# 《计算机视觉前沿方法》期末考试答案

**适用对象**: 研究生课程  
**总分**: 150分

---

## 第一部分：选择题答案（共40题，每题2分）

### 一、变分图像分割基础（1-12题）

| 题号 | 答案 | 解析 |
|:---:|:---:|:---|
| 1 | B | Mumford-Shah泛函三项：光滑项、长度项、数据拟合项 |
| 2 | B | Chan-Vese将分段光滑函数简化为分段常数，每区域为常数 |
| 3 | A | ROF = Rudin-Osher-Fatemi，1992年提出的全变分去噪模型 |
| 4 | A | ROF模型：$TV(u) + \frac{\mu}{2}\int_\Omega (u-f)^2 dx$ |
| 5 | A | T-ROF通过阈值化ROF解实现分割，避免PCMS的非凸优化 |
| 6 | A | SLaT = Smoothing + Lifting + Thresholding三阶段 |
| 7 | C | SLaT使用RGB+Lab双色彩空间，Lab提供感知均匀性 |
| 8 | B | 变分方法理论严谨，有存在性、唯一性、收敛性保证 |
| 9 | B | Chambolle-Pock算法用于求解凸优化问题的Primal-Dual形式 |
| 10 | B | PCMS = Piecewise Constant Mumford-Shah |
| 11 | B | 阈值更新：$\tau_i = \frac{m_{i-1}+m_i}{2}$，相邻区域均值的中点 |
| 12 | B | TV正则化保持边缘（允许跳跃）同时平滑区域内部，实现边缘保持去噪 |

### 二、张量分解方法（13-20题）

| 题号 | 答案 | 解析 |
|:---:|:---:|:---|
| 13 | A | Tucker分解：$\mathcal{X} \approx \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)}$ |
| 14 | A | HOOI = Higher-Order Orthogonal Iteration，高阶正交迭代 |
| 15 | C | Mode-n乘积结果仍是张量，形状中第n维变为矩阵的行数 |
| 16 | B | Sketching通过随机投影降低数据规模，降低计算复杂度 |
| 17 | A | CP分解可视为Tucker分解的特例，核心张量为对角张量 |
| 18 | B | TT分解存储复杂度线性增长，避免指数级维数灾难 |
| 19 | B | tCURLoRA将张量CUR分解应用于大语言模型的高效微调 |
| 20 | B | Leverage Score根据统计杠杆值重要性采样，保留关键信息 |

### 三、3D视觉与点云（21-28题）

| 题号 | 答案 | 解析 |
|:---:|:---:|:---|
| 21 | B | 点云数据表示为三维坐标$(x,y,z)$的集合 |
| 22 | A | Neural Varifold使用神经网络学习点云的变分几何表示 |
| 23 | B | 3D Orientation Field Transform用于分割和方向场分析 |
| 24 | A | MCGC = Multi-Class Graph Cut，多类图割算法 |
| 25 | C | 图像分辨率过高不是主要挑战，反而是数据质量问题 |
| 26 | A | CornerPoint3D使用角点特征进行3D目标检测 |
| 27 | B | Cross-Domain解决不同传感器数据的域迁移问题 |
| 28 | B | 3D Tree Delineation使用图割优化进行树木分割 |

### 四、医学影像分析（29-34题）

| 题号 | 答案 | 解析 |
|:---:|:---:|:---|
| 29 | B | Tight-Frame血管分割主要用于MRA（磁共振血管成像） |
| 30 | B | HiFi-Mamba用于MRI图像重建，利用Mamba的长序列建模能力 |
| 31 | D | 困惑度是NLP指标，不用于医学图像分割 |
| 32 | B | Few-shot解决标注样本稀缺问题 |
| 33 | C | IIHT是医疗报告生成方法 |
| 34 | B | Discrepancy-based Diffusion使用Diffusion Model |

### 五、深度学习融合（35-40题）

| 题号 | 答案 | 解析 |
|:---:|:---:|:---|
| 35 | B | SPSS使用语义比例进行弱监督分割，降低标注成本 |
| 36 | B | TransNet用于人体活动识别（HAR） |
| 37 | A | PEFT = Parameter-Efficient Fine-Tuning，参数高效微调 |
| 38 | B | CALM = Culturally Self-Aware Language Models |
| 39 | B | GAMED用于多模态虚假新闻检测 |
| 40 | B | 结合数据驱动（深度学习）和模型驱动（变分）的优点 |

---

## 第二部分：填空题答案（共30题，每题2分）

### 一、变分图像分割基础

1. **边界长度项 / Per(C) / Length(C)**
2. **平均灰度值 / 均值**
3. **全变分 / Total Variation / TV**
4. **ROF / Rudin-Osher-Fatemi**
5. **6**
6. **Dual / 对偶**
7. **Heaviside / 阶跃**
8. **$2\mu(m_1 - m_0)$**
9. **1**

### 二、张量分解方法

10. **$R_1 \times R_2 \times R_3$**
11. **第n模态的维度 / $I_n$**
12. **因子 / rank-1**
13. **公共 / bond**
14. **$I_n$ / 恒等矩阵**
15. **因子 / 基**
16. **列**

### 三、3D视觉与点云

17. **z**
18. **几何 / 几何特征**
19. **方向 / Orientation**
20. **Multi-Constrained / 多约束**
21. **鸟瞰 / Bird's Eye View**
22. **最远点 / Farthest Point**

### 四、医学影像分析

23. **完美重构 / Parseval / 紧框架**
24. **0**
25. **傅里叶 / Fourier / FFT**
26. **K**

### 五、深度学习融合

27. **GAP / Global Average Pooling / 全局平均池化**
28. **迁移 / Transfer**
29. **秩 / rank**
30. **高斯噪声 / 噪声**

---

## 第三部分：简答题参考答案（每题约200字）

### 题1：Mumford-Shah泛函的物理意义

Mumford-Shah泛函包含三个能量项：

1. **光滑项** $\int_{\Omega \setminus C} |\nabla u|^2 dx$：强制分割后的图像在除边界外的区域内光滑，惩罚区域内灰度变化。

2. **数据保真项** $\mu \int_\Omega (u-u_0)^2 dx$：保证分段光滑函数$u$与原始图像$u_0$的相似性，$\mu$控制保真度权重。

3. **边界长度项** $\nu|C|$：惩罚边界曲线长度，避免过分割产生过多细小区域。

这三个项的平衡实现了**光滑性、保真度、简洁性**的折中。在图像分割中，最小化该泛函可得到同时满足区域平滑、接近原图、边界简洁的分割结果。

### 题2：ROF边缘保持去噪

ROF模型通过**全变分(TV)正则化**实现边缘保持的去噪效果：

TV定义为$TV(u) = \int_\Omega |\nabla u| dx$，它允许函数在边缘处有跳跃（间断），而在平滑区域内约束梯度。这源于TV对梯度幅度的$L^1$范数惩罚，而非$L^2$范数。

与**高斯滤波**的本质区别：
- 高斯滤波是线性滤波，对整个图像均匀平滑，导致边缘模糊
- ROF是非线性的，在边缘处（$|\nabla u|$大）惩罚小，保留边缘；在平滑区（$|\nabla u|$小）施加强平滑

这种特性使ROF特别适合处理具有清晰边缘的图像。

### 题3：T-ROF算法步骤

T-ROF算法包含三个主要步骤：

**Step 1：初始化** - 使用FCM聚类获得初始区域均值$m_0, m_1, ..., m_{K-1}$

**Step 2：迭代优化**
- 2a：求解ROF模型得到去噪图像$u$
- 2b：使用阈值$\tau_i$对$u$进行阈值化得到分割$\Omega_i = \{x: \tau_i < u(x) \leq \tau_{i-1}\}$
- 2c：更新均值$m_i = mean_f(\Omega_i)$
- 2d：更新阈值$\tau_i = \frac{m_{i-1}+m_i}{2}$

**Step 3：收敛检查** - 直到阈值变化小于阈值

阈值公式设计原理：$\tau_i$取相邻两区域均值的**中点**，这是最优贝叶斯决策边界（假设等先验、等方差高斯分布）。

### 题4：SLaT双色彩空间

SLaT使用RGB+Lab双色彩空间的原因：

**RGB空间的局限性**：三个通道高度相关，某些颜色在RGB空间中难以分离，如肤色在不同光照下RGB值变化大。

**Lab空间的互补性**：
- L通道：亮度信息
- a/b通道：颜色对立维度（红-绿、蓝-黄）

**感知均匀性**：Lab空间中，欧氏距离与人类感知的颜色差异成比例。这意味着相同距离在任何颜色区域代表相同的感知差异，而RGB空间不具备此性质。

通过组合两个空间得到6维特征向量，即使在RGB相关性强的情况下，Lab仍提供独立的颜色判断信息，提高分割鲁棒性。

### 题5：Primal-Dual算法

Primal-Dual算法（Chambolle-Pock）用于求解形如$\min_x F(Kx) + G(x)$的凸优化问题。

**基本思想**：
- 引入对偶变量$y$，将问题转化为鞍点问题
- 交替更新Primal变量$x$和对偶变量$y$
- 使用外推（extrapolation）加速收敛

**ROF求解步骤**：
```
初始化：x̄ = x = f, y = 0
迭代：
  y = prox_{σF*}(y + σKx̄)  // 对偶更新
  x_new = prox_{τG}(x - τK*y)  // 原始更新
  x̄ = 2x_new - x  // 外推
  x = x_new
```

**收敛性保证**：当$\tau\sigma\|K\|^2 < 1$时，算法收敛到最优解。

### 题6：Chan-Vese与水平集方法

**Chan-Vese主动轮廓**：
- 基于Mumford-Shah泛函的两相分割特例
- 使用参数化曲线C（或隐式表示）
- 能量泛函包含边界长度和区域拟合项
- 优点：模型简单，理论完备
- 缺点：只能处理两相，对初始位置敏感

**水平集方法**：
- 将曲线表示为高维函数$\phi$的零水平集$\{x: \phi(x)=0\}$
- 曲线演化通过PDE：$\frac{\partial \phi}{\partial t} = F|\nabla \phi|$
- 优点：自动处理拓扑变化（分裂、合并）
- 缺点：需要重新初始化，计算量大

**关系**：Chan-Vese可用水平集实现，但水平集是更一般的框架，可嵌入各种速度函数F。

### 题7：Tucker vs CP分解

**结构差异**：
- **Tucker分解**：$\mathcal{X} \approx \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)}$
  - 有核心张量$\mathcal{G}$，每个模态有独立因子矩阵
  - 各模态的秩可以不同（多元秩$(R_1,R_2,R_3)$）
  
- **CP分解**：$\mathcal{X} \approx \sum_{r=1}^R a_r \circ b_r \circ c_r$
  - 无核心张量（等价于对角核心）
  - 只有一个秩R，所有模态共享

**适用场景**：
- Tucker：需要灵活秩控制的场景，数据压缩
- CP：需要唯一性的场景（如化学分析），因子可解释

### 题8：随机Sketching加速

**基本流程**：
1. **构建Sketch矩阵**：对每个模态$n$，生成随机采样矩阵$S^{(n)} \in \mathbb{R}^{k_n \times I_n}$
2. **降维投影**：计算投影后的张量$\mathcal{Y} = \mathcal{X} \times_1 S^{(1)} \times_2 S^{(2)} \times_3 S^{(3)}$
3. **小规模分解**：对小张量$\mathcal{Y}$执行Tucker分解
4. **恢复因子**：从投影结果恢复原规模因子矩阵

**复杂度改进**：
- 原始Tucker：$O(I^3 R^2)$（假设$I_1=I_2=I_3=I$）
- Sketching后：$O(k^3 R^2 + I k R)$，其中$k \ll I$
- 典型$k = O(R \log I)$，大幅降低复杂度

### 题9：Mode-n乘积

**数学定义**：
张量$\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times ... \times I_N}$与矩阵$M \in \mathbb{R}^{J \times I_n}$的Mode-n乘积：
$$\mathcal{Y} = \mathcal{X} \times_n M$$
其中$\mathcal{Y} \in \mathbb{R}^{I_1 \times ... \times I_{n-1} \times J \times I_{n+1} \times ... \times I_N}$

**元素形式**：
$$\mathcal{Y}_{i_1...i_{n-1}ji_{n+1}...i_N} = \sum_{i_n=1}^{I_n} \mathcal{X}_{i_1...i_N} \cdot M_{j i_n}$$

**几何意义**：
Mode-n乘积相当于对张量在第$n$个模态方向上进行线性变换，改变该模态的维度。可以理解为"张量版本的矩阵乘法"，在特定模态上压缩或扩展。

### 题10：Tensor Train与维数灾难

**TT分解形式**：
$$\mathcal{X}(i_1, ..., i_d) = \sum_{r_0,...,r_d} G_1(r_0, i_1, r_1) G_2(r_1, i_2, r_2) ... G_d(r_{d-1}, i_d, r_d)$$

**避免维数灾难**：
- 原始存储：$O(I^d)$，随维度指数增长
- TT存储：$O(d \cdot I \cdot r^2)$，其中$r$是TT-rank，线性增长

**TT-rank定义**：
向量$(r_0, r_1, ..., r_d)$，其中$r_0 = r_d = 1$，$r_k$是相邻核心间的公共维度。最大TT-rank决定存储和计算复杂度。

TT分解特别适合高维张量（如量子态、高阶相关矩阵），将指数复杂度降为多项式复杂度。

### 题11：3D数据表示方法比较

**体素化方法**：
- 优点：可直接使用3D卷积，与2D CNN自然扩展
- 缺点：内存占用大（$O(n^3)$），分辨率受限；稀疏性被忽略

**点云方法**（PointNet系列）：
- 优点：直接处理原始点数据，内存效率高；保持几何精度
- 缺点：缺乏局部结构建模（PointNet++改进）；点顺序敏感

**图方法**：
- 优点：自然建模点间关系；可处理不规则连接
- 缺点：图构建成本高；大规模图处理困难

**选择建议**：体素适合规则场景；点云适合原始数据；图方法适合需要明确关系建模的场景。

### 题12：Neural Varifold

**核心思想**：
将点云几何表示为**变分流形(varifold)**，使用神经网络学习变分泛函的最优表示。

**方法框架**：
1. **变分形式化**：定义点云上的变分泛函$E(\mu) = \int K(x,y)d\mu(x)d\mu(y)$
2. **神经逼近**：使用神经网络参数化测度$\mu_\theta$
3. **优化目标**：最小化变分能量，学习点云的几何表示

**学习过程**：
- 输入：点云$\{p_i\}_{i=1}^N$
- 网络：学习点特征和权重
- 输出：参数化几何表示，可用于分割、配准等任务

**优势**：结合变分方法的理论保证和神经网络的表达能力。

### 题13：3D Orientation Field

**方向场定义**：
在3D空间的每个点，方向场$O: \mathbb{R}^3 \to S^2$指定一个单位向量方向，表示局部结构的主方向。

**计算方法**：
1. 计算局部梯度$\nabla I$
2. 结构张量$S = \nabla I \cdot \nabla I^T$
3. 主方向 = $S$的最小特征值对应特征向量（垂直于梯度方向）

**分割应用**：
- 相似方向聚为一类
- 方向场的不连续性指示边界
- 结合强度和方向信息进行分割

在管状结构（血管、树干）分割中，方向场与结构方向一致，增强分割效果。

### 题14：MCGC算法

**Multi-Constrained Graph Cut算法原理**：

**图构建**：
- 节点：3D体素或超体素
- 边：邻接关系，权重反映相似度
- 约束：多种约束（空间、强度、形状）

**目标函数**：
$$\min_{S} \sum_{e \in E} w_e |S(e)| + \sum_{c \in C} \lambda_c \phi_c(S)$$

其中$S$是分割，$w_e$边权重，$\phi_c$约束代价。

**3D树木分割应用**：
1. 使用强度和形状特征构建图
2. 添加树干连续性约束
3. 添加树冠紧凑性约束
4. 图割优化得到树木分割

**优势**：多种约束自然融合，适合复杂结构分割。

### 题15：Tight-Frame与小波变换

**Tight-Frame与小波的关系**：
- 紧框架是正交小波的**过完备推广**
- 满足$A^T A = I$（完美重构），但$AA^T \neq I$（冗余）
- 提供更多方向和尺度的基函数

**血管分割应用流程**：
1. **分解**：$c = A f$，获取多尺度系数
2. **阈值化**：$c' = T_\lambda(c)$，去噪并增强血管
3. **重构**：$f' = A^T c'$，得到处理后的图像
4. **迭代细化**：重复上述过程，逐步细化血管边界
5. **二值化**：最终阈值化得到血管掩码

**优势**：多尺度表示捕捉不同粗细血管；冗余表示增强鲁棒性。

### 题16：HiFi-Mamba MRI重建

**Mamba架构**：
Mamba是状态空间模型(SSM)的高效实现，具有线性复杂度的长序列建模能力：
$$h_t = A h_{t-1} + B x_t, \quad y_t = C h_t$$

**MRI重建应用**：
1. **问题**：从欠采样k空间数据重建高质量图像
2. **方法**：使用HiFi-Mamba建模图像域的长程依赖
3. **层次结构**：多尺度特征提取，捕获局部和全局信息

**优势**：
- 相比Transformer：线性复杂度vs二次复杂度
- 相比CNN：更好的长程依赖建模
- 高保真重建：HiFi = High Fidelity

### 题17：Few-shot Medical Imaging

**标注稀缺问题**：
医学图像标注需要专家知识，成本高、耗时长，导致标注样本稀少。

**解决策略**：

1. **元学习(Meta-learning)**：
   - 在多个任务上学习"如何学习"
   - MAML、Prototypical Networks

2. **数据增强**：
   - 合成数据生成
   - 变换增强（旋转、缩放、弹性变形）

3. **迁移学习**：
   - 从大规模自然图像预训练
   - 域适应微调

4. **半监督学习**：
   - 利用未标注数据
   - 一致性正则化、伪标签

5. **度量学习**：
   - 学习样本相似度
   - 基于支持集分类

### 题18：SPSS弱监督分割

**核心思想**：
传统分割需要像素级标注（每个像素的类别），SPSS只使用**图像级语义比例**（每类占多少比例）作为监督信号。

**方法框架**：
```
输入图像 → CNN特征提取 → GAP层 → 预测比例ρ
                        ↓
                   分割网络 → 分割结果Y
```

**损失函数**：
$$L_{sp} = \frac{1}{|\Omega_T|}\sum_{i \in \Omega_T}\|\rho_i^* - \rho_i\|^2$$

**与传统方法区别**：
- 传统：$O(H \times W)$个标签点
- SPSS：$O(C)$个比例值（C是类别数）
- 标注成本降低几个数量级

**可行性**：比例信息约束了全局分布，结合特征学习可推断局部分割。

### 题19：TransNet迁移学习

**问题背景**：
人体活动识别(HAR)中，不同传感器、不同受试者、不同环境导致域偏移。

**TransNet策略**：

1. **特征提取器**：
   - 共享底层特征（通用运动模式）
   - 在源域预训练

2. **域适应**：
   - MMD（最大均值差异）对齐特征分布
   - 对抗训练使域分类器无法区分源域和目标域

3. **分类器微调**：
   - 在目标域少量数据上微调
   - 保持源域知识

**优势**：利用源域大量数据，适应目标域新场景，减少目标域标注需求。

### 题20：Diffusion vs GAN

**Diffusion模型原理**：
正向过程逐步添加噪声：$x_0 \to x_1 \to ... \to x_T \sim \mathcal{N}(0,I)$
逆向过程学习去噪：$x_T \to x_{T-1} \to ... \to x_0$

**医学图像应用潜力**：
- 图像重建：从稀疏数据重建
- 数据增强：生成合成训练数据
- 异常检测：比较重建误差

**与GAN对比**：

| 特性 | Diffusion | GAN |
|------|-----------|-----|
| 训练稳定性 | 高 | 低（模式崩溃） |
| 生成质量 | 高 | 高 |
| 生成速度 | 慢（多步迭代） | 快（单次前向） |
| 多样性 | 好 | 可能塌缩 |
| 可控性 | 好（条件生成） | 中等 |

**医学场景建议**：Diffusion适合生成质量要求高、速度要求不高的场景。

---

## 第四部分：计算题参考答案

### 题1：ROF模型最优性条件

**(1) Euler-Lagrange方程（4分）**

设$J(u) = TV(u) + \frac{\mu}{2}\int_\Omega (u-f)^2 dx$

$TV(u) = \int_\Omega |\nabla u| dx = \int_\Omega \sqrt{u_x^2 + u_y^2} dx$

对$J(u)$求变分：

$$\frac{\partial J}{\partial u} = -div\left(\frac{\nabla u}{|\nabla u|}\right) + \mu(u-f) = 0$$

因此Euler-Lagrange方程为：
$$\boxed{-div\left(\frac{\nabla u}{|\nabla u|}\right) + \mu(u-f) = 0}$$

**(2) 曲率项几何意义（3分）**

$div\left(\frac{\nabla u}{|\nabla u|}\right)$是水平集曲线的**曲率**。

- $\frac{\nabla u}{|\nabla u|}$是单位法向量
- 散度给出法向量场的发散程度
- 几何上等于曲线的曲率$\kappa$

曲率大的地方（尖锐边缘）正则化弱，曲率小的地方（平滑区域）正则化强。

**(3) 奇异性处理（3分）**

当$|\nabla u| = 0$时，分母为零，需要正则化：

**方法1：小常数正则化**
$$\frac{\nabla u}{|\nabla u|} \approx \frac{\nabla u}{\sqrt{|\nabla u|^2 + \epsilon^2}}$$

**方法2：变分正则化**
使用$\sqrt{|\nabla u|^2 + \epsilon^2}$代替$|\nabla u|$

### 题2：T-ROF阈值计算

**(1) 初始阈值计算（4分）**

给定$m_0 = 0.2, m_1 = 0.5, m_2 = 0.8$

阈值更新公式：$\tau_i = \frac{m_{i-1} + m_i}{2}$

$$\tau_1 = \frac{m_0 + m_1}{2} = \frac{0.2 + 0.5}{2} = \boxed{0.35}$$

$$\tau_2 = \frac{m_1 + m_2}{2} = \frac{0.5 + 0.8}{2} = \boxed{0.65}$$

**(2) 像素分类（3分）**

分割规则：
- $\Omega_0 = \{x: u(x) \leq \tau_2\} = \{x: u(x) \leq 0.65\}$（背景+过渡）
- $\Omega_2 = \{x: u(x) > \tau_1\} = \{x: u(x) > 0.35\}$（过渡+目标）

更精确地：
- $\Omega_0$（背景）：$u(x) \leq 0.35$
- $\Omega_1$（过渡）：$0.35 < u(x) \leq 0.65$
- $\Omega_2$（目标）：$u(x) > 0.65$

由于$u(x) = 0.6$，满足$0.35 < 0.6 \leq 0.65$，因此该像素属于$\boxed{\Omega_1\text{（过渡区域）}}$

**(3) 收敛条件（3分）**

T-ROF算法的收敛条件：
$$\|\tau^{(k)} - \tau^{(k-1)}\|_2 < \epsilon$$

或等价地：
$$|m_i^{(k)} - m_i^{(k-1)}| < \epsilon', \quad \forall i$$

### 题3：Tucker分解复杂度

**(1) 原始存储（2分）**

张量大小：$1000 \times 1000 \times 1000 = 10^9$元素

float64 = 8字节/元素

$$\text{内存} = 10^9 \times 8 = \boxed{8 \text{ GB}}$$

**(2) Tucker分解参数量（4分）**

$(100, 100, 100)$-Tucker分解：
- 核心张量$\mathcal{G}$：$100 \times 100 \times 100 = 10^6$元素
- 因子矩阵$U^{(1)}$：$1000 \times 100 = 10^5$元素
- 因子矩阵$U^{(2)}$：$1000 \times 100 = 10^5$元素
- 因子矩阵$U^{(3)}$：$1000 \times 100 = 10^5$元素

总参数量：$10^6 + 3 \times 10^5 = 1.3 \times 10^6$元素

$$\text{内存} = 1.3 \times 10^6 \times 8 = \boxed{10.4 \text{ MB}}$$

压缩比：$8000 / 10.4 \approx 770$倍

**(3) Sketching复杂度改进（4分）**

**原始HOOI复杂度**：$O(I^3 R^2) = O(10^9 \times 10^4) = O(10^{13})$

**Sketching方法**：
- 采样矩阵大小：$200 \times 1000$
- 投影张量大小：$200 \times 200 \times 200$
- 小规模分解复杂度：$O(200^3 \times 100^2) = O(8 \times 10^8 \times 10^4) = O(8 \times 10^{12})$
- 恢复因子：$O(I \times k \times R) = O(1000 \times 200 \times 100) = O(2 \times 10^7)$

主要改进来自降维后的分解操作。虽然本例中Sketching后仍较大，但对于更高维张量（$I \gg k$），改进显著。

### 题4：张量Mode-n乘积

**(1) 结果形状（3分）**

$\mathcal{X} \in \mathbb{R}^{4 \times 3 \times 2}$，$U \in \mathbb{R}^{5 \times 4}$

Mode-1乘积将第1维从4变为5，其他维不变。

$$\mathcal{Y} = \mathcal{X} \times_1 U \in \mathbb{R}^{\boxed{5 \times 3 \times 2}}$$

**(2) Mode-1展开（3分）**

$X_{(1)} \in \mathbb{R}^{4 \times (3 \times 2)} = \mathbb{R}^{4 \times 6}$

构造方法：将张量沿第1模态展开，每一行对应一个固定的第1索引，列为其他模态的元素按某种顺序排列。

例如，$X_{(1)}(i, :) = [X(i,1,1), X(i,2,1), X(i,3,1), X(i,1,2), X(i,2,2), X(i,3,2)]$

**(3) 矩阵乘法表示（4分）**

$$\mathcal{Y} = \mathcal{X} \times_1 U \iff Y_{(1)} = U \cdot X_{(1)}$$

即：Mode-1乘积等价于对Mode-1展开矩阵左乘$U$。

$Y_{(1)} \in \mathbb{R}^{5 \times 6}$，$U \in \mathbb{R}^{5 \times 4}$，$X_{(1)} \in \mathbb{R}^{4 \times 6}$

验证：$5 \times 4$ 与 $4 \times 6$ 相乘得到 $5 \times 6$，正确。

### 题5：Chan-Vese推导

**(1) 最优$c_1$和$c_2$（4分）**

固定$C$，对$c_1$求导：

$$\frac{\partial E}{\partial c_1} = -2\lambda_1\int_{inside(C)}(f-c_1)dx = 0$$

$$\int_{inside(C)}f dx - c_1 \cdot |inside(C)| = 0$$

$$c_1^* = \frac{\int_{inside(C)} f dx}{|inside(C)|} = \boxed{mean_{inside(C)}(f)}$$

同理：

$$c_2^* = \frac{\int_{outside(C)} f dx}{|outside(C)|} = \boxed{mean_{outside(C)}(f)}$$

**(2) 水平集形式（3分）**

使用Heaviside函数$H(\phi)$：

$$E(c_1, c_2, \phi) = \mu\int_\Omega \delta(\phi)|\nabla \phi|dx + \lambda_1\int_\Omega H(\phi)(f-c_1)^2dx + \lambda_2\int_\Omega (1-H(\phi))(f-c_2)^2dx$$

**(3) 梯度下降方程（3分）**

$$\frac{\partial \phi}{\partial t} = \delta(\phi)\left[\mu \cdot div\left(\frac{\nabla \phi}{|\nabla \phi|}\right) - \lambda_1(f-c_1)^2 + \lambda_2(f-c_2)^2\right]$$

### 题6：Tight-Frame分解

**(1) 验证完美重构（4分）**

需要验证：$\sum_k h_k^T h_k = I$

$h_0 = [1, 2, 1]/4$，$h_1 = [1, 0, -1]/(2\sqrt{2})$，$h_2 = [-1, 2, -1]/4$

计算卷积自相关并相加：
- $h_0^T * h_0$：中心系数 = $(1^2 + 2^2 + 1^2)/16 = 6/16 = 3/8$
- $h_1^T * h_1$：中心系数 = $(1^2 + 0^2 + 1^2)/8 = 2/8 = 1/4$
- $h_2^T * h_2$：中心系数 = $(1^2 + 2^2 + 1^2)/16 = 6/16 = 3/8$

总和：$3/8 + 1/4 + 3/8 = 3/8 + 2/8 + 3/8 = 8/8 = 1$

满足$\sum_k h_k^T h_k = \delta(n)$（Delta函数），即完美重构。

**(2) 2D滤波器（3分）**

$h_{00} = h_0^T \otimes h_0 = h_0^T h_0$（外积）

$$h_{00} = \frac{1}{16}\begin{bmatrix}1\\2\\1\end{bmatrix}\begin{bmatrix}1&2&1\end{bmatrix} = \frac{1}{16}\begin{bmatrix}1&2&1\\2&4&2\\1&2&1\end{bmatrix}$$

$h_{12} = h_1^T \otimes h_2$：

$$h_{12} = \frac{1}{8\sqrt{2}}\begin{bmatrix}1\\0\\-1\end{bmatrix}\begin{bmatrix}-1&2&-1\end{bmatrix} = \frac{1}{8\sqrt{2}}\begin{bmatrix}-1&2&-1\\0&0&0\\1&-2&1\end{bmatrix}$$

**(3) 软阈值作用（3分）**

$T_\lambda(v) = sgn(v)\max(|v|-\lambda, 0)$

作用：
- 当$|v| > \lambda$：收缩为$|v| - \lambda$，保留信号
- 当$|v| \leq \lambda$：置为0，去除噪声

在Tight-Frame中，小系数通常是噪声，大系数是信号。软阈值实现**非线性去噪**。

### 题7：Dice与IoU关系

**(1) 特殊情况证明（3分）**

当$|A| = |B|$时：

$|A \cup B| = |A| + |B| - |A \cap B| = 2|A| - |A \cap B|$

$IoU = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{2|A| - |A \cap B|}$

$Dice = \frac{2|A \cap B|}{|A| + |B|} = \frac{2|A \cap B|}{2|A|} = \frac{|A \cap B|}{|A|}$

设$|A \cap B| = x$，$|A| = m$：

$IoU = \frac{x}{2m-x}$，$Dice = \frac{x}{m}$

当$|A| = |B|$时，$Dice = \frac{2|A \cap B|}{2|A|} = \frac{|A \cap B|}{|A|}$

而$IoU = \frac{|A \cap B|}{2|A| - |A \cap B|} = \frac{1}{\frac{2|A|}{|A \cap B|}-1} \cdot \frac{|A \cap B|}{|A|}$

只有当$|A \cap B| = |A|$（完全重叠）时$Dice = IoU = 1$，一般情况下不成立。题目假设有误，正确关系见(2)。

**(2) 一般关系推导（4分）**

设$|A \cap B| = a$，$|A| = m$，$|B| = n$

$Dice = \frac{2a}{m+n}$

$IoU = \frac{a}{m+n-a}$

设$Dice = d$，则$a = \frac{d(m+n)}{2}$

$IoU = \frac{\frac{d(m+n)}{2}}{m+n-\frac{d(m+n)}{2}} = \frac{d}{2-d}$

因此：

$$\boxed{IoU = \frac{Dice}{2-Dice} \quad \text{或} \quad Dice = \frac{2 \cdot IoU}{1+IoU}}$$

**(3) 具体计算（3分）**

$|A| = 100$，$|B| = 120$，$|A \cap B| = 80$

$|A \cup B| = 100 + 120 - 80 = 140$

$Dice = \frac{2 \times 80}{100 + 120} = \frac{160}{220} = \boxed{0.727}$

$IoU = \frac{80}{140} = \boxed{0.571}$

验证关系：$IoU = \frac{0.727}{2-0.727} = \frac{0.727}{1.273} \approx 0.571$ ✓

### 题8：CP分解秩

**(1) CP秩定义与复杂度（3分）**

**定义**：张量$\mathcal{X}$的CP秩是最小的$R$使得$\mathcal{X} = \sum_{r=1}^R a_r \circ b_r \circ c_r$

**计算复杂度**：
- 判断给定$R$是否足够：$O(IJKR)$每次迭代
- 找到最小$R$：需要尝试多个$R$值
- 已知问题是NP-hard，没有多项式时间算法

**(2) 秩-1张量唯一性（4分）**

对于秩-1张量$\mathcal{X} = a \circ b \circ c$：

元素形式：$X_{ijk} = a_i b_j c_k$

唯一性分析：
- 若$a, b, c$是因子，则$(\alpha a) \circ (\beta b) \circ (\gamma c)$（$\alpha\beta\gamma=1$）也是有效因子
- 差别仅在**尺度重新分配**

因此因子向量在尺度意义上唯一确定。

**(3) NP-hard性质（3分）**

CP秩确定问题的NP-hard性质：
1. 可归约到3-SAT问题
2. 即使判断秩是否$\leq R$也是NP完全的
3. 对于$R \geq 2$的三阶张量，没有多项式时间算法

实践中使用启发式方法（如CORCONDIA）或基于误差的秩选择。

### 题9：KNN图构建

**(1) 邻接矩阵定义（3分）**

$A \in \mathbb{R}^{N \times N}$，$A_{ij} = 1$若$j$是$i$的K近邻之一，否则$A_{ij} = 0$

带权重版本：
$$A_{ij} = \begin{cases} \exp(-\|p_i - p_j\|^2 / \sigma^2), & \text{if } j \in KNN(i) \\ 0, & \text{otherwise} \end{cases}$$

**(2) 时间复杂度（4分）**

**朴素算法**：
- 计算所有点对距离：$O(N^2)$
- 每点找K近邻：$O(N \log N)$ per point
- 总复杂度：$O(N^2 + N^2 \log N) = O(N^2 \log N)$

**KD-tree加速**：
- 构建KD-tree：$O(N \log N)$
- K近邻查询：$O(\log N)$ per point（平均情况）
- 总复杂度：$O(N \log N)$

**注**：最坏情况下KD-tree退化为$O(N)$per query。

**(3) K值选择影响（3分）**

- **K太小**：图稀疏，可能断开连通分量；局部特征为主
- **K太大**：图稠密，计算成本高；全局特征掩盖局部结构
- **经验选择**：$K \in [10, 50]$通常合适，或使用自适应K

K值直接影响图的拓扑结构和后续处理（如谱聚类、图神经网络）的效果。

### 题10：Diffusion正向过程

**(1) 直接采样公式（4分）**

由递推关系：$x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}\epsilon_{t-1}$

设$\alpha_t = 1 - \beta_t$，$\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$

通过数学归纳可证：
$$x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$$

其中$\epsilon \sim \mathcal{N}(0, I)$

$$\boxed{q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)}$$

**(2) 极限分布（3分）**

当$t \to \infty$时：
- $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s \to 0$（因为$\alpha_s < 1$）
- $\sqrt{\bar{\alpha}_t} \to 0$，$\sqrt{1-\bar{\alpha}_t} \to 1$

因此：
$$x_\infty \sim \mathcal{N}(0, I)$$

即**标准高斯分布**。

**(3) 逆向过程目标（3分）**

逆向过程$p_\theta(x_{t-1}|x_t)$的目标是**学习逆向去噪**：

$$p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)$$

训练目标是最小化逆向过程与真实逆向分布的KL散度：

$$\mathcal{L} = \mathbb{E}_q\left[D_{KL}(q(x_{t-1}|x_t, x_0) \| p_\theta(x_{t-1}|x_t))\right]$$

实践中，网络学习预测噪声$\epsilon_\theta(x_t, t)$，然后用其估计均值$\mu_\theta$。

---

## 第五部分：论述题评分要点

### 题1：变分分割发展（15分）

**评分标准**：
- Mumford-Shah形式化定义（3分）
- Chan-Vese简化与主动轮廓（3分）
- 凸松弛方法突破（3分）
- SLaT/T-ROF范式联系（3分）
- PCMS-ROF定理数学意义（3分）

**关键点**：
- MS泛函：$\int|\nabla u|^2 + \mu\int(u-f)^2 + \nu|C|$
- CV：分段常数简化，水平集实现
- 凸松弛：$\lambda = 2\mu(m_1-m_0)$联系
- T-ROF：恢复+阈值化范式

### 题2：SLaT数学基础（15分）

**评分标准**：
- Stage 1变分模型唯一性条件（4分）
- RGB+Lab互补性分析（4分）
- Stage 3聚类有效性（3分）
- Chambolle算法关键步骤（4分）

**关键点**：
- 唯一性条件：$Ker(\omega_i A) \oplus Ker(\nabla) = \{0\}$
- Lab感知均匀性解释
- K-means在6维空间的聚类
- Primal-Dual迭代公式

### 题3：张量分解应用（15分）

**评分标准**：
- Tucker数学形式与SVD关系（3分）
- Sketching复杂度改进（4分）
- TT分解维数灾难（4分）
- tCURLoRA创新点（4分）

**关键点**：
- $\mathcal{X} \approx \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)}$
- 复杂度从$O(I^3)$到$O(k^3 + Ik)$
- TT存储$O(d \cdot I \cdot r^2)$
- CUR分解用于LoRA

### 题4：3D点云处理（15分）

**评分标准**：
- 体素/点云/图方法比较（4分）
- PointNet架构分析（4分）
- Neural Varifold变分方法（4分）
- 方向场在分割中作用（3分）

### 题5：医学影像多模态（15分）

**评分标准**：
- 结构与功能影像互补（3分）
- 图像-文本跨模态（4分）
- Few-shot应用（4分）
- Diffusion模型潜力（4分）

### 题6：深度学习与变分融合（15分）

**评分标准**：
- 深度网络作为求解器（4分）
- 变分正则嵌入损失（4分）
- 学习变分参数（4分）
- 物理约束与可解释性（3分）

### 题7-10：类似评分标准

每题关注：
- 理论基础（5分）
- 方法论分析（5分）
- 实践应用（5分）

---

**答案完成**
