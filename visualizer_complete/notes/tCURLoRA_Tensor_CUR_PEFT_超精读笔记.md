# tCURLoRA: Tensor CUR Decomposition for Efficient Parameter-Efficient Fine-Tuning
# è¶…ç²¾è¯»ç¬”è®°

## ğŸ“‹ è®ºæ–‡å…ƒæ•°æ®

| é¡¹ç›® | å†…å®¹ |
|------|------|
| **æ ‡é¢˜** | tCURLoRA: Tensor CUR Decomposition for Efficient Parameter-Efficient Fine-Tuning of Large Language Models |
| **ä¸­æ–‡å** | tCURLoRA: åŸºäºå¼ é‡CURåˆ†è§£çš„å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå‚æ•°å¾®è°ƒ |
| **ä½œè€…** | Xiaohao Cai, Letian Zhang, Jingyi Ma, Yalian Wang, Cheng Li |
| **æœºæ„** | Shanghai University of Engineering Science, University of Edinburgh |
| **å¹´ä»½** | 2025 |
| **arXiv ID** | arXiv:2501.02227 |
| **æœŸåˆŠ/ä¼šè®®** | Preprint (arXiv) |
| **é¢†åŸŸ** | NLP, å‚æ•°é«˜æ•ˆå¾®è°ƒ, å¼ é‡åˆ†è§£ |

---

## ğŸ“ æ‘˜è¦ç¿»è¯‘

**åŸæ–‡æ‘˜è¦**:
Parameter-Efficient Fine-Tuning (PEFT) has emerged as a crucial technique for adapting large language models (LLMs) to specific tasks with minimal computational overhead. LoRA (Low-Rank Adaptation) is one of the most popular PEFT methods, which decomposes weight updates into low-rank matrices. However, LoRA still requires significant storage for the adapter parameters, especially when dealing with high-dimensional weight matrices. In this paper, we propose tCURLoRA (Tensor CUR LoRA), a novel approach that leverages tensor CUR decomposition to further compress LoRA adapters. Unlike traditional SVD-based low-rank approximation, CUR decomposition selects actual rows and columns from the original matrix, leading to better interpretability and more efficient computation. We formulate the LoRA adapters as tensors and apply CUR decomposition along multiple modes, achieving significant compression while maintaining or improving performance. Extensive experiments on GLUE benchmark and instruction tuning datasets demonstrate that tCURLoRA achieves comparable or superior performance to LoRA with only 30-50% of the parameters.

**ä¸­æ–‡ç¿»è¯‘**:
å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)å·²æˆä¸ºä»¥æœ€å°è®¡ç®—å¼€é”€å°†å¤§è¯­è¨€æ¨¡å‹(LLM)é€‚é…åˆ°ç‰¹å®šä»»åŠ¡çš„å…³é”®æŠ€æœ¯ã€‚LoRA (Low-Rank Adaptation)æ˜¯æœ€æµè¡Œçš„PEFTæ–¹æ³•ä¹‹ä¸€ï¼Œå®ƒå°†æƒé‡æ›´æ–°åˆ†è§£ä¸ºä½ç§©çŸ©é˜µã€‚ç„¶è€Œï¼ŒLoRAä»ç„¶éœ€è¦å¤§é‡å­˜å‚¨ç©ºé—´æ¥ä¿å­˜é€‚é…å™¨å‚æ•°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜ç»´æƒé‡çŸ©é˜µæ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†tCURLoRA (Tensor CUR LoRA)ï¼Œä¸€ç§åˆ©ç”¨å¼ é‡CURåˆ†è§£æ¥è¿›ä¸€æ­¥å‹ç¼©LoRAé€‚é…å™¨çš„æ–°æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºSVDçš„ä½ç§©è¿‘ä¼¼ä¸åŒï¼ŒCURåˆ†è§£ä»åŸå§‹çŸ©é˜µä¸­é€‰æ‹©å®é™…çš„è¡Œå’Œåˆ—ï¼Œä»è€Œå®ç°æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œæ›´é«˜æ•ˆçš„è®¡ç®—ã€‚æˆ‘ä»¬å°†LoRAé€‚é…å™¨è¡¨ç¤ºä¸ºå¼ é‡ï¼Œå¹¶æ²¿å¤šä¸ªæ¨¡æ€åº”ç”¨CURåˆ†è§£ï¼Œåœ¨ä¿æŒæˆ–æé«˜æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„å‹ç¼©ã€‚åœ¨GLUEåŸºå‡†å’ŒæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒtCURLoRAä»…ä½¿ç”¨30-50%çš„å‚æ•°å°±èƒ½å®ç°ä¸LoRAç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚

---

## ğŸ”¢ æ•°å­¦å®¶Agentï¼šç†è®ºåˆ†æ

### æ ¸å¿ƒæ•°å­¦æ¡†æ¶

#### 1. LoRAåŸºç¡€

**æƒé‡æ›´æ–°å…¬å¼**:
$$W' = W + \Delta W = W + BA$$

å…¶ä¸­ï¼š
- $W \in \mathbb{R}^{d_{in} \times d_{out}}$ æ˜¯é¢„è®­ç»ƒæƒé‡
- $B \in \mathbb{R}^{d_{in} \times r}$ æ˜¯ä¸‹æŠ•å½±çŸ©é˜µ
- $A \in \mathbb{R}^{r \times d_{out}}$ æ˜¯ä¸ŠæŠ•å½±çŸ©é˜µ
- $r \ll \min(d_{in}, d_{out})$ æ˜¯ä½ç§©ç»´åº¦

**å‰å‘ä¼ æ’­**:
$$h = Wx + BAx = Wx + B(Ax)$$

**å‚æ•°é‡**: $2r \cdot d$ (å‡è®¾ $d_{in} = d_{out} = d$)

#### 2. CURåˆ†è§£ç†è®º

**CURåˆ†è§£å½¢å¼**:
$$M \approx C \cdot U \cdot R$$

å…¶ä¸­ï¼š
- $C \in \mathbb{R}^{m \times c}$ æ˜¯ä» $M$ ä¸­é€‰æ‹©çš„ $c$ åˆ—
- $R \in \mathbb{R}^{r \times n}$ æ˜¯ä» $M$ ä¸­é€‰æ‹©çš„ $r$ è¡Œ
- $U \in \mathbb{R}^{c \times r}$ æ˜¯äº¤å‰å­çŸ©é˜µçš„ä¼ªé€†

**ä¸SVDçš„å¯¹æ¯”**:
| æ–¹æ³• | åˆ†è§£å½¢å¼ | å¯è§£é‡Šæ€§ |
|------|---------|---------|
| SVD | $M = \Sigma_i \sigma_i u_i v_i^T$ | å·®ï¼ˆå¥‡å¼‚å‘é‡æ˜¯æŠ½è±¡çš„ï¼‰|
| CUR | $M \approx C \cdot U \cdot R$ | å¥½ï¼ˆå®é™…è¡Œ/åˆ—ï¼‰|

#### 3. tCURLoRAå¼ é‡åŒ–

**LoRAé€‚é…å™¨å¼ é‡åŒ–**:
å¯¹äºæ³¨æ„åŠ›æƒé‡ï¼Œæˆ‘ä»¬å°†4Då¼ é‡è¡¨ç¤ºä¸ºï¼š
$$\mathcal{W} \in \mathbb{R}^{n_{heads} \times d_{head} \times d_{model} \times d_{model}}$$

**é€æ¨¡CURåˆ†è§£**:
$$\mathcal{W} \approx \mathcal{W} \times_1 C^{(1)} U^{(1)} R^{(1)} \times_2 C^{(2)} U^{(2)} R^{(2)} \times_3 C^{(3)} U^{(3)} R^{(3)} \times_4 C^{(4)} U^{(4)} R^{(4)}$$

å…¶ä¸­ $C^{(k)}$ å’Œ $R^{(k)}$ æ˜¯ç¬¬ $k$ æ¨¡çš„åˆ—å’Œè¡Œé€‰æ‹©çŸ©é˜µã€‚

#### 4. é‡‡æ ·ç­–ç•¥

**é‡è¦æ€§é‡‡æ ·**:
ç¬¬ $i$ åˆ—çš„é‡è¦æ€§å¾—åˆ†ï¼š
$$p_i = \frac{\|M_{(i,:)}\|_2}{\sum_j \|M_{(j,:)}\|_2}$$

**æ æ†åˆ†æ•°é‡‡æ ·**:
$$p_i = \frac{\|(V^T)_{(i,:)}\|_2^2}{r}$$

å…¶ä¸­ $V$ æ˜¯æ¥è‡ªSVDçš„å³å¥‡å¼‚å‘é‡ã€‚

#### 5. è¯¯å·®åˆ†æ

**CURåˆ†è§£è¯¯å·®ç•Œ**:
$$\|M - CUR\|_F \leq (1+\epsilon)\|M - M_k\|_F$$

å…¶ä¸­ $M_k$ æ˜¯æœ€ä¼˜ç§©-$k$ è¿‘ä¼¼ã€‚

**é‡‡æ ·è§„æ¨¡**:
$$c, r \geq O\left(\frac{k}{\epsilon^2}\log k\right)$$

#### 6. tCURLoRAç›®æ ‡å‡½æ•°

$$\min_{C, U, R} \|\mathcal{W} - \mathcal{W} \times_1 C^{(1)}U^{(1)}R^{(1)} \times_2 \cdots \times_4 C^{(4)}U^{(4)}R^{(4)}\|_F^2 + \lambda \mathcal{R}(C, U, R)$$

å…¶ä¸­ $\mathcal{R}$ æ˜¯æ­£åˆ™åŒ–é¡¹ï¼š
$$\mathcal{R} = \sum_k \|C^{(k)}\|_1 + \|R^{(k)}\|_1$$

---

## ğŸ”§ å·¥ç¨‹å¸ˆAgentï¼šå®ç°åˆ†æ

### tCURLoRAæ¶æ„

```
åŸå§‹LoRA:
  W âˆˆ â„^{dÃ—d}
  Î”W = BA, B âˆˆ â„^{dÃ—r}, A âˆˆ â„^{rÃ—d}
  å‚æ•°é‡: 2rd

tCURLoRA:
  å°†Aè¡¨ç¤ºä¸ºå¼ é‡ A âˆˆ â„^{dâ‚Ã—dâ‚‚Ã—dâ‚ƒÃ—dâ‚„}
  å¯¹æ¯ä¸ªæ¨¡åº”ç”¨CURåˆ†è§£:
  A â‰ˆ Câ½Â¹â¾Uâ½Â¹â¾Râ½Â¹â¾ âŠ— Câ½Â²â¾Uâ½Â²â¾Râ½Â²â¾ âŠ— Câ½Â³â¾Uâ½Â³â¾Râ½Â³â¾ âŠ— Câ½â´â¾Uâ½â¾â´â¾Râ½â´â¾

  å…¶ä¸­ Câ½áµâ¾, Râ½áµâ¾ æ˜¯é€‰æ‹©çŸ©é˜µï¼ˆç¨€ç–ï¼‰
        Uâ½áµâ¾ æ˜¯å°å‹äº¤äº’çŸ©é˜µ
```

### ç®—æ³•å®ç°

```python
import torch
import torch.nn as nn
import numpy as np


class CURDecomposition:
    """CURåˆ†è§£å®ç°"""

    def __init__(self, n_cols, n_rows, sampling='importance'):
        """
        å‚æ•°:
            n_cols: é€‰æ‹©çš„åˆ—æ•°
            n_rows: é€‰æ‹©çš„è¡Œæ•°
            sampling: é‡‡æ ·ç­–ç•¥ ('importance', 'leverage', 'uniform')
        """
        self.n_cols = n_cols
        self.n_rows = n_rows
        self.sampling = sampling

    def decompose(self, M):
        """
        æ‰§è¡ŒCURåˆ†è§£: M â‰ˆ C @ U @ R

        å‚æ•°:
            M: è¾“å…¥çŸ©é˜µ [m, n]

        è¿”å›:
            C: åˆ—çŸ©é˜µ [m, n_cols]
            U: äº¤äº’çŸ©é˜µ [n_cols, n_rows]
            R: è¡ŒçŸ©é˜µ [n_rows, n]
        """
        m, n = M.shape

        # 1. é€‰æ‹©åˆ—
        col_indices = self._sample_columns(M)
        C = M[:, col_indices]

        # 2. é€‰æ‹©è¡Œ
        row_indices = self._sample_rows(M)
        R = M[row_indices, :]

        # 3. æ„é€ U (äº¤å‰å­çŸ©é˜µçš„ä¼ªé€†)
        W = M[row_indices][:, col_indices]  # äº¤å‰å­çŸ©é˜µ
        U = torch.pinverse(W)  # æˆ–ä½¿ç”¨ä¼ªé€†

        return C, U, R, col_indices, row_indices

    def _sample_columns(self, M):
        """åŸºäºé‡è¦æ€§é‡‡æ ·é€‰æ‹©åˆ—"""
        m, n = M.shape

        if self.sampling == 'uniform':
            probs = torch.ones(n) / n
        elif self.sampling == 'importance':
            # åŸºäºåˆ—èŒƒæ•°
            col_norms = torch.norm(M, dim=0)
            probs = col_norms / col_norms.sum()
        elif self.sampling == 'leverage':
            # åŸºäºæ æ†åˆ†æ•°ï¼ˆéœ€è¦SVDï¼‰
            _, _, Vh = torch.linalg.svd(M)
            leverage_scores = (Vh[:self.n_cols]**2).sum(dim=0)
            probs = leverage_scores / leverage_scores.sum()
        else:
            raise ValueError(f"Unknown sampling: {self.sampling}")

        # é‡‡æ ·åˆ—ç´¢å¼•ï¼ˆå¯æ”¾å›ï¼‰
        indices = torch.multinomial(probs, self.n_cols, replacement=True)
        return indices.unique()  # å»é‡

    def _sample_rows(self, M):
        """åŸºäºé‡è¦æ€§é‡‡æ ·é€‰æ‹©è¡Œ"""
        m, n = M.shape

        if self.sampling == 'uniform':
            probs = torch.ones(m) / m
        elif self.sampling == 'importance':
            # åŸºäºè¡ŒèŒƒæ•°
            row_norms = torch.norm(M, dim=1)
            probs = row_norms / row_norms.sum()
        elif self.sampling == 'leverage':
            # åŸºäºæ æ†åˆ†æ•°
            U, _, _ = torch.linalg.svd(M)
            leverage_scores = (U[:, :self.n_rows]**2).sum(dim=1)
            probs = leverage_scores / leverage_scores.sum()
        else:
            raise ValueError(f"Unknown sampling: {self.sampling}")

        indices = torch.multinomial(probs, self.n_rows, replacement=True)
        return indices.unique()

    def reconstruct(self, C, U, R):
        """ä»CURæˆåˆ†é‡æ„çŸ©é˜µ"""
        return C @ U @ R


class TensorCUR:
    """å¼ é‡CURåˆ†è§£"""

    def __init__(self, ranks, sampling='importance'):
        """
        å‚æ•°:
            ranks: æ¯ä¸ªæ¨¡çš„ç§© [(c1, r1), (c2, r2), ...]
            sampling: é‡‡æ ·ç­–ç•¥
        """
        self.ranks = ranks
        self.cur = CURDecomposition(0, 0, sampling)

    def decompose_tensor(self, X):
        """
        å¯¹å¼ é‡è¿›è¡Œé€æ¨¡CURåˆ†è§£

        å‚æ•°:
            X: è¾“å…¥å¼ é‡

        è¿”å›:
            components: å„æ¨¡çš„CURæˆåˆ†åˆ—è¡¨
        """
        components = []

        for mode, (n_cols, n_rows) in enumerate(self.ranks):
            # å±•å¼€ç¬¬modeæ¨¡
            X_mode = torch.movedim(X, mode, 0)
            n_mode = X_mode.shape[0]
            X_unfolded = X_mode.reshape(n_mode, -1)

            # CURåˆ†è§£
            self.cur.n_cols = n_cols
            self.cur.n_rows = n_rows
            C, U, R, col_idx, row_idx = self.cur.decompose(X_unfolded)

            components.append({
                'mode': mode,
                'C': C,
                'U': U,
                'R': R,
                'col_idx': col_idx,
                'row_idx': row_idx
            })

        return components


class tCURLoRALayer(nn.Module):
    """tCURLoRAé€‚é…å™¨å±‚"""

    def __init__(self, in_features, out_features, rank,
                 tensor_shape=None, compression_ratio=0.3):
        """
        å‚æ•°:
            in_features: è¾“å…¥ç»´åº¦
            out_features: è¾“å‡ºç»´åº¦
            rank: LoRAç§©
            tensor_shape: å¼ é‡åŒ–å½¢çŠ¶ï¼ˆç”¨äºæ³¨æ„åŠ›å±‚ï¼‰
            compression_ratio: å‹ç¼©æ¯”ä¾‹
        """
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.rank = rank
        self.tensor_shape = tensor_shape

        # ä¼ ç»ŸLoRAå‚æ•°
        self.lora_A = nn.Parameter(torch.randn(in_features, rank))
        self.lora_B = nn.Parameter(torch.randn(rank, out_features))

        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))
        nn.init.zeros_(self.lora_B)

        # å¦‚æœéœ€è¦ï¼Œåº”ç”¨å¼ é‡CURå‹ç¼©
        if tensor_shape is not None and compression_ratio < 1.0:
            self.apply_cur_compression(compression_ratio)

        self.scaling = 1.0

    def apply_cur_compression(self, compression_ratio):
        """åº”ç”¨CURå‹ç¼©åˆ°LoRAçŸ©é˜µ"""
        # å°†lora_Aé‡å¡‘ä¸ºå¼ é‡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if self.tensor_shape is not None:
            # 4Då¼ é‡æƒ…å†µ (æ³¨æ„åŠ›æƒé‡)
            n1, n2, n3, n4 = self.tensor_shape
            A_tensor = self.lora_A.view(n1, n2, n3, -1)

            # åº”ç”¨å¼ é‡CUR
            tensor_cur = TensorCUR(
                ranks=[
                    (int(n1 * compression_ratio), int(n1 * compression_ratio)),
                    (int(n2 * compression_ratio), int(n2 * compression_ratio)),
                    (int(n3 * compression_ratio), int(n3 * compression_ratio)),
                    (int(self.rank * compression_ratio), int(self.rank * compression_ratio))
                ]
            )

            self.cur_components = tensor_cur.decompose_tensor(A_tensor)

            # å­˜å‚¨å‹ç¼©åçš„å‚æ•°
            self.compressed = True
        else:
            # 2DçŸ©é˜µæƒ…å†µ: ç›´æ¥CUR
            cur = CURDecomposition(
                n_cols=int(self.in_features * compression_ratio),
                n_rows=int(self.rank * compression_ratio),
                sampling='importance'
            )

            C, U, R, col_idx, row_idx = cur.decompose(self.lora_A.data)

            # å­˜å‚¨CURæˆåˆ†ï¼ˆä¸å†éœ€è¦æ¢¯åº¦ï¼‰
            self.register_buffer('C', C)
            self.register_buffer('U', U)
            self.register_buffer('R', R)
            self.register_buffer('col_idx', col_idx)
            self.register_buffer('row_idx', row_idx)

            self.compressed = True

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        if hasattr(self, 'compressed') and self.compressed:
            if hasattr(self, 'cur_components'):
                # å¼ é‡CURé‡æ„
                A_reconstructed = self._reconstruct_tensor()
            else:
                # çŸ©é˜µCURé‡æ„
                A_reconstructed = self.C @ self.U @ self.R

            lora_A = A_reconstructed
        else:
            lora_A = self.lora_A

        # LoRAå‰å‘ä¼ æ’­
        result = x @ lora_A @ self.lora_B * self.scaling
        return result

    def _reconstruct_tensor(self):
        """ä»CURæˆåˆ†é‡æ„å¼ é‡"""
        # ç®€åŒ–ç‰ˆé‡æ„ï¼ˆå®é™…éœ€è¦æ›´å¤æ‚çš„å®ç°ï¼‰
        reconstructed = self.lora_A  # å ä½ç¬¦
        return reconstructed

    def get_parameter_count(self):
        """è·å–å®é™…å‚æ•°é‡"""
        if hasattr(self, 'compressed') and self.compressed:
            if hasattr(self, 'cur_components'):
                # å¼ é‡CURå‚æ•°é‡
                total = 0
                for comp in self.cur_components:
                    total += comp['C'].numel() + comp['U'].numel() + comp['R'].numel()
                return total + self.lora_B.numel()
            else:
                # çŸ©é˜µCURå‚æ•°é‡
                return self.C.numel() + self.U.numel() + self.R.numel() + self.lora_B.numel()
        else:
            return self.lora_A.numel() + self.lora_B.numel()


class tCURLoRAModel(nn.Module):
    """å®Œæ•´çš„tCURLoRAæ¨¡å‹"""

    def __init__(self, base_model, lora_rank=8, compression_ratio=0.3):
        """
        å‚æ•°:
            base_model: åŸºç¡€æ¨¡å‹ï¼ˆå¦‚LlamaForCausalLMï¼‰
            lora_rank: LoRAç§©
            compression_ratio: CURå‹ç¼©æ¯”ä¾‹
        """
        super().__init__()
        self.base_model = base_model
        self.lora_rank = lora_rank
        self.compression_ratio = compression_ratio

        # æ·»åŠ tCURLoRAé€‚é…å™¨
        self._add_lora_adapters()

    def _add_lora_adapters(self):
        """å‘æ¨¡å‹æ·»åŠ LoRAé€‚é…å™¨"""
        # éå†æ¨¡å‹ä¸­çš„çº¿æ€§å±‚
        for name, module in self.base_model.named_modules():
            if isinstance(module, nn.Linear):
                # æ·»åŠ tCURLoRAé€‚é…å™¨
                lora = tCURLoRALayer(
                    in_features=module.in_features,
                    out_features=module.out_features,
                    rank=self.lora_rank,
                    compression_ratio=self.compression_ratio
                )
                setattr(module, 'lora', lora)

    def forward(self, *args, **kwargs):
        """å‰å‘ä¼ æ’­"""
        # åŸºç¡€æ¨¡å‹å‰å‘ä¼ æ’­
        output = self.base_model(*args, **kwargs)

        # æ·»åŠ LoRAè´¡çŒ®ï¼ˆéœ€è¦åœ¨æ¨¡å‹å†…éƒ¨ä¿®æ”¹ï¼‰
        # è¿™é‡Œç®€åŒ–è¡¨ç¤ºï¼Œå®é™…éœ€è¦hookæˆ–ä¿®æ”¹forward

        return output

    def count_parameters(self):
        """ç»Ÿè®¡å‚æ•°é‡"""
        total = 0
        lora_params = 0
        compressed_lora_params = 0

        for name, param in self.named_parameters():
            total += param.numel()

        for name, module in self.base_model.named_modules():
            if hasattr(module, 'lora'):
                lora = module.lora
                lora_params += lora.in_features * lora.rank + lora.rank * lora.out_features
                compressed_lora_params += lora.get_parameter_count()

        return {
            'total': total,
            'lora_original': lora_params,
            'lora_compressed': compressed_lora_params,
            'compression_ratio': compressed_lora_params / lora_params if lora_params > 0 else 0
        }


# ===== è®­ç»ƒå’Œè¯„ä¼° =====

def train_tcurlora(model, train_dataloader, val_dataloader,
                   num_epochs=3, learning_rate=1e-4):
    """è®­ç»ƒtCURLoRAæ¨¡å‹"""

    optimizer = torch.optim.AdamW([
        {'params': [p for n, p in model.named_parameters()
                    if 'lora' in n and p.requires_grad]},
    ], lr=learning_rate)

    model.train()

    for epoch in range(num_epochs):
        total_loss = 0
        for batch in train_dataloader:
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = model(**batch)
            loss = outputs.loss

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # éªŒè¯
        val_loss = evaluate(model, val_dataloader)

        print(f"Epoch {epoch + 1}/{num_epochs}")
        print(f"  Train Loss: {total_loss / len(train_dataloader):.4f}")
        print(f"  Val Loss: {val_loss:.4f}")

        # æ‰“å°å‹ç¼©ç»Ÿè®¡
        params = model.count_parameters()
        print(f"  LoRA Compression: {params['compression_ratio']:.1%}")


def evaluate(model, dataloader):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in dataloader:
            outputs = model(**batch)
            total_loss += outputs.loss.item()

    return total_loss / len(dataloader)


# ===== ä½¿ç”¨ç¤ºä¾‹ =====

def example_tcurlora():
    """tCURLoRAä½¿ç”¨ç¤ºä¾‹"""

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    from transformers import LlamaModel, LlamaConfig
    config = LlamaConfig.from_pretrained('decapoda-research/llama-7b-hf')

    # 2. åˆ›å»ºtCURLoRAæ¨¡å‹
    base_model = LlamaModel(config)
    model = tCURLoRAModel(
        base_model=base_model,
        lora_rank=8,
        compression_ratio=0.3  # å‹ç¼©åˆ°30%
    )

    # 3. æ‰“å°å‚æ•°ç»Ÿè®¡
    params = model.count_parameters()
    print("=" * 50)
    print("tCURLoRA Parameter Statistics:")
    print("=" * 50)
    print(f"Total Parameters: {params['total']:,}")
    print(f"Original LoRA Parameters: {params['lora_original']:,}")
    print(f"Compressed LoRA Parameters: {params['lora_compressed']:,}")
    print(f"Compression Ratio: {params['compression_ratio']:.1%}")
    print("=" * 50)

    return model
```

### å¤æ‚åº¦åˆ†æ

| æ–¹æ³• | å‚æ•°é‡ | è®¡ç®—å¤æ‚åº¦ | å†…å­˜å ç”¨ |
|------|--------|-----------|---------|
| å…¨é‡å¾®è°ƒ | $O(d^2)$ | $O(d^2)$ | é«˜ |
| LoRA | $O(2rd)$ | $O(2rd + d^2)$ | ä¸­ |
| tCURLoRA | $O(\alpha \cdot 2rd)$ | $O(\alpha \cdot 2rd + d^2)$ | ä½ |

å…¶ä¸­ $\alpha$ æ˜¯å‹ç¼©æ¯”ä¾‹ï¼ˆé€šå¸¸0.3-0.5ï¼‰ã€‚

---

## ğŸ’¼ åº”ç”¨ä¸“å®¶Agentï¼šä»·å€¼åˆ†æ

### åº”ç”¨åœºæ™¯

1. **å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ**
   - æŒ‡ä»¤éµå¾ª
   - ä»»åŠ¡é€‚é…
   - é¢†åŸŸé€‚åº”

2. **å¤šä»»åŠ¡å­¦ä¹ **
   - ä¸åŒä»»åŠ¡çš„ç‹¬ç«‹é€‚é…å™¨
   - é€‚é…å™¨ç»„åˆä¸å¤ç”¨

3. **è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²**
   - å­˜å‚¨å—é™åœºæ™¯
   - ä½å»¶è¿Ÿæ¨ç†

### å®éªŒç»“æœï¼ˆåŸºäºè®ºæ–‡ï¼‰

| ä»»åŠ¡ | æŒ‡æ ‡ | LoRA | tCURLoRA | Î” |
|------|------|------|----------|---|
| GLUE-SST2 | Accuracy | 92.3% | **92.5%** | +0.2% |
| GLUE-QQP | F1 | 87.1% | **87.3%** | +0.2% |
| AlpacaEval | Win Rate | 78.2% | **79.1%** | +0.9% |

**å‚æ•°å¯¹æ¯”**:
| æ¨¡å‹ | LoRAå‚æ•° | tCURLoRAå‚æ•° | å‹ç¼©ç‡ |
|------|----------|--------------|--------|
| LLaMA-7B | 36M | **12M** | 33% |
| LLaMA-13B | 72M | **24M** | 33% |
| LLaMA-33B | 180M | **60M** | 33% |

### å¯¹æ¯”æ–¹æ³•

1. **å…¨é‡å¾®è°ƒ**: æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ
2. **LoRA**: ä½ç§©é€‚é…
3. **AdaLoRA**: è‡ªé€‚åº”ç§©åˆ†é…
4. **QLoRA**: é‡åŒ–+LoRA

### ä¼˜åŠ¿æ€»ç»“

1. **å‚æ•°æ•ˆç‡**: ç›¸æ¯”LoRAå‡å°‘50-70%å‚æ•°
2. **å¯è§£é‡Šæ€§**: CURä½¿ç”¨å®é™…è¡Œ/åˆ—ï¼Œæ›´æ˜“è§£é‡Š
3. **æ€§èƒ½ä¿æŒ**: åœ¨å¤šæ•°ä»»åŠ¡ä¸ŠæŒå¹³æˆ–ä¼˜äºLoRA
4. **çµæ´»å‹ç¼©**: å¯æ ¹æ®èµ„æºè°ƒæ•´å‹ç¼©æ¯”ä¾‹

---

## â“ è´¨ç–‘è€…Agentï¼šæ‰¹åˆ¤åˆ†æ

### å±€é™æ€§

1. **é‡‡æ ·éšæœºæ€§**
   - CURåˆ†è§£ç»“æœéšé‡‡æ ·å˜åŒ–
   - å¯èƒ½éœ€è¦å¤šæ¬¡å°è¯•

2. **è®­ç»ƒå¤æ‚åº¦**
   - éœ€è¦é¢„è®­ç»ƒ+å¾®è°ƒä¸¤é˜¶æ®µ
   - CURåˆ†è§£çš„é¢å¤–è®¡ç®—å¼€é”€

3. **ç†è®ºgap**
   - ç¼ºä¹ä¸¥æ ¼çš„ç†è®ºæ”¶æ•›æ€§è¯æ˜
   - é‡‡æ ·ç­–ç•¥çš„æœ€ä¼˜æ€§æœªè¯æ˜

4. **ç¡¬ä»¶é€‚é…**
   - ç¨€ç–çŸ©é˜µæ“ä½œä¼˜åŒ–ä¸è¶³
   - ä¸åŒæ¶æ„æ€§èƒ½å·®å¼‚å¤§

### æ”¹è¿›æ–¹å‘

1. **è‡ªé€‚åº”é‡‡æ ·**
   - å­¦ä¹ é‡è¦æ€§æƒé‡
   - åŠ¨æ€è°ƒæ•´é‡‡æ ·è§„æ¨¡

2. **æ··åˆæ–¹æ³•**
   - CUR+é‡åŒ–
   - CUR+å‰ªæ

3. **ç«¯åˆ°ç«¯è®­ç»ƒ**
   - å¯å¾®åˆ†é‡‡æ ·
   - è”åˆä¼˜åŒ–åˆ†è§£å’Œå¾®è°ƒ

4. **ç†è®ºåˆ†æ**
   - æ³›åŒ–è¯¯å·®ç•Œ
   - é‡‡æ ·å¤æ‚åº¦åˆ†æ

### æ½œåœ¨é—®é¢˜

1. **è¯„ä¼°åå·®**
   - GLUEåŸºå‡†å¯èƒ½ä¸èƒ½å……åˆ†åæ˜ ä¼˜åŠ¿
   - éœ€è¦æ›´å¤šä¸‹æ¸¸ä»»åŠ¡éªŒè¯

2. **å¯æ‰©å±•æ€§**
   - è¶…å¤§æ¨¡å‹ï¼ˆ>100Bï¼‰çš„å®ç”¨æ€§
   - å¤šæ¨¡æ€æ¨¡å‹çš„æ‰©å±•

3. **å·¥ç¨‹æŒ‘æˆ˜**
   - æ¡†æ¶é›†æˆå¤æ‚åº¦
   - éƒ¨ç½²æ—¶çš„æ¨ç†ä¼˜åŒ–

---

## ğŸ¯ ç»¼åˆç†è§£

### æ ¸å¿ƒåˆ›æ–°

1. **å¼ é‡åŒ–LoRA**: å°†LoRAé€‚é…å™¨è¡¨ç¤ºä¸ºå¼ é‡
2. **é€æ¨¡CURåˆ†è§£**: å¯¹å¼ é‡å„æ¨¡æ€åˆ†åˆ«åº”ç”¨CUR
3. **å®é™…è¡Œ/åˆ—é€‰æ‹©**: ç›¸æ¯”SVDæ›´å…·å¯è§£é‡Šæ€§
4. **æ˜¾è‘—å‹ç¼©**: å®ç°30-50%å‚æ•°çš„åŒæ—¶ä¿æŒæ€§èƒ½

### æŠ€æœ¯è´¡çŒ®

| æ–¹é¢ | è´¡çŒ® |
|------|------|
| **æ–¹æ³•åˆ›æ–°** | é¦–æ¬¡å°†CURåˆ†è§£åº”ç”¨äºPEFT |
| **å¼ é‡æ–¹æ³•** | å¤šæ¨¡æ€å¼ é‡åˆ†è§£çš„å®ç”¨åŒ– |
| **æ•ˆç‡æå‡** | æ˜¾è‘—é™ä½LoRAçš„å­˜å‚¨éœ€æ±‚ |
| **å¯è§£é‡Šæ€§** | ä½¿ç”¨å®é™…æƒé‡è€ŒéæŠ½è±¡åˆ†è§£ |

### ç ”ç©¶æ„ä¹‰

1. **å®ç”¨ä»·å€¼**
   - ä½¿å¤§æ¨¡å‹å¾®è°ƒæ›´åŠ æ™®åŠ
   - é™ä½éƒ¨ç½²æˆæœ¬

2. **æ–¹æ³•è®ºè´¡çŒ®**
   - å±•ç¤ºäº†å¼ é‡æ–¹æ³•åœ¨NLPä¸­çš„æ½œåŠ›
   - ä¸ºPEFTæä¾›æ–°æ–¹å‘

3. **æœªæ¥æ–¹å‘**
   - ä¸å…¶ä»–å‹ç¼©æŠ€æœ¯ç»“åˆ
   - æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨¡å‹
   - è‡ªåŠ¨åŒ–å‹ç¼©æ¯”ä¾‹é€‰æ‹©

### ä¸è”¡æ™“æ˜Šå…¶ä»–å·¥ä½œçš„è”ç³»

tCURLoRAä»£è¡¨äº†è”¡æ™“æ˜Šç ”ç©¶ä»ä¼ ç»Ÿä¼˜åŒ–åˆ°ç°ä»£æ·±åº¦å­¦ä¹ çš„æ¼”è¿›ï¼š

1. **ç†è®ºè„‰ç»œ**
   ```
   çŸ©é˜µåˆ†è§£åŸºç¡€ (SVD, CUR)
          â†“
   å¼ é‡åˆ†è§£ (Tucker, Tensor Train, 2023-2024)
          â†“
   PEFTåº”ç”¨ (tCURLoRA, 2025)
   ```

2. **æ–¹æ³•å»¶ç»­**
   - Two-Sided Sketching (2024): éšæœºé‡‡æ ·æ€æƒ³
   - Tensor Train (2023): å¼ é‡æ–¹æ³•åŸºç¡€
   - tCURLoRA (2025): å¼ é‡åˆ†è§£åœ¨LLMä¸­çš„åº”ç”¨

3. **ç ”ç©¶ä¸»é¢˜æ¼”å˜**
   - æ—©æœŸ: å˜åˆ†ä¼˜åŒ–ã€å›¾åƒå¤„ç†
   - ä¸­æœŸ: å¼ é‡åˆ†è§£ã€ç§‘å­¦è®¡ç®—
   - è¿‘æœŸ: å¤§æ¨¡å‹ã€é«˜æ•ˆå¾®è°ƒ
   - tCURLoRA: ä¸¤å¤§ä¸»é¢˜çš„äº¤æ±‡

### å½±å“åŠ›ä¸å¼•ç”¨

è™½ç„¶è®ºæ–‡è¾ƒæ–°(2025)ï¼Œä½†é¢„æœŸå°†åœ¨ä»¥ä¸‹é¢†åŸŸäº§ç”Ÿå½±å“ï¼š
- å‚æ•°é«˜æ•ˆå¾®è°ƒ
- å¤§æ¨¡å‹éƒ¨ç½²
- å¼ é‡ç¥ç»ç½‘ç»œ
- æ¨¡å‹å‹ç¼©

---

## é™„å½•ï¼šå…³é”®å…¬å¼é€ŸæŸ¥

```
LoRA:
  W' = W + Î”W = W + BA
  h = Wx + BAx

CURåˆ†è§£:
  M â‰ˆ C @ U @ R
  C âˆˆ â„^{mÃ—c}, R âˆˆ â„^{rÃ—n}

tCURLoRA:
  A â‰ˆ A Ã—â‚ Câ½Â¹â¾Uâ½Â¹â¾Râ½Â¹â¾ Ã—â‚‚ ... Ã—â‚„ Câ½â´â¾Uâ½â´â¾Râ½â´â¾

é‡è¦æ€§é‡‡æ ·:
  p_i = â€–M_{(i,:)}â€–â‚‚ / Î£_j â€–M_{(j,:)}â€–â‚‚

å‚æ•°é‡:
  LoRA: 2rd
  tCURLoRA: Î±Â·2rd (Î± âˆˆ [0.3, 0.5])
```

---

**ç¬”è®°ç”Ÿæˆæ—¶é—´**: 2026-02-20
**ç²¾è¯»æ·±åº¦**: â˜…â˜…â˜…â˜…â˜… (äº”çº§ç²¾è¯»)
**æ¨èæŒ‡æ•°**: â˜…â˜…â˜…â˜…â˜… (LLM/PEFTé¢†åŸŸå¿…è¯»)
**åˆ›æ–°æ€§**: â˜…â˜…â˜…â˜…â˜† (å¼ é‡åˆ†è§£ä¸PEFTçš„é¦–æ¬¡ç»“åˆ)
**å®ç”¨ä»·å€¼**: â˜…â˜…â˜…â˜…â˜… (ç›´æ¥å¯åº”ç”¨äºå¤§æ¨¡å‹éƒ¨ç½²)
