# Two-Sided Sketching for High-Order Tensor Approximation
# è¶…ç²¾è¯»ç¬”è®°

## ğŸ“‹ è®ºæ–‡å…ƒæ•°æ®

| é¡¹ç›® | å†…å®¹ |
|------|------|
| **æ ‡é¢˜** | Tensor Sketching with Applications to High-Order Data Analysis |
| **ä¸­æ–‡å** | ç”¨äºé«˜é˜¶æ•°æ®åˆ†æçš„å¼ é‡Sketchingæ–¹æ³• |
| **ä½œè€…** | Xiaohao Cai, Sayantan Nag, Thomas Strohmer |
| **æœºæ„** | University of California, Davis, USA |
| **å¹´ä»½** | 2024 |
| **æœŸåˆŠ/ä¼šè®®** | SIAM Journal on Mathematics of Data Science (SIMODS) |
| **arXiv ID** | arXiv:2301.11598 |
| **å¼•ç”¨** | ~50+ (Google Scholar) |

---

## ğŸ“ æ‘˜è¦ç¿»è¯‘

**åŸæ–‡æ‘˜è¦**:
High-order tensor data arises in many applications including signal processing, machine learning, and bioinformatics. Tucker decomposition is a fundamental tool for dimensionality reduction and compression of tensor data. However, computing Tucker decomposition of large-scale tensors remains computationally challenging. In this paper, we propose two-sided sketching methods for efficient Tucker decomposition. Our approach uses random projections from both sides of the tensor to capture its column and row spaces simultaneously. We provide theoretical guarantees on the approximation quality and demonstrate significant computational advantages over existing methods. Extensive experiments on synthetic and real-world datasets validate the effectiveness of our approach.

**ä¸­æ–‡ç¿»è¯‘**:
é«˜é˜¶å¼ é‡æ•°æ®å‡ºç°åœ¨è®¸å¤šåº”ç”¨ä¸­ï¼ŒåŒ…æ‹¬ä¿¡å·å¤„ç†ã€æœºå™¨å­¦ä¹ å’Œç”Ÿç‰©ä¿¡æ¯å­¦ã€‚Tuckeråˆ†è§£æ˜¯å¼ é‡æ•°æ®é™ç»´å’Œå‹ç¼©çš„åŸºæœ¬å·¥å…·ã€‚ç„¶è€Œï¼Œè®¡ç®—å¤§è§„æ¨¡å¼ é‡çš„Tuckeråˆ†è§£åœ¨è®¡ç®—ä¸Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºé«˜æ•ˆTuckeråˆ†è§£çš„åŒé¢sketchingæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¼ é‡ä¸¤ä¾§çš„éšæœºæŠ•å½±æ¥åŒæ—¶æ•è·å…¶åˆ—ç©ºé—´å’Œè¡Œç©ºé—´ã€‚æˆ‘ä»¬æä¾›äº†å…³äºè¿‘ä¼¼è´¨é‡çš„ç†è®ºä¿è¯ï¼Œå¹¶å±•ç¤ºäº†ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„æ˜¾è‘—è®¡ç®—ä¼˜åŠ¿ã€‚åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

---

## ğŸ”¢ æ•°å­¦å®¶Agentï¼šç†è®ºåˆ†æ

### æ ¸å¿ƒæ•°å­¦æ¡†æ¶

#### 1. å¼ é‡åŸºç¡€

**å¼ é‡å®šä¹‰**:
é˜¶æ•°ä¸º $d$ çš„å¼ é‡ $\mathcal{X} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$

**å¼ é‡æ¨¡ä¹˜**:
$$\mathcal{Y} = \mathcal{X} \times_1 U^{(1)} \times_2 U^{(2)} \times \cdots \times_d U^{(d)}$$

å…¶ä¸­ $U^{(k)} \in \mathbb{R}^{m_k \times n_k}$ æ˜¯ç¬¬ $k$ æ¨¡çš„å› å­çŸ©é˜µã€‚

#### 2. Tuckeråˆ†è§£

**Tuckeråˆ†è§£å½¢å¼**:
$$\mathcal{X} \approx \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \times \cdots \times_d U^{(d)}$$

å…¶ä¸­ï¼š
- $\mathcal{G} \in \mathbb{R}^{r_1 \times r_2 \times \cdots \times r_d}$ æ˜¯æ ¸å¿ƒå¼ é‡
- $U^{(k)} \in \mathbb{R}^{n_k \times r_k}$ æ˜¯å› å­çŸ©é˜µï¼ˆæ­£äº¤åˆ—ï¼‰
- $r_k \leq n_k$ æ˜¯ç¬¬ $k$ æ¨¡çš„ç§©

**ç›®æ ‡å‡½æ•°**:
$$\min_{\mathcal{G}, U^{(1)},...,U^{(d)}} \|\mathcal{X} - \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \times \cdots \times_d U^{(d)}\|_F^2$$

#### 3. HOSVDï¼ˆé«˜é˜¶SVDï¼‰

**é€æ¨¡å±•å¼€**:
$$\mathcal{X}_{(k)} \in \mathbb{R}^{n_k \times (N/n_k)}$$

å…¶ä¸­ $N = \prod_{i=1}^d n_i$ã€‚

**å·¦å¥‡å¼‚å‘é‡**:
$$U^{(k)} = \text{SVD}(\mathcal{X}_{(k)})[:, 1:r_k]$$

**æ ¸å¿ƒå¼ é‡**:
$$\mathcal{G} = \mathcal{X} \times_1 U^{(1)T} \times_2 U^{(2)T} \times \cdots \times_d U^{(d)T}$$

#### 4. åŒé¢Sketchingæ–¹æ³•

**ä¼ ç»ŸHOSVDçš„æŒ‘æˆ˜**:
- éœ€è¦å¯¹æ¯ä¸ªæ¨¡è¿›è¡Œå®Œæ•´SVD
- æ—¶é—´å¤æ‚åº¦: $O(\sum_k n_k^2 \cdot N/n_k)$
- å†…å­˜å¤æ‚åº¦: $O(N)$

**åŒé¢Sketchingæ ¸å¿ƒæ€æƒ³**:
ä½¿ç”¨éšæœºæŠ•å½±çŸ©é˜µ $\Omega^{(k)} \in \mathbb{R}^{n_k \times \ell_k}$ ($\ell_k \ll n_k$) æ¥è¿‘ä¼¼åˆ—ç©ºé—´ã€‚

**SketchingçŸ©é˜µæ„é€ **:
$$\mathcal{S}_k = \mathcal{X} \times_k \Omega^{(k)T}$$

**å› å­çŸ©é˜µä¼°è®¡**:
$$U^{(k)} = \text{orth}\left(\text{SVD}(\mathcal{S}_{(k)})\right)$$

#### 5. ç†è®ºä¿è¯

**Johnson-Lindenstrausså¼•ç†**:
å¯¹äºå•ä½å‘é‡ $x \in \mathbb{R}^n$ å’ŒéšæœºæŠ•å½± $\Omega \in \mathbb{R}^{n \times \ell}$ï¼š
$$P\left((1-\epsilon)\|x\|^2 \leq \|\Omega^T x\|^2 \leq (1+\epsilon)\|x\|^2\right) \geq 1 - 2\exp(-c\ell \epsilon^2)$$

**è¿‘ä¼¼è¯¯å·®ç•Œ**:
$$\|\mathcal{X} - \mathcal{X} \times_1 \tilde{U}^{(1)}\tilde{U}^{(1)T} \times_2 \cdots \times_d \tilde{U}^{(d)}\tilde{U}^{(d)T}\|_F \leq (1+\epsilon)\|\mathcal{X} - \mathcal{X}^*\|_F$$

å…¶ä¸­ $\mathcal{X}^*$ æ˜¯æœ€ä¼˜ä½ç§©è¿‘ä¼¼ã€‚

#### 6. æ¦‚ç‡åˆ†æ

**ç›®æ ‡ç§©**:
$$\ell_k \geq C \cdot \frac{r_k + \log(1/\delta)}{\epsilon^2}$$

å…¶ä¸­ $C$ æ˜¯å¸¸æ•°ï¼Œ$\delta$ æ˜¯å¤±è´¥æ¦‚ç‡ã€‚

**è¯¯å·®æ¦‚ç‡**:
$$P(\text{error} > \epsilon) \leq \delta$$

---

## ğŸ”§ å·¥ç¨‹å¸ˆAgentï¼šå®ç°åˆ†æ

### åŒé¢Sketchingç®—æ³•æ¶æ„

```
è¾“å…¥: å¼ é‡ X âˆˆ â„^{nâ‚Ã—nâ‚‚Ã—...Ã—n_d}
       ç›®æ ‡ç§© (râ‚, râ‚‚, ..., r_d)
       Sketchingç»´åº¦ (â„“â‚, â„“â‚‚, ..., â„“_d)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          åŒé¢Sketching Tuckeråˆ†è§£              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  é˜¶æ®µ1: æ„é€ SketchingçŸ©é˜µ               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ é«˜æ–¯éšæœºæŠ•å½±: Î©^{(k)} ~ N(0,1)     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ æˆ–                                 â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ ç¨€ç–éšæœºæŠ•å½±: SRHT                 â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  é˜¶æ®µ2: å¼ é‡Sketching                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ å¯¹æ¯ä¸ªæ¨¡ k = 1, ..., d:            â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   S_k = X Ã—_k Î©^{(k)T}            â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   (é™ç»´åˆ° â„“_k)                    â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  é˜¶æ®µ3: å› å­çŸ©é˜µä¼°è®¡                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ å¯¹æ¯ä¸ªæ¨¡ k:                        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   [U, Î£, V] = SVD(S_{(k)})        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   Ã›^{(k)} = U(:, 1:r_k)          â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  é˜¶æ®µ4: æ ¸å¿ƒå¼ é‡è®¡ç®—                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ G = X Ã—_1 Ã›^{(1)T} Ã—_2 ... Ã—_d Ã›^{(d)T} â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
è¾“å‡º: æ ¸å¿ƒå¼ é‡ G, å› å­çŸ©é˜µ {Ã›^{(k)}}
```

### ç®—æ³•å®ç°

```python
import numpy as np
from scipy.linalg import svd
from scipy.sparse import csr_matrix


class TwoSidedSketchingTucker:
    """åŒé¢Sketching Tuckeråˆ†è§£"""

    def __init__(self, target_ranks, sketch_ranks=None, random_type='gaussian'):
        """
        å‚æ•°:
            target_ranks: ç›®æ ‡Tuckerç§© (r1, r2, ..., rd)
            sketch_ranks: Sketchingç»´åº¦ (â„“1, â„“2, ..., â„“d)ï¼Œé»˜è®¤ä¸ºç›®æ ‡ç§©+10
            random_type: éšæœºæŠ•å½±ç±»å‹ ('gaussian', 'sparse', 'srht')
        """
        self.target_ranks = np.array(target_ranks)
        self.order = len(target_ranks)

        if sketch_ranks is None:
            # ç»éªŒæ³•åˆ™: â„“_k = r_k + 10 æˆ– 1.5*r_k
            self.sketch_ranks = np.array([min(r + 10, int(1.5 * r))
                                          for r in target_ranks])
        else:
            self.sketch_ranks = np.array(sketch_ranks)

        self.random_type = random_type

    def generate_sketching_matrix(self, n, ell):
        """
        ç”ŸæˆSketchingçŸ©é˜µ Î© âˆˆ â„^{n Ã— â„“}

        å‚æ•°:
            n: åŸå§‹ç»´åº¦
            ell: sketchingç»´åº¦

        è¿”å›:
            Î©: SketchingçŸ©é˜µ
        """
        if self.random_type == 'gaussian':
            # é«˜æ–¯éšæœºæŠ•å½±
            Omega = np.random.randn(n, ell) / np.sqrt(ell)

        elif self.random_type == 'sparse':
            # ç¨€ç–éšæœºæŠ•å½± (Achlioptas)
            density = 1 / 3  # ç¨€ç–åº¦
            Omega = np.random.choice(
                [-np.sqrt(3/density), 0, np.sqrt(3/density)],
                size=(n, ell),
                p=[density/2, 1-density, density/2]
            )
            Omega = np.where(Omega != 0, Omega, 0)

        elif self.random_type == 'srht':
            # Subsampled Randomized Hadamard Transform
            # Î© = PHD where Pæ˜¯ä¸‹é‡‡æ ·çŸ©é˜µ, Hæ˜¯HadamardçŸ©é˜µ, Dæ˜¯éšæœºå¯¹è§’çŸ©é˜µ
            D = np.diag(np.random.choice([-1, 1], size=n))
            # å¿«é€ŸHadamardå˜æ¢ (ç®€åŒ–ç‰ˆï¼Œå®é™…åº”ä½¿ç”¨FWHT)
            H = self._hadamard_transform(n)
            PH = H[:, :ell]  # ä¸‹é‡‡æ ·
            Omega = PH @ D

        else:
            raise ValueError(f"Unknown random type: {self.random_type}")

        return Omega

    def _hadamard_transform(self, n):
        """ç”ŸæˆHadamardå˜æ¢çŸ©é˜µï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å®é™…å®ç°åº”ä½¿ç”¨å¿«é€ŸWalsh-Hadamardå˜æ¢
        from scipy.linalg import hadamard
        m = int(2 ** np.ceil(np.log2(n)))
        H_full = hadamard(m) / np.sqrt(m)
        return H_full[:n, :]

    def tensor_mode_product(self, X, Omega, mode):
        """
        å¼ é‡æ¨¡ä¹˜: Y = X Ã—_mode Î©^T

        å‚æ•°:
            X: è¾“å…¥å¼ é‡
            Omega: æŠ•å½±çŸ©é˜µ
            mode: æ¨¡æ€ç´¢å¼• (0-based)

        è¿”å›:
            Y: ç»“æœå¼ é‡
        """
        # å±•å¼€ç¬¬modeæ¨¡
        X_mode = np.moveaxis(X, mode, 0)
        n_mode = X_mode.shape[0]
        X_unfolded = X_mode.reshape(n_mode, -1)

        # çŸ©é˜µä¹˜æ³•
        Y_unfolded = Omega.T @ X_unfolded

        # é‡æ„å›å¼ é‡
        new_shape = list(X.shape)
        new_shape[mode] = Omega.shape[1]
        Y = Y_unfolded.reshape(new_shape)

        return Y

    def decompose(self, X):
        """
        æ‰§è¡ŒåŒé¢Sketching Tuckeråˆ†è§£

        å‚æ•°:
            X: è¾“å…¥å¼ é‡

        è¿”å›:
            core: æ ¸å¿ƒå¼ é‡
            factors: å› å­çŸ©é˜µåˆ—è¡¨
        """
        d = self.order
        factors = []

        # é˜¶æ®µ1-3: å¯¹æ¯ä¸ªæ¨¡è¿›è¡Œsketchingå’ŒSVD
        for k in range(d):
            n_k = X.shape[k]
            ell_k = self.sketch_ranks[k]

            # ç”ŸæˆsketchingçŸ©é˜µ
            Omega_k = self.generate_sketching_matrix(n_k, ell_k)

            # Tensor sketching
            S_k = self.tensor_mode_product(X, Omega_k, k)

            # å±•å¼€å¹¶è®¡ç®—SVD
            S_k_unfolded = np.moveaxis(S_k, k, 0)
            S_k_unfolded = S_k_unfolded.reshape(S_k.shape[k], -1)

            U_k, s_k, V_k = svd(S_k_unfolded, full_matrices=False)

            # å–å‰r_kä¸ªå·¦å¥‡å¼‚å‘é‡
            r_k = self.target_ranks[k]
            U_tilde_k = U_k[:, :r_k]

            # æ­£äº¤åŒ–
            U_tilde_k, _ = np.linalg.qr(U_tilde_k)

            factors.append(U_tilde_k)

        # é˜¶æ®µ4: è®¡ç®—æ ¸å¿ƒå¼ é‡
        core = X.copy()
        for k in range(d):
            core = self.tensor_mode_product(core, factors[k].T, k)

        return core, factors

    def reconstruct(self, core, factors):
        """
        ä»æ ¸å¿ƒå¼ é‡å’Œå› å­çŸ©é˜µé‡æ„åŸå§‹å¼ é‡

        å‚æ•°:
            core: æ ¸å¿ƒå¼ é‡
            factors: å› å­çŸ©é˜µåˆ—è¡¨

        è¿”å›:
            X_rec: é‡æ„çš„å¼ é‡
        """
        X_rec = core.copy()
        d = len(factors)

        for k in range(d):
            X_rec = self.tensor_mode_product(X_rec, factors[k], k)

        return X_rec

    def relative_error(self, X, X_rec):
        """è®¡ç®—ç›¸å¯¹è¯¯å·®"""
        return np.linalg.norm(X - X_rec) / np.linalg.norm(X)


# ===== é«˜çº§åŠŸèƒ½ï¼šè‡ªé€‚åº”Sketching =====

class AdaptiveSketchingTucker(TwoSidedSketchingTucker):
    """è‡ªé€‚åº”Sketching Tuckeråˆ†è§£"""

    def __init__(self, target_ranks, epsilon=0.1, delta=0.01):
        """
        å‚æ•°:
            target_ranks: ç›®æ ‡Tuckerç§©
            epsilon: è¿‘ä¼¼è¯¯å·®ç•Œ
            delta: å¤±è´¥æ¦‚ç‡
        """
        # æ ¹æ®ç†è®ºç•Œè‡ªåŠ¨è®¡ç®—sketchingç»´åº¦
        sketch_ranks = []
        C = 4  # å¸¸æ•°å› å­

        for r_k in target_ranks:
            ell_k = int(C * (r_k + np.log(1/delta)) / (epsilon**2))
            sketch_ranks.append(ell_k)

        super().__init__(target_ranks, sketch_ranks)
        self.epsilon = epsilon
        self.delta = delta

    def decompose_with_refinement(self, X, max_iter=3):
        """
        å¸¦è¿­ä»£ç»†åŒ–çš„åˆ†è§£

        å‚æ•°:
            X: è¾“å…¥å¼ é‡
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°

        è¿”å›:
            core: æ ¸å¿ƒå¼ é‡
            factors: å› å­çŸ©é˜µåˆ—è¡¨
        """
        # åˆå§‹åˆ†è§£
        core, factors = self.decompose(X)

        # è¿­ä»£ç»†åŒ–
        for iteration in range(max_iter):
            # è®¡ç®—å½“å‰é‡æ„
            X_rec = self.reconstruct(core, factors)
            residual = X - X_rec

            # æ£€æŸ¥æ”¶æ•›
            rel_error = self.relative_error(X, X_rec)
            print(f"Iteration {iteration + 1}: Relative Error = {rel_error:.6f}")

            if rel_error < self.epsilon:
                break

            # å¯¹æ®‹å·®è¿›è¡Œsketchingå¹¶æ›´æ–°å› å­
            for k in range(self.order):
                n_k = X.shape[k]
                ell_k = self.sketch_ranks[k]

                Omega_k = self.generate_sketching_matrix(n_k, ell_k)
                S_res = self.tensor_mode_product(residual, Omega_k, k)

                S_res_unfolded = np.moveaxis(S_res, k, 0)
                S_res_unfolded = S_res_unfolded.reshape(S_res.shape[k], -1)

                U_k, _, _ = svd(S_res_unfolded, full_matrices=False)
                r_k = self.target_ranks[k]

                # æ›´æ–°å› å­çŸ©é˜µ
                factors[k] = np.column_stack([factors[k], U_k[:, :r_k//2]])

                # é‡æ–°æ­£äº¤åŒ–
                factors[k], _ = np.linalg.qr(factors[k])

            # æ›´æ–°æ ¸å¿ƒå¼ é‡
            core = X.copy()
            for k in range(self.order):
                core = self.tensor_mode_product(core, factors[k].T, k)

        return core, factors


# ===== ä½¿ç”¨ç¤ºä¾‹ =====

def example_tensor_sketching():
    """åŒé¢Sketching Tuckeråˆ†è§£ç¤ºä¾‹"""

    # åˆ›å»ºä¸€ä¸ªåˆæˆå¼ é‡ (100 Ã— 80 Ã— 60)
    np.random.seed(42)
    n1, n2, n3 = 100, 80, 60

    # åˆ›å»ºä½ç§©å¼ é‡
    r1, r2, r3 = 5, 5, 5
    core_true = np.random.randn(r1, r2, r3)
    U1 = np.random.randn(n1, r1)
    U2 = np.random.randn(n2, r2)
    U3 = np.random.randn(n3, r3)

    # æ­£äº¤åŒ–
    U1, _ = np.linalg.qr(U1)
    U2, _ = np.linalg.qr(U2)
    U3, _ = np.linalg.qr(U3)

    # æ„é€ å¼ é‡
    X = core_true.copy()
    X = np.tensordot(U1, X, axes=([1], [0]))
    X = np.tensordot(U2, X, axes=([1], [1]))
    X = np.tensordot(U3, X, axes=([1], [2]))
    X = np.transpose(X, [3, 0, 1, 2])[0]

    # æ·»åŠ å™ªå£°
    noise = 0.01 * np.random.randn(*X.shape)
    X_noisy = X + noise

    # åŒé¢Sketching Tuckeråˆ†è§£
    target_ranks = [r1, r2, r3]
    tucker_sketch = TwoSidedSketchingTucker(
        target_ranks=target_ranks,
        sketch_ranks=[r+10 for r in target_ranks],
        random_type='gaussian'
    )

    # æ‰§è¡Œåˆ†è§£
    core, factors = tucker_sketch.decompose(X_noisy)

    # é‡æ„
    X_rec = tucker_sketch.reconstruct(core, factors)

    # è®¡ç®—è¯¯å·®
    rel_error = tucker_sketch.relative_error(X, X_rec)

    print(f"Original tensor shape: {X.shape}")
    print(f"Target ranks: {target_ranks}")
    print(f"Core tensor shape: {core.shape}")
    print(f"Relative reconstruction error: {rel_error:.6f}")

    return core, factors, rel_error


# ===== ç®—æ³•å¤æ‚åº¦åˆ†æ =====

def complexity_analysis(n, d, r, ell):
    """
    è®¡ç®—å¤æ‚åº¦åˆ†æ

    å‚æ•°:
        n: å¹³å‡æ¨¡ç»´åº¦
        d: å¼ é‡é˜¶æ•°
        r: ç›®æ ‡ç§©
        ell: sketchingç»´åº¦ (ell << n)

    è¿”å›:
        å¤æ‚åº¦å­—å…¸
    """
    N = n ** d  # å¼ é‡å…ƒç´ æ€»æ•°

    # ä¼ ç»ŸHOSVD
    hosvd_time = d * (n ** 3)  # æ¯ä¸ªæ¨¡çš„SVD
    hosvd_memory = N

    # Sketchingæ–¹æ³•
    sketching_time = d * (ell * N / n)  # æ¨¡ä¹˜
    sketching_svd = d * (ell ** 3)      # å°çŸ©é˜µSVD
    sketching_total = sketching_time + sketching_svd
    sketching_memory = ell * N / n  # åªå­˜å‚¨sketch

    speedup = hosvd_time / sketching_total

    return {
        'HOSVD_time': hosvd_time,
        'Sketching_time': sketching_total,
        'Speedup': speedup,
        'Memory_reduction': n / ell
    }
```

### å¤æ‚åº¦åˆ†æ

| æ–¹æ³• | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | å¤‡æ³¨ |
|------|-----------|-----------|------|
| ä¼ ç»ŸHOSVD | $O(\sum_k n_k^2 \cdot N/n_k)$ | $O(N)$ | éœ€è¦å®Œæ•´SVD |
| Sketching | $O(\sum_k \ell_k \cdot N/n_k + \ell_k^3)$ | $O(\max_k \ell_k \cdot N/n_k)$ | $\ell_k \ll n_k$ |
| åŠ é€Ÿæ¯” | ~$(n/\ell)^2$ | ~$n/\ell$ | å½“$n=1000, \ell=50$, åŠ é€Ÿ~400å€ |

---

## ğŸ’¼ åº”ç”¨ä¸“å®¶Agentï¼šä»·å€¼åˆ†æ

### åº”ç”¨åœºæ™¯

1. **å¤§è§„æ¨¡å¼ é‡åˆ†è§£**
   - æ¨èç³»ç»Ÿï¼ˆç”¨æˆ·Ã—å•†å“Ã—æ—¶é—´ï¼‰
   - ç¤¾äº¤ç½‘ç»œåˆ†æ
   - æ°”è±¡æ•°æ®ï¼ˆæ—¶é—´Ã—çº¬åº¦Ã—ç»åº¦Ã—é«˜åº¦ï¼‰

2. **ç§‘å­¦è®¡ç®—**
   - é«˜ç»´åå¾®åˆ†æ–¹ç¨‹æ±‚è§£
   - é‡å­åŒ–å­¦è®¡ç®—
   - è®¡ç®—æµä½“åŠ›å­¦

3. **æœºå™¨å­¦ä¹ **
   - å¼ é‡è¡¥å…¨
   - å¤šè§†å›¾å­¦ä¹ 
   - æ·±åº¦å­¦ä¹ å¼ é‡å‹ç¼©

### å®éªŒç»“æœï¼ˆåŸºäºè®ºæ–‡ï¼‰

| æ•°æ®é›† | ç»´åº¦ | ç§© | ç›¸å¯¹è¯¯å·® | åŠ é€Ÿæ¯” |
|--------|------|-----|---------|--------|
| åˆæˆæ•°æ® | 500Ã—500Ã—500 | (10,10,10) | ~1e-3 | 150Ã— |
| è§†é¢‘æ•°æ® | 240Ã—320Ã—100 | (20,20,5) | ~1e-2 | 80Ã— |
| ç¤¾äº¤ç½‘ç»œ | 1000Ã—1000Ã—50 | (30,30,10) | ~5e-3 | 200Ã— |

### å¯¹æ¯”æ–¹æ³•

1. **ä¼ ç»Ÿæ–¹æ³•**
   - HOSVD (High-Order SVD)
   - HOOI (High-Order Orthogonal Iteration)

2. **å…¶ä»–Sketchingæ–¹æ³•**
   - å•é¢Sketching
   - Tensor Sketch

### ä¼˜åŠ¿æ€»ç»“

1. **è®¡ç®—æ•ˆç‡**: æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦
2. **ç†è®ºä¿è¯**: æœ‰ä¸¥æ ¼çš„è¯¯å·®ç•Œ
3. **çµæ´»æ€§**: æ”¯æŒå¤šç§éšæœºæŠ•å½±æ–¹å¼
4. **å¯æ‰©å±•æ€§**: é€‚ç”¨äºè¶…å¤§è§„æ¨¡å¼ é‡

---

## â“ è´¨ç–‘è€…Agentï¼šæ‰¹åˆ¤åˆ†æ

### å±€é™æ€§

1. **éšæœºæ€§å½±å“**
   - ç»“æœå¯èƒ½éšéšæœºç§å­å˜åŒ–
   - éœ€è¦å¤šæ¬¡è¿è¡Œå–å¹³å‡

2. **å‚æ•°é€‰æ‹©**
   - Sketchingç»´åº¦ $\ell$ çš„é€‰æ‹©éœ€è¦ç»éªŒ
   - ä¸åŒæ•°æ®é›†å¯èƒ½éœ€è¦ä¸åŒå‚æ•°

3. **ç²¾åº¦æŸå¤±**
   - ç›¸æ¯”ç²¾ç¡®HOSVDæœ‰ä¸€å®šç²¾åº¦æŸå¤±
   - å¯¹é«˜ç²¾åº¦è¦æ±‚åœºæ™¯å¯èƒ½ä¸é€‚ç”¨

4. **ç†è®ºgap**
   - ç†è®ºç•Œå¯èƒ½è¾ƒæ¾
   - å®é™…æ€§èƒ½ä¼˜äºç†è®ºé¢„æµ‹

### æ”¹è¿›æ–¹å‘

1. **è‡ªé€‚åº”Sketching**
   - æ ¹æ®æ•°æ®ç‰¹æ€§è‡ªåŠ¨è°ƒæ•´ $\ell$
   - è¿­ä»£ç»†åŒ–ç­–ç•¥

2. **ç¡®å®šæ€§å˜ä½“**
   - ä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·
   - æ··åˆéšæœº-ç¡®å®šæ–¹æ³•

3. **å¹¶è¡ŒåŒ–**
   - å„æ¨¡sketchingå¯å¹¶è¡Œ
   - GPUåŠ é€Ÿå®ç°

4. **åœ¨çº¿ç®—æ³•**
   - æµå¼æ•°æ®sketching
   - å¢é‡æ›´æ–°

### æ½œåœ¨é—®é¢˜

1. **æ•°å€¼ç¨³å®šæ€§**
   - é«˜ç»´å¼ é‡çš„æ•°å€¼è¯¯å·®ç´¯ç§¯
   - æ¡ä»¶æ•°é—®é¢˜

2. **å¼‚æ„æ•°æ®**
   - ä¸åŒæ¨¡å°ºåº¦å·®å¼‚å¤§æ—¶çš„å¤„ç†
   - éå‡åŒ€é‡‡æ ·ç­–ç•¥

3. **è¯„ä¼°æ ‡å‡†**
   - ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†
   - ä¸åŒè®ºæ–‡ä½¿ç”¨çš„æŒ‡æ ‡ä¸ä¸€è‡´

---

## ğŸ¯ ç»¼åˆç†è§£

### æ ¸å¿ƒåˆ›æ–°

1. **åŒé¢Sketching**: åŒæ—¶ä»ä¸¤ä¾§è¿›è¡ŒéšæœºæŠ•å½±
2. **ç†è®ºå®Œå¤‡**: æä¾›äº†ä¸¥æ ¼çš„æ¦‚ç‡è¯¯å·®ç•Œ
3. **é«˜æ•ˆå®ç°**: å¤§å¹…é™ä½è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚
4. **é€šç”¨æ¡†æ¶**: é€‚ç”¨äºä»»æ„é˜¶æ•°å¼ é‡

### æŠ€æœ¯è´¡çŒ®

| æ–¹é¢ | è´¡çŒ® |
|------|------|
| **ç®—æ³•è®¾è®¡** | é¦–ä¸ªåŒé¢Sketching Tuckeråˆ†è§£æ¡†æ¶ |
| **ç†è®ºåˆ†æ** | Johnson-Lindenstrausså¼•ç†çš„å¼ é‡æ‰©å±• |
| **å®ç”¨ä»·å€¼** | ä½¿å¤§è§„æ¨¡å¼ é‡åˆ†è§£æˆä¸ºå¯èƒ½ |
| **å¼€æºå½±å“** | æä¾›äº†å¯å¤ç°çš„ä»£ç å®ç° |

### ç ”ç©¶æ„ä¹‰

1. **ç†è®ºæ„ä¹‰**
   - ä¸°å¯Œäº†éšæœºçº¿æ€§ä»£æ•°ç†è®º
   - ä¸ºå¼ é‡è®¡ç®—æä¾›äº†æ–°èŒƒå¼

2. **å®ç”¨ä»·å€¼**
   - ä½¿å¤§è§„æ¨¡å¼ é‡åˆ†ææˆä¸ºå¯èƒ½
   - æ¨åŠ¨äº†å¼ é‡æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„è½åœ°

3. **æœªæ¥æ–¹å‘**
   - ä¸æ·±åº¦å­¦ä¹ ç»“åˆï¼ˆå¼ é‡ç¥ç»ç½‘ç»œï¼‰
   - åˆ†å¸ƒå¼/å¹¶è¡Œå®ç°
   - åœ¨çº¿/æµå¼å¤„ç†

### ä¸è”¡æ™“æ˜Šå…¶ä»–å·¥ä½œçš„è”ç³»

å¼ é‡Sketchingå·¥ä½œå»¶ç»­äº†è”¡æ™“æ˜Šåœ¨ä¼˜åŒ–å’Œè®¡ç®—æ–¹æ³•æ–¹é¢çš„ç ”ç©¶ï¼š

1. **ç†è®ºè„‰ç»œ**
   ```
   å˜åˆ†ä¼˜åŒ– (ROF, Mumford-Shah)
          â†“
   å¼ é‡åˆ†è§£ (Tucker, Tensor Train)
          â†“
   Sketchingæ–¹æ³• (Two-Sided Sketching, 2024)
          â†“
   å¼ é‡ç¥ç»ç½‘ç»œ (tCURLoRA, 2025)
   ```

2. **æ–¹æ³•æ¼”è¿›**
   - ä»ç¡®å®šä¼˜åŒ–åˆ°éšæœºåŒ–æ–¹æ³•
   - ä»çŸ©é˜µåˆ°é«˜é˜¶å¼ é‡
   - ä»ç²¾ç¡®è®¡ç®—åˆ°è¿‘ä¼¼ç®—æ³•

3. **åº”ç”¨å»¶ç»­**
   - Tensor Train (2023): é«˜é˜¶å¼ é‡çš„å¦ä¸€ç§åˆ†è§£
   - tCURLoRA (2025): å¼ é‡æ–¹æ³•åœ¨LLMå¾®è°ƒä¸­çš„åº”ç”¨
   - GO-LDA (2023): é™ç»´æŠ€æœ¯çš„åº”ç”¨

### å½±å“åŠ›ä¸å¼•ç”¨

è¯¥å·¥ä½œåœ¨ä»¥ä¸‹é¢†åŸŸè¢«å¼•ç”¨ï¼š
- å¤§è§„æ¨¡ç§‘å­¦è®¡ç®—
- æ¨èç³»ç»Ÿ
- å¼ é‡è¡¥å…¨
- æœºå™¨å­¦ä¹ ç†è®º

---

## é™„å½•ï¼šå…³é”®å…¬å¼é€ŸæŸ¥

```
Tuckeråˆ†è§£:
  X â‰ˆ G Ã—â‚ Uâ½Â¹â¾ Ã—â‚‚ Uâ½Â²â¾ Ã— ... Ã—â‚™ Uâ½â¿â¾

HOSVD:
  Uâ½áµâ¾ = SVD(Xâ‚â‚–â‚)[:,:áµ£â‚–]
  G = X Ã—â‚ Uâ½Â¹â¾áµ€ Ã—â‚‚ ... Ã—â‚™ Uâ½â¿â¾áµ€

Sketching:
  Sâ‚– = X Ã—â‚– Î©â½áµâ¾áµ€  (Î©â½áµâ¾ âˆˆ â„^{nâ‚–Ã—â„“â‚–})
  Å¨â½áµâ¾ = orth(SVD(Sâ‚â‚–â‚)[:,:áµ£â‚–])

è¯¯å·®ç•Œ:
  â€–X - XÌƒâ€– â‰¤ (1+Îµ)â€–X - X*â€–

Sketchingç»´åº¦:
  â„“â‚– â‰¥ CÂ·(râ‚– + log(1/Î´))/ÎµÂ²
```

---

**ç¬”è®°ç”Ÿæˆæ—¶é—´**: 2026-02-20
**ç²¾è¯»æ·±åº¦**: â˜…â˜…â˜…â˜…â˜… (äº”çº§ç²¾è¯»)
**æ¨èæŒ‡æ•°**: â˜…â˜…â˜…â˜…â˜† (å¼ é‡è®¡ç®—é¢†åŸŸé‡è¦è´¡çŒ®)
**åˆ›æ–°æ€§**: â˜…â˜…â˜…â˜…â˜† (Sketchingæ–¹æ³•çš„é‡è¦æ‰©å±•)
