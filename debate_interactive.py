"""
å¢å¼ºè¾©è®ºäº¤äº’ç³»ç»Ÿ - Agentä¸»åŠ¨è´¨ç–‘ä¸åé©³
Enhanced Debate Interaction System

æ ¸å¿ƒå¢å¼ºï¼š
1. Agentå¯ä»¥ä¸»åŠ¨å‘èµ·è´¨ç–‘ï¼ˆChallengeï¼‰
2. Agentå¯ä»¥é’ˆå¯¹è´¨ç–‘è¿›è¡Œåé©³ï¼ˆRebuttalï¼‰
3. æ™ºèƒ½å†³ç­–ï¼šä½•æ—¶è´¨ç–‘ã€è´¨ç–‘è°ã€è´¨ç–‘ä»€ä¹ˆ
4. è¾©è®ºæ”¶æ•›ï¼šå…±è¯†æ£€æµ‹ + æœ€å¤§è½®æ¬¡
"""

import asyncio
import json
import re
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

from debate_system import (
    AgentRole,
    AgentMessage,
    DebateStatus,
    DebateState,
    LLMClient,
    MockLLMClient,
    Paper,
    BaseAgent,
    AgentConfig,
    DebateConfig,
    load_config,
    AgentFactory,
    SynthesizerAgent,
)


class InteractionType(Enum):
    ANALYSIS = "analysis"
    CHALLENGE = "challenge"
    REBUTTAL = "rebuttal"
    CLARIFICATION = "clarification"
    AGREEMENT = "agreement"
    SYNTHESIS = "synthesis"
    COMMENT = "comment"


class ConsensusLevel(Enum):
    FULL_AGREEMENT = "full_agreement"
    PARTIAL_AGREEMENT = "partial_agreement"
    DISAGREEMENT = "disagreement"
    UNRESOLVED = "unresolved"


@dataclass
class InteractionMessage(AgentMessage):
    interaction_type: InteractionType = InteractionType.ANALYSIS
    target_agent: Optional[str] = None
    target_message_id: Optional[str] = None
    challenge_points: List[str] = field(default_factory=list)
    consensus_level: Optional[ConsensusLevel] = None


@dataclass
class AgentOpinion:
    agent_role: str
    stance: str
    confidence: float
    key_points: List[str]
    concerns: List[str]


@dataclass
class DebateTopic:
    topic_id: str
    description: str
    raised_by: str
    round_raised: int
    status: str = "active"
    opinions: List[AgentOpinion] = field(default_factory=list)
    consensus_reached: bool = False


@dataclass
class InteractiveDebateState(DebateState):
    topics: List[DebateTopic] = field(default_factory=list)
    pending_challenges: List[InteractionMessage] = field(default_factory=list)
    agent_opinions: Dict[str, AgentOpinion] = field(default_factory=dict)
    consensus_matrix: Dict[Tuple[str, str], float] = field(default_factory=dict)


class AgentDecisionEngine:
    """Agentå†³ç­–å¼•æ“ - å†³å®šæ˜¯å¦è´¨ç–‘ã€è´¨ç–‘è°ã€è´¨ç–‘ä»€ä¹ˆ"""

    CHALLENGE_KEYWORDS = {
        AgentRole.SKEPTIC: ["é—®é¢˜", "ç–‘è™‘", "ä¸è¶³", "ç¼ºä¹", "éœ€è¦éªŒè¯", "å‡è®¾ä¸æˆç«‹"],
        AgentRole.MATHEMATICIAN: ["å…¬å¼é”™è¯¯", "æ¨å¯¼ä¸ä¸¥è°¨", "ç¼ºå°‘è¯æ˜", "è¾¹ç•Œæ¡ä»¶"],
        AgentRole.ENGINEER: ["éš¾ä»¥å®ç°", "å¤æ‚åº¦è¿‡é«˜", "ä¸ç¨³å®š", "ä¸å¯å¤ç°"],
        AgentRole.APPLICATION_EXPERT: ["ä¸å®ç”¨", "æˆæœ¬è¿‡é«˜", "åœºæ™¯å—é™", "éš¾ä»¥è½åœ°"],
    }

    def __init__(self, agent: BaseAgent, llm_client: LLMClient):
        self.agent = agent
        self.llm_client = llm_client

    async def should_challenge(
        self, message: InteractionMessage, context: List[InteractionMessage]
    ) -> Tuple[bool, float]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è´¨ç–‘æŸæ¡æ¶ˆæ¯"""
        if message.agent_role == self.agent.role.value:
            return False, 0.0

        if message.interaction_type == InteractionType.CHALLENGE:
            target_is_me = message.target_agent == self.agent.role.value
            if not target_is_me:
                return False, 0.0

        challenge_score = await self._calculate_challenge_score(message, context)
        threshold = self._get_challenge_threshold()

        return challenge_score > threshold, challenge_score

    async def _calculate_challenge_score(
        self, message: InteractionMessage, context: List[InteractionMessage]
    ) -> float:
        """è®¡ç®—è´¨ç–‘åˆ†æ•°"""
        prompt = f"""åˆ†æä»¥ä¸‹å‘è¨€ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ä»{self.agent.name}çš„è§’åº¦æå‡ºè´¨ç–‘ã€‚

å‘è¨€è€…: {message.agent_name}
å†…å®¹: {message.content}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "need_challenge": true/false,
    "confidence": 0.0-1.0,
    "reason": "åŸå› è¯´æ˜",
    "challenge_points": ["è´¨ç–‘ç‚¹1", "è´¨ç–‘ç‚¹2"]
}}"""

        try:
            response = await self.llm_client.generate(
                prompt=prompt,
                system_prompt=self.agent.system_prompt,
                temperature=0.3,
                max_tokens=500,
            )
            result = self._parse_json_response(response)
            if result:
                return (
                    result.get("confidence", 0.0)
                    if result.get("need_challenge")
                    else 0.0
                )
        except:
            pass

        keywords = self.CHALLENGE_KEYWORDS.get(self.agent.role, [])
        keyword_score = sum(0.1 for kw in keywords if kw in message.content)
        return min(1.0, keyword_score)

    def _get_challenge_threshold(self) -> float:
        """è·å–è´¨ç–‘é˜ˆå€¼"""
        thresholds = {
            AgentRole.SKEPTIC: 0.3,
            AgentRole.MATHEMATICIAN: 0.5,
            AgentRole.ENGINEER: 0.5,
            AgentRole.APPLICATION_EXPERT: 0.5,
        }
        return thresholds.get(self.agent.role, 0.6)

    async def select_challenge_target(
        self, messages: List[InteractionMessage]
    ) -> Optional[InteractionMessage]:
        """é€‰æ‹©è¦è´¨ç–‘çš„ç›®æ ‡æ¶ˆæ¯"""
        candidates = []

        for msg in messages:
            if msg.agent_role == self.agent.role.value:
                continue
            if msg.interaction_type == InteractionType.REBUTTAL:
                continue

            should, score = await self.should_challenge(msg, messages)
            if should:
                candidates.append((msg, score))

        if not candidates:
            return None

        candidates.sort(key=lambda x: x[1], reverse=True)
        return candidates[0][0]

    async def generate_challenge(
        self, target_message: InteractionMessage, context: List[InteractionMessage]
    ) -> Tuple[str, List[str]]:
        """ç”Ÿæˆè´¨ç–‘å†…å®¹"""
        prompt = f"""ä½œä¸º{self.agent.name}ï¼Œè¯·å¯¹ä»¥ä¸‹è§‚ç‚¹æå‡ºè´¨ç–‘ã€‚

åŸå‘è¨€è€…: {target_message.agent_name}
åŸå†…å®¹: {target_message.agent_name}è¯´ï¼š{target_message.content}

è¾©è®ºèƒŒæ™¯:
{self._format_context(context[-5:])}

è¯·æå‡ºä½ çš„è´¨ç–‘ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
1. æ˜ç¡®æŒ‡å‡ºä½ è´¨ç–‘çš„å…·ä½“è§‚ç‚¹
2. è¯´æ˜è´¨ç–‘çš„ç†ç”±
3. æå‡ºä½ è®¤ä¸ºæ­£ç¡®æˆ–éœ€è¦è¡¥å……çš„å†…å®¹

ç›´æ¥è¾“å‡ºè´¨ç–‘å†…å®¹ï¼Œä¸è¦åŠ å‰ç¼€ã€‚"""

        response = await self.llm_client.generate(
            prompt=prompt,
            system_prompt=self.agent.system_prompt,
            temperature=self.agent.config.temperature,
            max_tokens=1500,
        )

        challenge_points = self._extract_key_points(response)
        return response, challenge_points

    async def generate_rebuttal(
        self, challenge: InteractionMessage, context: List[InteractionMessage]
    ) -> str:
        """ç”Ÿæˆåé©³"""
        prompt = f"""ä½œä¸º{self.agent.name}ï¼Œè¯·å›åº”é’ˆå¯¹ä½ è§‚ç‚¹çš„è´¨ç–‘ã€‚

è´¨ç–‘è€…: {challenge.agent_name}
è´¨ç–‘å†…å®¹: {challenge.content}

è´¨ç–‘çš„å…·ä½“ç‚¹:
{chr(10).join(f"- {p}" for p in challenge.challenge_points)}

è¾©è®ºèƒŒæ™¯:
{self._format_context(context[-5:])}

è¯·è¿›è¡Œåé©³ï¼š
1. é’ˆå¯¹æ¯ä¸ªè´¨ç–‘ç‚¹é€ä¸€å›åº”
2. æä¾›è¯æ®æˆ–è®ºè¯æ”¯æŒä½ çš„è§‚ç‚¹
3. å¦‚ç¡®å®å­˜åœ¨é—®é¢˜ï¼Œå¯ä»¥éƒ¨åˆ†æ‰¿è®¤å¹¶æå‡ºæ”¹è¿›

ç›´æ¥è¾“å‡ºåé©³å†…å®¹ã€‚"""

        return await self.llm_client.generate(
            prompt=prompt,
            system_prompt=self.agent.system_prompt,
            temperature=self.agent.config.temperature,
            max_tokens=1500,
        )

    def _format_context(self, messages: List[InteractionMessage]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        lines = []
        for msg in messages:
            lines.append(f"[{msg.agent_name}]: {msg.content[:200]}...")
        return "\n".join(lines)

    def _extract_key_points(self, text: str) -> List[str]:
        """æå–å…³é”®ç‚¹"""
        points = []
        patterns = [
            r"\d+\.\s*(.+?)(?=\d+\.|$)",
            r"[-â€¢]\s*(.+?)(?=[-â€¢]|$)",
        ]
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            points.extend([m.strip() for m in matches if m.strip()])

        if not points:
            sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)
            points = [s.strip() for s in sentences if len(s.strip()) > 10][:3]

        return points[:5]

    def _parse_json_response(self, text: str) -> Optional[Dict]:
        """è§£æJSONå“åº”"""
        try:
            json_match = re.search(r"\{[\s\S]*\}", text)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        return None


class ConsensusDetector:
    """å…±è¯†æ£€æµ‹å™¨"""

    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client

    async def check_consensus(
        self, state: InteractiveDebateState
    ) -> Tuple[float, bool]:
        """æ£€æµ‹æ˜¯å¦è¾¾æˆå…±è¯†"""
        if len(state.messages) < 4:
            return 0.0, False

        recent_messages = (
            state.messages[-6:] if len(state.messages) >= 6 else state.messages
        )

        prompt = f"""åˆ†æä»¥ä¸‹è¾©è®ºè®°å½•ï¼Œåˆ¤æ–­æ˜¯å¦å·²è¾¾æˆå…±è¯†ã€‚

è¾©è®ºè®°å½•:
{self._format_debate(recent_messages)}

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "consensus_score": 0.0-1.0,
    "consensus_reached": true/false,
    "remaining_issues": ["æœªè§£å†³é—®é¢˜1", "æœªè§£å†³é—®é¢˜2"],
    "agreed_points": ["å…±è¯†ç‚¹1", "å…±è¯†ç‚¹2"]
}}"""

        try:
            response = await self.llm_client.generate(
                prompt=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªè¾©è®ºå…±è¯†åˆ†æä¸“å®¶ï¼Œå®¢è§‚åˆ¤æ–­è¾©è®ºæ˜¯å¦å·²ç»å……åˆ†å¹¶è¾¾æˆå…±è¯†ã€‚",
                temperature=0.2,
                max_tokens=500,
            )
            result = self._parse_json(response)
            if result:
                score = result.get("consensus_score", 0.0)
                reached = result.get("consensus_reached", False)
                state.unresolved_issues = result.get("remaining_issues", [])
                return score, reached
        except:
            pass

        return self._heuristic_consensus(state)

    def _heuristic_consensus(self, state: InteractiveDebateState) -> Tuple[float, bool]:
        """å¯å‘å¼å…±è¯†æ£€æµ‹"""
        challenge_count = sum(
            1
            for m in state.messages
            if isinstance(m, InteractionMessage)
            and m.interaction_type == InteractionType.CHALLENGE
        )
        rebuttal_count = sum(
            1
            for m in state.messages
            if isinstance(m, InteractionMessage)
            and m.interaction_type == InteractionType.REBUTTAL
        )

        if challenge_count == 0:
            return 0.8, True

        resolution_rate = (
            rebuttal_count / challenge_count if challenge_count > 0 else 1.0
        )
        score = min(1.0, resolution_rate * 0.8)

        return score, score > 0.7

    def _format_debate(self, messages: List[AgentMessage]) -> str:
        lines = []
        for msg in messages:
            lines.append(f"[{msg.agent_name}]: {msg.content[:150]}...")
        return "\n".join(lines)

    def _parse_json(self, text: str) -> Optional[Dict]:
        try:
            json_match = re.search(r"\{[\s\S]*\}", text)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        return None


class InteractiveDebateScheduler:
    """äº¤äº’å¼è¾©è®ºè°ƒåº¦å™¨"""

    def __init__(
        self,
        agents: List[BaseAgent],
        config: DebateConfig,
        llm_client: LLMClient = None,
    ):
        self.agents = [a for a in agents if not isinstance(a, SynthesizerAgent)]
        self.synthesizer = next(
            (a for a in agents if isinstance(a, SynthesizerAgent)), None
        )
        self.config = config
        self.llm_client = llm_client or MockLLMClient()
        self.state: Optional[InteractiveDebateState] = None

        self.decision_engines: Dict[str, AgentDecisionEngine] = {}
        for agent in self.agents:
            self.decision_engines[agent.role.value] = AgentDecisionEngine(
                agent, self.llm_client
            )

        self.consensus_detector = ConsensusDetector(self.llm_client)
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    async def start_debate(self, paper: Paper) -> InteractiveDebateState:
        """å¯åŠ¨è¾©è®º"""
        self.state = InteractiveDebateState(
            paper=paper,
            messages=[],
            current_round=0,
            status=DebateStatus.INITIALIZED,
            consensus_score=0.0,
            unresolved_issues=[],
            max_rounds=self.config.max_rounds,
        )

        self._print_header(paper)

        await self._round_initial_analysis()

        while self._should_continue():
            await self._round_challenge_rebuttal()

            (
                consensus_score,
                consensus_reached,
            ) = await self.consensus_detector.check_consensus(self.state)
            self.state.consensus_score = consensus_score

            if consensus_reached:
                print(f"\n[å…±è¯†] å·²è¾¾æˆå…±è¯†ï¼å…±è¯†åº¦: {consensus_score:.2f}")
                break

        await self._generate_final_report()

        return self.state

    def _print_header(self, paper: Paper):
        print(f"\n{'=' * 60}")
        print(f"äº¤äº’å¼è¾©è®ºç³»ç»Ÿï¼š{paper.title}")
        print(f"å‚ä¸Agent: {', '.join([a.name for a in self.agents])}")
        print(f"è¾©è®ºæ¨¡å¼: ä¸»åŠ¨è´¨ç–‘ + åé©³å›åº”")
        print(f"{'=' * 60}\n")

    async def _round_initial_analysis(self):
        """ç¬¬ä¸€è½®ï¼šåˆå§‹åˆ†æ"""
        self.state.current_round = 1
        self.state.status = DebateStatus.IN_PROGRESS

        print(f"\n{'â”€' * 50}")
        print(f"ç¬¬1è½®ï¼šåˆå§‹è§‚ç‚¹é™ˆè¿°")
        print(f"{'â”€' * 50}\n")

        for agent in self.agents:
            print(f"[{agent.name}] æ­£åœ¨åˆ†æ...")
            content = await agent.analyze(self.state.paper, self.state.messages)

            message = InteractionMessage(
                agent_role=agent.role.value,
                agent_name=agent.name,
                content=content,
                timestamp=datetime.now(),
                round=1,
                message_id=str(uuid4()),
                interaction_type=InteractionType.ANALYSIS,
            )

            self.state.add_message(message)
            self._print_message(message)

    async def _round_challenge_rebuttal(self):
        """åç»­è½®æ¬¡ï¼šè´¨ç–‘ä¸åé©³"""
        self.state.current_round += 1

        print(f"\n{'â”€' * 50}")
        print(f"ç¬¬{self.state.current_round}è½®ï¼šè´¨ç–‘ä¸åé©³")
        print(f"{'â”€' * 50}\n")

        challenges = await self._collect_challenges()

        for challenge in challenges:
            self.state.add_message(challenge)
            self.state.pending_challenges.append(challenge)
            self._print_message(challenge)

            rebuttal = await self._generate_rebuttal_for_challenge(challenge)
            if rebuttal:
                self.state.add_message(rebuttal)
                self._print_message(rebuttal)

        await self._allow_follow_up_comments()

    async def _collect_challenges(self) -> List[InteractionMessage]:
        """æ”¶é›†æ‰€æœ‰Agentçš„è´¨ç–‘"""
        challenges = []

        for agent in self.agents:
            if agent.role == AgentRole.SYNTHESIZER:
                continue

            engine = self.decision_engines[agent.role.value]

            target = await engine.select_challenge_target(
                [m for m in self.state.messages if isinstance(m, InteractionMessage)]
            )

            if target:
                content, challenge_points = await engine.generate_challenge(
                    target,
                    [
                        m
                        for m in self.state.messages
                        if isinstance(m, InteractionMessage)
                    ],
                )

                challenge = InteractionMessage(
                    agent_role=agent.role.value,
                    agent_name=agent.name,
                    content=content,
                    timestamp=datetime.now(),
                    round=self.state.current_round,
                    message_id=str(uuid4()),
                    interaction_type=InteractionType.CHALLENGE,
                    target_agent=target.agent_role,
                    target_message_id=target.message_id,
                    challenge_points=challenge_points,
                )
                challenges.append(challenge)

        return challenges

    async def _generate_rebuttal_for_challenge(
        self, challenge: InteractionMessage
    ) -> Optional[InteractionMessage]:
        """ä¸ºè´¨ç–‘ç”Ÿæˆåé©³"""
        target_agent = self._get_agent_by_role(challenge.target_agent)
        if not target_agent:
            return None

        print(f"\n[{target_agent.name}] æ­£åœ¨å‡†å¤‡åé©³...")

        engine = self.decision_engines[target_agent.role.value]
        content = await engine.generate_rebuttal(
            challenge,
            [m for m in self.state.messages if isinstance(m, InteractionMessage)],
        )

        rebuttal = InteractionMessage(
            agent_role=target_agent.role.value,
            agent_name=target_agent.name,
            content=content,
            timestamp=datetime.now(),
            round=self.state.current_round,
            message_id=str(uuid4()),
            interaction_type=InteractionType.REBUTTAL,
            target_agent=challenge.agent_role,
            target_message_id=challenge.message_id,
        )

        if challenge in self.state.pending_challenges:
            self.state.pending_challenges.remove(challenge)

        return rebuttal

    async def _allow_follow_up_comments(self):
        """å…è®¸è¡¥å……å‘è¨€"""
        agents_spoken = set()
        for msg in self.state.get_messages_by_round(self.state.current_round):
            agents_spoken.add(msg.agent_role)

        for agent in self.agents:
            if agent.role.value not in agents_spoken:
                if await self._should_comment(agent):
                    print(f"\n[{agent.name}] è¡¥å……å‘è¨€...")
                    content = await agent.analyze(
                        self.state.paper, self.state.messages[-3:]
                    )

                    message = InteractionMessage(
                        agent_role=agent.role.value,
                        agent_name=agent.name,
                        content=content,
                        timestamp=datetime.now(),
                        round=self.state.current_round,
                        message_id=str(uuid4()),
                        interaction_type=InteractionType.COMMENT,
                    )

                    self.state.add_message(message)
                    self._print_message(message)

    async def _should_comment(self, agent: BaseAgent) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¡¥å……å‘è¨€"""
        if self.state.current_round < 2:
            return False

        recent_challenges = [
            m
            for m in self.state.messages[-4:]
            if isinstance(m, InteractionMessage)
            and m.interaction_type == InteractionType.CHALLENGE
        ]

        return len(recent_challenges) > 0 and agent.role == AgentRole.SKEPTIC

    def _should_continue(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­è¾©è®º"""
        if self.state.current_round >= self.state.max_rounds:
            print(f"\n[ç»ˆæ­¢] è¾¾åˆ°æœ€å¤§è½®æ¬¡ ({self.config.max_rounds}è½®)")
            return False

        if len(self.state.pending_challenges) == 0 and self.state.current_round >= 2:
            recent_activity = len(
                self.state.get_messages_by_round(self.state.current_round)
            )
            if recent_activity == 0:
                return False

        return True

    def _get_agent_by_role(self, role: str) -> Optional[BaseAgent]:
        """æ ¹æ®è§’è‰²è·å–Agent"""
        for agent in self.agents:
            if agent.role.value == role:
                return agent
        return None

    def _print_message(self, message: InteractionMessage):
        """æ‰“å°æ¶ˆæ¯"""
        type_icons = {
            InteractionType.ANALYSIS: "[åˆ†æ]",
            InteractionType.CHALLENGE: "[è´¨ç–‘]",
            InteractionType.REBUTTAL: "[åé©³]",
            InteractionType.COMMENT: "[è¯„è®º]",
            InteractionType.AGREEMENT: "[åŒæ„]",
        }
        icon = type_icons.get(message.interaction_type, "ğŸ“Œ")

        preview = message.content[:150]
        if len(message.content) > 150:
            preview += "..."

        if message.interaction_type == InteractionType.CHALLENGE:
            print(
                f"{icon} [{message.agent_name}] â†’ @{message.target_agent}: {preview}\n"
            )
        elif message.interaction_type == InteractionType.REBUTTAL:
            print(
                f"{icon} [{message.agent_name}] å›åº” @{message.target_agent}: {preview}\n"
            )
        else:
            print(f"{icon} [{message.agent_name}]: {preview}\n")

    async def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print(f"\n{'=' * 60}")
        print("ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
        print(f"{'=' * 60}\n")

        self.state.status = DebateStatus.CONSENSUS

        stats = self._calculate_statistics()
        self._save_outputs(stats)

        print(f"è¾©è®ºç»Ÿè®¡:")
        print(f"  - æ€»è½®æ¬¡: {self.state.current_round}")
        print(f"  - æ€»å‘è¨€æ•°: {len(self.state.messages)}")
        print(f"  - è´¨ç–‘æ•°: {stats['challenge_count']}")
        print(f"  - åé©³æ•°: {stats['rebuttal_count']}")
        print(f"  - å…±è¯†åº¦: {self.state.consensus_score:.2f}")

    def _calculate_statistics(self) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        messages = [m for m in self.state.messages if isinstance(m, InteractionMessage)]

        return {
            "challenge_count": sum(
                1 for m in messages if m.interaction_type == InteractionType.CHALLENGE
            ),
            "rebuttal_count": sum(
                1 for m in messages if m.interaction_type == InteractionType.REBUTTAL
            ),
            "analysis_count": sum(
                1 for m in messages if m.interaction_type == InteractionType.ANALYSIS
            ),
            "messages_by_role": {
                agent.role.value: len(
                    [m for m in messages if m.agent_role == agent.role.value]
                )
                for agent in self.agents
            },
        }

    def _save_outputs(self, stats: Dict):
        """ä¿å­˜è¾“å‡º"""
        import json

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        json_path = self.output_dir / f"interactive_debate_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "paper": self.state.paper.title,
                    "rounds": self.state.current_round,
                    "consensus_score": self.state.consensus_score,
                    "statistics": stats,
                    "messages": [
                        {
                            "round": m.round,
                            "agent": m.agent_name,
                            "type": m.interaction_type.value
                            if isinstance(m, InteractionMessage)
                            else "unknown",
                            "content": m.content,
                        }
                        for m in self.state.messages
                    ],
                },
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"\næŠ¥å‘Šå·²ä¿å­˜: {json_path}")


class GLM5Client(LLMClient):
    """GLM-5 APIå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key or os.getenv("GLM_API_KEY") or os.getenv("ZHIPU_API_KEY")
        self.base_url = base_url or os.getenv(
            "GLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4"
        )
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½®GLM_API_KEYæˆ–ZHIPU_API_KEYç¯å¢ƒå˜é‡")

    async def generate(
        self,
        prompt: str,
        system_prompt: str = "",
        temperature: float = 0.5,
        max_tokens: int = 4000,
    ) -> str:
        import asyncio

        try:
            import httpx

            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }

            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": "glm-4-plus",
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
            }

            for attempt in range(3):
                try:
                    await asyncio.sleep(2)
                    async with httpx.AsyncClient(timeout=120.0) as client:
                        response = await client.post(
                            f"{self.base_url}/chat/completions",
                            headers=headers,
                            json=payload,
                        )
                        if response.status_code == 429:
                            wait_time = 10 * (attempt + 1)
                            print(f"[APIé™æµ] ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                            await asyncio.sleep(wait_time)
                            continue
                        response.raise_for_status()
                        result = response.json()
                        return result["choices"][0]["message"]["content"]
                except httpx.HTTPStatusError as e:
                    if e.response.status_code == 429 and attempt < 2:
                        continue
                    raise

        except ImportError:
            raise ImportError("è¯·å®‰è£… httpx: pip install httpx")
        except Exception as e:
            raise RuntimeError(f"GLM-5 APIè°ƒç”¨å¤±è´¥: {e}")


import os


async def run_interactive_debate(
    paper_content: str,
    paper_title: str = "æµ‹è¯•è®ºæ–‡",
    max_rounds: int = 3,
    llm_type: str = "mock",
    api_key: str = None,
):
    """è¿è¡Œäº¤äº’å¼è¾©è®º"""
    config = load_config()
    config.max_rounds = max_rounds

    if llm_type == "mock":
        llm_client = MockLLMClient()
    elif llm_type == "glm":
        llm_client = GLM5Client(api_key=api_key)
    elif llm_type == "claude":
        from debate_system import ClaudeLLMClient

        llm_client = ClaudeLLMClient()
    else:
        llm_client = MockLLMClient()

    agents = AgentFactory.create_all_agents(llm_client, config)
    scheduler = InteractiveDebateScheduler(agents, config, llm_client)

    paper = Paper(
        title=paper_title,
        authors=["å¾…è¡¥å……"],
        year=2024,
        abstract="",
        content=paper_content,
    )

    return await scheduler.start_debate(paper)


async def main():
    """ç¤ºä¾‹è¿è¡Œ"""
    sample_paper = """
# åŸºäºå˜åˆ†æ–¹æ³•çš„å›¾åƒåˆ†å‰²ç®—æ³•ç ”ç©¶

## æ‘˜è¦
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå˜åˆ†ä¼˜åŒ–çš„å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œç»“åˆäº†å…¨å˜åˆ†(TV)æ­£åˆ™åŒ–å’Œæ°´å¹³é›†æ–¹æ³•ã€‚

## æ–¹æ³•
1. èƒ½é‡å‡½æ•°å®šä¹‰ï¼š
   E(u) = âˆ«|âˆ‡u| dx + Î»âˆ«(u-f)Â² dx

2. ä¼˜åŒ–ç­–ç•¥ï¼šé‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£

3. å®éªŒéªŒè¯ï¼šåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¾¾åˆ°92%çš„åˆ†å‰²ç²¾åº¦

## ç»“è®º
è¯¥æ–¹æ³•åœ¨å¤šç±»åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰è‰¯å¥½çš„ç†è®ºä¿è¯ã€‚
"""

    await run_interactive_debate(sample_paper, "å˜åˆ†å›¾åƒåˆ†å‰²æ–¹æ³•", max_rounds=3)


if __name__ == "__main__":
    asyncio.run(main())
