Journal of Scientiﬁc Computing (2024) 100:81
https://doi.org/10.1007/s10915-024-02644-9
An Efﬁcient and Versatile Variational Method for
High-Dimensional Data Classiﬁcation
Xiaohao Cai 1 · Raymond H. Chan 2 · Xiaoyu Xie 3 · Tieyong Zeng 4
Received: 16 October 2023 / Revised: 6 June 2024 / Accepted: 24 July 2024 /
Published online: 1 August 2024
© The Author(s) 2024
Abstract
High-dimensional data classiﬁcation is a fundamental task in machine learning and imaging
science. In this paper, we propose an efﬁcient and versatile multi-class semi-supervised
classiﬁcation method for classifying high-dimensional data and unstructured point clouds.
To begin with, a warm initialization is generated by using a fuzzy classiﬁcation method such
as the standard support vector machine or random labeling. Then an unconstraint convex
variational model is proposed to purify and smooth the initialization, followed by a step which
is to project the smoothed partition obtained previously to a binary partition. These steps can
be repeated, with the latest result as a new initialization, to keep improving the classiﬁcation
quality. We show that the convex model of the smoothing step has a unique solution and can
be solved by a speciﬁcally designed primal–dual algorithm whose convergence is guaranteed.
We test our method and compare it with the state-of-the-art methods on several benchmark
data sets. Thorough experimental results demonstrate that our method is superior in both the
classiﬁcation accuracy and computation speed for high-dimensional data and point clouds.
Keywords Semi-supervised clustering · Point cloud classiﬁcation · V ariational methods·
Graph Laplacian
B Xiaohao Cai
x.cai@soton.ac.uk
B Raymond H. Chan
raymond.chan@ln.edu.hk
B Xiaoyu Xie
Xiaoyu_Xie@Brown.edu
B Tieyong Zeng
zeng@math.cuhk.edu.hk
1 School of Electronics and Computer Science, University of Southampton, Southampton SO17 1BJ,
UK
2 Department of Operations and Risk Management and School of Data Science, Lingnan University,
Tuen Mun, Hong Kong
3 Applied Mathematics, Brown University, Providence, RI 02912, USA
4 Department of Mathematics, The Chinese University of Hong Kong, Shatin, Hong Kong
123
81 Page 2 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
1 Introduction
Data classiﬁcation is a fundamental task in remote sensing, machine learning, computer
vision, and imaging science [ 1–5]. The task, simply speaking, is to group the given data
into different classes such that, on one hand, data points within the same class share similar
characteristics (e.g. distance, edges, intensities, colors, and textures); on the other hand,
pairs of different classes are as dissimilar as possible with respect to certain features. In this
paper, we focus on the task of multi-class semi-supervised classiﬁcation. The total number
of classes K of the given data sets is assumed to be known, and a few samples, namely the
training points, in each class, have been labeled. The goal is therefore to infer the labels of
the remaining data points using the knowledge of the labeled ones.
For data classiﬁcation, previous methods are generally based on graphical models, see
e.g. [1, 2, 6] and references therein. In a weighted graph, the data points are vertices and the
edge weights signify the afﬁnity or similarity between pairs of data points, where the larger
the edge weight is, the closer or more similar the two vertices are. The basic assumption for
data classiﬁcation is that vertices in the graph that are connected by edges with large weight
should belong to the same class. Since a fully connected graph is dense and has the size as
large as the square of the number of vertices, it is computationally expensive to work on
it directly. In order to circumvent this, some cleverly designed approximations have been
developed. For example, spectral approaches are proposed in [ 7, 8] to efﬁciently calculate
the eigendecomposition of a dense graph Laplacian. In [ 9, 10], the nearest neighbor strategy
was adopted to build up a sparse graph where most of its entries are zero, and therefore is
computationally efﬁcient.
In the literature, various studies for semi-supervised classiﬁcation have been performed by
computing a local minimizer of some non-convex energy functional or minimizing a relevant
convex relaxation. To name just a few, we have the diffuse interface approaches using phase
ﬁeld representation based on partial differential equation techniques [ 11, 12], the MBO
scheme established for solving the diffusion equation [ 7, 8, 13], and the variational methods
b a s e do ng r a p hc u t[1, 2]. In particular, the convex relaxation models and special constraints on
class sizes were investigated in [ 1]. In [2], some novelty region-force terms were introduced
in the variational models to enforce the afﬁnity between vertices and training samples. To the
best of our knowledge, all these proposed variational models have the so-called no vacuum
and overlap constraint on the labeling functions, which gives rise to non-convex models with
NP-hard issues. By allowing labeling functions to take values in the unit simplex, the original
NP-hard combinatorial problem is rephrased into a continuous setting, see e.g. [ 1–4, 14–17]
for various continuous relaxation techniques (e.g. the ones based on solving the eigenvalue
problem, convex approximation, or non-linear optimization) and references therein.
Image segmentation can also be viewed as a special case of the data classiﬁcation problem
[4, 18], since the pixels in an image can be treated as individual points. V arious studies and
many algorithms have been considered for image segmentation. In particular, variational
methods are among the most successful image segmentation techniques, see e.g. [ 19–25].
The Mumford–Shah model [19], one of the most important variational segmentation models,
was proposed to ﬁnd piecewise smooth representations of different segments. It is, however,
difﬁcult to solve since the model is non-convex and non-smooth. Then substantially rich
follow-up works were conducted, and many of them considered compromise techniques
such as: (i) simplifying the original complex model, e.g. ﬁnding piecewise constant solutions
instead of piecewise smooth solutions [26–28]); (ii) performing convex approximations, e.g.
using convex regularization terms like total variation [29, 30]; or (iii) using the smoothing and
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 3 of 25 81
thresholding (SaT) segmentation methodology [17, 22, 31–33]; for more details please refer
to e.g. [ 34, 34–41] and references therein. Moreover, various applications were put forward
for instance in optical ﬂow [ 42], tomographic imaging [ 43], and medical imaging [ 44–49].
In this paper, we propose a semi-supervised data classiﬁcation method inspired by the SaT
segmentation methodology [17, 22, 31–33]. The SaT methodology has been shown to be very
promising in terms of segmentation quality and computation speed for images corrupted by
many different types of blurs and noises. Brieﬂy speaking, the SaT methodology includes
two main steps: the ﬁrst step is to obtain a smooth approximation of the given image through
minimizing some convex models; and the second step is to get the segmentation results by
thresholding the smooth approximation, e.g. using thresholds determined by the K-means
algorithm [50]. Since the models used are convex, the non-convex and NP-hard issues in many
existing variational segmentation methods (e.g. the Mumford–Shah model and its piecewise
constant versions mentioned above) are naturally avoided.
Our proposed data classiﬁcation method mainly contains two steps with a warm initial-
ization. The warm initialization is a fuzzy classiﬁcation result which can be generated by
any standard classiﬁcation method such as the support vector machine (SVM) [ 51], or by
labeling the given data randomly if no proper method is available for the given data (e.g. the
data set is too large or complicated). Its accuracy is not critical since our proposed method
will improve the accuracy signiﬁcantly from this starting point.
With the warm initialization, the ﬁrst step which is also the key point of our method is to
ﬁnd a set of smooth labeling functions, where each gives the probability of every point being
in a particular class. They are obtained by minimizing a properly-chosen convex objective
functional. In detail, the convex objective functional contains K independent convex sub-
minimization problems, where each corresponds to one labeling function, with no constraints
between these K labeling functions. For each sub-minimization problem, the model is formed
by three terms: (i) the data ﬁdelity term restricting the distance between the smooth labeling
function and the initialization; (ii) the graph Laplacian ( ℓ
2-norm) term, and (iii) the total
variation (ℓ1-norm) built on the graph of the given data. The graph Laplacian and the total
variation terms regularize the labeling functions to be smooth but at the same time close to
a representation on the unit simplex.
After obtaining the set of labeling functions, the second step of our method is just to project
the fuzzy classiﬁcation results obtained at step one onto the unit simplex to obtain a binary
classiﬁcation result. This step can be done straightforwardly. To improve the classiﬁcation
accuracy, these two steps can be repeated iteratively, where at each iteration the result at the
previous iteration is used as a new initialization.
The main advantage of our method is threefold. Firstly, it performs outstandingly in com-
putation speed, since the proposed model at the ﬁrst step is convex and theK sub-minimization
problems are independent of each other (with no constraint on the K labeling functions). The
parallelism strategy can thus be applied straightforwardly to improve the computation per-
formance further. On the contrary, the standard start-of-the-art variational data classiﬁcation
methods e.g. [2, 7, 12] have the constraint on unit simplex in their minimization models, and
thus the non-convex or NP-hard issues can affect seriously the efﬁciency of these methods,
even though some convex relaxations may be applied. Secondly, in addition to the multi-class
classiﬁcation problem, our method can also be used to tackle other problems like the one-
class classiﬁcation problem [52] (see Sect. 5.6) beneﬁting from its robustness in dealing with
extremely unbalanced data sets. Thirdly, our method is generally superior in classiﬁcation
accuracy, due to its ﬂexibility of merging the warm initialization and the two-step iterations
which are tractable to improve the accuracy gradually. Note again that we are solving a con-
vex model in the ﬁrst step of each iteration, which guarantees a unique global minimizer.
123
81 Page 4 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
In contrast, there is, however, no guarantee that the results obtained by the standard start-
of-the-art variational data classiﬁcation methods e.g. [ 2, 7, 12] are global minimizers. The
effectiveness of iterations in our proposed method will be shown in the experiments. For
most cases, the clustering accuracy would be increased by a signiﬁcant margin compared to
the ﬁrst initialization and generally outperforms the state-of-the-art variational classiﬁcation
methods.
The paper is organized as follows. In Sect. 2, the basic notation used throughout the paper
is introduced. Our method for data sets classiﬁcation is proposed in Sect. 3. In Sect. 4,w e
present the algorithm for solving the proposed model and its convergence proof. In Sect. 5,
we test our method on benchmark data sets and compare it with the start-of-the-art methods.
Conclusions are drawn in Sect. 6.
2 Basic Notation
Let G = (V, E,w) be a weighted undirected graph representing a given point cloud, where
V is the vertex set (in which each vertex represents a point) containing N vertices, E is the
edge set consisting of pairs of vertices, and w : E → R+ is the weight function deﬁned on
the edges in E. The weights w(x, y) on the edges (x, y) ∈ E measure the similarity between
the two vertices x and y; the larger the weight is, the more similar (e.g. closer in distance)
the pair of the vertices is.
There are many different ways to deﬁne the weight function. Let d(·,·) be a distance
metric. Several particularly popular deﬁnitions of weight functions are as follows: (i) radial
basis function
w(x, y) := exp(− d(x, y)
2/(2ξ)), ∀(x, y) ∈ E, (1)
for a preﬁxed constant ξ> 0; (ii) Zelnic-Manor and Perona weight function
w(x, y) := exp
( − d(x, y)2
var(x)var( y)
)
, ∀(x, y) ∈ E, (2)
where var(·) denotes the local variance; and (iii) the cosine similarity
w(x, y) := ⟨x, y⟩√ ⟨x, x⟩⟨ y, y⟩, ∀(x, y) ∈ E, (3)
where ⟨·,·⟩ represents the inner product.
Let W = (w(x, y))(x, y)∈E ∈ RN×N , the so-called afﬁnity matrix, which is usually
assumed to be a symmetric matrix with non-negative entries. Let a diagonal matrix be D =
(h(x, y))(x, y)∈E ∈ RN×N , where its diagonal entries are equal to the sum of the entries on
t h es a m er o wi nW , i.e.,
h(x, y) :=
{ ∑
z∈V w(x, z), x = y,
0, otherwise. (4)
Let u = (u(x))⊤
x∈V ∈ RN , i.e., an N -length column vector. Deﬁne the graph Laplacian as
L = D − W , and the gradient operator ∇ on u(x),∀x ∈ V ,a s
∇ u(x) := (w(x, y)[u(x) − u( y)])(x, y)∈E . (5)
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 5 of 25 81
The ℓ1- n o r mo fa nN -length vector is deﬁned as
∥∇ u∥1 :=
∑
x∈V
|∇ u(x)|
=
∑
(x, y)∈E
|w(x, y)[u(x) − u( y)]|. (6)
The ℓ2-norm (also known as Dirichlet energy) is deﬁned as
∥∇ u∥2
2 := 1
2 u⊤ L u
= 1
2
∑
(x, y)∈E
w(x, y)[u(x) − u( y)]2. (7)
Note, however, that working with the fully connected graph E—like the settings in Eqs. (5),
(6)a n d( 7)—can be highly computational demanding.
In order to reduce the computational burden, one often only considers the set of edges
with large weights. In this paper, thek-nearest-neighbor (k-NN) of a point x, N(x),i su s e dt o
replace the whole edge set starting from the point x in E. Besides the computational saving,
one additional beneﬁt of using k-NN graph is its capability to capture the local property of
points lying close to a manifold. With the k-NN graph, the deﬁnitions in Eqs. (5), (6)a n d(7)
become
∇ u(x) = (w(x, y)[u(x) − u( y)]) y∈N(x) , (8)
∥∇ u∥1 :=
∑
x∈V
|∇ u(x)|
=
∑
x∈V
∑
y∈N(x)
|w(x, y)[u(x) − u( y)]|, (9)
and
∥∇ u∥2
2 := 1
2 u⊤ L u
= 1
2
∑
x∈V
∑
y∈N(x)
w(x, y)[u(x) − u( y)]2, (10)
respectively, see e.g. [2, 12] for more detail.
3 Proposed Data Classiﬁcation Method
3.1 Preliminary
Given a point cloud V containing N points in RM . We aim to partition V into K classes
V1,..., VK based on their similarities (the points in the same class possess high similarity),
with a set of training points T ={ Tj }K
j=1 ⊂ V , |T|= NT . Note that Tj ⊂ Vj for j =
1,..., K . In other words, we aim to assign the points in V\T certain labels between 1 to K
using the training set T in which the labels of points are known, and the partition satisﬁes no
123
81 Page 6 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
vacuum and overlap constraint, i.e.,
V =
K⋃
j=1
Vj and Vi ∩ Vj =∅ , ∀i ̸= j, 1 ≤ i, j ≤ K. (11)
In the rest of the paper, we denote the points in V needed to be labeled as S = V\T , and call
S the test set in V .
The constraint ( 11) can be described by a binary matrix function U := (u1,..., uK ) ∈
RN×K (also called partition matrix), with u j = (u j (x))⊤
x∈V ∈ RN : V →{ 0, 1} deﬁned as
u j(x) :=
{
1, x ∈ Vj,
0, otherwise, ∀x ∈ V, j = 1,..., K. (12)
Clearly, the above deﬁnition yields ∑K
j=1 u j (x) = 1,∀x ∈ V . The constraint ( 12)i sa l s o
known as the indicator constraint on the unit simplex. Since the binary representation in the
constraint (12) generally requires solving a non-convex model with NP-hard issue, a common
strategy—the convex unit simplex—is considered as an alternative
K∑
j=1
u j(x) = 1, ∀x ∈ V,
s.t. u j (x) ∈[ 0, 1], j = 1,..., K.
(13)
Note, importantly, that the convex constraint (13) can overcome the NP-hard issue and make
some subproblems convex, but generally the whole model can still be non-convex. Therefore,
solving a model with constraint ( 11), (12)o r( 13) can be time consuming, see e.g. [ 2, 12]f o r
more detail.
If a result satisfying the constraint ( 13) is not completely binary, a common way to obtain
an approximate binary solution satisfying the constraint ( 12) is to select the binary function
as the nearest vertex in the unit simplex by the magnitude of the components, i.e.,
(u
1(x) ,..., u K (x)) ↦→ ei,
where i = argmax
j
{u j (x)}K
j=1,∀x ∈ V. (14)
Here, ei is the K -length unit normal vector, which is 1 at the i-th component and 0 for all
other components.
3.2 Proposed Method
In this section, we present our novel method for data (e.g. point clouds) classiﬁcation inspired
by the SaT strategy which has been validated very effective in image segmentation. Our
method can be summarized brieﬂy as follows: ﬁrst, a classiﬁcation result is obtained as a
warm initialization by using a classical and fast, but need not be very accurate classiﬁcation
method such as SVM [ 51]; then, a proposed two-step iteration scheme is implemented until
no change in the labels of the test points could be made between consecutive iterations.
Speciﬁcally, at the ﬁrst step, we propose to minimize a novel convex model free of constraint
(cf. those constraints ( 11), (12)a n d( 13)) to obtain a fuzzy partition, say U , while keeping
the training labels unchanged. At the second step, a binary result is obtained by just applying
the binary rule in Eq. ( 14) directly on the fuzzy partition obtained at the previous step. This
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 7 of 25 81
binary result could be the ﬁnal classiﬁcation result for the original classiﬁcation problem or,
if necessary, be set as a new initialization to search a better one in the same manner. In the
following, we give the details of each step.
Initialization. Given a point cloud V containing N points in R
M and training set T
containing NT points with correct labels, we use SVM, which is a standard and fast clus-
tering method as an example, to obtain the ﬁrst clustering. Let the partition matrix be
ˆU = (ˆu1,..., ˆuK ) ∈ RN×K ,w h e r eˆu j = (ˆu j (x))⊤
x∈V ∈ RN for j = 1,..., K .O n e
could acquire an initialization by any other methods which have better performance than
SVM. If no proper method is available (e.g. the data set is too large), then an initialization
generated by setting labels to the test points randomly can be used as an alternative.
Step one. We now put forward our convex model to ﬁnd a fuzzy partition U with
initialization ˆU = (ˆu
1,..., ˆuK ), i.e.,
argmin
U
K∑
j=1
{ β
2 ∥u j − ˆu j∥2
2 + α
2 u⊤
j L u j +∥ ∇u j ∥1
}
, (15)
where the ﬁrst term is the data ﬁdelity term constraining the fuzzy partition not far away from
the initialization; the second term is related to ∥∇ u∥2
2 with graph Laplacian L; the last term
is the total variation constructed on the graph; and α, β >0 are regularization parameters.
Speciﬁcally, the second term in model ( 15) is used to impose smooth features on the labels
of the points, and the last term is used to force the points with similar information to group
together.
It is worth emphasizing that we already have the labels on the points in the training
set T , with ¯U = (¯u
1,..., ¯uK ) ∈ RNT ×K being the partition matrix on T ,w h e r e¯u j =
(¯u j (x))⊤
x∈T ∈ RNT for j = 1,..., K . Therefore, we only assign labels to points in the test
set S, i.e., we have
ˆu j(x) =¯u j (x), ∀x ∈ T, j = 1,..., K. (16)
Let ˆuS j represent the part of ˆu j deﬁned on the test set S, and then we have
ˆu j = (ˆu⊤
S j , ¯u⊤
j )⊤, j = 1,..., K. (17)
Analogous notations are used for the partition matrix U = (u1,..., uK ), with
u j = (u⊤
S j , ¯u⊤
j )⊤, j = 1,..., K. (18)
In Sect. 4,E q s .(17)a n d(18) are going to be used to derive an efﬁcient algorithm to solve the
minimization problem (15).
The following Theorem 1 proves that our proposed model ( 15) has a unique solution.
Theorem 1 Given ˆU ∈ RN×K and α, β >0, the proposed model (15) has a unique solution
U ∈ RN×K .
Proof According to [ 53, Chapter 9], we know that a strongly convex function has a unique
minimum. The conclusion follows directly from the strong convexity of the proposed model
(15). ⊓⊔
Many algorithms can be used to solve model ( 15) efﬁciently due to the convexity of the
model without constraint. For example, the split-Bregman algorithm [ 54], which is speciﬁ-
cally devised for ℓ
1 regularized problems; the primal–dual algorithm [55], which is designed
to solve general saddle point problems; and the powerful alternative, ADMM algorithm [56].
123
81 Page 8 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
In particular, model (15) actually contains K independent sub-minimization problems, where
each corresponds to a labeling function u j , and therefore the parallelism strategy is ideal to
apply. This is one of the important advantages of our method for large data sets. The algorithm
aspects to solve our proposed convex model ( 15) are detailed in Sect. 4.
Step two. This step is to project the fuzzy partition result U obtained at step one to a binary
partition. Here, formula (14) is applied to the fuzzy partition U to generate a binary partition,
which naturally satisﬁes no vacuum and overlap constraint ( 11). We remark that compared
to the computation time at step one, the time at step two is negligible.
Normally, the classiﬁcation work is complete after we obtain a binary partition matrix at
step two. However, since the way of obtaining an initialization in our scheme is open and the
quality of the initialization could be poor, we suggest going back to step one with the latest
obtained partition as a new initialization and repeating the above two steps until no more
change in the partition matrix is observed. More precisely, we set U as ˆU and repeat steps
one and two again to obtain a new U . Then the ﬁnal classiﬁcation result is the converged
stationary partition matrix, sayU
∗. Moreover, to accelerate the convergence speed, we update
β in model ( 15) by a factor of 2 if we are to repeat the steps. This will obviously enforce
the closeness between two consecutive clustering results during iterations, which will ensure
the algorithm converges fast. We stop the algorithm when no changes are observed in the
clustering result compared to the previous one. We remark that a few iterations ( ∼ 10) are
generally enough in practice, see the experimental results in Sect. 5 for more detail.
Note, importantly, that our classiﬁcation method here is totally different from other vari-
ational methods like [ 2, 12] which need to minimize variational models with constraints
like ( 11), ( 12), ( 13), or other kinds of constraints (e.g. minimum and maximum number
of points imposed on individual classes V
i ). Even though our proposed model ( 15)h a sn o
constraint, the ﬁnal classiﬁcation result of our method naturally satisﬁes the no vacuum and
overlap constraint (11). Therefore, our method, namely SaT (inheriting the name of the SaT
methodology) classiﬁcation method for high-dimensional data, is much easier to solve in
each iteration. Its whole procedure is summarized in Algorithm 1.
Algorithm 1 SaT classiﬁcation method for high-dimensional data
Initialization: Generate initialization ˆU by e.g. SVM method.
Output: Binary partition U∗.
For l = 0, 1,..., until the stopping criterion reached (e.g. ∥U(l) − U(l+1)∥= 0)
Step one: Compute fuzzy partition U by solving model ( 15).
Step two: Compute binary partition U(l+1) by using formula ( 14)o n U .
Set ˆU = U(l+1) and β = 2β.
Endfor
Set U
∗ = U(l+1).
4 Algorithm Aspects
In this section, we present an algorithm to solve the proposed convex model ( 15) based on
the primal–dual algorithm [ 55].
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 9 of 25 81
4.1 Primal–Dual Algorithm
Let Xi be a ﬁnite dimensional vector space equipped with a proper inner product ⟨·,·⟩Xi and
norm ∥·∥ Xi , i = 1, 2. Let map K : X1 → X2 be a bounded linear operator. The primal–dual
algorithm is, generally speaking, to solve the following saddle-point problem
min
x∈X1
max
˜x∈X2
{
⟨Kx, ˜x⟩+ G(x) − F∗(˜x)
}
, (19)
where G : X1 →[ 0,+∞] and F : X2 →[ 0,+∞] are proper, convex and lower-
semicontinuous functions, and F∗ represents the convex conjugate of F . Given proper
initializations, the primal–dual algorithm to solve problem ( 19) can be summarized in the
following iterative way of updating the primal and dual variables, i.e.,
˜x(l+1) = (I + σ∂F∗)− 1(˜x(l) + σKz(l)), (20)
x(l+1) = (I + τ∂G)− 1(x(l) − τK∗˜x(l+1)), (21)
z(l+1) = x(l+1) + θ(x(l+1) − x(l)), (22)
where θ∈[ 0, 1],τ,σ > 0 are algorithm parameters.
4.2 Algorithm to Solve Our Proposed Model
We ﬁrst deﬁne some useful notations which will be used to present our algorithm.
4.2.1 Preliminary
For ease of explanation, in the following, when we say (i, j) ∈ E,t h e i and j represent the
i-th and j-th vertices in E, respectively. Let
E′ =
{
(i, j) | i < j,∀(i, j) ∈ E
}
. (23)
The graph Laplacian L = D − W ∈ RN×N can be decomposed as
L =
∑
(i, j)∈E′
Lij , (24)
where
Lij =
⎛
⎜⎜
⎜
⎜
⎜
⎜
⎜⎝
ij
...
...
i ··· w(i, j) ··· − w(i, j) ···
...
...
j ··· − w(i, j) ··· w(i, j) ···
...
...
⎞
⎟⎟
⎟
⎟
⎟
⎟
⎟⎠
∈ R
N×N (25)
is a matrix with only four nonzero entries which locate at positions (i, i), (i, j), (j, i) and
( j, j).L e t E′ = E′
a ∪ E′
b ∪ E′
c,w h e r e
E′
a =
{
(i, j) | i, j ∈ S,∀(i, j) ∈ E′}
, (26)
E′
b =
{
(i, j) | i, j ∈ T,∀(i, j) ∈ E′}
, (27)
123
81 Page 10 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
E′
c = E′\(E′
a ∪ E′
b). (28)
Then the decomposition L in Eq. (24) can be rewritten as
L =
∑
(i, j)∈E′a
Lij +
∑
(i, j)∈E′
b
Lij +
∑
(i, j)∈E′c
Lij . (29)
Note that, the terms ∑
(i, j)∈E′a
Lij and ∑
(i, j)∈E′
b
Lij only have nonzero entries which are
associated to the test set S and the training set T , respectively. Let
∑
(i, j)∈E′a
Lij =
⎛
⎝
L S 0
00
⎞
⎠ ,
∑
(i, j)∈E′
b
Lij =
⎛
⎝
00
0 ¯L
⎞
⎠ ,
∑
(i, j)∈E′c
Lij =
⎛
⎝
L1 L3
L⊤
3 L2
⎞
⎠ ,
(30)
where L S, L1 ∈ R(N− NT )×(N− NT ) are related to the test set S, ¯L, L2 ∈ RNT ×NT are related
to the training set T ,a n d L3 ∈ R(N− NT )×NT .T h e nw eh a v e
L =
⎛
⎝
L S + L1 L3
L⊤
3 ¯L + L2
⎞
⎠ . (31)
According to Eq. (8), the gradient operator ∇ can be regarded as a linear transformation
between RN and RN×(k− 1) (where k =| N(x)|). For a vector u j = (u⊤
S j , ¯u⊤
j )⊤ deﬁned in
Eq. (18), let
AS(uS j ) =∇
( uS j
0
)
∈ RN×(k− 1),
H j =∇
( 0
¯u j
)
∈ RN×(k− 1).
(32)
Clearly, AS : RN− NT → RN×(k− 1) is an operator corresponding to the test set S,a n d H j is
the gradient matrix corresponding to the training set T which is ﬁxed since ¯u j is ﬁxed. Then,
we have
∇ u j =∇
( uS j
¯u j
)
=∇
( uS j
0
)
+∇
( 0
¯u j
)
= AS(uS j ) + H j . (33)
4.2.2 Algorithm
Substituting the decomposition of L in Eq. ( 31), ∇ in Eq. ( 33), ˆu j in Eq. ( 17)a n d u j in
Eq. (18) into the proposed minimization model ( 15) yields
argmin
{uS j }K
j=1
K∑
j=1
{ β
2 ∥ˆuS j − uS j ∥2
2 + α
2 u⊤
S j L S uS j
+ αu⊤
S j L3 ¯u j +∥ AS(uS j ) + H j ∥1
}
.
(34)
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 11 of 25 81
Note, obviously, that solving the above model ( 34) is equivalent to solving K sub-
minimization problems corresponding to each uS j , j = 1,..., K , indicating that our
proposed model inherently beneﬁts from the parallelism computation. For 1 ≤ j ≤ K ,
let
G j (uS j ) = β
2 ∥ˆuS j − uS j ∥2
2 + α
2 u⊤
S j L S uS j + αu⊤
S j L3 ¯u j , (35)
F j(˜x) =∥ ˜x + H j ∥1. (36)
Using the deﬁnition of the ℓ1-norm given in Eq. ( 9), the conjugate of F j , F∗
j , can then be
calculated as
F∗
j ( p) = sup
˜x∈RN×(k− 1)
⟨˜x, p⟩−∥ ˜x + H j ∥1
=− ⟨ p, H j ⟩+ χP( p), (37)
where P ={ p ∈ RN×(k− 1) :∥ p∥∞ ≤ 1},a n dχP( p) is the characteristic function of set P
with value 0 if p ∈ P, otherwise +∞ .
Using the primal–dual formulation ( 19) with the deﬁnitions of G j and F∗
j respectively
given in Eqs. (35)a n d( 37), then the minimization problem ( 34) corresponding to each uS j
can be reformulated as
argmin
uS j
maxp
{
⟨AS(uS j ), p⟩+ G j(uS) +⟨ p, h j ⟩− χP( p)
}
. (38)
To apply the primal–dual method, it remains to compute (I + σ∂F∗
j )− 1 and (I + τ∂G j )− 1.
Firstly, for ∀˜x ∈ RN×(k− 1),w eh a v e
(I + σ∂F∗)− 1(˜x)
= argmin
p∈RN×(k− 1)
F∗
j ( p) + 1
2σ∥ p − ˜x∥2
2
= argmin
p∈RN×(k− 1)
χP( p) + 1
2σ∥ p − ˜x∥2
2 −⟨ p, H j ⟩
= argmin
p∈RN×(k− 1)
χP( p) + 1
2σ∥ p − ˜x − σH j∥2
2
=ιP(˜x + σH j), (39)
where the operator ιP(·) is the pointwise projection operator onto the set P, i.e., ∀p ∈ R,
ιP( p) =
{
1, | p| > 1,
p, otherwise. (40)
Secondly, for ∀x ∈ RN− NT ,w eh a v e
(I + τ∂G j )− 1(x)
= argmin
uS j ∈RN− NT
G j(uS j ) + 1
2τ∥uS j − x∥2
2. (41)
123
81 Page 12 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
Using the deﬁnition ofG j (uS j ) g i v e ni nE q .(35), problem (41) becomes solving the following
linear system
(αL S + βI + 1
τI)uS j = βˆuS j + 1
τx − αL3 ¯u j . (42)
Since (α¯L + βI + 1
τ I) is positive deﬁnite, the above linear system can be solved efﬁciently
by e.g. conjugate gradient method [ 57].
Finally, by exploiting the strong convexity of G j,∀1 ≤ j ≤ K ,w h i c hi ss h o w ni nt h e
next lemma, the work in [55] suggests that we could adaptively modify σ,τ to accelerate the
convergence or the primal–dual method.
Lemma 2 The functions G j,∀1 ≤ j ≤ K , are strongly convex with parameter β.
Proof For simplicity, we omit the subscript j and S j in the following proof. First, by Eq. (30),
L S is semi-positive deﬁnite. Therefore, (α
2 u⊤ L S u + αu⊤ L3 ¯u) is convex. Now the strong
convexity of G follows from the fact that the remaining term in Eq. (35), which is β
2 ∥u− ˆu∥2
2,
is strongly convex with parameter β. ⊓⊔
The algorithm solving our proposed classiﬁcation model ( 34) (i.e., model ( 15)) is sum-
marized in Algorithm 2. Its convergence proof is given in Theorem 3 below. For each
sub-minimization problem, the relative error between two consecutive iterations and/or a
given maximum iteration number can be used as stopping criteria to terminate the algorithm.
Finally, we emphasize again that our method is quite suitable for parallelism since the K
sub-minimization problems are independent of each other and therefore can be computed in
parallel.
Algorithm 2 Algorithm solving the proposed model ( 34) (i.e., model ( 15))
Initialization: ˜x(0) ∈ RN×(k− 1), x(0), z(0) ∈ RN− NT , θ∈[ 0, 1],τ(0),σ(0) > 0.
Output: {uS j }K
j=1.
For j = 1,..., K (parallelism strategy can be applied)
For l = 0, 1,..., until the stopping criterion reached
Let ˜x = ˜x(l) + σ(l)AS z(l), and compute ˜x(l+1) = (I + σ(l)∂F∗)− 1(˜x) by Eq. (39);
Let x = x(l) − τ(l)A∗
S ˜x(l+1), and compute x(l+1) = (I + τ(l)∂G)− 1(x) by Eq. (41);
Let θ(l) = 1/
√
1 + βτ(l),a n ds e tτ(l+1) = θ(l)τ(l),σ(l+1) = σ(l)/θ(l);
Compute z(l+1) = x(l+1) + θ(x(l+1) − x(l));
Endfor
Set uS j = x(l+1).
Endfor
Theorem 3 Algorithm 2 converges if τ(0)σ(0) < 1
N 2(k− 1) .
Proof By Theorem 2 in [55], Algorithm 2 converges as long as ∥AS∥2
2 < 1
τ(0)σ(0) . Therefore,
it sufﬁces to ﬁnd a suitable upper bound for ∥AS∥2. By our implementation in Eq. ( 32)a n d
since the weight functions Eqs. ( 1)–(3) take values in [− 1, 1], each entry in AS is between
[− 1, 1]. Therefore, the 1-norm and ∞ -norm of AS can be easily estimated as
∥AS∥1 = max
1≤ j≤ N− NT
N(k− 1)∑
i=1
|(AS)ij |≤ N(k − 1)
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 13 of 25 81
Fig. 1 Three-class classiﬁcation for the Three Moon synthetic data. a Ground truth; b and c results of
method TVRF [ 2] and our proposed method, respectively
and
∥AS∥∞ = max
1≤i≤ N(k− 1)
N− NT∑
j=1
|(AS)ij |≤ N − NT .
We then have
∥AS∥2 ≤
√
∥AS∥1∥AS∥∞ ≤ N
√
k − 1.
Therefore, we conclude that the algorithm converges as long as we choose τ(0),σ(0) > 0,
such that τ(0)σ(0) < 1
N 2(k− 1). ⊓⊔
The convergence of Algorithm 2 proved in Theorem 3 ensures the convergence of step
one of our proposed method (i.e., Algorithm 1). After the binary partition step two at the l-th
iteration of Algorithm 1,w eh a v e ˆU = U(l+1) and β = 2β. The increasing regularization
parameter β will lead to the dominance of the ﬁrst term of our model ( 15), which yields
∥U − ˆU∥→ 0w h e nl becomes large; in particular, this will ﬁnally lead to the satisfaction of
the stopping criterion ∥U(l) − U(l+1)∥ of Algorithm 1. Extensive experiments in Sect. 5 will
show that our Algorithm 1 converges very quickly, e.g. generally no more than ten iterations
(i.e., l ≤ 10); see Fig. 4 for the convergence history.
5 Numerical Results
In this section, we evaluate the performance of our proposed method on four benchmark
data sets—including Three Moon, COIL, Opt- Digits and MINST—for semi-supervised
learning. Three Moon is a synthetic data set which has been used frequently e.g. in [2, 7, 12].
The COIL, Opt- Digits,a n d MNIST data sets can be found in the supplementary material
of [ 58], the UCI machine learning repository, 1 and the MNIST Database of Handwritten
Digits,2 respectively.
The basic properties of these test data sets are shown in Table1. It indicates that the number
of classes in these data sets ranges from small to large (i.e., 3 to 10), which is analogous to
the dimensions and number of points. The individual points in these data sets may have no
texture/feature information like those in Three Moon or may be images with low resolution
like those in COIL, Opt- Digits and MNIST. In particular, the number of labeled points (see
details below) may be very small, e.g. less than 1% of the given data set, and signiﬁcantly
unbalanced across the classes.
1 http://archive.ics.uci.edu/ml/datasets.html.
2 http://yann.lecun.com/exdb/mnist/ .
123
81 Page 14 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
Table 1 Basic properties of the
test benchmark data sets Data set No. of classes Dim. No. of points
Three Moon 3 100 1500
COIL 6 241 1500
Opt- Digits 10 64 5620
MNIST 10 784 70 ,000
“Dim.” means dimension, i.e., the length of every vector representing
individual points in the given data sets
To implement our method, k-NN graphs are constructed for the test data sets, using the
randomized kd-tree [59] to ﬁnd the nearest neighbors with Euclidean distance as the metric.
The radial basis function (1) is used to compute the weight matrix W , except for the MNIST
data set where the Zelnic-Manor and Perona weight function (2) is used with the eight closest
neighbors. The training samplesT —samples with labels known—are selected randomly from
each test data set. The classiﬁcation accuracy is deﬁned as the percentage of correctly labeled
data points.
Unless otherwise speciﬁed, the regularization parameter α is ﬁxed to 1 and the regular-
ization parameter β is initially ﬁxed to 0.01. Indeed, this combination of α, βprovides good
results for almost all data sets; moreover, the results are robust for α ∈[ 0.5, 2] and for
β ∈[ 0.001, 0.1]. The choice of initial β needs ﬁne-tuning for the COIL data set, i.e., a much
smaller initial β is required to achieve reasonable accuracy. We comment that the accuracy
of the proposed method can be improved further after ﬁne-tuning the values of α and β
for individual test data sets. The ways of selecting the optimized parameters are, however,
beyond the scope of this work and will be conducted in future investigation. All the codes
were implemented in Matlab 2017a and run on a MacBook with 2.8 GHz processor and 16
GB RAM.
5.1 Methods Comparison
As mentioned in previous sections, we use the SVM method [ 51] to generate initializations
for our proposed method. If it is not proper for a data set (e.g. very slow due to the large size
of the data set), we could just use an initialization generated by assigning clustering labels
randomly.
The SVM is a technique aiming to ﬁnd the best hyperplane that separates data points of
one class from the others. In practice, data may not be separable by a hyperplane. In that case,
soft margin is used so that the hyperplane would separate many data points if not all. It is also
common to kernelize data points, and then ﬁnd a separating hyperplane in the transformed
space. The SVM method used in our experiments is trained with a linear kernel.
The properties shown in Table 1, including the individual points with no clear tex-
ture/feature information and lack of labeled points, justify the necessity and importance
of the methods based on variational models like this work, which can exploit the structure
of the whole data set except for the individual points, against another type of methods based
on deep learning which generally require the individual points to have rich texture/feature
information and quite a large number of training points/samples. In such sense, the methods
based on deep learning are not the main focus of this paper and will not be included for
comparison here.
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 15 of 25 81
Fig. 2 Unbalanced sampling
from the Three Moon data,
where sampled points are
highlighted with their
corresponding labels
We compare our proposed method with the state-of-the-art methods proposed recently,
e.g. CVM [ 1], GL [ 7], MBO [ 7], TVRF [ 2], LapRF [ 2], LapRLS [ 60], MP [ 60], and SQ-
Loss-I [58]. The code TVRF was provided by the authors and the parameters used in it were
chosen by trial and error to give the best results. The classiﬁcation accuracies of methods
GL, MBO, LapRF, LapRLS, MP and SQ-Loss-I were taken from [ 1, 2], in which methods
CVM and TVRF were shown to be superior in most of the cases.
5.2 Three Moon Data
The synthetic Three Moon data used here is constructed by following the way performed in
[1, 2] exactly. We brieﬂy repeat the procedure as follows. First, generate three half circles in
R2—two half top unit circles and one half bottom circle with radius of 1.5 which are centered
at (0, 0), (3, 0) and (1.5, 0.4), respectively. Then 500 points are uniformly sampled from each
half circle and embedded into R
100 by appending zeros to the remaining dimensions. Finally,
an i.i.d. Gaussian noise with standard deviation 0.14 is added to each dimension of the data.
An illustration of the ﬁrst two dimensions of the Three Moon data is shown in Fig. 1a
where different colors are applied on each half circle. This is a three-class classiﬁcation
problem with the goal of classifying each half circle using a small number of supervised
points from each class. This classiﬁcation problem is challenging due to the noise and the
high dimensionality of all the points with high similarity in R
98.
A k-NN graph with k = 10 is built for this data set, parameter ξ = 3i su s e di nt h e
Gaussian weight function, and the distance metric chosen is Euclidean metric for R100.W e
ﬁrst test the methods using uniformly distributed supervised points, where a total number of
75 points is sampled uniformly from this data set as training points.
The accuracies of method TVRF and ours are obtained by running the methods ten times
with randomly selected labeled samples, and taking the average of the accuracies. The accu-
racies of method CVM are obtained from the original paper [ 1]. The accuracy comparison is
reported in Table 2, showing that our proposed method gives the highest accuracy; see also
Fig. 1 for visual validation of the results between methods of TVRF and ours. The average
number of iterations taken for our proposed method is 3 .8. Figure 4a gives the convergence
history and partition accuracy of our proposed method corresponding to iteration steps, which
clearly shows the accuracy increment during iterations (note that the accuracy at iteration
0 is the result of the initialization which is obtained by the SVM method). Table 7 reports
the comparison in terms of computation time, indicating the superior performance of our
proposed method in computation speed.
In the following, as a showcase, we test the methods using non-uniformly distributed
supervised points, which is used to investigate the robustness of these methods on training
points. In this case for the 75 training points, as an example, we respectively pick 5 points
123
81 Page 16 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
Table 2 Accuracy comparison
for the Three Moon synthetic
data set, with uniformly selected
training points
Method Accuracy (%)
CVM 98.7
GL 98.4
MBO 99.1
TVRF 98.6
LapRF 98.4
Proposed 99.4
Highest accuracy value given in bold
Table 3 Accuracy comparison
for the Three Moon synthetic
data set, with non-uniformly
selected training points
Method Accuracy (%)
TVRF 97.8
Proposed 99.3
Highest accuracy value given in bold
Fig. 3 Examples of digits 0–9
from the MNIST data set
Table 4 Accuracy comparison
for the COIL data set, with
uniformly selected training points
Method Accuracy (%)
CVM 93.3
TVRF 92.5
LapRF 87.7
GL 91.2
MBO 91.5
Proposed 94.0
Highest accuracy value given in bold
from the left and the bottom half circles, and pick the rest 65 points from the right half circle.
This sampling is illustrated in Fig. 2.
The accuracies of TVRF and our method are shown in Table 3, from which we see clearly
that our method gives much higher accuracy. The standard deviation of the accuracy for
our method is 0.11%. In particular, compared to the results in Table 2 using training points
selected uniformly, the accuracy of TVRF decreases by 0.8%, whereas we observe only a
very small decrease (i.e., 0.1%) in our proposed method. This shows the robustness of our
method with respect to the way that training points are selected. Note that in the case of
training points chosen non-uniformly, the initialization obtained by SVM is poor, because of
which more iterations are needed to converge for our method—average 12.0 iterations in 10
trials versus 3.3 iterations needed for the case of training points selected uniformly.
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 17 of 25 81
Table 5 Accuracy comparison
for the MINST data set, with
uniformly selected training points
Method Accuracy (%)
CVM 97.7
TVRF 96.9
LapRF 96.9
GL 96.8
MBO 96.9
Proposed 97.4
Highest accuracy value given in bold
5.3 COIL Data
The benchmark COIL data comes from the Columbia object image library. It contains a set
of color images of 100 different objects. These images, with size of 128× 128 each, are taken
from different angles in steps of 5 degrees, i.e., 72 ( = 360/5) images for each object. In the
following, without loss of generality, we also call an image a point for ease of reference.
The test data set here is constructed the same way as depicted in e.g. [ 1, 2] and is brieﬂy
described as follows. First, the red channel of each image is down-sampled to 16 × 16 pixels
by averaging over blocks of 8 × 8 pixels. Then, 24 out of the 100 objects are randomly
selected, which amounts to 1728 (= 24 × 360/5) images. After that, these 24 objects are
partitioned into six classes with four objects—288 images (= 4×72)—in each class. Finally,
after discarding 38 images randomly from each class, a data set of 1500 images where 250
images in each of the six classes are constructed. To construct a graph, each image, which
is a vector with length of 241 after randomly masking (i.e., removing) 15 pixels from the
original 256 (= 16× 16) pixels (see [58, Algorithm 21.1]), is treated as a node on the graph.
For accuracy test, a k-NN graph with k = 4 is built for this data set, parameter ξ= 250 is
used in the Gaussian weight function, and the distance metric chosen is Euclidean metric for
R
241. The initial β is chosen as 10 − 5. The training points, amount to 10% of the points, are
selected randomly from the data set. Again, we run the test methods 10 times and compare the
average accuracy. The resulting accuracy listed in Table4 shows that our method outperforms
other methods. The standard deviation of the accuracy for our method is 0.84%. Moreover, the
average number of iterations of our method is 12.2. Figure 4b gives the convergence history
of our proposed method in partition accuracy corresponding to iterations, which again shows
an increasing trend in accuracy.
5.4 MNIST Data
The MNIST data set consists of 70,000 images of handwritten digits 0–9, where each image
has a size of 28 × 28. Figure 3 shows some images of the ten digits from the data set. Each
image is a node on a constructed graph. The objective is to classify the data set into 10 disjoint
classes corresponding to different digits. For accuracy test, a k-NN graph with k = 8 is built
for this data set, and Zelnik-Manor and Perona weight function in Eq. ( 2) is used to compute
the weight matrix. The training 2500 (i.e., 3.57%) points (images) are selected randomly
from the total 70,000 points.
The experimental results of the test methods are obtained by running them 10 times with
randomly selected training set with a ﬁxed number of points 2500, and the average accuracy is
computed for comparison. The accuracy of the test results is shown in Table5, indicating that
123
81 Page 18 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
Fig. 4 Accuracy convergence history of our proposed method corresponding to iteration steps for all the test
data sets. The training samples are uniformly selected in each class. Blue and orange curves correspond to
cases with the least and the largest number of iterations among the 10 trials, respectively
Table 6 Accuracy comparison
for the Opt- Digits data set, with
uniformly selected training points
Sample rate 0.89% (50) 1.78% (100) 2.67% (150)
k-NN 85.5 92.0 93.8
SGT 91.4 97.4 97.4
LapRLS 92.3 97.6 97.3
SQ-Loss-I 95.9 97.3 97.7
MP 94.7 97.0 97.1
TVRF 95.9 98.3 98.2
LapRF 94.1 97.7 98.1
Proposed 97.09 8 .49 8 .5
Highest accuracy value given in bold
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 19 of 25 81
Table 7 Computation time comparison in seconds
Method Three Moon COIL MINST Opt- Digits
TVRF 0.71 0.65 66.00 3.42
Proposed 0.30 (3.3) 0.76 (11.7) 82.04 (9.4) 4.45 (9.3)
Highest accuracy value given in bold
The value in the brackets for our method represents the average number of iterations of the 10 trials. More
computation time of the related methods can be found in [ 2], which indicates that the TVRF method is quite
efﬁcient among the methods compared, e.g., it is at least 10 times faster than the multi-class MBO [ 7]. (For
the Opt-Digits data, we select 100 sample points.)
our method is comparable to or better than the state-of-the-art methods compared here. The
standard deviation of the accuracy for our method is 0.03%. Table 7 shows the computation
time comparison, from which we again see that our method is very competitive in computation
speed. The convergence history of our proposed method in partition accuracy corresponding
to iterations is given in Fig. 4c, which also demonstrates a clear increasing trend in accuracy.
5.5 Opt- Digits Data
The Opt- Digits data set is constructed as follows. It contains 5620 bitmaps of handwritten
digits (i.e., 0–9). Each bitmap has the size of 32 × 32 and is divided into non-overlapping
blocks of 4× 4, and then the number of “on" pixels is counted in each block. Therefore, each
bitmap corresponds to a matrix of 8 × 8 where each element is an integer in [0, 16]. The
classiﬁcation problem is to partition the data set into 10 classes.
For accuracy test, a k-NN graph with k = 8 is built for this data set, parameter ξ= 30 is
used in the Gaussian weight function, and the distance metric chosen is Euclidean metric for
R
64. For the experiments on this data set, we generate three training sets respectively with the
number of points 50, 100 and 150, which are all selected randomly. All the methods are run 10
times for each training set and the average accuracy is used for comparison. The quantitative
results in accuracy are listed in Table 6, from which we see that our proposed method is
consistently better than the state-of-the-art methods compared for all the cases. The standard
deviations of the accuracy for our method are 1.25%, 0.53% and 0.28% corresponding to 50,
100 and 150 number of training points, respectively. We also observe the improvement of the
accuracy of these methods w.r.t. the increasing number of points in the training set. Finally,
we show the convergence history of our proposed method in partition accuracy corresponding
to iterations using the training set with 150 points in Fig. 4d, which again clearly shows an
increasing trend in accuracy.
5.6 One-Class Classification
Apart from the aforementioned multi-class classiﬁcation problem, our proposed method can
also be naturally extended to tackle the one-class classiﬁcation problem, also known as unary
classiﬁcation [52, 61]. The goal of one-class classiﬁcation is to distinguish one speciﬁc class
from the others by learning primarily from the speciﬁc class in the data set. We regard the
speciﬁc class (the class of interest) as the true data, while the others as outliers. The goal then
is to discriminate between the true data and outliers in the given data set. It is natural to treat
123
81 Page 20 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
Table 8 One-class classiﬁcation results of our proposed method
Data set # true samples: # outlier samples Accuracy (%)
Three Moon 50: 25 (= 2:1) 99.58
38: 37 (≈ 1:1) 99.62
COIL 100: 50 (= 2:1) 91.30
75: 75 (= 1:1) 94.42
MNIST 1667: 833 (≈ 2:1) 99.80
1250: 1250 (= 1:1) 99.81
Opt- Digits 67: 33 (≈ 2:1) 99.97
50: 50 (= 1:1) 99.97
# true samples and # outlier samples represent the number of samples with labels in the speciﬁc class and the
outliers, respectively
the true data and outliers as two classes, where the main challenge now is the highly uneven
sampling of these two classes.
We test the performance of our proposed method on all of the above four data sets following
the same parameter settings. Two types of tests are conducted for each data set, i.e., the ratios
of 2:1 and 1:1 in the speciﬁc class and the outliers regarding the number of samples labeled
uniformly in each class are applied. Table 8 summarizes the results in terms of classiﬁcation
accuracy of our proposed method (the accuracy of the TVRF method is withdrawn due to its
inferior and unstable performance for unbalanced data set shown in Sect. 5.2). The accuracy
after each iteration versus the iteration number for the four test data sets is given in Fig. 5,
which repeatedly shows an increasing trend in accuracy. It is evident that our proposed method
consistently performs excellently in this problem even if the two classes—the speciﬁc class
and the outliers—are extremely uneven, demonstrating the versatility and robustness of our
proposed method in classiﬁcation.
5.7 Further Discussion
The above experimental results on the benchmark data sets in terms of classiﬁcation accuracy,
s h o w ni nT a b l e s2, 3, 4, 5 and 6, indicate that our proposed method outperforms the state-of-
the-art methods for high-dimensional data and point clouds classiﬁcation.
Compared to the start-of-the-art variational classiﬁcation models proposed e.g. in [ 1, 2],
in addition to the data ﬁdelity term and ℓ
1 term (e.g. TV), our proposed model ( 15) contains
an additional ℓ2 term on the labeling functions which is used to smooth the classiﬁcation
results so as to reduce the non-smooth artifact (the so-called staircase artifact in images)
introduced by the ℓ
1 term. This is one reason that our method can generally achieve bet-
ter results. Moreover, the warm initialization used in our method can also play a role in
improving the classiﬁcation quality. Apart from generating the initialization manually, any
classiﬁcation methods can practically be used to generate the initialization. Starting from
the initialization, our proposed method can then be applied to achieve a better classiﬁcation
result by improving the accuracy iteratively. Theoretically speaking, the poorer the quality
of the initialization, the more iterations are needed for our method. Nevertheless, we found
that even for poor initializations (e.g. the ones generated randomly), 20 iterations are already
enough to achieve competitive results. Generally, no more than 15 iterations are needed when
using an initialization computed by standard classiﬁcation methods (e.g. SVM).
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 21 of 25 81
Fig. 5 Accuracy convergence history for one-class classiﬁcation using our proposed method corresponding
to iteration steps for all the test data sets. The training samples are uniformly selected in each class. Blue
and orange curves correspond to cases with the least and the largest number of iterations among the 10 trials,
respectively
Another distinction of our proposed model compared to the variational classiﬁcation
models in e.g. [1, 2] is that there are no constraints on these labeling functions in our objective
functional. In other words, in each iteration, we just need to ﬁnd the minimizer of the objective
functional corresponding to each labeling function, but these minimizers do not need to
satisfy the constraint that their summation equal to 1. Therefore, the computation speed for
every single iteration is improved in our method compared to other methods which have
constraints. We emphasize again that, since minimizing each sub-problem with respect to
each labeling function is irrelevant to minimizing the sub-problems with respect to other
labeling functions, parallelism techniques can be used straightforwardly to further improve
the computation performance of our algorithm. Theoretically, we just require 1 /K of the
computation time needed for the non-parallelism scheme. This will be extremely important
for large data sets. From Table 7, we see that, for all the computation time of our method,
when considering the effect of parallel computing, our method should be able to outperform
the state-of-the-art methods by a large margin.
123
81 Page 22 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
The efﬁciency, versatility and robustness of our proposed method have also been validated
in the one-class classiﬁcation problem. It is indeed that our proposed method can be used
to deal with different types of data sets which have e.g. extremely small number of labeled
samples where individual samples have little to none texture/feature information (e.g. the
samples in the Three Moon data set which only contain the coordinate information). These
are the scenarios that the methods based on deep learning generally struggle. Therefore, our
proposed method in this sense can complement deep learning methods in classiﬁcation rather
than be mutually exclusive. In particular, it would be of great interest in the future to further
investigate the integration of the variational methods including ours with deep learning meth-
ods, e.g., using variational methods to classify features extracted by deep learning methods,
involving graph Laplacian in deep learning frameworks [ 62, 63], etc.
6 Conclusions
In this paper, an efﬁcient and versatile multi-class semi-supervised method based on varia-
tional models is proposed for classifying high-dimensional data or unstructured point clouds.
The method is inspired by the SaT strategy which has been shown very effective for segmen-
tation problems such as gray or color images corrupted by different degradations. Starting
with a proper initialization which can be obtained by using any standard classiﬁcation algo-
rithm (e.g. SVM) or constructed by users, the ﬁrst step of our method is to solve a convex
variational model without constraint. Most importantly, our proposed model is a lot easier
to solve than the state-of-the-art variational models (e.g. [ 1, 2]) for the point clouds classi-
ﬁcation problem since they all need no vacuum and overlap constraint ( 11) on the labeling
functions in the unit simplex, which could make their models to be non-convex. The second
step of our method is to ﬁnd a binary partition via thresholding the smoothed result obtained
from the ﬁrst step. We proved that our proposed model has a unique solution and the derived
primal–dual algorithm converges.
We tested our proposed method on four benchmark data sets and compared with the
state-of-the-art methods. We also investigated the inﬂuence of the training sets selected
uniformly and non-uniformly. For our method, different ways of generating initializations
were implemented and validated. The performance of the proposed method on the one-class
classiﬁcation problem was also validated except for the multi-class problem. On the whole,
the experimental results demonstrated that our method is superior in terms of classiﬁcation
accuracy and computation speed when parallel computing is considered. Our method is
therefore an efﬁcient and versatile classiﬁcation method for data sets like high-dimensional
data or unstructured point clouds.
Acknowledgements This work of R. Chan is partially supported by HKRGC Grants No. CityU12500915,
CityU14306316, HKRGC CRF Grant C1007-15 G, and HKRGC AoE Grant AoE/M-05/12. This work of T.
Zeng is partially supported by the National Natural Science Foundation of China under Grant 11671002,
CUHK start-up and CUHK DAG 4053296, 4053342. We thank Prof. Xue-Cheng Tai, Dr Ke Yin, Dr Egil Bae
and Prof. Ekaterina Merkurjev for providing the codes of their methods [ 1, 2].
Funding The authors have not disclosed any funding. A funding declaration is mandatory for publication in
this journal. Please conﬁrm that this declaration is accurate, or provide an alternative.
Data Availability The data sets generated during the current study are available from the corresponding authors
on reasonable request.
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 23 of 25 81
Declarations
Conﬂict of interest The authors have no relevant ﬁnancial or non-ﬁnancial interests to disclose.
Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which
permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,
and indicate if changes were made. The images or other third party material in this article are included in the
article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is
not included in the article’s Creative Commons licence and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.
To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .
References
1. Bae, E., Merkurjev, E.: Convex variational methods on graphs for multiclass segmentation of high-
dimensional data and point clouds. J. Math. Imaging Vis. 58(3), 468–493 (2017). https://doi.org/10.
1007/s10851-017-0713-9
2. Yin, K., Tai, X.-C.: An effective region force for some variational models for learning and clustering. J.
Sci. Comput. 74, 175–196 (2018)
3. Merkurjev, E., Bertozzi, A., Y an, X., Lerman, K.: Modiﬁed Cheeger and ratio cut methods using the
Ginzburga–Landau functional for classiﬁcation of high-dimensional data. Inverse Prob. 33(7), 074003
(2017)
4. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 22(8),
888–905 (2000)
5. Lee, J., Cai, X., Lellmann, J., Dalponte, M., et al.: Individual tree species classiﬁcation from airborne
multisensor imagery using robust PCA. IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens. 9(6), 2554–
2567 (2016)
6. Osting, B., White, C., Oudet, E.: Minimal Dirichlet energy partitions for graphs. SIAM J. Imag. Sci.
36(4), 1635–1651 (2014)
7. Garcia-Cardona, C., Merkurjev, E., Bertozzi, A.L., Flenner, A., Percus, A.G.: Multiclass data segmentation
using diffuse interface methods on graphs. IEEE Trans. Pattern Anal. Mach. Intell. 36(8), 1600–1613
(2014). https://doi.org/10.1109/TPAMI.2014.2300478
8. Merkurjev, E., Kostic, T., Bertozzi, A.L.: An MBO scheme on graphs for classiﬁcation and image
processing. SIAM J. Imaging Sci. 6(4), 1903–1930 (2013)
9. Elmoataz, A., Lezoray, O., Bougleux, S.: Nonlocal discrete regularization on weighted graphs: a
framework for image and manifold processing. IEEE Trans. Image Process. 17, 1047–1060 (2008)
10. Merkurjev, E., Bae, E., Bertozzi, A.L., Tai, X.-C.: Global binary optimization on graphs for classiﬁcation
of high-dimensional data. J. Math. Imaging Vis. 52(3), 414–435 (2015)
11. Bertozzi, A.L., Flenner, A.: Diffuse interface models on graphs for classiﬁcation of high dimensional
data. Multiscale Model. Simul. 10(3), 1090–1118 (2012)
12. Lézoray, O., Elmoataz, A., Ta, V .T.: Nonlocal PDEs on graphs for active contours models with applications
to image segmentation and data clustering. In: IEEE International Conference on Acoustics, Speech and
Signal Processing, pp. 873–876 (2012)
13. Merriman, B., Ruuth, S.J.: Diffusion generated motion of curves on surfaces. J. Comput. Phys. 225(2),
2267–2282 (2007). https://doi.org/10.1016/j.jcp.2007.03.034
14. Y u, S.X., Shi, J.: Multiclass spectral clustering. In: Proceedings Ninth IEEE International Conference on
Computer Vision, pp. 313–3191 (2003). https://doi.org/10.1109/ICCV .2003.1238361
15. Hein, M., Setzer, S.: Beyond spectral clustering—tight relaxations of balanced graph cuts. In: Shawe-
Taylor, J., Zemel, R.S., Bartlett, P .L., Pereira, F., Weinberger, K.Q. (eds.) Advances in Neural Information
Processing Systems 24, pp. 2366–2374 (2011)
16. Bresson, X., Tai, X.-C., Chan, T.F., Szlam, A.: Multi-class transductive learning based on l1 relaxations
of Cheeger cut and Mumford–Shah–Potts model. J. Math. Imaging Vis. 49(1), 191–201 (2014). https://
doi.org/10.1007/s10851-013-0452-5
17. Cai, X., Chan, R.H., Zeng, T.: A two-stage image segmentation method using a convex variant of the
Mumford–Shah model and thresholding. SIAM J. Imaging Sci. 6(1), 368–390 (2013)
18. Boykov, Y ., Funka-Lea, G.: Graph cuts and efﬁcient N-D image segmentation. Int. J. Comput. Vis. 70(2),
109–131 (2006)
123
81 Page 24 of 25 Journal of Scientiﬁc Computing (2024) 100 :81
19. Mumford, D., Shah, J.: Optimal approximations by piecewise smooth functions and associated variational
problems. Commun. Pure Appl. Math. 42(5), 577–685 (1989)
20. Cremers, D., Rousson, M., Deriche, R.: A review of statistical approaches to level set segmentation:
integrating color, texture, motion and shape. Int. J. Comput. Vis. 72(2), 195–215 (2007)
21. Cai, X.: V ariational image segmentation model coupled with image restoration achievements. Pattern
Recognit. 48, 2029–2042 (2015)
22. Cai, X., Chan, R.H., Nikolova, M., Zeng, T.: A three-stage approach for segmenting degraded color
images: smoothing, lifting and thresholding (SLaT). J. Sci. Comput. 72(3), 1313–1332 (2017). https://
doi.org/10.1007/s10915-017-0402-2
23. Dong, B., Chien, A., Shen, Z.: Frame based segmentation for medical images. Commun. Math. Sci. 9,
551–559 (2010). https://doi.org/10.4310/CMS.2011.v9.n2.a10
24. Bar, L., Chan, T.F., Chung, G., Jung, M., Kiryati, N., Mohieddine, R., Sochen, N., V ese, L.A.: Mumford
and Shah model and its applications to image segmentation and image restoration. In: Scherzer, O. (ed.)
Handbook of Mathematical Methods in Imaging, pp. 1095–1157. Springer, New Y ork (2011).https://doi.
org/10.1007/978-0-387-92920-0_25
25. Bresson, X., Esedoglu, S., V andergheynst, P ., Thiran, J., Osher, S.: Fast global minimization of the active
contour/snake model. J. Math. Imaging Vis. 28(2), 151–167 (2007)
26. Chan, T.F., V ese, L.A.: Active contours without edges. IEEE Trans. Image Process.10(2), 266–277 (2001)
27. Li, F., Ng, M., Zeng, T., Shen, C.: A multiphase image segmentation method based on fuzzy region
competition. SIAM J. Imaging Sci. 3(2), 277–299 (2010)
28. V ese, L., Chan, T.F.: A multiphase level set framework for image segmentation using the Mumford and
Shah model. Int. J. Comput. Vis. 50(3), 271–293 (2002)
29. Rudin, L.I., Osher, S., Fatemi, E.: Nonlinear total variation based noise removal algorithms. Physica D
60(1), 259–268 (1992)
30. Chambolle, A., Novaga, M., Cremers, D., Pock, T.: An introduction to total variation for image analysis.
In: Theoretical Foundations and Numerical Methods for Sparse Recovery. De Gruyter (2010)
31. Cai, X., Chan, R.H., Schönlieb, C.-B., Steidl, G., Zeng, T.: Linkage between piecewise constant Mumford–
Shah model and ROF model and its virtue in image segmentation. arXiv:1807.10194 (2018)
32. Cai, X., Steidl, G.: Multiclass segmentation by iterated ROF thresholding. In: Heyden, A., Kahl, F.,
Olsson, C., Oskarsson, M., Tai, X.-C. (eds.) Energy Minimization Methods in Computer Vision and
Pattern Recognition, pp. 237–250. Springer Berlin Heidelberg, Berlin (2013)
33. Chan, R.H., Y ang, H., Zeng, T.: A two-stage image segmentation method for blurry images with Poisson
or multiplicative Gamma noise. SIAM J. Imaging Sci. 7(1), 98–127 (2014)
34. Chan, T.F., Esedoglu, S., Nikolova, M.: Algorithms for ﬁnding global minimizers of image segmentation
and denoising models. SIAM J. Appl. Math. 66(5), 1632–1648 (2006)
35. He, Y ., Hussaini, M.Y ., Ma, J., Shafei, B., Steidl, G.: A new Fuzzy C-means method with total variation
regularization for image segmentation of images with noisy and incomplete data. Pattern Recognit. 45,
3463–3471 (2012)
36. Brown, E., Chan, T., Bresson, X.: Completely convex formulation of the Chan–V ese image segmentation
model. Int. J. Comput. Vis. 98, 103–121 (2012)
37. Lellmann, J., Schnörr, C.: Continuous multiclass labeling approaches and algorithms. SIAM J. Imaging
Sci. 44(4), 1049–1096 (2011)
38. Pock, T., Chambolle, A., Cremers, D., Bischof, H.: A convex relaxation approach for computing minimal
partitions. In: IEEE Conference on Computer Vision and Pattern Recognition, pp. 810–817 (2009)
39. Pock, T., Cremers, D., Bischof, H., Chambolle, A.: An algorithm for minimizing the Mumford–Shah
functional. In: 2009 IEEE 12th International Conference on Computer Vision, pp. 1133–1140 (2009)
40. Y uan, J., Bae, E., Tai, X.-C., Boykov, Y .: A continuous max-ﬂow approach to Potts model. In: European
Conference on Computer Vision, pp. 379–392 (2010)
41. Bezdek, J.C., Ehrlich, R., Full, W.: FCM: the fuzzy C-means clustering algorithm. Comput. Geosci.
10(2–3), 191–203 (1984). https://doi.org/10.1016/0098-3004(84)90020-7
42. Cai, X., Fitschen, J., Nikolova, M., Steidl, G., Storath, M.: Disparity and optical ﬂow partitioning using
extended Potts priors. Inf. Inference A J. IMA 4(1), 43–62 (2014)
43. Bauer, B., Cai, X., Peth, S., Schladitz, K., Steidl, G.: V ariational-based segmentation of bio-pores in
tomographic images. Comput. Geosci. 98, 1–8 (2017)
44. Zhang, Y ., Matuszewski, B., Shark, L., Moore, C.: Medical image segmentation using new hybrid level-set
method. In: 2008 Fifth International Conference BioMedical Visualization: Information Visualization in
Medical and Biomedical Informatics, pp. 71–76 (2008)
45. Cai, X., Chan, R.H., Morigi, S., Sgallari, F.: Framelet-based algorithm for segmentation of tubular struc-
tures. In: Bruckstein, A.M., ter Haar Romeny, B.M., Bronstein, A.M., Bronstein, M.M. (eds.) Scale Space
and V ariational Methods in Computer Vision, pp. 411–422. Springer Berlin Heidelberg, Berlin (2012)
123
Journal of Scientiﬁc Computing (2024) 100 :81 Page 25 of 25 81
46. Cai, X., Chan, R.H., Morigi, S., Sgallari, F.: V essel segmentation in medical imaging using a tight-frame-
based algorithm. SIAM J. Imaging Sci. 6(1), 464–486 (2013)
47. Burnet, N., Scaife, J., Romanchikova, M., Thomas, S., et al.: Applying physical science techniques
and CERN technology to an unsolved problem in radiation treatment for cancer: the multidisciplinary
‘V oxTox’ research programme. CERN ideaSquare J. Exp. Innov. 1(1), 3–12 (2017)
48. Scaife, J., Harrison, K., Drew, A., et al.: Accuracy of manual and automated rectal contours using helical
tomotherapy image guidance scans during prostate radiotherapy. J. Clin. Oncol. 33(7-suppl), 94 (2015)
49. Cai, X., Schönlieb, C.-B., Lee, J., et al.: Automatic contouring of soft organs for image-guided prostate
radiotherapy. Radiother. Oncol. 119, 895–896 (2016)
50. Kanungo, T., Mount, D.M., Netanyahu, N.S., Piatko, C.D., Silverman, R., Wu, A.Y .: An efﬁcient k-
means clustering algorithm: analysis and implementation. IEEE Trans. Pattern Anal. Mach. Intell. 24(7),
881–892 (2002)
51. Cortes, C., V apnik, V .: Support-vector networks. Mach. Learn. 20(3), 273–297 (1995)
52. Khan, S., Madden, M.: One-class classiﬁcation: taxonomy of study and review of techniques. Knowl.
Eng. Rev. 9(3), 345–374 (2014)
53. Boyd, S., V andenberghe, L.: Convex Optimization. Cambridge University Press, New Y ork (2004)
54. Goldstein, T., Osher, S.: The split Bregman method for l1-regularized problems. SIAM J. Imaging Sci.
2(2), 323–343 (2009)
55. Chambolle, A., Pock, T.: A ﬁrst-order primal–dual algorithm for convex problems with applications to
imaging. J. Math. Imaging Vis. 40(1), 120–145 (2011)
56. Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J.: Distributed optimization and statistical learning
via the alternating direction method of multipliers. Found. Trends Mach. Learn. 3(1), 1–122 (2011)
57. Chan, R.H., Ng, M.: Conjugate gradient method for Toeplitz systems. SIAM Rev. 38, 427–482 (1996)
58. Chapelle, O., Schlkopf, B., Zien, A.: Semi-Supervised Learning, 1st edn. The MIT Press, Cambridge
(2006)
59. Silpa-Anan, C., Hartley, R.: Optimised KD-trees for fast image descriptor matching. In: IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1–8 (2008)
60. Subramanya, A., Bilmes, J.: Semi-supervised learning with measure propagation. J. Mach. Learn. Res.
12, 3311–3370 (2011)
61. Moya, M., Hush, D.: Network constraints and multi-objective optimization for one-class classiﬁcation.
Neural Netw. 9(3), 463–474 (1996)
62. Wang, B., Luo, X., Li, Z., Zhu, W., Shi, Z., Osher, S.: Deep neural nets with interpolating function as
output activation. In: Advances in Neural Information Processing Systems, vol. 32 (2018)
63. Wang, B., Osher, S.: Graph interpolating activation improves both natural and robust accuracies in data-
efﬁcient deep learning. Eur. J. Appl. Math. 32(3), 540–569 (2021)
Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and
institutional afﬁliations.
123
