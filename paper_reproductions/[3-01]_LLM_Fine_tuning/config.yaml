# LoRA微调配置文件

# 模型配置
model:
  # 基础模型名称或路径
  name: "gpt2"
  # 模型缓存目录
  cache_dir: "./cache"
  # 是否使用量化 (节省显存)
  load_in_8bit: false
  load_in_4bit: false

# LoRA配置
lora:
  # LoRA秩 (低秩矩阵的维度)
  r: 8
  # LoRA alpha参数 (缩放因子)
  alpha: 32
  # dropout率
  dropout: 0.05
  # 目标模块 (应用LoRA的层)
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  # 偏置类型 ("none", "all", "lora_only")
  bias: "none"
  # 任务类型 ("CAUSAL_LM", "SEQ_2_SEQ_LM", "TOKEN_CLS")
  task_type: "CAUSAL_LM"

# 训练配置
training:
  # 输出目录
  output_dir: "./outputs"
  # 批次大小
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  # 梯度累积步数
  gradient_accumulation_steps: 1
  # 学习率
  learning_rate: 3.0e-4
  # 训练轮数
  num_train_epochs: 3
  # 最大序列长度
  max_seq_length: 512
  #  warmup步数
  warmup_steps: 100
  # 日志记录步数
  logging_steps: 10
  # 评估步数
  eval_steps: 500
  # 保存步数
  save_steps: 500
  # 是否 fp16 训练
  fp16: false
  # 是否 bf16 训练 (Ampere GPU推荐)
  bf16: false
  # 梯度检查点 (节省显存)
  gradient_checkpointing: false
  # 优化器
  optim: "adamw_torch"
  # 权重衰减
  weight_decay: 0.001
  # 最大梯度范数 (用于梯度裁剪)
  max_grad_norm: 1.0

# 数据集配置
data:
  # 数据集名称或路径
  dataset_name: "wikitext"
  # 数据集配置
  dataset_config: "wikitext-2-raw-v1"
  # 文本字段名
  text_column: "text"
  # 训练集比例
  train_test_split: 0.1
  # 预处理批大小
  preprocessing_num_workers: 4

# 评估配置
evaluation:
  # 评估指标
  metrics:
    - "perplexity"
    - "bleu"
  # 是否生成样本
  generate_samples: true
  # 样本数量
  num_samples: 10

# 日志配置
logging:
  # 日志级别 (DEBUG, INFO, WARNING, ERROR)
  level: "INFO"
  # 是否使用 wandb
  use_wandb: false
  # wandb项目名
  wandb_project: "llm-finetuning"
  # 是否使用 tensorboard
  use_tensorboard: true
