# [3-01] å¤§æ¨¡å‹é«˜æ•ˆå¾®è°ƒ (LLM Fine-tuning)

## è®ºæ–‡ä¿¡æ¯

**æ ‡é¢˜**: Parameter-Efficient Fine-Tuning of Large Language Models

**ä½œè€…**: Xiaohao Cai ç­‰

**å‘è¡¨**: 2023

**è®ºæ–‡è·¯å¾„**: `xiaohao_cai_papers/[3-01] å¤§æ¨¡å‹é«˜æ•ˆå¾®è°ƒ LLM Fine-tuning.pdf`

---

## æ ¸å¿ƒè´¡çŒ®ç®€ä»‹

æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š

1. **LoRAæ”¹è¿›**: åœ¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åŸºç¡€ä¸Šæå‡ºæ–°çš„å‚æ•°åˆ†è§£ç­–ç•¥
2. **è®¡ç®—æ•ˆç‡**: å¤§å¹…å‡å°‘å¾®è°ƒæ‰€éœ€çš„è®¡ç®—èµ„æºå’Œå­˜å‚¨å¼€é”€
3. **æ€§èƒ½ä¿æŒ**: åœ¨å‚æ•°å‡å°‘çš„æƒ…å†µä¸‹ä¿æŒæ¨¡å‹æ€§èƒ½
4. **é€šç”¨æ€§**: æ–¹æ³•å¯åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡

### å…³é”®åˆ›æ–°ç‚¹

- **ä½ç§©åˆ†è§£ä¼˜åŒ–**: æ”¹è¿›ä¼ ç»Ÿçš„ä½ç§©é€‚åº”æ–¹æ³•
- **åŠ¨æ€ç§©é€‰æ‹©**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è‡ªé€‚åº”é€‰æ‹©ç§©çš„å¤§å°
- **æ¢¯åº¦ä¼˜åŒ–ç­–ç•¥**: è®¾è®¡é«˜æ•ˆçš„æ¢¯åº¦æ›´æ–°è§„åˆ™

---

## å¤ç°çŠ¶æ€

| ç»„ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|:---|:---:|:---|
| LoRAæ ¸å¿ƒå®ç° | ğŸŸ¡ è¿›è¡Œä¸­ | åŸºç¡€LoRAæ¨¡å—å·²å®Œæˆ |
| åŠ¨æ€ç§©é€‰æ‹© | ğŸ”´ å¾…å®Œæˆ | éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ |
| è®­ç»ƒè„šæœ¬ | ğŸŸ¡ è¿›è¡Œä¸­ | åŸºç¡€æ¡†æ¶å·²æ­å»º |
| è¯„ä¼°æŒ‡æ ‡ | ğŸ”´ å¾…å®Œæˆ | å¾…å®ç° |
| ç¤ºä¾‹ä»£ç  | ğŸŸ¡ è¿›è¡Œä¸­ | å¿«é€Ÿå¼€å§‹ç¤ºä¾‹å¯ç”¨ |

**æ€»ä½“çŠ¶æ€**: ğŸŸ¡ **è¿›è¡Œä¸­** (çº¦60%å®Œæˆ)

---

## æ–‡ä»¶ç»“æ„è¯´æ˜

```
[3-01]_LLM_Fine_tuning/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt             # Pythonä¾èµ–
â”œâ”€â”€ config.yaml                  # é…ç½®æ–‡ä»¶
â”œâ”€â”€ src/                         # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py             # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ lora_finetune.py        # LoRAå¾®è°ƒæ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ model.py                # æ¨¡å‹å®šä¹‰ä¸åŒ…è£…
â”‚   â”œâ”€â”€ dataset.py              # æ•°æ®å¤„ç†
â”‚   â””â”€â”€ trainer.py              # è®­ç»ƒå™¨
â””â”€â”€ examples/                    # ç¤ºä¾‹ä»£ç 
    â””â”€â”€ quickstart.py           # å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
```

---

## ä½¿ç”¨æ–¹æ³•

### ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»ç¯å¢ƒ (Windows)
venv\Scripts\activate

# æ¿€æ´»ç¯å¢ƒ (Linux/Mac)
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### å¿«é€Ÿå¼€å§‹

```bash
# è¿è¡Œå¿«é€Ÿå¼€å§‹ç¤ºä¾‹
python examples/quickstart.py
```

### é…ç½®æ–‡ä»¶è¯´æ˜

ç¼–è¾‘ `config.yaml` ä»¥è°ƒæ•´è®­ç»ƒå‚æ•°ï¼š

```yaml
model:
  name: "gpt2"  # åŸºç¡€æ¨¡å‹
  lora_rank: 8  # LoRAç§©

training:
  batch_size: 8
  learning_rate: 3e-4
  num_epochs: 3
```

### è‡ªå®šä¹‰è®­ç»ƒ

```python
from src.lora_finetune import LoRAModel
from src.dataset import load_dataset

# åŠ è½½æ¨¡å‹
model = LoRAModel(base_model="gpt2", lora_rank=8)

# åŠ è½½æ•°æ®
dataset = load_dataset("your_dataset")

# å¼€å§‹è®­ç»ƒ
model.finetune(dataset)
```

---

## ä¾èµ–è¦æ±‚

- Python >= 3.8
- PyTorch >= 2.0
- Transformers >= 4.30
- PEFT >= 0.4.0
- è¯¦è§ `requirements.txt`

---

## å‚è€ƒæ–‡çŒ®

1. Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ICLR 2022.
2. Cai, X., et al. (2023). Parameter-Efficient Fine-Tuning of Large Language Models.
3. Lewis, M., et al. (2020). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.

---

## æ›´æ–°æ—¥å¿—

- **2024-XX-XX**: åˆ›å»ºå¤ç°æ¡†æ¶
- **2024-XX-XX**: å®ç°åŸºç¡€LoRAæ¨¡å—

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒè®ºæ–‡æˆ–è”ç³»åŸä½œè€…ã€‚
