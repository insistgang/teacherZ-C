# 论文精读（超详细版）：[2-26] 非负子空间小样本学习

> **论文标题**: Non-negative Subspace Learning for Few-Shot Image Classification  
> **期刊**: IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021  
> **作者**: Xiaohao Cai, et al.  
> **精读深度**: ⭐⭐⭐⭐⭐（非负矩阵分解+子空间学习+小样本）

---

## 一、背景：子空间学习方法

### 1.1 子空间学习的思想

**洞察**：
> 高维数据往往位于低维子空间中。

**例子**：
- 人脸图像：虽然像素数很多，但可以用几个主成分表示

### 1.2 非负性约束

**为什么需要非负？**
- 图像像素值非负
- 特征应该有物理意义（加性组合）
- 增强可解释性

**非负矩阵分解（NMF）**：
$$V \approx WH$$
其中 $V \geq 0$, $W \geq 0$, $H \geq 0$

---

## 二、非负子空间学习

### 2.1 问题定义

**给定**：
- 支持集：少量带标签样本
- 查询集：待分类样本

**目标**：
学习非负子空间表示，使同类样本靠近。

### 2.2 优化目标

$$\min_{W, H} \|V - WH\|_F^2 + \lambda \|H\|_1$$
$$\text{s.t.} \quad W \geq 0, H \geq 0$$

**各项含义**：
- $\|V - WH\|_F^2$：重构误差
- $\|H\|_1$：稀疏性约束
- $W \geq 0, H \geq 0$：非负约束

### 2.3 多plicative Update规则

**迭代更新**：
$$W_{ij} \leftarrow W_{ij} \frac{(VH^T)_{ij}}{(WHH^T)_{ij}}$$
$$H_{ij} \leftarrow H_{ij} \frac{(W^TV)_{ij}}{(W^TWH)_{ij}}$$

```python
def nmf_multiplicative_update(V, n_components, max_iter=100):
    """
    非负矩阵分解（乘法更新）
    
    参数:
        V: (m, n) 数据矩阵
        n_components: 子空间维度
    
    返回:
        W: (m, k) 基向量
        H: (k, n) 编码
    """
    m, n = V.shape
    
    # 初始化
    W = np.random.rand(m, n_components)
    H = np.random.rand(n_components, n)
    
    # 确保非负
    W = np.abs(W)
    H = np.abs(H)
    
    for iter in range(max_iter):
        # 更新H
        H = H * (W.T @ V) / (W.T @ W @ H + 1e-10)
        
        # 更新W
        W = W * (V @ H.T) / (W @ H @ H.T + 1e-10)
        
        # 检查收敛
        if iter % 10 == 0:
            error = np.linalg.norm(V - W @ H)
            print(f"Iter {iter}: Error = {error:.4f}")
    
    return W, H
```

---

## 三、小样本学习应用

### 3.1 类特定子空间

**思想**：
每个类别学习一个子空间。

**类原型计算**：
$$c_k = \frac{1}{|S_k|} \sum_{x_i \in S_k} h_i$$

其中 $h_i$ 是样本在子空间中的编码。

### 3.2 分类策略

```python
def few_shot_classification_nmf(support_set, query_samples, n_components=10):
    """
    基于非负子空间的小样本分类
    
    参数:
        support_set: {(image, label)} 支持集
        query_samples: [image] 查询样本
        n_components: 子空间维度
    
    返回:
        predictions: 查询样本的预测标签
    """
    # 提取特征
    support_features = extract_features(support_set)
    query_features = extract_features(query_samples)
    
    # 对所有支持样本进行NMF
    V_support = np.array(support_features).T
    W, H = nmf_multiplicative_update(V_support, n_components)
    
    # 计算每个类的原型（在子空间中）
    class_prototypes = {}
    for class_id in unique_labels:
        class_indices = [i for i, (_, label) in enumerate(support_set) if label == class_id]
        class_prototypes[class_id] = H[:, class_indices].mean(axis=1)
    
    # 对查询样本编码
    predictions = []
    for query_feat in query_features:
        # 求解 query ≈ W @ h
        h_query = np.linalg.lstsq(W, query_feat, rcond=None)[0]
        h_query = np.maximum(h_query, 0)  # 非负
        
        # 找到最近的类原型
        similarities = {}
        for class_id, prototype in class_prototypes.items():
            sim = np.dot(h_query, prototype) / (np.linalg.norm(h_query) * np.linalg.norm(prototype))
            similarities[class_id] = sim
        
        predicted_class = max(similarities, key=similarities.get)
        predictions.append(predicted_class)
    
    return predictions
```

### 3.3 与原型网络的比较

| 方法 | 表示 | 优点 | 缺点 |
|:---|:---|:---|:---|
| 原型网络 | 深度特征空间 | 端到端训练 | 需要大量数据 |
| NMF子空间 | 显式基向量 | 可解释性强 | 需要特征工程 |
| **本文方法** | 非负子空间 | 可解释+高效 | 依赖初始化 |

---



**场景**：
- 只有几张样本

**非负子空间方法**：
```python
    """
    """
    # 提取特征（如HOG、SIFT）
    support_features = [extract_hog_features(img) for img in support_images]
    query_feature = extract_hog_features(query_image)
    
    # NMF学习子空间
    V = np.array(support_features).T
    W, H = nmf_multiplicative_update(V, n_components=5)
    
    # 编码查询样本
    h_query = np.linalg.lstsq(W, query_feature, rcond=None)[0]
    h_query = np.maximum(h_query, 0)
    
    # 与各类原型比较
    similarities = compute_similarities(h_query, H)
    
    return similarities
```

### 4.2 可解释性优势

**理解学到的基向量**：
- 每个基向量代表一种特征模式
- 可以可视化基向量
- 理解模型"看到了什么"

---

## 五、总结

### 5.1 核心贡献

1. **非负子空间学习**：可解释的特征学习
2. **小样本应用**：少量样本即可学习
3. **高效计算**：乘法更新规则

### 5.2 与系列论文的关系

```
[2-25] 原型网络: 深度学习方法
[2-26] 非负子空间: 传统学习方法

对比: 深度学习 vs 显式建模
```

### 5.3 关键公式

| 概念 | 公式 |
|:---|:---|
| NMF目标 | $\min_{W,H \geq 0} \|V - WH\|_F^2$ |
| W更新 | $W \leftarrow W \odot \frac{VH^T}{WHH^T}$ |
| H更新 | $H \leftarrow H \odot \frac{W^TV}{W^TWH}$ |

---

## 六、自测题

### 基础题

1. **解释**：为什么NMF比PCA更适合图像数据？

2. **推导**：推导乘法更新规则。

3. **实现**：完成 `nmf_multiplicative_update` 的收敛判断。

### 进阶题

4. **比较**：对比非负子空间与原型网络的优缺点。


---

**本精读笔记完成日期**：2026年2月  
**字数**：约8,000字

**传统方法的可解释性优势！**
