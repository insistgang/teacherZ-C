A TWO-STAGE CLASSIFICATION METHOD FOR
HIGH-DIMENSIONAL DATA AND POINT CLOUDS
XIAOHAO CAIâˆ—, RAYMOND CHANâ€ , XIAOYU XIEâ€¡, AND TIEYONG ZENGâ€¡
Abstract. High-dimensional data classiï¬cation is a fundamental task in machine learning and
imaging science.
In this paper, we propose a two-stage multiphase semi-supervised classiï¬cation
method for classifying high-dimensional data and unstructured point clouds. To begin with, a fuzzy
classiï¬cation method such as the standard support vector machine is used to generate a warm
initialization.
We then apply a two-stage approach named SaT (smoothing and thresholding) to
improve the classiï¬cation. In the ï¬rst stage, an unconstraint convex variational model is implemented
to purify and smooth the initialization, followed by the second stage which is to project the smoothed
partition obtained at stage one to a binary partition. These two stages can be repeated, with the
latest result as a new initialization, to keep improving the classiï¬cation quality. We show that the
convex model of the smoothing stage has a unique solution and can be solved by a speciï¬cally designed
primal-dual algorithm whose convergence is guaranteed. We test our method and compare it with
the state-of-the-art methods on several benchmark data sets. The experimental results demonstrate
clearly that our method is superior in both the classiï¬cation accuracy and computation speed for
high-dimensional data and point clouds.
Key words. Semi-supervised clustering, point cloud classiï¬cation, variational methods, Graph
Laplacian, SaT (smoothing and thresholding).
1. Introduction. Data sets classiï¬cation is a fundamental task in remote sens-
ing, machine learning, computer vision, and imaging science [1, 56, 43, 52, 38]. The
task, simply speaking, is to group the given data into diï¬€erent classes such that, on
one hand, data points within the same class shares similar characteristics (e.g. dis-
tance, edges, intensities, colors, and textures); on the other hand, pairs of diï¬€erent
classes are as dissimilar as possible with respect to certain features. In this paper, we
focus on the task of multi-class semi-supervised classiï¬cation. The total number of
classes K of the given data sets is assumed to be known, and a few samples, namely
the training points, in each class have been labeled. The goal is therefore to infer the
labels of the remaining data points using the knowledge of the labeled ones.
For data classiï¬cation, previous methods are generally based on graphical models,
see e.g. [1, 47, 56], and references therein. In a weighted graph, the data points are
vertices and the edge weights signify the aï¬ƒnity or similarity between pairs of data
points, where the larger the edge weight is, the closer or more similar the two vertices
are. The basic assumption for data classiï¬cation is that vertices in the graph that are
connected by edges with large weight should belong to the same class. Since a fully
connected graph is dense and has the size as large as the square of the number of
vertices, it is computationally expensive to work on it directly. In order to circumvent
this, some cleverly designed approximations have been developed. For example, in [33,
44], spectral approaches are proposed to eï¬ƒciently calculate the eigendecomposition
of a dense graph Laplacian. In [32, 42], the nearest neighbor strategy was adopted
to build up a sparse graph where most of its entries are zero, and therefore it is
computationally eï¬ƒcient.
âˆ—Mullard Space Science Laboratory (MSSL), University College London, Surrey RH5 6NT, UK.
Email: x.cai@ucl.ac.uk.
â€ Department of Mathematics, City University of Hong Kong, Kowloon Tong, Hong Kong. Email:
rchan.sci@cityu.edu.hk.
â€¡Department of Mathematics, The Chinese University of Hong Kong, Shatin, Hong Kong. Emails:
xyxie@math.cuhk.edu.hk and zeng@math.cuhk.edu.hk.
1
arXiv:1905.08538v1  [math.NA]  21 May 2019
In the literature, various studies for semi-supervised classiï¬cation have been per-
formed by computing a local minimizer of some non-convex energy functional or
minimizing a relevant convex relaxation. To name just a few, we have the diï¬€use in-
terface approaches using phase ï¬eld representation based on partial diï¬€erential equa-
tion techniques [4, 40], the MBO scheme established for solving the diï¬€usion equation
[45, 44, 33], and the variational methods based on graph cut [1, 56]. In particular,
in [1], the convex relaxation models and special constraints on class sizes were inves-
tigated. In [56], some novelty region-force terms were introduced in the variational
models to enforce the aï¬ƒnity between vertices and training samples. To the best of
our knowledge, all these proposed variational models have the so-called no vacuum
and overlap constraint on the labeling functions, which gives rise to non-convex model
with NP-hard issues. By allowing labeling functions to take values in the unit simplex,
the original NP-hard combinatorial problem is rephrased into a continuous setting,
see e.g. [1, 56, 43, 52, 57, 36, 10, 18] for various continuous relaxation techniques
(e.g. the ones based on solving the eigenvalue problem, convex approximation, or
non-linear optimization) and references therein.
Image segmentation can also be viewed as a special case of the data classiï¬ca-
tion problem [52, 8], since the pixels in an image can be treated as individual points.
Various studies and many algorithms have been considered for image segmentation.
In particular, variational methods are among the most successful image segmentation
techniques, see e.g. [46, 30, 13, 16, 31, 2, 9]. The Mumford-Shah model [46], one of
the most important variational segmentation models, was proposed to ï¬nd piecewise
smooth representations of diï¬€erent segments. It is, however, diï¬ƒcult to solve since the
model is non-convex and non-smooth. Then substantially rich follow-up works were
conducted, and many of them considered compromise techniques such as: (i) simpli-
fying the original complex model, e.g. ï¬nding piecewise constant solutions instead of
piecewise smooth solutions [27, 41, 55]); (ii) performing convex approximations, e.g.
using convex regularization terms like total variation [50, 22]; or (iii) using the smooth-
ing and thresholding (SAT) segmentation methodology [18, 17, 16, 21, 25]; for more
details refer to e.g. [26, 35, 11, 39, 26, 48, 49, 58, 5] and references therein. Moreover,
various applications were put forward for instance in optical ï¬‚ow [19], tomographic
imaging [3], and medical imaging [59, 14, 15, 12, 51, 20].
In this paper, we propose a multi-class semi-supervised data classiï¬cation method
based on the SaT segmentation methodology [18, 17, 16, 21, 25]. It has been shown
to be very promising in terms of segmentation quality and computation speed for im-
ages corrupted by many diï¬€erent types of blurs and noises. Brieï¬‚y speaking, the SaT
methodology includes two main steps: the ï¬rst step is to obtain a smooth approxima-
tion of the given image through minimizing some convex models; and the second step
is to get the segmentation results by thresholding the smooth approximation, e.g. us-
ing thresholds determined by the K-means algorithm [37]. Since the models used are
convex, the non-convex and NP-hard issues in many existing variational segmenta-
tion methods (e.g. the Mumford-Shah model and piecewise constant Mumford-Shah
model mentioned above) were naturally avoided.
Our proposed data classiï¬cation method mainly contains two stages with a warm
initialization. The warm initialization is a fuzzy classiï¬cation result which can be
generated by any standard classiï¬cation methods such as the support vector machine
(SVM) [29]; or by labeling the given data randomly if no proper method is available
for the given data (e.g. the data set is too large). Its accuracy is not critical since our
proposed method will improve the accuracy signiï¬cantly from this starting point.
2
With the warm initialization, the ï¬rst stage of our method is to ï¬nd a set of
smooth labeling functions, where each gives the probability of every point being in a
particular class. They are obtained by minimizing a properly-chosen convex objective
functional. In detail, the convex objective functional contains K independent convex
sub-minimization problems, where each corresponds to one labeling function, with no
constraints between these K labeling functions. For each sub-minimization problem,
the model is formed by three terms: (i) the data ï¬delity term restricting the distance
between the smooth labeling function and the initialization; (ii) the graph Laplacian
(â„“2-norm) term, and (iii) the total variation (â„“1-norm) built on the graph of the
given data. The graph Laplacian and the total variation terms regularize the labeling
functions to be smooth but at the same time close to a representation on the unit
simplex.
After obtaining the set of labeling functions, the second stage of our method is just
to project the fuzzy classiï¬cation results obtained at stage one onto the unit simplex
to obtain a binary classiï¬cation result. This step can be done straightforwardly. To
improve the classiï¬cation accuracy, these two stages can be repeated iteratively, where
at each iteration the result at the previous iteration is used as a new initialization.
The main advantage of our proposed method is twofold. First, it performs out-
standingly in computation speed, since the proposed model is convex and the K
sub-minimization problems are independent with each other (with no constraint on
the K labeling functions). The parallelism strategy can be applied straightforwardly
to improve computation performance further. On the contrary, the standard start-of-
the-art variational data classiï¬cation methods e.g. [56, 33, 40] have the constraint on
unit simplex in their minimization models, so that the non-convex or NP-hard issues
can aï¬€ect seriously the eï¬ƒciency of these methods, even though some convex relax-
ations may be applied. Secondly, our method is generally superior in classiï¬cation
accuracy, due to its ï¬‚exibility of merging the warm initialization and the two-stage
iterations which are tractable and manage to improve the accuracy gradually. Note
again that we are solving a convex model in the ï¬rst stage of each iteration, which
guarantees a unique global minimizer. In contrast, there is however no guarantee
that the results obtained by the standard start-of-the-art variational data classiï¬ca-
tion methods e.g. [56, 33, 40] are global minimizers. The eï¬€ectiveness of iterations in
our proposed method will be shown in the experiments. For most cases, the clustering
accuracy would be increased by a signiï¬cant margin compared to the ï¬rst initialization
and generally outperforms the state-of-the-art variational classiï¬cation methods.
The paper is organized as follows. In Section 2, we give the basic notation used
throughout the paper. In Section 3, we present our method for data sets classiï¬ca-
tion. In Section 4, we present the algorithm for solving the proposed model and its
convergence proof. In Section 5, we test our method on benchmark data sets and
compare it with the start-of-the-art methods. Conclusions are drawn in Section 6.
2. Basic notation. Let G = (V, E, w) be a weighted undirected graph repre-
senting a given point cloud, where V is the vertex set (in which each vertex represents
a point) containing N vertices, E is the edge set consisting of pairs of vertices, and
w : E â†’R+ is the weight function deï¬ned on the edges in E. The weights w(x, y)
on the edges (x, y) âˆˆE measure the similarity between the two vertices x and y; the
larger the weight is, the more similar (e.g. closer in distance) the pair of the vertices
is.
There are many diï¬€erent ways to deï¬ne the weight function.
Let d(Â·, Â·) be a
distance metric. Several particularly popular deï¬nitions of weight functions are as
3
follows: (i) radial basis function
w(x, y) := exp(âˆ’d(x, y)2/(2Î¾)),
âˆ€(x, y) âˆˆE,
(2.1)
for a preï¬xed constant Î¾ > 0; (ii) Zelnic-Manor and Perona weight function
w(x, y) := exp(âˆ’d(x, y)2/(var(x)var(y))),
âˆ€(x, y) âˆˆE,
(2.2)
where var(Â·) denotes the local variance; and (iii) the cosine similarity
w(x, y) :=
âŸ¨x, yâŸ©
p
âŸ¨x, xâŸ©âŸ¨y, yâŸ©
,
âˆ€(x, y) âˆˆE,
(2.3)
where âŸ¨Â·, Â·âŸ©is the inner product.
Let W = (w(x, y))(x,y)âˆˆE âˆˆRNÃ—N, the so-called aï¬ƒnity matrix, which is usually
assumed to be a symmetric matrix with non-negative entries. Let D = (h(x, y))(x,y)âˆˆE
âˆˆRNÃ—N be the diagonal matrix, where its diagonal entries are equal to the sum of
the entries on the same row in W, i.e.
h(x, y) :=
(P
zâˆˆV w(x, z),
x = y,
0,
otherwise.
(2.4)
Let u = (u(x))âŠ¤
xâˆˆV âˆˆRN, an N-length column vector. Deï¬ne the graph Laplacian as
L = D âˆ’W, and the gradient operator âˆ‡on u(x), âˆ€x âˆˆV , as
âˆ‡u(x) := (w(x, y)(u(x) âˆ’u(y)))(x,y)âˆˆE .
(2.5)
Then deï¬ne the â„“1-norm of an N-length vector as
âˆ¥âˆ‡uâˆ¥1 :=
X
xâˆˆV
|âˆ‡u(x)| =
X
(x,y)âˆˆE
|w(x, y)(u(x) âˆ’u(y))|,
(2.6)
and the â„“2-norm (also known as Dirichlet energy)
âˆ¥âˆ‡uâˆ¥2 := 1
2uâŠ¤Lu = 1
2
X
(x,y)âˆˆE
w(x, y)(u(x) âˆ’u(y))2.
(2.7)
Note, however, that working with the fully connected graph Eâ€”like the setting in
(2.5), (2.6) and (2.7)â€”can be highly computational demanding.
In order to reduce the computational burden, one often only considers the set of
edges with large weights. In this paper, the k-nearest-neighbor (k-NN) of a point x,
N(x), is used to replace the whole edge set starting from the point x in E. Besides
the computational saving, one additional beneï¬t of using k-NN graph is its capability
to capture local property of points lying close to a manifold. With the k-NN graph,
then the deï¬nitions in (2.5), (2.6) and (2.7) become
âˆ‡u(x) = (w(x, y)(u(x) âˆ’u(y)))yâˆˆN(x) ,
(2.8)
âˆ¥âˆ‡uâˆ¥1 :=
X
xâˆˆV
|âˆ‡u(x)| =
X
xâˆˆV
X
yâˆˆN(x)
|w(x, y)(u(x) âˆ’u(y))|,
(2.9)
and
âˆ¥âˆ‡uâˆ¥2 := 1
2uâŠ¤Lu = 1
2
X
xâˆˆV
X
yâˆˆN(x)
w(x, y)(u(x) âˆ’u(y))2,
(2.10)
respectively, see e.g. [40, 56] for more detail.
4
3. Proposed data classiï¬cation method.
3.1. Preliminary. Given a point cloud V containing N points in RM. We aim
to partition V into K classes V1, Â· Â· Â· , VK based on their similarities (the points in the
same class possess high similarity), with a set of training points T = {Tj}K
j=1 âŠ‚V ,
|T| = NT . Note that Tj âŠ‚Vj for j = 1, . . . , K. In other words, we aim to assign
the points in V \ T certain labels between 1 to K using the training set T in which
the labels of points are known, and the partition satisï¬es no vacuum and overlap
constraint:
V =
K
[
j=1
Vj
and
Vi âˆ©Vj = âˆ…,
âˆ€i Ì¸= j, 1 â‰¤i, j â‰¤K.
(3.1)
In the rest of the paper, we denote the points in V needed to be labeled as S = V \T,
and call S the test set in V .
The constraint (3.1) can be described by a binary matrix function U := (u1, Â· Â· Â· , uK) âˆˆ
RNÃ—K (also called partition matrix), with uj = (uj(x))âŠ¤
xâˆˆV âˆˆRN : V â†’{0, 1} de-
ï¬ned as
uj(x) :=
(
1,
x âˆˆVj,
0,
otherwise,
âˆ€x âˆˆV, j = 1, . . . , K.
(3.2)
Clearly, the above deï¬nition yields PK
j=1 uj(x) = 1, âˆ€x âˆˆV . The constraint (3.2) is
also known as the indicator constraint on the unit simplex. Since the binary repre-
sentation in (3.2) generally requires solving a non-convex model with NP-hard issue,
a common strategyâ€”the convex unit simplexâ€”is considered as an alternative
K
X
j=1
uj(x) = 1,
âˆ€x âˆˆV,
s.t.
uj(x) âˆˆ[0, 1], j = 1, . . . , K.
(3.3)
Note, importantly, that the convex constraint (3.3) can overcome the NP-hard issue
and make some subproblems convex, but generally the whole model can still be non-
convex. Therefore, solving a model with constraint (3.1), (3.2), or (3.3) can be time
consuming, see e.g. [40, 56] for more detail.
If a result satisfying (3.3) is not completely binary, a common way to obtain an
approximate binary solution satisfying (3.2) is to select the binary function as the
nearest vertex in the unit simplex by the magnitude of the components, i.e.
(u1(x), Â· Â· Â· , uK(x)) 7â†’ei,
where i = argmax
j
{uj(x)}K
j=1, âˆ€x âˆˆV.
(3.4)
Here, ei is the K-length unit normal vector which is 1 at the i-th component and 0
for all other components.
3.2. Proposed method. In this section, we present our novel two-stage method
for data (e.g. point cloud) classiï¬cation based on the SaT strategy which has been
validated very eï¬€ective in image segmentation. Our method can be summarized brieï¬‚y
as follows: ï¬rst, a classiï¬cation result is obtained as a warm initialization by using a
classical, fast, but need not be very accurate classiï¬cation method such as SVM [29];
then, a proposed two-stage iteration scheme is implemented until no change in the
labels of the test points could be made between consecutive iterations. Speciï¬cally,
5
at the ï¬rst stage, we propose to minimize a novel convex model free of constraint
(like those in (3.1), (3.2) and (3.3)), to obtain a fuzzy partition, say U, while keeping
the training labels unchanged; at the second stage, a binary result is obtained by
just applying the binary rule in (3.4) directly on the fuzzy partition obtained at the
ï¬rst stage. This binary result could be the ï¬nal classiï¬cation result for the original
classiï¬cation problem, or, if necessary, be set as a new initialization to search a better
one in the same manner. In the following, we give the details of each step.
Initialization. Given a point cloud V containing N points in RM and training
set T containing NT points with correct labels, we use SVM, which is a standard
and fast clustering method as an example, to obtain the ï¬rst clustering.
Let the
partition matrix be Ë†U = (Ë†u1, Â· Â· Â· , Ë†uK) âˆˆRNÃ—K, where Ë†uj = (Ë†uj(x))âŠ¤
xâˆˆV âˆˆRN for
j = 1, . . . , K. One could acquire an initialization by any other methods which have
better performance than SVM. If no proper method is available (e.g. the data set is too
large), then an initialization generated by setting labels to the test points randomly
can be used as an alternative.
Stage one. Now we put forward our convex model to ï¬nd a fuzzy partition U
with initialization Ë†U = (Ë†u1, Â· Â· Â· , Ë†uK). It is
argmin
U
K
X
j=1
Î²
2 âˆ¥uj âˆ’Ë†ujâˆ¥2
2 + Î±
2 uâŠ¤
j Luj + âˆ¥âˆ‡ujâˆ¥1

,
(3.5)
where the ï¬rst term is the data ï¬delity term constraining the fuzzy partition not far
away from the initialization; the second term is related to âˆ¥âˆ‡uâˆ¥2
2 with graph Laplacian
L; the last term is the total variation constructed on the graph; and Î±, Î² > 0 are
regularization parameters. Speciï¬cally, the second term in (3.5) is used to impose
smooth features on the labels of the points, and the last term is used to force the
points with similar information to group together.
We emphasize that we already have the labels on the points in the training set
T, with Â¯U = (Â¯u1, Â· Â· Â· , Â¯uK) âˆˆRNT Ã—K being the partition matrix on T, where Â¯uj =
(Â¯uj(x))âŠ¤
xâˆˆT âˆˆRNT for j = 1, . . . , K. Therefore, we only assign labels to points in the
test set S, i.e. we have
Ë†uj(x) = Â¯uj(x),
âˆ€x âˆˆT, j = 1, . . . , K.
(3.6)
Let Ë†uSj represent the part of Ë†uj deï¬ned on the test set S, then we have
Ë†uj = (Ë†uâŠ¤
Sj, Â¯uâŠ¤
j )âŠ¤,
j = 1, . . . , K.
(3.7)
We use analogous notations for the partition matrix U = (u1, Â· Â· Â· , uK), with
uj = (uâŠ¤
Sj, Â¯uâŠ¤
j )âŠ¤, j = 1, . . . , K.
(3.8)
In Section 4, (3.7) and (3.8) are going to be used to derive an eï¬ƒcient algorithm to
solve (3.5).
The following Theorem 3.1 proves that our proposed model (3.5) has a unique
solution.
Theorem 3.1. Given Ë†U âˆˆRNÃ—K and Î±, Î² > 0, the proposed model (3.5) has a
unique solution U âˆˆRNÃ—K.
Proof.
According to [7, Chapter 9], a strongly convex function has a unique
minimum. The conclusion follows easily from the strong convexity of model (3.5).
6
Many algorithms can be used to solve model (3.5) eï¬ƒciently due to the convexity
of the model without constraint. For example, the split-Bregman algorithm [34], which
is speciï¬cally devised for â„“1 regularized problems; the primal-dual algorithm [23],
which is designed to solve general saddle point problems; and the powerful alternative,
ADMM algorithm [6]. In particular, model (3.5) actually contains K independent
sub-minimization problems, where each corresponds to a labeling function uj, and
therefore the parallelism strategy is ideal to apply. This is an important advantage of
our method for large data sets. The algorithm aspects to solve our proposed convex
model (3.5) are detailed in Section 4.
Stage two. This stage is to project the fuzzy partition result U obtained at stage
one to a binary partition. Here, formula (3.4) is applied to the fuzzy partition U to
generate a binary partition, which naturally satisï¬es no vacuum and overlap constraint
(3.1). We remark that compared to the computation time at stage one, the time at
stage two is negligible.
Normally, the classiï¬cation work is complete after we obtain a binary partition
matrix at stage two. However, since the way of obtaining an initialization in our
scheme is open, and the quality of the initialization could be poor, we suggest going
back to stage one with the latest obtained partition as a new initialization and repeat
the above two stages until no more change in the partition matrix is observed. More
precisely, we set U as Ë†U and repeat Stages 1 and 2 again to obtain a new U. Then
the ï¬nal classiï¬cation result is the converged stationary partition matrix, say U âˆ—.
Moreover, to accelerate the convergence speed, we update Î² in (3.5) by a factor of
2 if we are to repeat the stages. This will obviously enforce the closeness between
two consecutive clustering results during iterations, which will ensure the algorithm
converges fast. We stop the algorithm when no changes are observed in the clustering
result compared to the previous one. We remark that a few iterations (â‰ˆ10) are
generally enough in practice, see the experimental results in Section 5 for more detail.
Note, importantly, that our classiï¬cation method here is totally diï¬€erent from
other variational methods like [40, 56] which need to minimize variational models
with constraint like (3.1), (3.2), (3.3), or other kinds of constraints (e.g. minimum
and maximum number of points imposed on individual classes Vi). Even though our
proposed model (3.5) has no constraint, the ï¬nal classiï¬cation result of our method
naturally satisï¬es no vacuum and overlap constraint (3.1). Therefore, our method is
much easier to solve for each iteration. Our proposed method, namely SaT (smoothing
and thresholding) method for high-dimensional data classiï¬cation, is summarized in
Algorithm 1.
Algorithm 1 SaT method for high-dimensional data classiï¬cation
Initialization: Generate initialization Ë†U by e.g. SVM method.
Output: Binary partition U âˆ—.
For l = 0, 1, . . . , until the stopping criterion reached (e.g. âˆ¥U (l) âˆ’U (l+1)âˆ¥= 0)
Stage one: Compute fuzzy partition U by solving model (3.5).
Stage two: Compute binary partition U (l+1) by using formula (3.4) on U.
Set Ë†U = U (l+1) and Î² = 2Î².
Endfor
Set U âˆ—= U (l+1).
4. Algorithm aspects. In this section, we present an algorithm to solve the
proposed convex model (3.5) based on the primal-dual algorithm [23] which is brieï¬‚y
7
recalled below.
4.1. Primal-dual algorithm. Let Xi be a ï¬nite dimensional vector space equipped
with a proper inner product âŸ¨Â·, Â·âŸ©Xi and norm âˆ¥Â· âˆ¥Xi, i = 1, 2. Let map K : X1 â†’X2
be a bounded linear operator. The primal-dual algorithm is, generally speaking, to
solve the following saddle-point problem
min
xâˆˆX1 max
ËœxâˆˆX2
n
âŸ¨Kx, ËœxâŸ©+ G(x) âˆ’Fâˆ—(Ëœx)
o
,
(4.1)
where G : X1 â†’[0, +âˆ], F : X2 â†’[0, +âˆ] are proper, convex, lower-semicontinuous
functions, and Fâˆ—represents the convex conjugate of F. Given proper initializations,
the primal-dual algorithm to solve (4.1) can be summarized in the following iterative
way of updating the primal and dual variables
Ëœx(l+1) = (I + Ïƒâˆ‚Fâˆ—)âˆ’1(Ëœx(l) + ÏƒKz(l)),
(4.2)
x(l+1) = (I + Ï„âˆ‚G)âˆ’1(x(l) âˆ’Ï„Kâˆ—Ëœx(l+1)),
(4.3)
z(l+1) = x(l+1) + Î¸(x(l+1) âˆ’x(l)),
(4.4)
where Î¸ âˆˆ[0, 1], Ï„, Ïƒ > 0 are algorithm parameters.
4.2. Algorithm to solve our proposed model. We ï¬rst deï¬ne some useful
notations which will be used to present our algorithm.
4.2.1. Preliminary. For ease of explanation, in the following, when we say
(i, j) âˆˆE, the i and j represent the i-th and j-th vertices in E, respectively. Let
Eâ€² =

(i, j) | i < j, âˆ€(i, j) âˆˆE
	
.
(4.5)
The graph laplacian L = D âˆ’W âˆˆRNÃ—N can be decomposed as
L =
X
(i,j)âˆˆEâ€²
Lij,
(4.6)
where
Lij =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
i
j
...
...
i
Â· Â· Â·
w(i, j)
Â· Â· Â·
âˆ’w(i, j)
Â· Â· Â·
...
...
j
Â· Â· Â·
âˆ’w(i, j)
Â· Â· Â·
w(i, j)
Â· Â· Â·
...
...
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
âˆˆRNÃ—N
(4.7)
is a matrix with only four nonzero entries which locate at positions (i, i), (i, j), (j, i)
and (j, j). Let Eâ€² = Eâ€²
a âˆªEâ€²
b âˆªEâ€²
c, where
Eâ€²
a =

(i, j) | i, j âˆˆS, âˆ€(i, j) âˆˆEâ€²	
,
(4.8)
Eâ€²
b =

(i, j) | i, j âˆˆT, âˆ€(i, j) âˆˆEâ€²	
,
(4.9)
Eâ€²
c = Eâ€² \ (Eâ€²
a âˆªEâ€²
b).
(4.10)
8
Then the decomposition L in (4.6) can be rewritten as
L =
X
(i,j)âˆˆEâ€²a
Lij +
X
(i,j)âˆˆEâ€²
b
Lij +
X
(i,j)âˆˆEâ€²c
Lij.
(4.11)
Note that, the terms P
(i,j)âˆˆEâ€²a Lij and P
(i,j)âˆˆEâ€²
b Lij only have nonzero entries which
are associated to the test set S and the training set T, respectively. Let
X
(i,j)âˆˆEâ€²a
Lij =
ï£«
ï£­
LS
0
0
0
ï£¶
ï£¸,
X
(i,j)âˆˆEâ€²
b
Lij =
ï£«
ï£­
0
0
0
Â¯L
ï£¶
ï£¸,
X
(i,j)âˆˆEâ€²c
Lij =
ï£«
ï£­
L1
L3
LâŠ¤
3
L2
ï£¶
ï£¸, (4.12)
where LS, L1 âˆˆR(Nâˆ’NT )Ã—(Nâˆ’NT ) are related to the test set S, Â¯L, L2 âˆˆRNT Ã—NT are
related to the training set T, and L3 âˆˆR(Nâˆ’NT )Ã—NT . Then we have
L =
ï£«
ï£­
LS + L1
L3
LâŠ¤
3
Â¯L + L2
ï£¶
ï£¸.
(4.13)
According to (2.8), the gradient operator âˆ‡can be regarded as a linear transfor-
mation between RN and RNÃ—(kâˆ’1) (where k = |N(x)|). For a vector uj = (uâŠ¤
Sj, Â¯uâŠ¤
j )âŠ¤
deï¬ned in (3.8), let
AS(uSj) = âˆ‡
uSj
0

âˆˆRNÃ—(kâˆ’1),
Hj = âˆ‡
 0
Â¯uj

âˆˆRNÃ—(kâˆ’1).
(4.14)
Clearly, AS : RNâˆ’NT â†’RNÃ—(kâˆ’1) is an operator corresponding to the test set S, and
Hj is the gradient matrix corresponding to the training set T which is ï¬xed since Â¯uj
is ï¬xed. Then, we have
âˆ‡uj = âˆ‡
uSj
Â¯uj

= âˆ‡
uSj
0

+ âˆ‡
 0
Â¯uj

= AS(uSj) + Hj.
(4.15)
4.2.2. Algorithm. Substituting the decomposition of L in (4.13) and âˆ‡in
(4.15), Ë†uj in (3.7) and uj in (3.8) into the proposed minimization model (3.5) yields
argmin
{uSj }K
j=1
K
X
j=1
Î²
2 âˆ¥Ë†uSj âˆ’uSjâˆ¥2
2 + Î±
2 uâŠ¤
SjLSuSj + Î±uâŠ¤
SjL3Â¯uj + âˆ¥AS(uSj) + Hjâˆ¥1

.
(4.16)
Note, obviously, that solving the above model (4.16) is equivalent to solving K sub-
minimization problems corresponding to each uSj, j = 1, . . . , K, which means that
our proposed model inherently beneï¬ts from the parallelism computation.
For 1 â‰¤j â‰¤K, let
Gj(uSj) = Î²
2 âˆ¥Ë†uSj âˆ’uSjâˆ¥2
2 + Î±
2 uâŠ¤
SjLSuSj + Î±uâŠ¤
SjL3Â¯uj,
(4.17)
Fj(Ëœx) = âˆ¥Ëœx + Hjâˆ¥1.
(4.18)
Using the deï¬nition of the â„“1-norm given in (2.9), the conjugate of Fj, Fâˆ—
j , can then
be calculated as
Fâˆ—
j (p) =
sup
ËœxâˆˆRNÃ—(kâˆ’1) âŸ¨Ëœx, pâŸ©âˆ’âˆ¥Ëœx + Hjâˆ¥1
= âˆ’âŸ¨p, HjâŸ©+ Ï‡P (p),
(4.19)
9
where P = {p âˆˆRNÃ—(kâˆ’1) : âˆ¥pâˆ¥âˆâ‰¤1}, and Ï‡P (p) is the characteristic function of
set P with value 0 if p âˆˆP, otherwise +âˆ.
Using the primal-dual formulation in (4.1) with the deï¬nitions of Gj and Fâˆ—
j
respectively given in (4.17) and (4.19), then the minimization problem (4.16) corre-
sponding to each uSj can be reformulated as
argmin
uSj
max
p
n
âŸ¨AS(uSj), pâŸ©+ Gj(uS) + âŸ¨p, hjâŸ©âˆ’Ï‡P (p)
o
.
(4.20)
To apply the primal-dual method, it remains to compute (I+Ïƒâˆ‚Fâˆ—
j )âˆ’1 and (I + Ï„âˆ‚Gj)âˆ’1.
Firstly, for âˆ€Ëœx âˆˆRNÃ—(kâˆ’1), we have
(I + Ïƒâˆ‚Fâˆ—)âˆ’1(Ëœx) =
argmin
pâˆˆRNÃ—(kâˆ’1) Fâˆ—
j (p) + 1
2Ïƒ âˆ¥p âˆ’Ëœxâˆ¥2
2
=
argmin
pâˆˆRNÃ—(kâˆ’1) Ï‡P (p) + 1
2Ïƒ âˆ¥p âˆ’Ëœxâˆ¥2
2 âˆ’âŸ¨p, HjâŸ©
=
argmin
pâˆˆRNÃ—(kâˆ’1) Ï‡P (p) + 1
2Ïƒ âˆ¥p âˆ’Ëœx âˆ’ÏƒHjâˆ¥2
2
= Î¹P (Ëœx + ÏƒHj),
(4.21)
where the operator Î¹P (Â·) is the pointwise projection operator onto the set P, i.e.,
âˆ€p âˆˆR,
Î¹P (p) =
(
1,
|p| > 1
p,
otherwise.
(4.22)
Secondly, for âˆ€x âˆˆRNâˆ’NT , we have
(I + Ï„âˆ‚Gj)âˆ’1(x) =
argmin
uSj âˆˆRNâˆ’NT
Gj(uSj) + 1
2Ï„ âˆ¥uSj âˆ’xâˆ¥2
2.
(4.23)
Using the deï¬nition of Gj(uSj) given in (4.17), problem (4.23) becomes solving the
following linear system
(Î±LS + Î²I + 1
Ï„ I)uSj = Î²Ë†uSj + 1
Ï„ x âˆ’Î±L3Â¯uj.
(4.24)
Since (Î±Â¯L + Î²I + 1
Ï„ I) is positive deï¬nite, the above linear system can be solved
eï¬ƒciently by e.g. conjugate gradient method [24].
Finally, by exploiting the strong convexity of Gj, âˆ€1 â‰¤j â‰¤K, which is shown in
the next lemma, [23] suggests that we could adaptively modify Ïƒ, Ï„ to accelerate the
convergence or the primal-dual method.
Lemma 4.1. The functions Gj, âˆ€1 â‰¤j â‰¤K are strongly convex with parameter
Î².
Proof. For simplicity, we omit the subscript j and Sj in the following proof. First,
by (4.12), LS is semi-positive deï¬nite. Therefore, ( Î±
2 uâŠ¤LSu + Î±uâŠ¤L3Â¯u) is convex.
Now the strong convexity of G follows from the fact that the remaining term in (4.17),
which is Î²
2 âˆ¥u âˆ’Ë†uâˆ¥2
2, is strongly convex with parameter Î².
The algorithm solving our proposed classiï¬cation model (4.16) (i.e. model (3.5))
is summarized in Algorithm 2, and its convergence proof is given in Theorem 4.2
10
below. For each sub-minimization problem, the relative error between two consecutive
iterations and/or a given maximum iteration number can be used as stopping criteria
to terminate the algorithm. Finally, we emphasize again that our method is quite
suitable for parallelism since the K sub-minimization problems are independent with
each other and therefore can be computed in parallel.
Algorithm 2 Algorithm solving the proposed model (4.16) (i.e. model (3.5))
Initialization: Ëœx(0) âˆˆRNÃ—(kâˆ’1), x(0), z(0) âˆˆRNâˆ’NT , Î¸ âˆˆ[0, 1], Ï„ (0), Ïƒ(0) > 0.
Output: {uSj}K
j=1.
For j = 1, Â· Â· Â· , K (parallelism strategy can be applied)
For l = 0, 1, . . . , until the stopping criterion reached
Let Ëœx = Ëœx(l) + Ïƒ(l)ASz(l), and compute Ëœx(l+1) = (I + Ïƒ(l)âˆ‚Fâˆ—)âˆ’1(Ëœx) by (4.21);
Let x = x(l) âˆ’Ï„ (l)Aâˆ—
SËœx(l+1), and compute x(l+1) = (I +Ï„ (l)âˆ‚G)âˆ’1(x) by (4.23);
Let Î¸(l) = 1/
p
1 + Î²Ï„ (l), and set Ï„ (l+1) = Î¸(l)Ï„ (l), Ïƒ(l+1) = Ïƒ(l)/Î¸(l);
Compute z(l+1) = x(l+1) + Î¸(x(l+1) âˆ’x(l));
Endfor
Set uSj = x(l+1).
Endfor
Theorem 4.2. Algorithm 2 converges if Ï„ (0)Ïƒ(0) <
1
N2(kâˆ’1).
Proof. By Theorem 2 in [23], Algorithm 2 converges as long as âˆ¥ASâˆ¥2
2 <
1
Ï„ (0)Ïƒ(0) .
Therefore it suï¬ƒces to ï¬nd a suitable upper bound for âˆ¥ASâˆ¥2. By our implementation
in (4.14) and since the weight functions (eqs. (2.1) to (2.3)) take value between [âˆ’1, 1],
each entry in AS is between [âˆ’1, 1]. Therefore, the 1-norm and âˆ-norm of AS can
be easily estimated as
âˆ¥ASâˆ¥1 =
max
1â‰¤jâ‰¤Nâˆ’NT
N(kâˆ’1)
X
i=1
|(AS)ij| â‰¤N(k âˆ’1)
and
âˆ¥ASâˆ¥âˆ=
max
1â‰¤iâ‰¤N(kâˆ’1)
Nâˆ’NT
X
j=1
|(AS)ij| â‰¤N âˆ’NT .
Now, we have
âˆ¥ASâˆ¥2 â‰¤
p
âˆ¥ASâˆ¥1âˆ¥ASâˆ¥âˆâ‰¤N
âˆš
k âˆ’1.
Therefore, we conclude that, the algorithm converges as long as we choose Ï„ (0), Ïƒ(0) >
0, such that Ï„ (0)Ïƒ(0) <
1
N2(kâˆ’1).
5. Numerical results. In this section, we evaluate the performance of our pro-
posed method on four benchmark data setsâ€”including Three Moon, COIL, Opt-
Digits and MINSTâ€”for semi-supervised learning.
Three Moon is a synthetic
data set which has been used frequently e.g. [33, 40, 56]. The COIL, Opt-Digits,
and MNIST data set can be found from the supplementary material of [28], the
UCI machine learning repository1, and the MNIST Database of Handwritten Digits2,
respectively. The basic properties of these test data sets are shown in Table 5.1.
1http://archive.ics.uci.edu/ml/datasets.html
2http://yann.lecun.com/exdb/mnist/
11
Table 5.1
Basic properties of the test benchmark data sets. â€œDimensionâ€ means the length of every vector
representing individual points in the given data sets.
Data set
Number of classes
Dimension
Number of points
Three Moon
3
100
1500
COIL
6
241
1500
Opt-Digits
10
64
5620
MNIST
10
784
70000
To implement our method, k-NN graphs are constructed for the test data sets,
using the randomized kd-tree [53] to ï¬nd the nearest neighbors with Euclidean dis-
tance as the metric. The radial basis function (2.1) is used to compute the weight
matrix W, except for the MNIST data set where the Zelnic-Manor and Perona weight
function (2.2) is used with eight closed neighbors. The training samples Tâ€”samples
with labels knownâ€”are selected randomly from each test data set. The classiï¬cation
accuracy is deï¬ned as the percentage of correctly labeled data points.
For our proposed method, the regularization parameter Î² is ï¬xed to 10âˆ’4 for
MNIST, 10âˆ’5 for COIL, and 10âˆ’2 for Three Moon, Opt-Digits. In practice, one
could choose the value of Î² based on the accuracy of initialization. The better the
initialized accuracy, the larger Î² we could choose. The regularization parameter Î± is
set to 1 for Three Moon, Opt-Digits, 0.4 for MNIST, and 10âˆ’2 for COIL. The
accuracy of the proposed method can be improved further after ï¬ne-tuning the values
of Î± and Î² for individual test data sets. All the codes were run on a MacBook with
2.8 GHz processor and 16 GB RAM, and Matlab 2017a.
5.1. Methods comparison. As mentioned in previous sections, we use SVM
method [29] to generate initializations for our proposed method. If it is not proper
for a data set (e.g. very slow due to the large size of the data set), we could just use
an initialization generated by assigning clustering labels randomly.
The SVM is a technique aiming to ï¬nd the best hyperplane that separates data
points of one class from the others.
In practice, data may not be separable by a
hyperplane. In that case, soft margin is used so that the hyperplane would separate
many data points if not all. It is also common to kernelize data points, and then
ï¬nd separating hyperplane in the transformed space. The SVM method used in our
experiments is trained with linear kernel.
We compare our proposed method with the state-of-the-art methods proposed
recently, e.g. CVM [1], GL [33], MBO [33], TVRF [56], LapRF [56], LapRLS [54],
MP [54], and SQ-Loss-I [28]. The code TVRF was provided by the authors and the
parameters used in it were chosen by trial and error to give the best results. The
classiï¬cation accuracies of methods GL, MBO, LapRF, LapRLS, MP and SQ-Loss-I
were taken from [1, 56], in which methods CVM and TVRF were shown to be superior
in most of the cases.
5.2. Three moon data. The synthetic Three Moon data used here is con-
structed by following the way performed in [1, 56] exactly.
We brieï¬‚y repeat the
procedure as follows. First, generate three half circles in R2â€”two half top unit circles
and one half bottom circle with radius 1.5 which are centered at (0, 0), (3, 0) and
(1.5, 0.4), respectively. Then 500 points are uniformly sampled from each half circle
and embedded into R100 by appending zeros to the remaining dimensions. Finally,
an i.i.d Gaussian noise with standard deviation 0.14 is added to each dimension of
12
the data. An illustration of the ï¬rst two dimensions of the Three Moon data is
shown in Fig. 5.1 (a) where diï¬€erent colors are applied on each half circle. This is
a three-class classiï¬cation problem with the goal of classifying each half circle using
a small number of supervised points from each class. This classiï¬cation problem is
challenging due to the noise and the high dimensionality of all the points with high
similarity in R98.
(a) Ground truth
(b) TVRF
(c) Our proposed method
Fig. 5.1. Three-class classiï¬cation for Three Moon synthetic data. (a) Ground truth; (b) and
(c) Results of method TVRF [56] and our proposed method, respectively.
A k-NN graph with k = 10 is built for this data set, parameter Ïƒ = 3 is used in
the Gaussian weight function, and the distance metric chosen is Euclidean metric for
R100. We ï¬rst test the methods using uniformly distributed supervised points, where
a total number of 75 points are sampled uniformly from this data set as training
points.
The accuracies of method TVRF and ours are obtained by running the methods
ten times with randomly selected labeled samples, and taking the average of the
accuracies. The accuracies of method CVM are copied from the original paper [1].
The accuracy comparison is reported in Table 5.2, which shows that our proposed
method gives the highest accuracy; also, see Fig.
5.1 for visual validation of the
results between methods of TVRF and ours. The average number of iterations taken
for our proposed method is 3.8. Fig. 5.4 (a) gives the convergence history and partition
13
Table 5.2
Accuracy comparison for Three
Moon synthetic data set, with uniformly
selected training points.
Method
Accuracy(%)
CVM
98.7
GL
98.4
MBO
99.1
TVRF
98.6
LapRF
98.4
Proposed
99.4
Table 5.3
Accuracy comparison for Three
Moon synthetic data set,
with non-
uniformly selected training points.
Method
Accuracy(%)
TVRF
97.8
Proposed
99.3
accuracy of our proposed method corresponding to iteration steps, which clearly shows
the accuracy increment during iterations (note that the accuracy at iteration 0 is the
result of the initialization which is obtained by SVM method). Table 5.7 reports the
comparison in terms of computation time, which indicates the superior performance
of our proposed method in computation speed.
In the following, as a showcase, we test the methods using non-uniformly dis-
tributed supervised points, which is used to investigate the robustness of these meth-
ods on training points. In this case for the 75 training points, as an example, we
respectively pick 5 points from the left and the bottom half circles, and pick the rest
65 points from the right half circle. This sampling is illustrated in Fig 5.2.
Fig. 5.2. Unbalanced sampling from Three Moon data, where sampled points are highlighted
with their corresponding labels.
The accuracies of TVRF and our method are shown in Table 5.3, from which
we see clearly that the proposed method gives much higher accuracy. In particular,
compared to the results in Table 5.2 using training points chosen uniformly, while
the accuracy of TVRF method decreases by 0.8 percent, we observe only a very
small decrease in our proposed method. This shows the robustness of our method
with respect to the way that training points are selected. Note that in the case of
training points chosen non-uniformly, the initialization obtained by SVM is poor,
because of which more iterations are needed to converge for our methodâ€”average
12.0 iterations in 10 trials versus 3.3 iterations needed for the case of training points
selected uniformly.
14
5.3. COIL data. The benchmark COIL data comes from the Columbia object
image library. It contains a set of color images of 100 diï¬€erent objects. These images,
with size 128 Ã— 128 each, are taken from diï¬€erent angles in steps of 5 degrees, i.e.,
72 (= 360/5) images for each object. In the following, without loss of generality, we
also call an image a point for ease of reference. The test data set here is constructed
the same way as depicted in e.g. [1, 56] and is brieï¬‚y described as follows. First,
the red channel of each image is down-sampled to 16 Ã— 16 pixels by averaging over
blocks of 8 Ã— 8 pixels. Then, 24 out of the 100 objects are randomly selected, which
amounts to 1728 (= 24 Ã— 360/5) images. After that, these 24 objects are partitioned
into six classes with four objectsâ€”288 images (= 4Ã—72)â€”in each class. Finally, after
discarding 38 images randomly from each class, a data set of 1500 images where 250
images in each of the six classes are constructed. To construct a graph, each image,
which is a vector with length 241, is treated as a node on the graph,
For accuracy test, a k-NN graph with k = 4 is built for this data set, parameter
Ïƒ = 250 is used in the Gaussian weight function, and the distance metric chosen
is Euclidean metric for R241.
The training points, amount to 10% of the points,
are selected randomly from the data set. Again, we run the test methods 10 times
and compare the average accuracy. The resulting accuracies are listed in Table 5.4,
showing that our method outperforms other methods. Moreover, the average number
of iterations of our method is 12.2. Fig. 5.4 (b) gives the convergence history of
our proposed method in partition accuracy corresponding to iterations, which again
shows an increasing trend in accuracy.
Table 5.4
Accuracy
comparison
for
COIL
data
set,
with
uniformly
selected training points.
Method
Accuracy(%)
CVM
93.3
TVRF
92.5
LapRF
87.7
GL
91.2
MBO
91.5
Proposed
94.0
Table 5.5
Accuracy
comparison
for
MINST data set, with uniformly
selected training points.
Method
Accuracy(%)
CVM
97.7
TVRF
96.9
LapRF
96.9
GL
96.8
MBO
96.9
Proposed
97.5
5.4. MNIST data. The MNIST data set consists of 70,000 images of handwrit-
ten digits 0â€“9, where each image has a size of 28 Ã— 28. Fig. 5.3 shows some images of
the ten digits from the data set. Each image is a node on a constructed graph. The
objective is to classify the data set into 10 disjoint classes corresponding to diï¬€erent
digits. For accuracy test, a k-NN graph with k = 8 is built for this data set, and
Zelnik-Manor and Perona weight function (2.2) is used to compute the weight ma-
trix. The training 2500 (3.57%) points (images) are selected randomly from the total
70,000 points.
The experimental results of the test methods are obtained by running them 10
times with randomly selected training set with a ï¬xed number of points 2500, and
the average accuracy is computed for comparison. The accuracies of the test results
are shown in Table 5.5, which indicates that our method is comparable to or better
than the state-of-the-art methods compared here. Table 5.7 shows the computation
time comparison, from which we again see that our method is very competitive in
15
computation speed. The convergence history of our proposed method in partition
accuracy corresponding to iterations is given in Fig. 5.4 (c), which also demonstrates
a clear increasing trend in accuracy.
Fig. 5.3. Examples of digits 0â€“9 from the MNIST data set.
5.5. Opt-Digits data. The Opt-Digits data set is constructed as follows. It
contains 5620 bitmaps of handwritten digits (i.e. 0â€“9). Each bitmap has the size of
32 Ã— 32 and is divided into non-overlapping blocks of 4 Ã— 4, and then the number
of â€œonâ€ pixels are counted in each block. Therefore, each bitmap corresponds to a
matrix of 8 Ã— 8 where each element is an integer in [0, 16]. The classiï¬cation problem
is to partition the data set into 10 classes.
For accuracy test, a k-NN graph with k = 8 is built for this data set, parameter
Ïƒ = 30 is used in the Gaussian weight function, and the distance metric chosen is
Euclidean metric for R64. For the experiments on this data set, we generate three
training sets respectively with the number of points 50, 100 and 150, which are all
selected randomly. All the methods are run 10 times for each training set and the
average accuracy is used for comparison. The quantitative results in accuracy are
listed in Table 5.6, from which we see that our proposed method is consistently better
than the state-of-the-art methods compared for all the cases. We also observe the
improvement of the accuracy of these methods w.r.t. the increasing number of points
in the training set. Finally, we show the convergence history of our proposed method
in partition accuracy corresponding to iterations using the training set with 150 points
in Fig. 5.4 (d), which again clearly shows an increasing trend in accuracy.
Table 5.6
Accuracy comparison for Opt-Digits data set, with uniformly selected training points.
Sample rate
0.89%(50)
1.78%(100)
2.67%(150)
k-NN
85.5
92.0
93.8
SGT
91.4
97.4
97.4
LapRLS
92.3
97.6
97.3
SQ-Loss-I
95.9
97.3
97.7
MP
94.7
97.0
97.1
TVRF
95.9
98.3
98.2
LapRF
94.1
97.7
98.1
Proposed
96.6
98.5
98.6
5.6. Further discussion. The above experimental results on the benchmark
data sets in terms of classiï¬cation accuracy, shown in Tables 5.2â€“5.6, indicate that
16
1
2
3
4
Iterations
0.95
0.96
0.97
0.98
0.99
1
Accuracy
2
4
6
8
10
12
Iterations
0.9
0.91
0.92
0.93
0.94
Accuracy
(a) Three Moon
(b) COIL
2
4
6
8
10
Iterations
0.966
0.968
0.97
0.972
0.974
Accuracy
2
4
6
8
10
Iterations
0.976
0.978
0.98
0.982
0.984
0.986
Accuracy
(c) MINST
(d) Opt-Digits
Fig. 5.4. Accuracy convergence history of our proposed method corresponding to iteration steps
for all the test data sets; training samples are uniformly selected in each class. Blue curves corre-
spond to cases with the least number of iterations among the 10 trials and orange curves correspond
to cases with the largest number of iterations among the 10 trials.
Table 5.7
Computation time comparison in seconds. The value in the brackets of our method represents
the average number of iterations of the 10 trials. (For Opt-Digits data, we select 100 sample points.)
Method
Computation time in seconds
Three Moon
COIL
MINST
Opt-Digits
TVRF
0.71
0.65
66.00
3.42
Proposed
0.30 (3.3)
0.76 (11.7)
82.04 (9.4)
4.45 (9.3)
our proposed method outperforms the state-of-the-art methods for high-dimensional
data and point clouds classiï¬cation.
Compared to the start-of-the-art variational classiï¬cation models proposed e.g.
in [1, 56], in addition to the data ï¬delity term and â„“1 term (e.g. TV), our proposed
model in (3.5) contains an additional â„“2 term on the labeling functions which is used
to smooth the classiï¬cation results so as to reduce the non-smooth artifact (the so-
called staircase artifact in images) introduced by the â„“1 term. This is one reason that
our method can generally achieve better results. Moreover, the warm initialization
used in our method can also play a role to improve the classiï¬cation quality. Apart
from generating the initialization manually, any classiï¬cation methods can practically
be used to generate the initialization. Starting from the initialization, our proposed
17
method can then be applied to achieve a better classiï¬cation result by improving the
accuracy iteratively. Theoretically speaking, the poorer the quality of the initializa-
tion, the more iterations are needed for our method. Nevertheless, we found that
even for poor initializations (e.g. the ones generated randomly), 20 iterations are
already enough to achieve competitive results. Generally, no more than 15 iterations
are needed when using an initialization computed by standard classiï¬cation methods
(e.g. SVM).
Another distinction of our proposed model compared to the variational classiï¬ca-
tion models in e.g. [1, 56] is that there are no constraints on these labeling functions
in our objective functional. In other words, in each iteration, we just need to ï¬nd
the minimizer of the objective functional corresponding to each labeling function, but
these minimizers do not need to satisfy the constraint that their summation equal
to 1. Therefore, the computation speed for every single iteration is improved in our
method compared to other methods which have constraints. We emphasize again that,
since minimizing each sub-problem with respect to each labeling function is irrelevant
to minimizing the sub-problems with respect to other labeling functions, parallelism
techniques can be used straightforwardly to further improve the computation perfor-
mance of our algorithm; theoretically, we just require 1/K of the computation time
needed for the non-parallelism scheme. This will be extremely important for large
data sets. From Table 5.7, we see that, for all the computation time of our method,
when considering the eï¬€ect of parallel computing, our method should be able to out-
perform the state-of-the-art methods by a large margin.
6. Conclusions. In this paper, a two-stage multiphase semi-supervised method
is proposed for classifying high-dimensional data or unstructured point clouds. The
method is based on the SaT strategy which has been shown very eï¬€ective for seg-
mentation problems such as gray or color images corrupted by diï¬€erent degradations.
Starting with a proper initialization which can be obtained by using any standard
classiï¬cation algorithm (e.g. SVM) or constructed by users, the ï¬rst stage of our
method is to solve a convex variational model without constraint. Most importantly,
our proposed model is a lot easier to solve than the state-of-the-art variational models
proposed recently (e.g. [1, 56]) for point clouds classiï¬cation problem since they all
need no vacuum and overlap constraint (3.1) on the labeling functions in the unit
simplex which could make their models to be non-convex. The second stage of our
method is to ï¬nd a binary partition via thresholding the smoothed result obtained
from stage one. We proved that our proposed model has a unique solution and the
derived primal-dual algorithm converges.
We tested our proposed method on four benchmark data sets and compared with
the state-of-the-art methods. We also investigated the inï¬‚uence of the training sets
selected uniformly and non-uniformly. For our method, diï¬€erent ways of generating
initializations were implemented and validated. On the whole, the experimental re-
sults demonstrated that our method is superior in terms of classiï¬cation accuracy and
when parallel computing is considered, computation speed too. Therefore our method
is an eï¬ƒcient and eï¬€ective classiï¬cation method for data sets like high-dimensional
data or unstructured point clouds.
Acknowledgements. This work of R. Chan is partially supported by HKRGC
Grants No. CityU12500915, CityU14306316, HKRGC CRF Grant C1007-15G, and
HKRGC AoE Grant AoE/M-05/12. This work of T. Zeng is partially supported by
the National Natural Science Foundation of China under Grant 11671002, CUHK
start-up and CUHK DAG 4053296, 4053342. We thank Prof. Xue-Cheng Tai, Dr.
18
Ke Yin, Dr. Egil Bae and Prof. Ekaterina Merkurjev for providing the codes of their
methods [1, 56].
REFERENCES
[1] E. Bae and E. Merkurjev, Convex variational methods on graphs for multiclass segmentation
of high-dimensional data and point clouds, Journal of Mathematical Imaging and Vision,
58 (2017), pp. 468â€“493.
[2] L. Bar, T. F. Chan, G. Chung, M. Jung, N. Kiryati, R. Mohieddine, N. Sochen, and
L. A. Vese, Mumford and Shah model and its applications to image segmentation and
image restoration, Springer New York, New York, NY, 2011, pp. 1095â€“1157.
[3] B. Bauer, X. Cai, S. Peth, K. Schladitz, and G. Steidl, Variational-based segmentation
of bio-pores in tomographic images, Computers & Geosciences, 98 (2017), pp. 1â€“8.
[4] A. Bertozzi and A. Flenner, Diï¬€use interface models on graphs for classiï¬cation of high
dimensional data, Multiscale Modeling and Simulation, 10 (2012), pp. 1090â€“1118.
[5] J. C. Bezdek, R. Ehrlich, and W. Full, FCM: The Fuzzy C-means clustering algorithm,
Computers & Geosciences, 10 (1984), pp. 191â€“203.
[6] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, Distributed optimization and
statistical learning via the alternating direction method of multipliers, Foundations and
Trends in Machine Learning, 3 (2011), pp. 1â€“122.
[7] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, New York,
NY, USA, 2004.
[8] Y. Boykov and G. Funka-Lea, Graph cuts and eï¬ƒcient N-D image segmentation, Interna-
tional Journal of Computer Vision, 70 (2006), pp. 109â€“131.
[9] X. Bresson, S. Esedoglu, P. Vandergheynst, J. Thiran, and S. Osher, Fast global mini-
mization of the active contour/snake model, Journal of Mathematical Imaging and Vision,
28 (2007), pp. 151â€“167.
[10] X. Bresson, X.-C. Tai, T. F. Chan, and A. Szlam, Multi-class transductive learning based
on l1 relaxations of cheeger cut and mumford-shah-potts model, Journal of Mathematical
Imaging and Vision, 49 (2014), pp. 191â€“201.
[11] E. Brown, T. Chan, and X. Bresson, Completely convex formulation of the Chan-Vese image
segmentation model, International Journal of Computer Vision, 98 (2012), pp. 103â€“121.
[12] N. Burnet, J. Scaife, M. Romanchikova, S. Thomas, and et al., Applying physical sci-
ence techniques and CERN technology to an unsolved problem in radiation treatment for
cancer: the multidisciplinary â€˜VoxToxâ€™ research programme, CERN ideaSquare journal of
experimental innovation, 1 (2017).
[13] X. Cai, Variational image segmentation model coupled with image restoration achievements,
Pattern Recognition, 48 (2015), pp. 2029â€“2042.
[14] X. Cai, R. H. Chan, S. Morigi, and F. Sgallari, Framelet-based algorithm for segmentation
of tubular structures, in Scale Space and Variational Methods in Computer Vision, A. M.
Bruckstein, B. M. ter Haar Romeny, A. M. Bronstein, and M. M. Bronstein, eds., Berlin,
Heidelberg, 2012, Springer Berlin Heidelberg, pp. 411â€“422.
[15] X. Cai, R. H. Chan, S. Morigi, and F. Sgallari, Vessel segmentation in medical imaging
using a tight-frameâ€“based algorithm, SIAM Journal on Imaging Sciences, 6 (2013), pp. 464â€“
486.
[16] X. Cai, R. H. Chan, M. Nikolova, and T. Zeng, A three-stage approach for segmenting
degraded color images: Smoothing, lifting and thresholding (SLaT), Journal of Scientiï¬c
Computing, 72 (2017), pp. 1313â€“1332.
[17] X. Cai, R. H. Chan, C.-B. SchÂ¨onlieb, G. Steidl, and T. Zeng, Linkage between piecewise
constant Mumford-Shah model and ROF model and its virtue in image segmentation,
arXiv:1807.10194, (2018).
[18] X. Cai, R. H. Chan, and T. Zeng, A two-stage image segmentation method using a convex
variant of the Mumford-Shah model and thresholding, SIAM Journal on Imaging Sciences,
6 (2013), pp. 368â€“390.
[19] X. Cai, J. Fitschen, M. Nikolova, G. Steidl, and M. Storath, Disparity and optical ï¬‚ow
partitioning using extended Potts priors, Information and Inference: A Journal of the IMA,
4 (2014), pp. 43â€“62.
[20] X. Cai, C.-B. SchÂ¨onlieb, J. Lee, and et al., Automatic contouring of soft organs for image-
guided prostate radiotherapy, Radiotherapy and Oncology, 119 (2016), pp. S895â€“S896.
[21] X. Cai and G. Steidl, Multiclass segmentation by iterated ROF thresholding, in Energy Min-
imization Methods in Computer Vision and Pattern Recognition, A. Heyden, F. Kahl,
19
C. Olsson, M. Oskarsson, and X.-C. Tai, eds., Berlin, Heidelberg, 2013, Springer Berlin
Heidelberg, pp. 237â€“250.
[22] A. Chambolle, M. Novaga, D. Cremers, and T. Pock, An introduction to total variation for
image analysis, in in Theoretical Foundations and Numerical Methods for Sparse Recovery,
De Gruyter, 2010.
[23] A. Chambolle and T. Pock, A ï¬rst-order primal-dual algorithm for convex problems with
applications to imaging, Journal of Mathematical Imaging and Vision, 40 (2011), pp. 120â€“
145.
[24] R. H. Chan and M. Ng, Conjugate gradient method for Toeplitz systems, SIAM Review, 38
(1996), pp. 427â€“482.
[25] R. H. Chan, H. Yang, and T. Zeng, A two-stage image segmentation method for blurry
images with Poisson or multiplicative Gamma noise, SIAM Journal on Imaging Sciences,
7 (2014), pp. 98â€“127.
[26] T. F. Chan, S. Esedoglu, and M. Nikolova, Algorithms for ï¬nding global minimizers of
image segmentation and denoising models, SIAM Journal on Applied Mathematics, 66
(2006), pp. 1632â€“1648.
[27] T. F. Chan and L. A. Vese, Active contours without edges, IEEE Transactions on Image
Processing, 10 (2001), pp. 266â€“277.
[28] O. Chapelle, B. Schlkopf, and A. Zien, Semi-Supervised Learning, The MIT Press, 1st ed.,
2010.
[29] C. Cortes and V. Vapnik, Support-vector networks, Machine Learning, 20 (1995), pp. 273â€“
297.
[30] D. Cremers, M. Rousson, and R. Deriche, A review of statistical approaches to level set
segmentation: integrating color, texture, motion and shape, International Journal of Com-
puter Vision, 72 (2007), pp. 195â€“215.
[31] B. Dong, A. Chien, and Z. Shen, Frame based segmentation for medical images, Communi-
cations in Mathematical Sciences, 9 (2010), pp. 551â€“559.
[32] A. Elmoataz, O. Lezoray, and S. Bougleux, Nonlocal discrete regularization on weighted
graphs: a framework for image and manifold processing, IEEE Transactions on Image
Processing, 17 (2008), pp. 1047â€“1060.
[33] C. Garcia-Cardona, E. Merkurjev, A. L. Bertozzi, A. Flenner, and A. G. Percus,
Multiclass data segmentation using diï¬€use interface methods on graphs, IEEE Transactions
on Pattern Analysis and Machine Intelligence, 36 (2014), pp. 1600â€“1613.
[34] T. Goldstein and S. Osher, The split Bregman method for l1-regularized problems, SIAM
Journal on Imaging Sciences, 2 (2009), pp. 323â€“343.
[35] Y. He, M. Y. Hussaini, J. Ma, B. Shafei, and G. Steidl, A new Fuzzy C-means method with
total variation regularization for image segmentation of images with noisy and incomplete
data, Pattern Recognition, 45 (2012), pp. 3463â€“3471.
[36] M. Hein and S. Setzer, Beyond spectral clustering - tight relaxations of balanced graph cuts,
in Advances in Neural Information Processing Systems 24, J. Shawe-Taylor, R. S. Zemel,
P. L. Bartlett, F. Pereira, and K. Q. Weinberger, eds., Curran Associates, Inc., 2011,
pp. 2366â€“2374.
[37] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu,
An eï¬ƒcient k-means clustering algorithm: analysis and implementation, IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 24 (2002), pp. 881â€“892.
[38] J. Lee, X. Cai, J. Lellmann, M. Dalponte, and et al., Individual tree species classiï¬cation
from airborne multisensor imagery using robust PCA, IEEE Journal of Selected Topics in
Applied Earth Observations and Remote Sensing, 9 (2016), pp. 2554â€“2567.
[39] J. Lellmann and C. SchnÂ¨orr, Continuous multiclass labeling approaches and algorithms,
SIAM Journal on Imaging Sciences, 44 (2011), pp. 1049â€“1096.
[40] O. LÂ´ezoray, A. Elmoataz, and V. T. Ta, Nonlocal PDEs on graphs for active contours
models with applications to image segmentation and data clustering, IEEE International
Conference on Acoustics, Speech and Signal Processing, (2012), pp. 873â€“876.
[41] F. Li, M. Ng, T. Zeng, and C. Shen, A multiphase image segmentation method based on
fuzzy region competition, SIAM Journal on Imaging Sciences, 3 (2010), pp. 277â€“299.
[42] E. Merkurjev, E. Bae, A. L. Bertozzi, and X.-C. Tai, Global binary optimization on graphs
for classiï¬cation of high-dimensional data, Journal of Mathematical Imaging and Vision,
52 (2015), pp. 414â€“435.
[43] E. Merkurjev, A. Bertozzi, X. Yan, and K. Lerman, Modiï¬ed Cheeger and ratio cut meth-
ods using the Ginzburg-Landau functional for classiï¬cation of high-dimensional data, In-
verse Problems, 33 (2017), p. 074003.
[44] E. Merkurjev, T. Kostic, and A. Bertozzi, An MBO scheme on graphs for classiï¬cation
20
and image processing, SIAM Journal on Imaging Sciences, 6 (2013), pp. 1903â€“1930.
[45] B. Merriman and S. J. Ruuth, Diï¬€usion generated motion of curves on surfaces, Journal of
Computational Physics, 225 (2007), pp. 2267â€“2282.
[46] D. Mumford and J. Shah, Optimal approximations by piecewise smooth functions and associ-
ated variational problems, Communications on Pure and Applied Mathematics, 42 (1989),
pp. 577â€“685.
[47] B. Osting, C. White, and E. Oudet, Minimal Dirichlet energy partitions for graphs, SIAM
Journal on Imaging Sciences, 36 (2014), pp. 1635â€“1651.
[48] T. Pock, A. Chambolle, D. Cremers, and H. Bischof, A convex relaxation approach for
computing minimal partitions, IEEE Conference on Computer Vision and Pattern Recog-
nition, (2009), pp. 810â€“817.
[49] T. Pock, D. Cremers, H. Bischof, and A. Chambolle, An algorithm for minimizing the
mumford-shah functional, in 2009 IEEE 12th International Conference on Computer Vi-
sion, IEEE, 2009, pp. 1133â€“1140.
[50] L. I. Rudin, S. Osher, and E. Fatemi, Nonlinear total variation based noise removal algo-
rithms, Physica D: Nonlinear Phenomena, 60 (1992), pp. 259â€“268.
[51] J. Scaife, K. Harrison, A. Drew, and et al., Accuracy of manual and automated rectal
contours using helical tomotherapy image guidance scans during prostate radiotherapy,
Journal of Clinical Oncology, 33 (2015), p. 94.
[52] J. Shi and J. Malik, Normalized cuts and image segmentation, IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22 (2000), pp. 888â€“905.
[53] C. Silpa-Anan and R. Hartley, Optimised KD-trees for fast image descriptor matching,
IEEE Conference on Computer Vision and Pattern Recognition, (2008), pp. 1â€“8.
[54] A. Subramanya and J. Bilmes, Semi-supervised learning with measure propagation, Journal
of Machine Learning Research, 12 (2011), pp. 3311â€“3370.
[55] L. Vese and T. F. Chan, A multiphase level set framework for image segmentation using the
Mumford and Shah model, International Journal of Computer Vision, 50 (2002), pp. 271â€“
293.
[56] K. Yin and X.-C. Tai, An eï¬€ective region force for some variational models for learning and
clustering, Journal of Scientiï¬c Computing, 74 (2018), pp. 175â€“196.
[57] S. Yu and J. Shi, Multiclass spectral clustering, in Proceedings Ninth IEEE International
Conference on Computer Vision, Oct 2003, pp. 313â€“319 vol.1.
[58] J. Yuan, E. Bae, X.-C. Tai, and Y. Boykov, A continuous max-ï¬‚ow approach to potts model,
in European Conference on Computer Vision, 2010, pp. 379â€“392.
[59] Y. Zhang, B. Matuszewski, L. Shark, and C. Moore, Medical image segmentation using new
hybrid level-set method, in 2008 Fifth International Conference BioMedical Visualization:
Information Visualization in Medical and Biomedical Informatics, 2008, pp. 71â€“76.
21
