# ç¬¬äºŒåäºŒè®²ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒ

## Parameter-Efficient Fine-Tuning (PEFT)

---

### ğŸ“‹ æœ¬è®²å¤§çº²

1. å¤§æ¨¡å‹å¾®è°ƒæŒ‘æˆ˜
2. Adapteræ–¹æ³•
3. LoRAæŠ€æœ¯
4. å…¶ä»–PEFTæ–¹æ³•
5. è§†è§‰ä»»åŠ¡åº”ç”¨

---

### 22.1 å¾®è°ƒæŒ‘æˆ˜

#### å…¨å‚æ•°å¾®è°ƒé—®é¢˜

```
æ¨¡å‹å‚æ•°ï¼š10äº¿+
å¾®è°ƒæˆæœ¬ï¼šå­˜å‚¨ã€è®¡ç®—
å¤šä»»åŠ¡ï¼šæ¯ä¸ªä»»åŠ¡ä¸€ä»½æ¨¡å‹
```

#### è§£å†³æ€è·¯

åªæ›´æ–°å°‘é‡å‚æ•°

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° |
|------|-----------|
| å…¨å¾®è°ƒ | 100% |
| Adapter | ~5% |
| LoRA | ~1% |
| Prompt Tuning | <0.1% |

---

### 22.2 Adapteræ–¹æ³•

#### åŸºæœ¬æ€æƒ³

åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­æ’å…¥å°å‹ç½‘ç»œæ¨¡å—

#### ç»“æ„

```
Transformer Layer
â”œâ”€â”€ Self-Attention
â”œâ”€â”€ Adapter (è®­ç»ƒ)
â”œâ”€â”€ Feed-Forward
â””â”€â”€ Adapter (è®­ç»ƒ)
```

#### Adapteræ¨¡å—

$$h' = h + \text{MLP}_{down}(\text{ReLU}(\text{MLP}_{up}(h)))$$

- é™ç»´ â†’ éçº¿æ€§ â†’ å‡ç»´
- ç“¶é¢ˆç»“æ„å‡å°‘å‚æ•°

---

### 22.3 Adapterå˜ä½“

#### AdapterFusion

å¤šä»»åŠ¡é€‚é…å™¨èåˆ

#### AdapterDrop

åŠ¨æ€ä¸¢å¼ƒé€‚é…å™¨

#### Parallel Adapter

å¹¶è¡Œè€Œéä¸²è¡Œ

#### Compacter

ä½ç§©åˆ†è§£é€‚é…å™¨

---

### 22.4 LoRAåŸç†

#### Low-Rank Adaptation

å‡è®¾æƒé‡æ›´æ–°æ˜¯ä½ç§©çš„ï¼š
$$\Delta W = BA$$

å…¶ä¸­ $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$, $r \ll \min(d, k)$

#### å‰å‘è®¡ç®—

$$h = W_0 x + \Delta W x = W_0 x + BAx$$

- $W_0$ï¼šå†»ç»“
- $B, A$ï¼šè®­ç»ƒ

#### å‚æ•°é‡

$$\frac{dk}{d \times r + r \times k} \approx \frac{d}{r} \ll 1$$

---

### 22.5 LoRAåˆå§‹åŒ–

#### æ ‡å‡†åˆå§‹åŒ–

- $A$ï¼šéšæœºé«˜æ–¯
- $B$ï¼šé›¶åˆå§‹åŒ–

æ•ˆæœï¼šåˆå§‹æ—¶ $\Delta W = 0$ï¼Œä¿æŒé¢„è®­ç»ƒæƒé‡

#### å‰å‘ä¼ æ’­

```python
# æ ‡å‡†å½¢å¼
h = W0 @ x + (B @ A) @ x

# ç¼©æ”¾å½¢å¼
h = W0 @ x + (alpha/r) * (B @ A) @ x
```

$\alpha$ï¼šç¼©æ”¾å› å­ï¼Œæ§åˆ¶LoRAçš„å½±å“

---

### 22.6 LoRAåº”ç”¨ä½ç½®

#### æ³¨æ„åŠ›å±‚

- Query ($W_q$)
- Key ($W_k$)
- Value ($W_v$)
- Output ($W_o$)

#### FFNå±‚

- $W_1, W_2$

#### å»ºè®®

é€šå¸¸åªå¯¹ $W_q, W_v$ åº”ç”¨LoRAå³å¯

---

### 22.7 LoRAä¼˜åŠ¿

| ä¼˜åŠ¿ | æè¿° |
|------|------|
| å‚æ•°æ•ˆç‡ | åªéœ€0.1%-1%å‚æ•° |
| æ— æ¨ç†å»¶è¿Ÿ | å¯åˆå¹¶åˆ°åŸæƒé‡ |
| æ¨¡å—åŒ– | å¤šä»»åŠ¡åˆ‡æ¢ |
| æ— éœ€æ”¹æ¶æ„ | å³æ’å³ç”¨ |

#### æƒé‡åˆå¹¶

éƒ¨ç½²æ—¶ï¼š$W_{new} = W_0 + BA$

æ— é¢å¤–è®¡ç®—å¼€é”€

---

### 22.8 è§†è§‰ä»»åŠ¡ä¸­çš„PEFT

#### ViTé€‚é…

åœ¨Vision Transformerä¸­åº”ç”¨ï¼š
- Attentionå±‚LoRA
- é€‚é…åˆ†ç±»å¤´

#### åº”ç”¨åœºæ™¯

- å›¾åƒåˆ†ç±»
- ç›®æ ‡æ£€æµ‹
- è¯­ä¹‰åˆ†å‰²

#### æ–¹æ³•

- ViT-Adapter
- Visual Prompt Tuning

---

### 22.9 Prefix Tuning

#### åŸç†

åœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ å¯å­¦ä¹ çš„"å‰ç¼€"å‘é‡

$$h = [P_1, ..., P_k, x_1, ..., x_n]$$

$P_i$ï¼šå¯å­¦ä¹ å‘é‡

#### å®ç°

åªè®­ç»ƒå‰ç¼€å‚æ•°ï¼Œå†»ç»“æ¨¡å‹

#### ä¼˜åŠ¿

- ä¸æ”¹å˜æ¨¡å‹ç»“æ„
- å‚æ•°é‡æå°‘

---

### 22.10 Prompt Tuning

#### è½¯æç¤º

$$h = \text{Embed}(x) + [P_1, ..., P_k]$$

å¯å­¦ä¹ çš„åµŒå…¥å‘é‡

#### ç¡¬æç¤º vs è½¯æç¤º

| ç±»å‹ | ç¤ºä¾‹ |
|------|------|
| ç¡¬æç¤º | "è¿™æ˜¯ä¸€å¼ {ç±»åˆ«}çš„å›¾ç‰‡" |
| è½¯æç¤º | å¯å­¦ä¹ å‘é‡ |

#### Visual Prompt Tuning

åœ¨å›¾åƒpatchå‰æ·»åŠ å¯å­¦ä¹ tokens

---

### 22.11 æ–¹æ³•æ¯”è¾ƒ

| æ–¹æ³• | å‚æ•°é‡ | æ¨ç†å»¶è¿Ÿ | çµæ´»æ€§ |
|------|--------|----------|--------|
| å…¨å¾®è°ƒ | 100% | æ—  | é«˜ |
| Adapter | 5% | å°å¢ | ä¸­ |
| LoRA | 1% | æ—  | é«˜ |
| Prefix Tuning | 0.1% | å°å¢ | ä½ |
| Prompt Tuning | <0.1% | å°å¢ | ä½ |

---

### 22.12 å®è·µå»ºè®®

#### é€‰æ‹©æŒ‡å—

- **è¿½æ±‚æ€§èƒ½**ï¼šLoRAæˆ–Adapter
- **è¿½æ±‚æ•ˆç‡**ï¼šPrompt Tuning
- **å¤šä»»åŠ¡**ï¼šLoRAï¼ˆæ˜“åˆ‡æ¢ï¼‰

#### è¶…å‚æ•°

- LoRA rank $r$ï¼š4-64ï¼ˆé€šå¸¸8-16ï¼‰
- $\alpha$ï¼šè®¾ä¸º$r$æˆ–$2r$
- å­¦ä¹ ç‡ï¼šæ¯”å…¨å¾®è°ƒå¤§

#### è®­ç»ƒæŠ€å·§

- åªè®­ç»ƒLoRAå‚æ•°
- ä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡
- ç›‘æ§è¿‡æ‹Ÿåˆ

---

### ğŸ“Š æœ¬è®²æ€»ç»“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚   æ ¸å¿ƒæ€æƒ³ï¼šåªæ›´æ–°å°‘é‡å‚æ•°                       â”‚
â”‚                                                 â”‚
â”‚   Adapterï¼šæ’å…¥ç“¶é¢ˆæ¨¡å—                          â”‚
â”‚   h' = h + MLP_down(ReLU(MLP_up(h)))           â”‚
â”‚                                                 â”‚
â”‚   LoRAï¼šä½ç§©åˆ†è§£                                 â”‚
â”‚   Î”W = BA, r << d                               â”‚
â”‚   éƒ¨ç½²æ—¶å¯åˆå¹¶ï¼Œæ— é¢å¤–å¼€é”€                       â”‚
â”‚                                                 â”‚
â”‚   Prefix/Prompt Tuningï¼š                        â”‚
â”‚   æ·»åŠ å¯å­¦ä¹ å‘é‡/å‰ç¼€                            â”‚
â”‚                                                 â”‚
â”‚   é€‰æ‹©ï¼šLoRAï¼ˆå¹³è¡¡ï¼‰ > Adapter > Prompt Tuning  â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ“š è¯¾åä½œä¸š

1. **å®ç°é¢˜**ï¼šå®ç°LoRAæ¨¡å—å¹¶åº”ç”¨äºå¾®è°ƒ

2. **å®éªŒé¢˜**ï¼šæ¯”è¾ƒä¸åŒrankå¯¹æ€§èƒ½çš„å½±å“

3. **åˆ†æé¢˜**ï¼šåˆ†æLoRAä¸Adapterçš„ä¼˜ç¼ºç‚¹

4. **åº”ç”¨é¢˜**ï¼šä½¿ç”¨PEFTæ–¹æ³•å¾®è°ƒè§†è§‰æ¨¡å‹

---

### ğŸ“– æ‰©å±•é˜…è¯»

1. **ç»å…¸è®ºæ–‡**ï¼š
   - Houlsby et al., "Parameter-Efficient Transfer Learning", ICML 2019
   - Hu et al., "LoRA: Low-Rank Adaptation", ICLR 2022
   - Lester et al., "The Power of Scale for Parameter-Efficient Prompt Tuning", ICML 2021

2. **ä»£ç åº“**ï¼š
   - HuggingFace PEFT
   - LoRA implementation (PyTorch)

3. **åº”ç”¨**ï¼š
   - Stable Diffusion LoRA
   - LLaMA-Adapter

---

### ğŸ“– å‚è€ƒæ–‡çŒ®

1. Houlsby, N., et al. (2019). Parameter-efficient transfer learning for NLP. *ICML*.

2. Hu, E.J., et al. (2022). LoRA: Low-rank adaptation of large language models. *ICLR*.

3. Lester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. *ICML*.

4. Chen, M., et al. (2023). ViT-Adapter: Adapting pretrained vision transformers. *CVPR*.
