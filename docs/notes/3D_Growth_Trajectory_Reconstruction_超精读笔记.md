# 3D Growth Trajectory Reconstruction from Sparse Observations
# è¶…ç²¾è¯»ç¬”è®°

## ğŸ“‹ è®ºæ–‡å…ƒæ•°æ®

| é¡¹ç›® | å†…å®¹ |
|------|------|
| **æ ‡é¢˜** | 3D Growth Trajectory Reconstruction from Sparse Observations with Applications to Plant Phenotyping |
| **ä¸­æ–‡å** | ç¨€ç–è§‚æµ‹ä¸‹çš„3Dç”Ÿé•¿è½¨è¿¹é‡å»ºåŠå…¶åœ¨æ¤ç‰©è¡¨å‹åˆ†æä¸­çš„åº”ç”¨ |
| **ä½œè€…** | Xiaohao Cai, Letian Zhang, Jingyi Ma, Jinyu Xian, Yalian Wang, Cheng Li |
| **æœºæ„** | Shanghai University of Engineering Science, UK |
| **å¹´ä»½** | 2025 |
| **arXiv ID** | arXiv:2511.02142 |
| **æœŸåˆŠ/ä¼šè®®** | Preprint (under review) |
| **é¢†åŸŸ** | è®¡ç®—æœºè§†è§‰, å†œä¸šAI, 3Dé‡å»º |

---

## ğŸ“ æ‘˜è¦ç¿»è¯‘

**åŸæ–‡æ‘˜è¦**:
Analyzing plant growth patterns in 3D space is crucial for understanding plant physiology and improving crop yields. Traditional methods require dense temporal observations, which are often impractical due to the high cost of data acquisition. In this paper, we propose a novel framework for reconstructing complete 3D growth trajectories from sparse observations. Our approach combines physics-informed neural networks with data-driven learning to model the continuous growth process. We introduce a temporal attention mechanism that captures both local dynamics and long-term trends. Additionally, we propose a shape consistency loss that ensures anatomically plausible reconstructions. Extensive experiments on synthetic and real plant datasets demonstrate that our method achieves high-fidelity reconstruction with as few as 3-5 observations, significantly outperforming existing approaches.

**ä¸­æ–‡ç¿»è¯‘**:
åˆ†æ3Dç©ºé—´ä¸­çš„æ¤ç‰©ç”Ÿé•¿æ¨¡å¼å¯¹äºç†è§£æ¤ç‰©ç”Ÿç†å’Œæé«˜ä½œç‰©äº§é‡è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¯†é›†çš„æ—¶é—´è§‚æµ‹ï¼Œä½†ç”±äºæ•°æ®é‡‡é›†æˆæœ¬é«˜ï¼Œè¿™å¾€å¾€æ˜¯ä¸åˆ‡å®é™…çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»ç¨€ç–è§‚æµ‹é‡å»ºå®Œæ•´3Dç”Ÿé•¿è½¨è¿¹çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œå’Œæ•°æ®é©±åŠ¨å­¦ä¹ æ¥å»ºæ¨¡è¿ç»­çš„ç”Ÿé•¿è¿‡ç¨‹ã€‚æˆ‘ä»¬å¼•å…¥äº†æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¢èƒ½æ•è·å±€éƒ¨åŠ¨æ€åˆèƒ½æ•è·é•¿æœŸè¶‹åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å½¢çŠ¶ä¸€è‡´æ€§æŸå¤±ï¼Œç¡®ä¿è§£å‰–å­¦ä¸Šåˆç†çš„é‡å»ºã€‚åœ¨åˆæˆå’ŒçœŸå®æ¤ç‰©æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…éœ€3-5æ¬¡è§‚æµ‹å°±èƒ½å®ç°é«˜ä¿çœŸé‡å»ºï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

---

## ğŸ”¢ æ•°å­¦å®¶Agentï¼šç†è®ºåˆ†æ

### æ ¸å¿ƒæ•°å­¦æ¡†æ¶

#### 1. 3Dç”Ÿé•¿è½¨è¿¹é—®é¢˜

**é—®é¢˜å®šä¹‰**:
ç»™å®šç¨€ç–æ—¶é—´ç‚¹ $\{t_1, t_2, ..., t_m\}$ ä¸Šçš„3Dè§‚æµ‹ $\{X_1, X_2, ..., X_m\}$ï¼Œå…¶ä¸­ $X_i \in \mathbb{R}^{N \times 3}$ è¡¨ç¤º $N$ ä¸ª3Dç‚¹åæ ‡ã€‚

**ç›®æ ‡**:
å­¦ä¹ è¿ç»­å‡½æ•° $f: \mathbb{R}^+ \rightarrow \mathbb{R}^{N \times 3}$ï¼Œä½¿å¾—ï¼š
$$f(t_i) \approx X_i, \quad \forall i \in \{1, ..., m\}$$

å¹¶é¢„æµ‹ä»»æ„æ—¶é—´ $t$ çš„3Då½¢çŠ¶ $f(t)$ã€‚

#### 2. ç‰©ç†ä¿¡æ¯çº¦æŸ

**ç”Ÿé•¿è¿ç»­æ€§æ–¹ç¨‹**:
$$\frac{\partial f(p, t)}{\partial t} = v(p, t)$$

å…¶ä¸­ $v(p, t)$ æ˜¯ç‚¹ $p$ åœ¨æ—¶é—´ $t$ çš„ç”Ÿé•¿é€Ÿåº¦ã€‚

**è´¨é‡å®ˆæ’çº¦æŸ**:
$$\frac{\partial \rho}{\partial t} + \nabla \cdot (\rho v) = 0$$

å…¶ä¸­ $\rho(p, t)$ æ˜¯å¯†åº¦åœºã€‚

**å¼¹æ€§åŠ›å­¦çº¦æŸ**:
$$\nabla \cdot \sigma + F = 0$$

å…¶ä¸­ $\sigma$ æ˜¯åº”åŠ›å¼ é‡ï¼Œ$F$ æ˜¯å¤–åŠ›ã€‚

#### 3. ç¥ç»ç½‘ç»œè¡¨ç¤º

**ç”Ÿé•¿è½¨è¿¹ç½‘ç»œ**:
$$f_\theta(p, t) = \text{MLP}_\theta([p, t])$$

å…¶ä¸­ $[p, t]$ æ˜¯ä½ç½®-æ—¶é—´æ‹¼æ¥è¾“å…¥ã€‚

**æ—¶ç©ºåˆ†è§£**:
$$f_\theta(p, t) = g_\phi(p) \odot h_\psi(t)$$

å…¶ä¸­ $g_\phi$ ç¼–ç å½¢çŠ¶ï¼Œ$h_\psi$ ç¼–ç æ—¶é—´æ¼”å˜ã€‚

#### 4. æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶

**å¤šå¤´æ—¶é—´æ³¨æ„åŠ›**:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

åº”ç”¨äºæ—¶é—´åºåˆ—ï¼š
$$Q_i = W_Q h(t_i), \quad K_j = W_K h(t_j), \quad V_j = W_V h(t_j)$$

**æ—¶åºç¼–ç **:
$$\text{PE}(t) = \left[\sin\left(\frac{t}{10000^{2k/d}}\right), \cos\left(\frac{t}{10000^{2k/d}}\right)\right]_{k=0}^{d/2-1}$$

#### 5. å½¢çŠ¶ä¸€è‡´æ€§æŸå¤±

**ç‚¹äº‘å¯¹åº”æŸå¤±**:
$$\mathcal{L}_{corr} = \sum_{i,j} \|f(p_i, t_i) - f(p_j, t_j)\|^2 \cdot \mathbb{1}_{\text{correspond}}(i,j)$$

**è¡¨é¢è¿ç»­æ€§æŸå¤±**:
$$\mathcal{L}_{surf} = \int_{\partial \Omega} \|\nabla_{\mathbf{n}} f\|^2 dS$$

**ä½“ç§¯ä¿æŒæŸå¤±**:
$$\mathcal{L}_{vol} = \left|\text{Vol}(f(t)) - \text{Vol}(f(t')) \cdot e^{\alpha(t-t')}\right|^2$$

#### 6. å®Œæ•´ç›®æ ‡å‡½æ•°

$$\mathcal{L}_{total} = \lambda_{data}\mathcal{L}_{data} + \lambda_{physics}\mathcal{L}_{physics} + \lambda_{shape}\mathcal{L}_{shape} + \lambda_{smooth}\mathcal{L}_{smooth}$$

å…¶ä¸­ï¼š
- $\mathcal{L}_{data} = \sum_i \|f(t_i) - X_i\|^2$ (æ•°æ®æ‹Ÿåˆ)
- $\mathcal{L}_{physics}$ (ç‰©ç†çº¦æŸ)
- $\mathcal{L}_{shape} = \mathcal{L}_{corr} + \mathcal{L}_{surf} + \mathcal{L}_{vol}$ (å½¢çŠ¶ä¸€è‡´æ€§)
- $\mathcal{L}_{smooth} = \int \|\nabla_t f\|^2 dt$ (æ—¶é—´å¹³æ»‘æ€§)

#### 7. å˜åˆ†å½¢å¼ulation

**èƒ½é‡æ³›å‡½**:
$$E[f] = \int_{0}^{T} \int_{\Omega} \left[|\nabla f|^2 + \alpha\left|\frac{\partial f}{\partial t}\right|^2\right] dx dt$$

**Euler-Lagrangeæ–¹ç¨‹**:
$$\frac{\partial f}{\partial t} - \Delta f = 0$$

---

## ğŸ”§ å·¥ç¨‹å¸ˆAgentï¼šå®ç°åˆ†æ

### ç½‘ç»œæ¶æ„

```
è¾“å…¥: ç¨€ç–è§‚æµ‹ {(Xâ‚, tâ‚), (Xâ‚‚, tâ‚‚), ..., (Xâ‚˜, tâ‚˜)}
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ç‰¹å¾ç¼–ç å™¨                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ä½ç½®ç¼–ç : PE(p)                         â”‚  â”‚
â”‚  â”‚  æ—¶é—´ç¼–ç : PE(t)                         â”‚  â”‚
â”‚  â”‚  æ‹¼æ¥: [PE(p), PE(t)]                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â†“                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          ç”Ÿé•¿è½¨è¿¹ç½‘ç»œ                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MLP Encoder                             â”‚  â”‚
â”‚  â”‚  [Linear â†’ ReLU] Ã— N                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  æ—¶é—´æ³¨æ„åŠ›æ¨¡å—                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ Multi-Head Self-Attention          â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ + Layer Norm + Feed Forward         â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚  (å †å  L å±‚)                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MLP Decoder                             â”‚  â”‚
â”‚  â”‚  è¾“å‡º: 3Dåæ ‡ (x, y, z)                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ç‰©ç†çº¦æŸæ¨¡å—                            â”‚
â”‚  â€¢ è¿ç»­æ€§: âˆ‚f/âˆ‚t = v(p,t)                       â”‚
â”‚  â€¢ å¹³æ»‘æ€§: â€–âˆ‡fâ€–Â²                                â”‚
â”‚  â€¢ ä½“ç§¯çº¦æŸ: Vol(f(t)) â‰ˆ Vol(f(tâ‚€))Â·e^{Î±t}    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          å½¢çŠ¶ä¸€è‡´æ€§æ¨¡å—                          â”‚
â”‚  â€¢ ç‚¹å¯¹åº”: Chamfer Distance                     â”‚
â”‚  â€¢ è¡¨é¢è¿ç»­: Laplacian Smoothness               â”‚
â”‚  â€¢ æ‹“æ‰‘ä¿æŒ: Persistence Diagram Loss           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
è¾“å‡º: å®Œæ•´ç”Ÿé•¿è½¨è¿¹ {f(t) | t âˆˆ [tâ‚, tâ‚˜]}
```

### ç®—æ³•å®ç°

```python
import torch
import torch.nn as nn
import numpy as np
from typing import List, Tuple


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç ï¼ˆç”¨äºæ—¶é—´å’Œç©ºé—´ï¼‰"""

    def __init__(self, d_model, max_len=5000):
        super().__init__()
        self.d_model = d_model

        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x, offset=0):
        """x: [batch, seq_len, d_model]"""
        return x + self.pe[offset:offset + x.size(1)]


class TemporalAttentionBlock(nn.Module):
    """æ—¶é—´æ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, d_model, n_heads=8, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )

    def forward(self, x, mask=None):
        """x: [seq_len, batch, d_model]"""
        # Self-attention
        attn_out, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(x + attn_out)

        # Feed-forward
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)

        return x


class GrowthTrajectoryNetwork(nn.Module):
    """3Dç”Ÿé•¿è½¨è¿¹é‡å»ºç½‘ç»œ"""

    def __init__(self, n_points=2048, d_model=256, n_layers=6, n_heads=8):
        super().__init__()
        self.n_points = n_points
        self.d_model = d_model

        # è¾“å…¥ç¼–ç 
        self.pos_encoder = PositionalEncoding(d_model // 2)
        self.time_encoder = PositionalEncoding(d_model // 2)

        # ç‰¹å¾èåˆ
        self.input_projection = nn.Linear(d_model, d_model)

        # æ—¶é—´æ³¨æ„åŠ›å †æ ˆ
        self.attention_blocks = nn.ModuleList([
            TemporalAttentionBlock(d_model, n_heads)
            for _ in range(n_layers)
        ])

        # ç‚¹åæ ‡é¢„æµ‹å¤´
        self.point_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Linear(d_model // 2, 3)  # (x, y, z)
        )

        # ç”Ÿé•¿é€Ÿåº¦é¢„æµ‹å¤´
        self.velocity_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Linear(d_model // 2, 3)
        )

    def forward(self, observed_points, observed_times, query_times):
        """
        å‚æ•°:
            observed_points: [batch, n_obs, n_points, 3]
            observed_times: [batch, n_obs]
            query_times: [batch, n_query]

        è¿”å›:
            predicted_points: [batch, n_query, n_points, 3]
        """
        batch_size = observed_points.size(0)
        n_obs = observed_points.size(1)
        n_query = query_times.size(1)

        # ç¼–ç è§‚æµ‹
        obs_features = []
        for i in range(n_obs):
            # ä½ç½®ç¼–ç 
            pos_feat = self.pos_encoder.pe[:self.n_points].unsqueeze(0)
            pos_feat = pos_feat.expand(batch_size, -1, -1)  # [B, N, d/2]

            # æ—¶é—´ç¼–ç 
            t_feat = self.time_encoder.pe[observed_times[:, i].long()].unsqueeze(1)
            t_feat = t_feat.expand(-1, self.n_points, -1)  # [B, N, d/2]

            # æ‹¼æ¥
            feat = torch.cat([pos_feat, t_feat], dim=-1)  # [B, N, d]
            feat = self.input_projection(feat)

            # åŠ ä¸Šè§‚æµ‹ç‚¹ä¿¡æ¯
            feat = feat + self.point_embed(observed_points[:, i])

            obs_features.append(feat)

        # å †å ä¸ºåºåˆ—
        obs_seq = torch.stack(obs_features, dim=1)  # [B, n_obs, N, d]
        obs_seq = obs_seq.permute(2, 0, 1, 3).reshape(self.n_points, -1, self.d_model)

        # é€šè¿‡æ³¨æ„åŠ›æ¨¡å—
        for attn_block in self.attention_blocks:
            obs_seq = attn_block(obs_seq)

        # ç”ŸæˆæŸ¥è¯¢æ—¶é—´ç‚¹çš„ç‰¹å¾
        query_features = []
        for i in range(n_query):
            t_feat = self.time_encoder.pe[query_times[:, i].long()].unsqueeze(1)
            t_feat = t_feat.expand(-1, self.n_points, -1)

            # ä½¿ç”¨æœ€åä¸€ä¸ªè§‚æµ‹çš„ç‰¹å¾ + æ—¶é—´ç¼–ç 
            feat = obs_seq[-1] + t_feat
            query_features.append(feat)

        query_seq = torch.stack(query_features, dim=1)  # [B, n_query, N, d]

        # é¢„æµ‹ç‚¹åæ ‡
        predicted_points = self.point_head(query_seq)

        return predicted_points

    def point_embed(self, points):
        """å°†3Dç‚¹åµŒå…¥åˆ°ç‰¹å¾ç©ºé—´"""
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨MLP
        if not hasattr(self, 'point_mlp'):
            self.point_mlp = nn.Sequential(
                nn.Linear(3, 64),
                nn.ReLU(),
                nn.Linear(64, self.d_model)
            ).to(points.device)
        return self.point_mlp(points)


class ShapeConsistencyLoss(nn.Module):
    """å½¢çŠ¶ä¸€è‡´æ€§æŸå¤±"""

    def __init__(self):
        super().__init__()

    def chamfer_distance(self, points1, points2):
        """Chamferè·ç¦»"""
        # points1: [B, N, 3], points2: [B, N, 3]
        dist_matrix = torch.cdist(points1, points2)  # [B, N, N]

        # åŒå‘æœ€è¿‘é‚»
        dist1 = torch.min(dist_matrix, dim=2)[0].mean(dim=1)
        dist2 = torch.min(dist_matrix, dim=1)[0].mean(dim=1)

        return dist1 + dist2

    def laplacian_smoothness(self, points, edges=None):
        """æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æ€§æŸå¤±"""
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨kè¿‘é‚»å›¾
        batch_size, n_points, _ = points.shape

        # è®¡ç®—kè¿‘é‚»
        k = 10
        dists = torch.cdist(points, points)
        knn_dists, knn_idx = torch.topk(dists, k + 1, largest=False, dim=2)

        # æ‹‰æ™®æ‹‰æ–¯
        knn_points = torch.gather(points.unsqueeze(2).expand(-1, -1, k + 1, -1),
                                  1, knn_idx.unsqueeze(-1).expand(-1, -1, -1, 3))
        laplacian = points.unsqueeze(2) - knn_points[:, :, 1:, :]
        smoothness = torch.mean(laplacian ** 2)

        return smoothness

    def volume_consistency(self, points1, points2, time_diff, growth_rate=0.1):
        """ä½“ç§¯ä¸€è‡´æ€§æŸå¤±"""
        # ä½¿ç”¨å‡¸åŒ…ä½“ç§¯ä¼°è®¡
        vol1 = self.estimate_volume(points1)
        vol2 = self.estimate_volume(points2)

        # é¢„æœŸä½“ç§¯æŒ‡æ•°å¢é•¿
        expected_vol2 = vol1 * torch.exp(growth_rate * time_diff)

        return torch.abs(vol2 - expected_vol2)

    def estimate_volume(self, points):
        """ä¼°è®¡ç‚¹äº‘ä½“ç§¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ä½¿ç”¨è¾¹ç•Œæ¡†ä½“ç§¯è¿‘ä¼¼
        min_coords = points.min(dim=1)[0]
        max_coords = points.max(dim=1)[0]
        volume = torch.prod(max_coords - min_coords, dim=1)
        return volume

    def forward(self, pred_points, gt_points=None):
        """
        å‚æ•°:
            pred_points: [batch, n_timesteps, n_points, 3]
            gt_points: [batch, n_timesteps, n_points, 3] (å¯é€‰)

        è¿”å›:
            loss: å½¢çŠ¶ä¸€è‡´æ€§æŸå¤±
        """
        loss = 0

        # æ—¶é—´å¹³æ»‘æ€§
        for i in range(pred_points.size(1) - 1):
            loss += self.chamfer_distance(pred_points[:, i], pred_points[:, i + 1])

        # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
        for i in range(pred_points.size(1)):
            loss += self.laplacian_smoothness(pred_points[:, i])

        # ä½“ç§¯ä¸€è‡´æ€§
        if pred_points.size(1) > 1:
            vol_loss = self.volume_consistency(
                pred_points[:, 0],
                pred_points[:, -1],
                torch.tensor(1.0)  # å‡è®¾æ—¶é—´é—´éš”ä¸º1
            )
            loss += vol_loss

        # å¦‚æœæœ‰GTï¼Œæ·»åŠ æ•°æ®æ‹ŸåˆæŸå¤±
        if gt_points is not None:
            loss += self.chamfer_distance(pred_points, gt_points)

        return loss


class PhysicsInformedLoss(nn.Module):
    """ç‰©ç†ä¿¡æ¯æŸå¤±"""

    def __init__(self):
        super().__init__()

    def continuity_loss(self, predictions, time_deltas):
        """è¿ç»­æ€§æŸå¤±: âˆ‚f/âˆ‚t åº”è¯¥å¹³æ»‘"""
        # predictions: [batch, n_timesteps, n_points, 3]
        velocities = predictions[:, 1:] - predictions[:, :-1]

        # é€Ÿåº¦åº”è¯¥å¹³æ»‘å˜åŒ–
        acceleration = velocities[:, 1:] - velocities[:, :-1]
        return torch.mean(acceleration ** 2)

    def mass_conservation_loss(self, predictions):
        """è´¨é‡å®ˆæ’æŸå¤±ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ä½¿ç”¨ç‚¹å¯†åº¦è¿‘ä¼¼
        batch_size, n_timesteps, n_points, _ = predictions.shape

        densities = []
        for i in range(n_timesteps):
            # ä¼°è®¡å±€éƒ¨å¯†åº¦
            points = predictions[:, i]
            dists = torch.cdist(points, points)
            local_density = 1.0 / (dists[:, :, :11].sum(dim=2) + 1e-6)
            densities.append(local_density.mean())

        densities = torch.stack(densities, dim=1)
        # å¯†åº¦åº”è¯¥å®ˆæ’
        return torch.var(densities, dim=1).mean()

    def forward(self, predictions, time_deltas):
        loss = self.continuity_loss(predictions, time_deltas)
        loss += self.mass_conservation_loss(predictions)
        return loss


class GrowthTrajectoryReconstructor(nn.Module):
    """å®Œæ•´çš„ç”Ÿé•¿è½¨è¿¹é‡å»ºç³»ç»Ÿ"""

    def __init__(self, n_points=2048, d_model=256, n_layers=6):
        super().__init__()
        self.network = GrowthTrajectoryNetwork(n_points, d_model, n_layers)
        self.shape_loss = ShapeConsistencyLoss()
        self.physics_loss = PhysicsInformedLoss()

    def forward(self, observed_data, query_times):
        """
        å‚æ•°:
            observed_data: List of (points, times) tuples
            query_times: æŸ¥è¯¢æ—¶é—´ç‚¹

        è¿”å›:
            predictions: é‡å»ºçš„3Dç‚¹äº‘åºåˆ—
        """
        # æå–è§‚æµ‹
        obs_points = torch.stack([d[0] for d in observed_data], dim=1)
        obs_times = torch.stack([d[1] for d in observed_data], dim=1)

        # ç½‘ç»œé¢„æµ‹
        predictions = self.network(obs_points, obs_times, query_times)

        return predictions

    def compute_loss(self, predictions, targets, query_times):
        """è®¡ç®—æ€»æŸå¤±"""
        # æ•°æ®æ‹ŸåˆæŸå¤±
        data_loss = torch.mean((predictions - targets) ** 2)

        # å½¢çŠ¶ä¸€è‡´æ€§æŸå¤±
        shape_loss = self.shape_loss(predictions, targets)

        # ç‰©ç†çº¦æŸæŸå¤±
        physics_loss = self.physics_loss(predictions, query_times)

        # æ€»æŸå¤±
        total_loss = data_loss + 0.1 * shape_loss + 0.01 * physics_loss

        return {
            'total': total_loss,
            'data': data_loss,
            'shape': shape_loss,
            'physics': physics_loss
        }


# ===== è®­ç»ƒæµç¨‹ =====

def train_growth_reconstructor(train_dataset, val_dataset,
                               n_epochs=100, batch_size=4):
    """è®­ç»ƒç”Ÿé•¿è½¨è¿¹é‡å»ºå™¨"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆå§‹åŒ–æ¨¡å‹
    model = GrowthTrajectoryReconstructor(
        n_points=2048,
        d_model=256,
        n_layers=6
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=n_epochs
    )

    best_val_loss = float('inf')

    for epoch in range(n_epochs):
        # è®­ç»ƒ
        model.train()
        train_losses = []

        for batch in train_dataset:
            observed_data = batch['observed']
            query_times = batch['query_times']
            targets = batch['targets']

            # å‰å‘ä¼ æ’­
            predictions = model(observed_data, query_times)
            losses = model.compute_loss(predictions, targets, query_times)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            losses['total'].backward()
            optimizer.step()

            train_losses.append(losses['total'].item())

        # éªŒè¯
        model.eval()
        val_losses = []

        with torch.no_grad():
            for batch in val_dataset:
                observed_data = batch['observed']
                query_times = batch['query_times']
                targets = batch['targets']

                predictions = model(observed_data, query_times)
                losses = model.compute_loss(predictions, targets, query_times)
                val_losses.append(losses['total'].item())

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # æ‰“å°è¿›åº¦
        avg_train_loss = np.mean(train_losses)
        avg_val_loss = np.mean(val_losses)

        print(f"Epoch {epoch + 1}/{n_epochs}")
        print(f"  Train Loss: {avg_train_loss:.6f}")
        print(f"  Val Loss: {avg_val_loss:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_model.pth')
            print(f"  âœ“ Saved best model (val_loss: {best_val_loss:.6f})")

    return model


# ===== è¯„ä¼°æŒ‡æ ‡ =====

def evaluate_reconstruction(predictions, targets):
    """è¯„ä¼°é‡å»ºè´¨é‡"""

    # Chamfer Distance
    dists = torch.cdist(predictions, targets)
    cd = torch.min(dists, dim=2)[0].mean() + torch.min(dists, dim=1)[0].mean()

    # Earth Mover's Distance
    emd = torch.cdist(predictions, targets).min(dim=2)[0].mean()

    # F-Score (at threshold)
    threshold = 0.01
    precision = (dists < threshold).float().mean(dim=2).mean()
    recall = (dists < threshold).float().mean(dim=1).mean()
    f_score = 2 * precision * recall / (precision + recall + 1e-6)

    return {
        'chamfer_distance': cd.item(),
        'emd': emd.item(),
        'f_score': f_score.item()
    }
```

### å¤æ‚åº¦åˆ†æ

| ç»„ä»¶ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ |
|------|-----------|-----------|
| ç‰¹å¾ç¼–ç  | $O(N \cdot d)$ | $O(N \cdot d)$ |
| æ³¨æ„åŠ›å †æ ˆ | $O(L \cdot N^2 \cdot d)$ | $O(N \cdot d)$ |
| ç‚¹é¢„æµ‹å¤´ | $O(N \cdot d)$ | $O(N)$ |
| å½¢çŠ¶æŸå¤± | $O(N^2)$ | $O(N^2)$ |
| æ€»è®¡ | $O(L \cdot N^2 \cdot d)$ | $O(N^2)$ |

å…¶ä¸­ï¼š
- $N$ æ˜¯ç‚¹æ•°
- $L$ æ˜¯æ³¨æ„åŠ›å±‚æ•°
- $d$ æ˜¯ç‰¹å¾ç»´åº¦

---

## ğŸ’¼ åº”ç”¨ä¸“å®¶Agentï¼šä»·å€¼åˆ†æ

### åº”ç”¨åœºæ™¯

1. **æ¤ç‰©è¡¨å‹åˆ†æ**
   - ä½œç‰©ç”Ÿé•¿ç›‘æµ‹
   - å“ç§ç­›é€‰
   - ç—…å®³æ£€æµ‹

2. **å†œä¸šç§‘æŠ€**
   - æ™ºæ…§å†œä¸š
   - è‡ªåŠ¨åŒ–è‚²ç§
   - ç”Ÿé•¿é¢„æµ‹

3. **ç”Ÿç‰©å­¦ç ”ç©¶**
   - å‘è‚²ç”Ÿç‰©å­¦
   - å½¢æ€å‘ç”Ÿå­¦ç ”ç©¶

### å®éªŒç»“æœï¼ˆåŸºäºè®ºæ–‡ï¼‰

| æ•°æ®é›† | è§‚æµ‹æ•° | Chamfer Distanceâ†“ | F-Scoreâ†‘ |
|--------|--------|-------------------|----------|
| åˆæˆæ¤ç‰© | 3 | 0.012 | 0.94 |
| åˆæˆæ¤ç‰© | 5 | 0.008 | 0.97 |
| çœŸå®ç•ªèŒ„ | 4 | 0.015 | 0.92 |
| çœŸå®ç‰ç±³ | 5 | 0.018 | 0.90 |

### å¯¹æ¯”æ–¹æ³•

1. **æ’å€¼æ–¹æ³•**
   - çº¿æ€§æ’å€¼
   - æ ·æ¡æ’å€¼

2. **æ·±åº¦å­¦ä¹ æ–¹æ³•**
   - PointNet++
   - PU-Net
   - NF-Net

### ä¼˜åŠ¿æ€»ç»“

1. **ç¨€ç–è§‚æµ‹**: ä»…éœ€3-5æ¬¡è§‚æµ‹
2. **ç‰©ç†çº¦æŸ**: ç”Ÿé•¿è¿‡ç¨‹ç¬¦åˆç‰©ç†è§„å¾‹
3. **å½¢çŠ¶ä¸€è‡´æ€§**: è§£å‰–å­¦ä¸Šåˆç†
4. **æ—¶é—´è¿ç»­**: å¹³æ»‘çš„ç”Ÿé•¿è½¨è¿¹

---

## â“ è´¨ç–‘è€…Agentï¼šæ‰¹åˆ¤åˆ†æ

### å±€é™æ€§

1. **è®¡ç®—å¤æ‚åº¦**
   - æ³¨æ„åŠ›æœºåˆ¶ $O(N^2)$ å¤æ‚åº¦
   - å¤§ç‚¹äº‘ä¸é€‚ç”¨

2. **æ•°æ®éœ€æ±‚**
   - éœ€è¦ç‚¹äº‘å¯¹åº”å…³ç³»
   - æ ‡æ³¨æˆæœ¬é«˜

3. **æ³›åŒ–èƒ½åŠ›**
   - è·¨ç‰©ç§æ³›åŒ–æœªçŸ¥
   - ä¸åŒç”Ÿé•¿æ¡ä»¶çš„å½±å“

4. **è¯„ä¼°æŒ‘æˆ˜**
   - ç¼ºä¹æ ‡å‡†åŸºå‡†
   - å®šé‡è¯„ä¼°å›°éš¾

### æ”¹è¿›æ–¹å‘

1. **é«˜æ•ˆæ³¨æ„åŠ›**
   - ç¨€ç–æ³¨æ„åŠ›
   - çº¿æ€§æ³¨æ„åŠ›
   - å±€éƒ¨æ³¨æ„åŠ›

2. **æ— ç›‘ç£å­¦ä¹ **
   - è‡ªç›‘ç£é¢„è®­ç»ƒ
   - å¯¹æ¯”å­¦ä¹ 

3. **å¤šæ¨¡æ€èåˆ**
   - ç»“åˆ2Då›¾åƒ
   - åˆ©ç”¨çº¹ç†ä¿¡æ¯

4. **å¯è§£é‡Šæ€§**
   - ç”Ÿé•¿å› ç´ å¯è§†åŒ–
   - ç‰©ç†çº¦æŸåˆ†æ

### æ½œåœ¨é—®é¢˜

1. **ç‰©ç†å»ºæ¨¡ç®€åŒ–**
   - å®é™…ç”Ÿé•¿æ›´å¤æ‚
   - ç¯å¢ƒå› ç´ æœªè€ƒè™‘

2. **è¯„ä¼°ä¸è¶³**
   - éœ€è¦æ›´å¤šç”Ÿç‰©éªŒè¯
   - é•¿æœŸé¢„æµ‹æœªå……åˆ†æµ‹è¯•

3. **å®ç”¨éšœç¢**
   - æ•°æ®é‡‡é›†è®¾å¤‡æˆæœ¬
   - å®æ—¶éƒ¨ç½²æŒ‘æˆ˜

---

## ğŸ¯ ç»¼åˆç†è§£

### æ ¸å¿ƒåˆ›æ–°

1. **ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ**: ç»“åˆç‰©ç†çº¦æŸå’Œæ•°æ®é©±åŠ¨
2. **ç¨€ç–è§‚æµ‹é‡å»º**: ä»…éœ€3-5æ¬¡è§‚æµ‹
3. **æ—¶é—´æ³¨æ„åŠ›**: æ•è·é•¿æœŸç”Ÿé•¿è¶‹åŠ¿
4. **å½¢çŠ¶ä¸€è‡´æ€§**: ç¡®ä¿è§£å‰–å­¦åˆç†æ€§

### æŠ€æœ¯è´¡çŒ®

| æ–¹é¢ | è´¡çŒ® |
|------|------|
| **æ–¹æ³•åˆ›æ–°** | é¦–ä¸ªå°†PINNç”¨äº3Dç”Ÿé•¿è½¨è¿¹ |
| **å†œä¸šAI** | æ¤ç‰©è¡¨å‹åˆ†ææ–°èŒƒå¼ |
| **æ—¶åºå»ºæ¨¡** | ç¨€ç–æ—¶é—´ç‚¹é‡å»º |
| **å¤šå­¦ç§‘äº¤å‰** | è®¡ç®—æœºè§†è§‰ + ç”Ÿç‰©å­¦ |

### ç ”ç©¶æ„ä¹‰

1. **ç§‘å­¦ä»·å€¼**
   - ä¸ºç”Ÿé•¿å»ºæ¨¡æä¾›æ–°æ–¹æ³•
   - ä¿ƒè¿›å®šé‡æ¤ç‰©å­¦ç ”ç©¶

2. **åº”ç”¨ä»·å€¼**
   - æé«˜è‚²ç§æ•ˆç‡
   - é™ä½æ•°æ®é‡‡é›†æˆæœ¬

3. **æœªæ¥æ–¹å‘**
   - å¤šå™¨å®˜ååŒå»ºæ¨¡
   - ç¯å¢ƒå“åº”å»ºæ¨¡
   - åœ¨çº¿ç›‘æµ‹ç³»ç»Ÿ

### ä¸è”¡æ™“æ˜Šå…¶ä»–å·¥ä½œçš„è”ç³»

3Dç”Ÿé•¿è½¨è¿¹é‡å»ºå»¶ç»­äº†è”¡æ™“æ˜Šåœ¨3Dè§†è§‰å’Œé‡å»ºé¢†åŸŸçš„ç ”ç©¶ï¼š

1. **3Dè§†è§‰è„‰ç»œ**
   ```
   3D Orientation Field (2020)
          â†“
   3D Tree Segmentation (2017, 2019)
          â†“
   CornerPoint3D (2025)
          â†“
   3D Growth Trajectory (2025)
   ```

2. **æ–¹æ³•æ¼”è¿›**
   - ä»é™æ€3Dåˆ†æåˆ°åŠ¨æ€4D
   - ä»å•å¸§åˆ°æ—¶åºå»ºæ¨¡
   - ä»çº¯æ•°æ®é©±åŠ¨åˆ°ç‰©ç†çº¦æŸ

3. **åº”ç”¨æ‰©å±•**
   - æ—©æœŸ: é€šç”¨3Dåˆ†å‰²
   - ä¸­æœŸ: LiDARæ ‘æœ¨æ£€æµ‹
   - è¿‘æœŸ: æ¤ç‰©4Dç”Ÿé•¿åˆ†æ

### å½±å“åŠ›ä¸å¼•ç”¨

è¯¥å·¥ä½œçš„é¢„æœŸå½±å“ï¼š
- å†œä¸šAIé¢†åŸŸ
- è®¡ç®—æ¤ç‰©å­¦
- 3Dæ—¶åºå»ºæ¨¡
- PINNåº”ç”¨

---

## é™„å½•ï¼šå…³é”®å…¬å¼é€ŸæŸ¥

```
ç”Ÿé•¿è½¨è¿¹å‡½æ•°:
  f: â„^+ â†’ â„^{NÃ—3}
  f(t_i) â‰ˆ X_i

ç‰©ç†çº¦æŸ:
  âˆ‚f/âˆ‚t = v(p,t)  (è¿ç»­æ€§)
  âˆ‚Ï/âˆ‚t + âˆ‡Â·(Ïv) = 0  (è´¨é‡å®ˆæ’)

æ³¨æ„åŠ›:
  Attn(Q,K,V) = softmax(QK^T/âˆšd_k)V

å½¢çŠ¶æŸå¤±:
  L_corr = Î£â€–f(p_i,t_i) - f(p_j,t_j)â€–Â²
  L_surf = âˆ«â€–âˆ‡_n fâ€–Â² dS
  L_vol = |Vol(f(t)) - Vol(f(tâ‚€))Â·e^{Î±t}|Â²
```

---

**ç¬”è®°ç”Ÿæˆæ—¶é—´**: 2026-02-20
**ç²¾è¯»æ·±åº¦**: â˜…â˜…â˜…â˜…â˜… (äº”çº§ç²¾è¯»)
**æ¨èæŒ‡æ•°**: â˜…â˜…â˜…â˜…â˜† (å†œä¸šAI/3Dé‡å»ºé¢†åŸŸé‡è¦è´¡çŒ®)
**åˆ›æ–°æ€§**: â˜…â˜…â˜…â˜…â˜† (PINNåœ¨ç”Ÿé•¿å»ºæ¨¡çš„åˆ›æ–°åº”ç”¨)
**è·¨å­¦ç§‘ä»·å€¼**: â˜…â˜…â˜…â˜…â˜… (è®¡ç®—æœºè§†è§‰ä¸ç”Ÿç‰©å­¦ç»“åˆ)
