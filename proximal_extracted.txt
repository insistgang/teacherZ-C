Statistics and Computing (2022) 32:87
https://doi.org/10.1007/s11222-022-10152-9
Proximal nested sampling for high-dimensional Bayesian model
selection
Xiaohao Cai1,2
¬∑ Jason D. McEwen1,3 ¬∑ Marcelo Pereyra4
Received: 15 April 2022 / Accepted: 9 September 2022 / Published online: 5 October 2022
¬© The Author(s) 2022
Abstract
Bayesian model selection provides a powerful framework for objectively comparing models directly from observed data,
without reference to ground truth data. However, Bayesian model selection requires the computation of the marginal likelihood
(model evidence), which is computationally challenging, prohibiting its use in many high-dimensional Bayesian inverse
problems. With Bayesian imaging applications in mind, in this work we present the proximal nested sampling methodology
to objectively compare alternative Bayesian imaging models for applications that use images to inform decisions under
uncertainty. The methodology is based on nested sampling, a Monte Carlo approach specialised for model comparison, and
exploits proximal Markov chain Monte Carlo techniques to scale efÔ¨Åciently to large problems and to tackle models that are
log-concave and not necessarily smooth (e.g., involving ‚Ñì1 or total-variation priors). The proposed approach can be applied
computationally to problems of dimension O(106) and beyond, making it suitable for high-dimensional inverse imaging
problems. It is validated on large Gaussian models, for which the likelihood is available analytically, and subsequently
illustrated on a range of imaging problems where it is used to analyse different choices of dictionary and measurement model.
Keywords Nested sampling ¬∑ MCMC sampling ¬∑ Marginal likelihood ¬∑ Bayesian evidence ¬∑ Inverse problems ¬∑ Proximal
optimisation ¬∑ Model selection
1 Introduction
High-dimensionalinverseproblemsareubiquitousinthedata
and imaging sciences, as well as in the physical and engineer-
ing sciences more generally. Due to limitations of the data
observation process and measurement noise, or even just due
to the nature of the problem at hand, most inverse problems
encountered are seriously ill-conditioned or ill-posed (canon-
B Xiaohao Cai
x.cai@soton.ac.uk
Jason D. McEwen
jason.mcewen@ucl.ac.uk
Marcelo Pereyra
m.pereyra@hw.ac.uk
1
Mullard Space Science Laboratory (MSSL), University
College London (UCL), Dorking RH5 6NT, UK
2
School of Electronics and Computer Science, University of
Southampton, Southampton SO17 1BJ, UK
3
Alan Turing Institute, London NW1 2DB, UK
4
School of Mathematical and Computer Sciences, Heriot-Watt
University, Edinburgh EH14 4AS, UK
ical examples include, e.g., medical and radio interferometric
imaging;Durmusetal.2018;Caietal.2019;Zhouetal.2020;
Lunz et al. 2021). Developing better methodology for solv-
ing challenging inverse problems is a signiÔ¨Åcant focus of the
community. The Bayesian statistical framework is currently
one of the predominant frameworks to perform inference in
inverse problems (Robert and Casella 2004; Pereyra et al.
2016). The choice of the Bayesian model used has a profound
impact on the solutions delivered, as alternative models can
lead to signiÔ¨Åcantly different point estimations and uncer-
tainty quantiÔ¨Åcation results.
In this article we develop methodology to objectively
compare alternative Bayesian models in performing infer-
ence in the regime of high-dimensional inverse problems,
directly form the observed data and in the absence of ground
truth. Motivated by applications in computational imaging,
we focus on the comparison of models with posterior distri-
butions that are log-concave and potentially not smooth. In
this context, model selection has been traditionally addressed
through benchmark experiments involving ground truth data
and expert supervision. However, for many applications it
is difÔ¨Åcult and expensive to produce reliable ground truth
123
87
Page 2 of 22
Statistics and Computing (2022) 32 :87
data. Moreover, for many problems it is simply impossible.
Bayesian model selection provides a framework for selecting
the most appropriate model directly from the observed data
in an objective manner and without reference to ground truth
data.
Bayesian model selection requires the computation of the
marginal likelihood of the data ‚Äì the average likelihood of a
model over its prior probability space ‚Äì which is also called
the Bayesian evidence. This quantity is a key ingredient of
model selection statistics such as Bayes factors and like-
lihood ratio tests (Robert 2007). The computation of the
marginal likelihood for high-dimensional models is highly
non-trivial because it requires the computation of integrals
over the (high-dimensional) solution space. For example,
in the context of Bayesian imaging problems, the dimen-
sion is given by the number of parameters (e.g. pixels) of
interest, which frequently reach sizes of O(105)‚ÄìO(106) and
beyond. For such settings, the evaluation of the marginal like-
lihood has been previously considered to be computationally
intractable.
Broadly speaking, general purpose Monte Carlo methods
can only handle model selection tasks for problems of dimen-
sion O(10) to O(102) (for reviews see Clyde et al. 2007;
Friel and Wyse 2012; Llorente et al. 2020). Nested sam-
pling (Skilling 2006), a state-of-the-art Monte Carlo strategy
designed speciÔ¨Åcally for model selection, has enabled model
selection for moderate dimensional problems of size O(102)
to O(103) (Mukherjee et al. 2006; Feroz and Hobson 2008;
Feroz et al. 2009; Brewer et al. 2011; Feroz and Skilling
2013; Handley et al. 2015). To the best of our knowledge,
model selection for larger problems is currently possible only
for models with very speciÔ¨Åc structures (e.g., conditionally
Gaussian models; Harroue 2020).
In this work, we address the difÔ¨Åcult computation of the
marginal likelihood by proposing a new methodology that
carefully integrates nested sampling (Skilling 2006) with
proximal Markov chain Monte Carlo (MCMC) (Pereyra
2016; Durmus et al. 2018). This leads to a proximal
nested sampling methodology specialised for comparing
high-dimensional posterior distributions that are log-concave
but potentially not smooth. The proposed approach can be
applied computationally to log-concave models of dimension
O(106) and beyond, making it suitable for model comparison
in Bayesian imaging problems. We demonstrate the approach
with a range of scientiÔ¨Åc imaging applications.
The remainder of the article is organised as follows. In
Sect. 2 we recall the Bayesian model selection approach,
highlight the associated computational challenges, and dis-
cuss proximal MCMC methodology for Bayesian com-
putation for inverse problems with an underlying convex
geometry. Section 3 recalls the standard nested sampling
method. Our proposed proximal nested sampling framework
is presented in general form in Sect. 4. In Sect. 5 explicit
formsofproximalnestedsamplingarepresentedforcommon
forms of the likelihood and prior that arise in imaging sci-
ences. Experimental results validating the proposed method
and showcasing its use in scientiÔ¨Åc imaging applications are
reported in Sect. 6. Finally, we conclude in Sect. 7.
2 Bayesian inference for high-dimensional
inverse problems
In this section we brieÔ¨Çy recall the Bayesian decision-
theoretic approach to model comparison, introduce some
elements of convex analysis which are essential for our
method, and review proximal MCMC methods, which are
an important component of the proximal nested sampling
methodology proposed in Sect. 4. We conclude the section by
brieÔ¨Çy explaining the computational difÔ¨Åculties encountered
in high-dimensional Bayesian model selection and why it is
necessary to develop new methodology for this task. Read-
ers familiar with Bayesian model selection and with proximal
MCMC methodology may prefer to skip this section and con-
tinue reading from Sect. 3.
2.1 Bayesian estimation and model selection
Let  ‚äÜRd. We consider the estimation of a quantity
of interest x ‚àà from observed data y. Bayesian meth-
ods address such problems by postulating a statistical model
M relating x and y, from which estimators of x and other
inferences can be derived. More precisely, M deÔ¨Ånes a
joint probability distribution p(x, y|M) speciÔ¨Åed via the
decomposition p(x, y|M) = p(y|x, M)p(x|M), where
p(y|x, M) denotes the likelihood of x for the observed data
y, and the marginal p(x|M) is the so-called prior of x. Fol-
lowing Bayes‚Äô theorem, inferences on x|y are then based on
the posterior distribution
p(x|y, M) = p(y|x, M)p(x|M)
p(y|M)
,
(1)
which models our beliefs about x after observing y. With
applications in Bayesian imaging sciences in mind, we focus
on posterior distributions that are log-concave and assume
that the potential function x ‚Üí‚àílog p(x|y, M) is convex
lower semicontinuous (l.s.c.) on , but possibly not smooth.
This is an important class of models in modern Bayesian
imaging sciences because it leads to point estimators that are
by construction well-posed and that can be efÔ¨Åciently esti-
mated by using scalable proximal convex optimisation and
stochastic sampling methods (Kaipio and Somersalo 2005;
Robert and Casella 2004; Pereyra et al. 2016).
We condition on M explicitly in (1) because our focus
is model selection, where one entertains several alterna-
123
Statistics and Computing (2022) 32 :87
Page 3 of 22
87
tive posterior distributions for x|y stemming from different
underlying modelling assumptions. As a result, rather than
the posterior p(x|y, M), our main object of interest is the
marginal likelihood or model evidence
p(y|M)=


p(y, x|M)dx =


p(y|x, M)p(x|M)dx,
(2)
which measures the likelihood of the observed data under
model M, and which we use to objectively compare dif-
ferent models relating x and y (Robert 2007). Notice that
the likelihood of the observed data y under the model M
is essentially the expectation (or average value) of the like-
lihood function p(y|x, M) with respect to (w.r.t.) the prior
p(x|M). Therefore, a model that allocates its prior mass
to solutions that agree with the observed data achieves a
large marginal likelihood value. Conversely, a low marginal
likelihood value indicates that only a small proportion of
the solutions favoured by the prior agree with the observed
data. In other words, the marginal likelihood (2) measures
the degree to which the observed data is in agreement with
the assumptions of the model, and in doing so it provides a
goodness-of-Ô¨Åt summary. Moreover, because all priors have
the same total probability mass (i.e.,

 p(x)dx = 1), the
likelihood (2) naturally incorporates Occams‚Äôs razor, trading
off model simplicity and accuracy and penalising over-Ô¨Åtting
(Robert 2007).
Bayesian model selection arises from the common and
natural inquiry of which model is the most suitable to anal-
yse x|y from a set of models M1, . . . , MK available. For
simplicity and without loss of generality, we suppose two
alternative models M1 and M2 (the generalisation to addi-
tional models is straightforward). From Bayesian decision
theory, to objectively compare the two models in settings
without ground truth available, one should calculate the
Bayes factor (Robert 2007)
œÅ12 = p(M1|y)
p(M2|y)
p(M2)
p(M1)
(3)
where p(M1) and p(M2) denote the prior probabilities
assigned to the two competing models, and where, from
Bayes‚Äô theorem, we have that for any i ‚àà{1, 2}
p(Mi|y) =
p(y|Mi)p(Mi)
p(y|M1)p(M1) + p(y|M2)p(M2) .
(4)
By developing (3) we can easily express the Bayes factor as
the likelihood ratio
œÅ12 = p(y|M1)
p(y|M2),
(5)
highlighting that œÅ12 is invariant to choice of the prior prob-
abilities p(M1) and p(M2). If one assumes p(M1) =
p(M2) = 1/2 to reÔ¨Çect the absence of prior information,
then the factor also coincides with the posterior probability
ratio p(M1|y)/p(M2|y).
Being a likelihood ratio, the factor œÅ12 is straightforward
to read: if œÅ12 ‚â´1, we prefer model M1 over the alternative
M2; conversely, if œÅ12 ‚â™1, we prefer model M2; and if
œÅ12 ‚âà1, we do not prefer either, inasmuch as the data y are
insufÔ¨Åcient for us to make an informed judgement. The fact
that œÅ12 is a likelihood ratio is also appealing from a frequen-
tist viewpoint, as it is associated with the most powerful test
for these two model hypotheses (Casella and Berger 2002).
Unfortunately, calculating œÅ12 is generally not possi-
ble in large-scale settings because the dimensionality of x
renders the marginal likelihoods p(y|M1) and p(y|M2)
computationally intractable. More precisely, the marginal
likelihoods are doubly-intractable because they require com-
puting two intractable integrals over the space of solutions
: the marginalisation of x denoted explicitly in (2); and
the normalising constant of the priors p(x|Mi) when these
are not available analytically, which otherwise implicitly also
requires integrating over .
It is worth emphasising at this point that this major dif-
Ô¨Åculty related to model selection is not encountered when
performing inferences with the posteriors p(x|y, M1) and
p(x|y, M2) individually, as one can use MCMC methods to
sample from p(x|y, M) without ever having to evaluate the
marginal likelihood p(y|M). As a result, efÔ¨Åcient Bayesian
model selection remains an open problem in many areas of
science and engineering that have widerly adopted Bayesian
inference techniques for point estimation and uncertainty
quantiÔ¨Åcation.
In the following we brieÔ¨Çy recall MCMC sampling meth-
ods derived from the overdamped Langevin diffusion pro-
cess, particularly proximal MCMC techniques specialised
for large models that are log-concave, and explain why it is
necesary to modify them to enable efÔ¨Åcient model compari-
son.
2.2 Bayesian computation and proximal MCMC
methods
2.2.1 Convex analysis
Let f : Rd ‚Üí[‚àí‚àû, +‚àû]. The function f is said to be
proper if there exists x0 ‚ààRd such that f (x0) < +‚àû.
Denote for all M ‚ààR, { f ‚â§M} = {z ‚ààRd | f (z) ‚â§M}.
The function f is l.s.c. if for all M ‚ààR, { f ‚â§M} is a closed
subset of Rd. For k ‚â•0, denoted by Ck(Rd) the set of k-
times continuouslydifferentiablefunctions. For f ‚ààC1(Rd),
denote by ‚àáf the gradient of f . We say that f ‚ààC1(Rd) is a
Lipschitz continuously differentiable function if there exists
123
87
Page 4 of 22
Statistics and Computing (2022) 32 :87
C ‚â•0 such that for all x, y ‚ààRd, ‚à•‚àáf (x) ‚àí‚àáf (y)‚à•‚â§
C‚à•x ‚àíy‚à•.
Given a convex, proper, l.s.c. function h
: Rd
‚Üí
(‚àí‚àû, +‚àû] and Œª > 0, the proximal operator (Bauschke
and Combettes 2011) associated with function h at x ‚ààRd
is deÔ¨Åned as
proxŒª
h(x) = argmin
u‚ààRd

h(u) + ‚à•u ‚àíx‚à•2
2/2Œª

.
(6)
When Œª = 1, we denote prox1
h(x) by proxh(x) for simplicity.
Let K be a closed convex set in Rd and let œáK be the
characteristic function for K, deÔ¨Åned by œáK(x) = 0 if x ‚àà
K and +‚àûotherwise. The proximal operator of œáK is the
projection onto K, given by
projK(x) = argmin
u‚ààRd

œáK(u) + ‚à•u ‚àíx‚à•2
2/2

.
(7)
The convex conjugate of function h, denoted by h‚àó, is
deÔ¨Åned as
h‚àó(x) = sup
u‚ààRd

x‚ä§u ‚àíh(u)

.
(8)
Its proximal operator can be related to the proximal operator
of h by
proxh‚àó(x) = x ‚àíproxh(x).
(9)
The Œª-Moreau-Yosida envelope of h (Bauschke and Com-
bettes 2011) is given for any x ‚ààRd and Œª > 0 by
hŒª(x) = min
u‚ààRd

h(u) + ‚à•u ‚àíx‚à•2
2/2Œª

.
(10)
The envelope hŒª is continuously differentiable with Lips-
chitz gradient. In particular, using the proximal operator, the
gradient of hŒª can be written
‚àáhŒª(x) =

x ‚àíproxŒª
h(x)

/Œª,
(11)
with Œª simultaneously controlling the Lipschitz constant of
‚àáhŒª as well as the error between h and its smooth approxi-
mation hŒª. This approximation error can be made arbitrarily
small by reducing Œª, at the expense of deteriorating the reg-
ularity of ‚àáhŒª, and consequently the speed of convergence
of proximal Bayesian computation algorithms rely on hŒª.
2.2.2 Proximal Langevin MCMC sampling
Consider the problem of calculating probabilities or expec-
tations with respect to (w.r.t.) some distribution œÄ(dx)
which admits a density œÄ(x) w.r.t. the usual d-dimensional
Lebesgue measure. In the context of Bayesian inference,
this is typically the posterior p(x|y, M). Evaluating expec-
tations and probabilities w.r.t. œÄ is non-trivial in problems
of moderate and high dimension because of the integrals
involved, which are usually beyond the scope of analyti-
cal and deterministic numerical integration schemes. These
calculations are further complicated when the normalising
constant of œÄ is not known, as this requires evaluating an
additional d-dimensional integral. Monte Carlo sampling
methods address these difÔ¨Åculties by simulating a set of
samples from œÄ followed by Monte Carlo stochastic inte-
gration to compute probabilities and expectations w.r.t. œÄ.
While there are different ways of simulating samples from
œÄ, we focus on MCMC strategies where one proceeds by
constructing a Markov chain that has œÄ as its invariant sta-
tionary distribution. Again, there are different methods for
constructing such Markov chains (see Robert and Casella
2004 for an excellent introduction to MCMC methodology
and Green et al. 2015 for a survey of recent developments in
the Bayesian computation literature).
The fastest provably convergent MCMC methods for
Bayesian inference models can be derived from the Langevin
diffusion process, which we recall below. For simplicity,
rather than presenting the approach in full generality, we
focus our presentation on proximal overdamped Langevin
sampling for non-smooth models, which we later use in the
proximal nested sampling method proposed in Sect. 4. For a
more exhaustive introduction to the topic please see Vargas
et al. (2020, Section 2) and references therein.
Assume that œÄ
admits a decomposition œÄ(x)
‚àù
exp{‚àíf (x) ‚àíg(x)} for all x ‚ààRd, where f ‚ààC1(Rd) with
‚àáf Lipschitz continuous with constant L f , and where g is
a proper l.s.c. function that is convex on Rd but potentially
non-smooth (e.g., g could encode constraints on the solu-
tion space and involve non-smooth regularisers such as the
‚Ñì1 norm). To simulate from œÄ, we construct the overdamped
Langevin stochastic differential equation (SDE) on Rd given
by Durmus et al. (2018)
dXt =‚àí[‚àáf (Xt)+‚àágŒª(Xt)]dt+
‚àö
2dWt,
X0 =x0,
(12)
where (Wt)t‚â•0 is a d-dimensional Brownian motion, gŒª is
the Moreau-Yosida envelop of g given by (10), Œª > 0 is a
smoothing parameter that we will discuss later, and x0 ‚ààRd.
When x ‚Üíf (x) + gŒª(x) is convex, the SDE has a unique
strong solution and Xt converges exponentially fast (as t ‚Üí
‚àû) to an invariant measure that is in the neighbourhood of
œÄ.
To use (12) for Bayesian computation, we use a numerical
solver to compute a discrete-time approximation of Xt over
some time period t ‚àà[0, T ]; the resulting discrete sample
path constitutes our set of Monte Carlo samples. In particu-
lar, in this article we use the conventional Euler-Maruyama
123
Statistics and Computing (2022) 32 :87
Page 5 of 22
87
approximation
Xn+1 = Xn ‚àíŒ¥
2‚àáf (Xn) ‚àíŒ¥
2‚àágŒª(Xn) +
‚àö
Œ¥Zn+1,
(13)
where Œ¥ ‚àà[0, 1/(L f + 1/Œª)] is a given stepsize and
(Zn)n‚â•1 is a sequence of i.i.d. d-dimensional standard Gaus-
sian random variables. This MCMC method is known as the
Moreau-Yosida unadjusted Langevin algorithm (MYULA)
(Durmus et al. 2018). The Markov chain (13) is usually
implemented by using (11) and reads
Xn+1 = Xn‚àíŒ¥
2‚àáf (Xn)‚àíŒ¥
2Œª

Xn‚àíproxŒª
g(Xn)
	
+
‚àö
Œ¥Zn+1.
(14)
The smoothing parameter Œª and the stepsize Œ¥ jointly control
a bias-variance trade-off between the asymptotic estimation
errors and non-asymptotic errors associated with using a
Ô¨Ånite number of iterations. In this article, we use Œª = 1/L f
and Œ¥ = 0.8/(L f + 1/Œª) as recommended in Durmus et al.
(2018) (recall that ‚àáf is Lipschitz continuous with constant
L f , please see Durmus et al. 2018; Vargas et al. 2020 for
further details).
The samples generated by (14) can be directly used
for biased Monte Carlo estimation (Durmus et al. 2018).
Alternatively, at the expense of additional computation,
one can supplement each iteration of MYULA with an
MH (Metropolis-Hastings) correction step to asymptotically
remove the approximation errors related to the discretisa-
tion of the SDE and the use of gŒª instead of g, leading to
a type of Metropolis-adjusted Langevin algorithm (MALA)
(see Pereyra 2016 for details).
2.3 Estimation of marginal likelihoods and Bayes
factors
Let {Xn}N
n=1 be a set of samples from œÄ (or an approxima-
tion of œÄ), generated by using a proximal MCMC method or
otherwise. Following a Monte Carlo integration approach,
the expectation of any function œÜ : Rd ‚ÜíR w.r.t. œÄ is
approximated by
ÀÜEœÄ(œÜ) = 1
N
N

n=1
œÜ(Xn) ,
(15)
which, under assumptions, converges to the truth EœÄ(œÜ) =

 œÜ(x)œÄ(x)dx as N increases (or to a biased estimate if
the samples are not exactly from œÄ). The accuracy of Monte
Carlo estimates depends of course on the number of sam-
ples N and on the properties of the MCMC method used, but
it also depends crucially on the variance VarœÄ(œÜ). Unfortu-
nately, VarœÄ(œÜ) is often very large for the kinds of functions
œÜ required for estimating the marginal likelihood (2) (and in
some cases VarœÄ(œÜ) is not even deÔ¨Åned), leading to Monte
Carlo estimators of the marginal that behave poorly (Newton
and Raftery 1994). As a result, it is difÔ¨Åcult to use the sam-
ples {Xn}N
n=1 to perform model selection. Several strategies
have been proposed to address the aforementioned difÔ¨Åculty
and derive well-posed estimators for the marginal likelihood
(2) and the Bayes factor (3) (for reviews of classical methods
see Friel and Wyse 2012; Clyde et al. 2007).
One avenue is to generate samples from a sequence of
distributions bridging œÄ to some tractable reference œÄ0 such
as the prior distribution or a Gaussian approximation of œÄ,
e.g., thermodynamic integration (O‚ÄôRuanaidh and Fitzgerald
1996) and annealed important sampling (Neal 2001). Such
strategies struggle with large problems because the number
of intermediate distributions grows quickly as d increases.
Another promising approach to derive computationally
efÔ¨Åcient estimators is to construct Rao-Blackwellized esti-
mators by carefully introducing auxiliary variables, as pro-
posed in the seminal papers Chib (1995) and Chib and
Jeliazkov (2001). This strategy has been successfully applied
recently to signal and image processing models that are con-
ditionally Gaussian given conjugate model hyper-parameters
(Harroue 2020). Some generalisations are possible, but con-
structing efÔ¨Åcient Rao-Blackwellized estimators for more
general classes of models, e.g., of the form (1), is highly
non-trivial.
An alternative natural strategy for stable Monte Carlo esti-
mators for (2) and (3) is to construct a truncated estimator by
Ô¨Årst using the samples {Xn}N
n=1 to identify a suitable truncat-
ing set A, followed by a sample average (15) only with the
samples verifying Xn ‚ààA (Brosse et al. 2017). Although
by construction well-posed, truncated estimators need to be
de-biased by using the volume of A, which is usually very
expensive to compute when the dimension d is large. From
the results of Brosse et al. (2017), we believe that this strat-
egy is unlikely to produce scalable methods suitable for large
problems. One can circumvent or simplify the calculation of
the volume of A (e.g., see Durmus et al. 2018), but in our
experience the resulting estimators become unstable and are
difÔ¨Åcult to use.
Another alternative approach, which is agnostic to the
sampling method, is the harmonic mean estimator (Newton
and Raftery 1994); although, in its original form the vari-
ance of the estimator can be very poorly behaved such that
the estimator can be highly inaccurate in practice. Strategies
to resolve this issue have been developed in the recently pro-
posed learnt harmonic mean estimator (McEwen et al. 2022),
which has been shown to be highly effective and can scale
to dimension O(103) and beyond. Nevertheless, it may be
challenging to scale this approach to the high-dimensional
settings considered in this paper.
123
87
Page 6 of 22
Statistics and Computing (2022) 32 :87
One can also consider the widely used Laplace‚Äôs method
(Tierney and Kadane 1986), which relies on the assumption
that the posterior distribution can be adequately approxi-
mated by a Gaussian distribution. Unfortunately, this is a
strong assumption that often leads to inaccurate estimates in
inverse problems that are ill-conditioned or ill-posed, partic-
ularly if d ‚â•dim(y). Many other alternatives are described
in the literature, e.g., the Savage-Dickey density ratio (Trotta
2007) and Reversible Jump MCMC (Green 1995), which are
mainly useful for nested or small models. It is worth mention-
ing that there are also some model selection strategies that do
not rely on the computation of the marginal likelihood (see,
e.g., Kamary et al. 2018; Pereyra and McLaughlin 2016);
however these are usually very computationally intensive.
Finally, nested sampling provides a distinctively differ-
ent approach for efÔ¨Åciently estimating (2) and (3) (Skilling
2006). The key idea underpinning nested sampling is the
re-parameterisation of the marginal likelihood (2) as a one-
dimensional integral of the likelihood with respected to the
enclosed prior volume. This greatly reduces the computa-
tion costs involved, provided that one can efÔ¨Åciently sample
from the prior distribution subject to a hard constraint on
the likelihood value. Nested sampling therefore shifts the
computational challenge from the direct evaluation of a
high-dimensional integral to sampling of the prior subject
to a hard likelihood constraint. The generation of samples
is challenging and previous works have considered a range
of sampling strategies. For example, conventional MCMC
sampling (Skilling 2006), rejection sampling (e.g. Mukher-
jee et al. 2006; Feroz and Hobson 2008; Feroz et al. 2009),
slice sampling (e.g. Handley et al. 2015), and more advanced
MCMC samplers such as Galilean Monte Carlo (Feroz and
Skilling 2013) and diffusive nested sampling (Brewer et al.
2011). Following over a decade of active research, nested
sampling is now a well-established technique for computing
the marginal likelihood that has found widespread applica-
tion, particularly in astronomy (e.g. Feroz and Hobson 2008;
Feroz et al. 2009; Trotta 2007). Nevertheless, broadly speak-
ing, current nested sampling techniques remain restricted to
moderate dimensional problems of size O(102) to O(103).
With imaging problems in mind, this article presents an
efÔ¨Åcient nested sampling methodology speciÔ¨Åcally designed
for high-dimensional log-concave models of the form (1).
A signiÔ¨Åcant novelty of the proposed approach is that we
address the difÔ¨Åcult generation of samples by using a prox-
imal MCMC technique that is naturally suited for dealing
with high-dimensional log-concave distributions subject to
hard convex constraints. Moreover, the proximal nature of
the method straightforwardly allows the use of the non-
smooth priors that are frequently encountered in imaging
(e.g., involving the ‚Ñì1 and total-variation regularisers), which
would not be easily addressed by using alternative gradient-
based samplers. Section 3 below reviews the nested sampling
approach. The proposed proximal nested sampling method-
ology is presented in Sect. 4.
3 Nested sampling
For ease of notation, given a model M, let L(x)
=
p(y|x, M) denote the likelihood function, œÄ(x) = p(x|M)
the prior, and
Z = p(y|M) =


L(x)œÄ(x)dx,
(16)
the marginal likelihood or evidence associated with a given
model M (to simplify notation, we henceforth omit the
dependence of Z and L on y).
Nested sampling (Skilling 2006) was proposed speciÔ¨Å-
cally to facilitate the efÔ¨Åcient evaluation of Z for Bayesian
model selection, while also supporting posterior inferences.
As mentioned previously, the calculation of the multidi-
mensional marginal likelihood integral (16) is generally
computationally intractable. Nested sampling addresses this
difÔ¨Åculty by cleverly converting (16) to a one-dimensional
integral by re-parameterising the likelihood in terms of the
enclosed prior volume. In addition, nested sampling involves
the prior via simulation and hence does not require knowl-
edge of the prior normalising constant. As a result, it also
circumvents the second level of intractability of Z that arises
in imaging problems.
Let L‚àó= {x|L(x) > L‚àó}, which groups the parameter
space  into a series of nested subspaces according to the
level-set or iso-likelihood contour L(x) = L‚àó‚â•0. Note that
L‚àó=0 = , since the likelihood values cannot be negative.
DeÔ¨Åne the prior volume Œæ by
Œæ(L‚àó) =

L‚àó
œÄ(x)dx.
(17)
Note that Œæ(0) = 1 and Œæ(Lmax) = 0, where Lmax is the max-
imum of the likelihood in . Let L‚Ä†(Œæ) be the inverse of the
prior volume Œæ(L‚àó) such that L‚Ä†(Œæ(L‚àó)) = L‚àó1, and assume
it is a monotonically decreasing function of Œæ (which, when L
is continuous and œÄ has connected support, is satisÔ¨Åed theo-
retically and up to practical numerical considerations that can
be trivially overcome; Sivia and Skilling 2006). The marginal
likelihood integral (16) can then be rewritten as
Z =
 1
0
L‚Ä†(Œæ)dŒæ,
(18)
1 In other words, L‚Ä† is a tail quantile function such that, for any L‚àó> 0,
the inverse of L‚Ä†(Œæ(L‚àó)) represents the probability that a draw x from
the prior œÄ will have a likelihood L(x) > L‚àó.
123
Statistics and Computing (2022) 32 :87
Page 7 of 22
87
which is a one-dimensional integral over the prior volume Œæ.
To evaluate (18) in practice it is necessary to compute
likelihood level-sets (iso-contours) Li, which correspond to
prior volumes 0 < Œæi ‚â§1 satisfying (17). A strategy to
generate the likelihoods Li and associated prior volumes Œæi is
discussed in Sect. 3.2. Once the likelihoods Li = L‚Ä†(Œæi) are
obtained,(18)canbeusedtoevaluatethemarginallikelihood,
where {Œæi}N
i=0 is a sequence of decreasing prior volumes, i.e.,
0 < ŒæN < ¬∑ ¬∑ ¬∑ < Œæ1 < Œæ0 = 1.
(19)
After discretising the integral (18) and associating each like-
lihood Li a quadrature weight wi, the marginal likelihood can
be computed numerically using standard quadrature methods
to give
Z ‚âà
N

i=1
Liwi.
(20)
The simplest assignment of the quadrature weights is wi =
Œæi‚àí1 ‚àíŒæi. The trapezium rule can also be used, i.e., wi =
(Œæi‚àí1 + Œæi+1)/2. The approximation error related to the dis-
cretisation of (18) can be made arbitrarily small by increasing
N.
3.1 Posterior inferences
Posterior inferences can be easily computed once Z is found.
Any sample taken randomly in the prior volume interval
(Œæi‚àí1, Œæi) is simply assigned an importance weight
pi = Liwi
Z
.
(21)
Samples with the assigned weights {pi} can then be used to
calculate posterior inferences such as the posterior moments,
probabilities, and credible regions.
3.2 Marginal likelihood evaluation
We now recall the basic procedure of the standard nested
sampling framework for evaluating the marginal likelihood,
i.e. to compute the summation (20). In particular, it is neces-
sary to generate samples of the likelihoods Li and to estimate
the corresponding enclosed prior volume Œæi.
Firstly, set the iteration number i = 0, the prior volume
Œæ0 = 1, and draw Nlive live samples of the unknown image x
from the prior distribution œÄ(x). Secondly, remove the sam-
ple with the smallest likelihood, say Li+1, from the live set
and replace it with a new sample. This new sample is again
drawn from the prior, but constrained to a higher likelihood
than Li+1.
It is necessary to then determine the prior volume Œæi+1
enclosed by the likelihood level-set (iso-contour) deÔ¨Åned by
Li+1. This is estimated in a stochastic manner. The enclosed
prior volume for each step i can be estimated by a shrinkage
ratio (random variable) ti+1, i.e. by Œæi+1 = ti+1Œæi, where ti+1
follows the distribution2
p(t) = Nlivet Nlive‚àí1.
(22)
Repeat the above step (removing the sample with the
smallest likelihood and estimating the updated prior volume)
until the entire prior volume (and the nested shells of like-
lihood) has been traversed. We Ô¨Ånally obtain {Li} and {Œæi}
which can then be used to compute the marginal likelihood by
(20). Moreover, we also simultaneously obtain a set of sam-
ples of the parameter x comprising all the discarded (dead)
samples and the Nlive Ô¨Ånal live samples, which can be used
for posterior parameter inferences (refer to Sect. 3.1 for fur-
ther detail).
The volume prior at step i of the nested sampling algo-
rithm, is Œæi = i
k=1 tk; recall that tk is the shrinkage ratio
and is independently distributed following the probability
density function given in (22). Since the mean and standard
deviation of log t are respectively
E(log t) = ‚àí1/Nlive
and œÉ(log t) = 1/Nlive,
(23)
we have
log Œæi ‚âà‚àíi/Nlive ¬±
‚àö
i/Nlive.
(24)
Ignoring uncertainty, one thus takes
Œæi = exp(‚àíi/Nlive).
(25)
A convergence criteria for the nested sampling algorithm
should be adopted. Terminating the algorithm too early or
late should be avoided to ensure the marginal likelihood is
estimated accurately without unnecessary additional compu-
tational cost. One stopping criterion is that the difference in
marginal likelihood estimates between two iterations falls
below a predeÔ¨Åned threshold, while another is to ensure a
sufÔ¨Åcient number of dead samples is used.
The pseudo code for the nested sampling algorithm is
given in Algorithm 1. Observe that the most challenging task
in the nested sampling algorithm is drawing samples from
the prior with the hard constraint that samples lie within Li ,
i.e. within the space deÔ¨Åned by the likelihood level-set (see
lines 8‚Äì10 in Algorithm 1). This constrained sampling step is
2 The probability distribution (22) is for the largest of Nlive samples
drawn uniformly from the interval [0, 1]. This follows since the param-
eter x is uniformly sampled from the prior œÄ(x) and {Œæi} are uniformly
distributed (by the relation dŒæ = œÄ(x)dx).
123
87
Page 8 of 22
Statistics and Computing (2022) 32 :87
relatively easy in small problems but can become very com-
putationally challenging as problem dimension increases. As
a result, nested sampling is usually restricted to problems of
moderate size.
Algorithm 1 Nested sampling algorithm
Initialization: Data Y. Set Z = 0, Œæ0 = 1 and i = 0. Draw Nlive
samples {xn}Nlive
n=1 from the prior distribution œÄ(x) in the prior space .
Output: Evidence Z and posterior probabilities {pi}.
for i = 1, . . . , until the stopping criterion reached
- Find the lowest likelihood, say Li, in the set of live samples.
- Compute weight wi = (Œæi‚àí1 ‚àíŒæi+1)/2, where Œæi = exp(‚àíi/Nlive).
- Update evidence by Z = Z + Liwi.
- Draw a new sample from the prior distribution œÄ(x) in the restricted
parameter space Li , and replace the individual sample associated with
the lowest likelihood Li in the set of live samples.
end for
Update the evidence by Z = Z + wi+1
Nlive
Nlive
n=1 L(xn).
Compute the posterior probability for each individual sample pi =
Liwi/Z.
3.3 Error estimation
If the prior volumes {Œæi} considered in the discretised inte-
gral (20) used to evaluate the marginal likelihood could be
assigned exactly, then the only error in the estimate of the
marginal likelihood would be due to the discretisation of the
integral, which is trivially O(1/N 2) and negligible when N
is sufÔ¨Åciently large. However, since the shrinkage ratio ti is
generated randomly, each prior volume Œæi is then assigned
approximately, which tends to overwhelm the error brought
by the discretisation of the integral and will therefore cause
the dominant source of uncertainty in the Ô¨Ånal computed
evidence Z. This uncertainty, fortunately, can be estimated
easily. We recall below the error estimation scheme presented
in Skilling (2006) using the entropy of the prior volumes.
This approach is highly efÔ¨Åcient since it does not require any
additional sampling.
Let P(Œæ) = L(Œæ)/Z be the posterior distribution regard-
ing the prior volume Œæ. Then the negative relative entropy H
can be deÔ¨Åned as
H =

P(Œæ) log[P(Œæ)]dŒæ ‚âà
N

i=1
Liwi
Z
log
 Li
Z

,
(26)
which can be computed directly from the obtained likeli-
hoods {Li}, weights {wi} and the evidence Z. Following
Skilling (2006), the standard deviation of the uncertainty of
log Z using the nested sampling algorithm reads ‚àöH/Nlive,
i.e.,
log Z = log
 N

i=1
Liwi

¬±

H
Nlive
.
(27)
In Chopin and Robert (2010), it is established that, under
someregularityconditions,theapproximationerrorisasymp-
totically Gaussian in the limit N ‚Üí‚àûand vanishes at the
usual Monte Carlo rate O(N ‚àí1/2). Moreover, the error scales
approximately linearly with the model dimension d.
4 Proximal nested sampling framework
The main difÔ¨Åculty in applying nested sampling to large
inverse problems is to efÔ¨Åciently simulate from the prior
distribution subject to a hard likelihood constraint. More
precisely, at iteration i, the samples from the prior are con-
strained to the region Li deÔ¨Åned by the likelihood level-set
corresponding to Li (i.e. where a new sample must have a
likelihood value greater than Li at iteration i).
In this section we present our proposed proximal nested
sampling method to address this challenging constrained
samplingproblem. Moreover, theproximal natureof thesam-
pling method ensures that non-differentiable distributions,
such as popular sparsity-promoting priors involving the ‚Ñì1
norm, are supported. We Ô¨Årst present the methodology of
proximal nested sampling for arbitrary log-concave distri-
butions of the form (1). Explicit forms of proximal nested
sampling for common choices of priors and likelihoods in
imaging sciences are presented in Sect. 5.
4.1 General constrained sampling problem
Following (1) and adopting the notation of Sect. 3, assume
that the prior and the likelihood are of the form œÄ(x) =
exp(‚àíf (x)) and L(x) = exp(‚àíg(x)), where f and g are
convex l.s.c. (lower semicontinuous) functions on .
We consider sampling from the prior œÄ(x), such that
L(x) > L‚àófor some generic likelihood value L‚àó> 0. Let
ŒπL‚àó(x) and œáL‚àó(x) be the indicator function and characteristic
function, respectively, deÔ¨Åned as
ŒπL‚àó(x) =

1, L(x) > L‚àó,
0, otherwise,
and
œáL‚àó(x) =

0,
L(x) > L‚àó,
+‚àû, otherwise.
(28)
Sincelogismonotonic,L(x) > L‚àóisequivalentto g(x) < œÑ,
where
œÑ = ‚àílog L‚àó.
(29)
123
Statistics and Computing (2022) 32 :87
Page 9 of 22
87
Let BœÑ := {x | g(x) < œÑ}. Then it is apparent that œáL‚àó(x),
as a constraint for x, is equivalent to œáBœÑ (x), where
œáBœÑ (x) =

0,
x ‚ààBœÑ,
+‚àû, otherwise.
(30)
Let œÄL‚àó(x) = œÄ(x)ŒπL‚àó(x) represent the prior distribu-
tion with the hard likelihood constraint L(x) > L‚àó. Since
ŒπL‚àó(x) = exp(‚àíœáL‚àó(x)), then we have
œÄL‚àó(x) = œÄ(x)ŒπL‚àó(x)
= exp(‚àíf (x))exp(‚àíœáL‚àó(x))
= exp(‚àí[ f (x) + œáL‚àó(x)])
= exp(‚àí[ f (x) + œáBœÑ (x)]).
(31)
Note that taking logarithm of œÄL‚àó(x) reads
‚àílog œÄL‚àó(x) = f (x) + œáBœÑ (x).
(32)
In the following section we introduce our proximal nested
sampling algorithm for parameter x to sample from the con-
strained prior distribution exp(‚àí[ f (x) + œáBœÑ (x)]).
4.2 Drawing a sample from the constrained prior
Sampling distributions over  is usually challenging because
of the dimensionality involved. Sampling from the con-
strained prior (32) is particularly difÔ¨Åcult because of the
hard constraint that x ‚ààBœÑ, encoded in the character-
istic function œáBœÑ (x). Sampling is further complicated if
the log-prior f (x) is not Lipschitz differentiable over 
(e.g. for non-differentiable sparsity-promoting priors), since
high-dimensional sampling methods rely heavily on gra-
dient information. To circumvent these issues we adopt a
proximal MCMC approach, which is particularly suitable
for high-dimensional distributions that are log-concave but
not smooth. More precisely, in a manner akin to Durmus
et al. (2018), we use the unadjusted Langevin algorithm
(ULA) MCMC sampling strategy combined with Moreau-
Yosida approximations of non-differential terms, followed
by Metropolis Hastings correction step to control the approx-
imations made, as described in Pereyra (2016).
Using the ULA iterative formula, for each given œÑ (recall
that œÑ corresponds to a likelihood value L‚àóby œÑ = ‚àílog L‚àó;
see (29)), we can generate the following Markov chain
x(k+1) =x(k) ‚àíŒ¥
2‚àá

f (x(k)) + œáBœÑ (x(k))

+
‚àö
Œ¥w(k+1),
(33)
where Œ¥ > 0 is the step size and wk+1 ‚àºN(0, 1K ) (a K-
sequence of standard Gaussian random variables).
The non-differentiable characteristic function œáBœÑ (x) can
be approximated by its Moreau-Yosida envelope œáŒª
BœÑ (x),
with approximation controlled by Œª > 0. It is straightfor-
ward to show that
œáŒª
BœÑ (x) = 1
2Œª‚à•x ‚àíx‚àó‚à•2
2,
(34)
where x‚àóistheclosestpointinBœÑ to x,givenbytheprojection
of x onto BœÑ, i.e. x‚àó= projBœÑ (x) = proxœáBœÑ (x). Critically,
the Œª-Moreau-Yosida envelope is 1
Œª-Lipschitz differentiable.
Its gradient can be calculated directly from (34) or by noting
(11), yielding
‚àáœáŒª
BœÑ (x) = (x ‚àíx‚àó)/Œª = (x ‚àíproxœáBœÑ (x))/Œª.
(35)
Replacing the characteristic function by its Moreau-
Yosida approximation in (33) , and noting the gradient (35),
yields
x(k+1) = x(k) ‚àíŒ¥
2‚àáf (x(k)) ‚àíŒ¥
2Œª

x(k) ‚àíproxœáBœÑ (x(k))

+
‚àö
Œ¥w(k+1).
(36)
When
f (x) is differentiable its gradient can be com-
puted directly (we consider the case where f (x) is non-
differentiable shortly). For differential log-priors f (x), (36)
provides the general strategy for sampling from the prior
subject to the hard likelihood constraint (with a subsequent
Metropolis-Hasting step as discussed below).
If the sample x(k) is already in BœÑ, i.e. x ‚ààBœÑ, the
term

x(k)‚àíproxŒª
œáBœÑ (x(k))

disappears and the Markov chain
iteration simply involves taking a noisy step to descent the
gradient. In contrast, if x(k) is not in BœÑ, i.e. x /‚ààBœÑ, then a
step is taken in the direction ‚àí

x(k) ‚àíproxŒª
œáBœÑ (x(k))

, which
acts to move the next iteration in the Markov chain in the
direction of the projection of x(k) onto the convex set BœÑ. This
term therefore acts to push the Markov chain back into the
constraintsetBœÑ ifitwandersoutsideofit,althoughduetothe
Moreau-Yosida approximation of œáBœÑ it does not guarantee
the constraint is satisÔ¨Åed (the subsequent Metropolis-Hasting
step does guarantee the hard likelihood constraint is satisÔ¨Åed
as discussed below).
When f (x) is non-differentiable, it may be approximated
by its differentiable Moreau-Yosida envelope f Œª(x). By not-
ing (11), the gradient of the term involving the sum of the
two Moreau-Yosida approximations then reads
‚àá( f Œª(x)+œáŒª
BœÑ (x))=(x‚àíproxŒª
f (x))/Œª
+(x‚àíproxœáBœÑ (x))/Œª.
(37)
123
87
Page 10 of 22
Statistics and Computing (2022) 32 :87
Here we have used the same regularisation parameter Œª >
0 for both approximations for notational brevity, although
clearly different parameters can be considered for f Œª(x) and
œáŒª
BœÑ (x) if desired.
Replacing in (33) both f (x) and œáBœÑ (x) by their Moreau-
Yosida approximations, and noting the gradient (37), yields
x(k+1) = (1 ‚àíŒ¥
Œª)x(k) + Œ¥
2ŒªproxŒª
f (x(k))
+ Œ¥
2ŒªproxœáBœÑ (x(k)) +
‚àö
Œ¥w(k+1).
(38)
For non-differentiable log-concave priors, (38) provides the
general strategy for sampling from the prior subject to the
hard likelihood constraint.
To summarise, given a proper initial sample, say x(0), we
generate a Markov chain by iteratively applying the Markov
kernel (36) if f is Lipschitz differentiable or the regularised
surrogate (38) if it is not, which allows drawing samples from
the prior that are likely to be within the likelihood iso-contour
L‚àó. This is the main challenge in nested sampling.
The Markov chains generated by ULA-type kernels
exhibit some bias resulting from the discretisation of the
Langevin stochastic differential equation and from the use of
Moreau-Yosida regularisations. This bias can be asymptoti-
cally removed by introducing a Metropolis-Hasting correc-
tion step to ensure convergence to the required target density.
In detail, at each iteration, a new candidate x‚Ä≤ generated using
formula (36) or (38) is then accepted with probability
min

1, q(x(k)|x‚Ä≤)œÄL‚àó(x‚Ä≤)
q(x‚Ä≤|x(k))œÄL‚àó(x(k))

,
(39)
where q(¬∑|¬∑) is a transition kernel, which we deÔ¨Åne by a
Gaussian related to the ULA random component (following
Pereyra 2016), i.e.,
q(x‚Ä≤|x(k)) ‚àºexp

‚àí

x‚Ä≤ ‚àíx(k) ‚àíŒ¥
2‚àálog œÄL‚àó(x(k))
2
2Œ¥
	
.
(40)
If the candidate sample x‚Ä≤ is outside of BœÑ, i.e. x‚Ä≤ /‚ààBœÑ,
then œÄL‚àó(x‚Ä≤) = 0 and according to the Metropolis-Hasting
update the candidate will not be accepted, ensuring the hard
likelihood constraint is satisÔ¨Åed.
We summarise our proximal technique to draw an individ-
ual sample from the prior under the hard likelihood constraint
in Algorithm 2.
4.3 Initialisation from the unconstrained prior
The initialisation of the nested sampling method is to draw
Nlive samples {xn}Nlive
n=1 from the prior distribution œÄ(x) in the
Algorithm 2 Proximal individual sample draw algorithm
ProxSampleDraw(x(0), L‚àó)
Initialization: k = 0, Kgap.
Input: x(0), L‚àó(starting point of Markov chain and likelihood thresh-
old).
Output: Individual sample xnew fulÔ¨Ålling the constraint L(xnew) > L‚àó.
Compute œÑ = ‚àílog L‚àó.
for k = 1, . . .
- Compute x(k) using the iterative formula (36) if f is differentiable;
otherwise (38).
- Metropolis-Hasting step following (39) to remove the estimation
bias.
if L(x(k)) > L‚àóand k ‚â•Kgap
break.
end if
end for
Set xnew = x(k).
prior space . If the log-prior f (x) is differentiable this may
be applied trivially with the ULA iterative formula. Other-
wise f (x) may again be approximated by its Moreau-Yosida
envelope and samples from the prior can be generated by the
iterative formula
x(k+1) = (1 ‚àíŒ¥
2Œª)x(k) + Œ¥
2ŒªproxŒª
f (x(k)) +
‚àö
Œ¥w(k+1).
(41)
To draw Nlive samples from the prior, it is necessary to
Ô¨Årst discard initial samples generated before converging on
the target prior distribution. Initial samples corresponding
to a number of burn-in iterations, say Kburn, are discarded.
Due to correlations between samples and the algorithm‚Äôs
memory footprint, the chain is thinned by discarding a
number of intermediate iterations between samples (the
chain‚Äôs thinning factor), say (Kgap ‚àí1). That is, only the
Kgap-th sample generated by the iterative formula is kept.
Only 1-in-Kgap samples are stored when k > Kburn and
mod(k ‚àíKburn, Kgap) = 0, where mod(¬∑, ¬∑) represents mod-
ulus after division. A Metropolis-Hasting step can also be
introduced here to remove the estimation bias. We summarise
the technique for drawing Nlive live samples from the prior
in Algorithm 3.
4.4 Proximal nested sampling algorithm
After embedding Algorithms 2 and 3 into Algorithm 1 (i.e.,
the standard nested sampling algorithm), we obtain our
proposed proximal nested sampling algorithm, which is sum-
marised in Algorithm 4. Recall that Algorithm 2 generates
a new single sample from the prior subject to the hard like-
lihood constraint, which is used to replace the sample with
the lowest likelihood value in the live sample set. We suggest
using a sample randomly selected from the live sample set
as a starting point for Algorithm 2.
123
Statistics and Computing (2022) 32 :87
Page 11 of 22
87
Algorithm 3 Proximal algorithm of drawing live samples
(from prior)
Initialization: Nlive, Kburn, Kgap, and x(0).
Output: Nlive live samples {xn}Nlive
n=1 (draw from the prior with no con-
straint).
for k = 1, . . . , Kburn
- Compute x(k) using the iterative formula (41).
end for
n = 1;
for k = Kburn + 1, . . . , Kburn + NliveKgap
- Compute x(k) using the iterative formula (41).
- Metropolis-Hasting step to remove the estimation bias.
if mod(k ‚àíKburn, Kgap) = 0
xn = x(k); n = n + 1.
end if
end for
So far we have presented the proximal nested sampling
framework in its most general form for arbitrary log-concave
distributions, which is based on the iterative formula (36)
or (38) to sample from the constrained prior. These itera-
tive formula involve computing proximal operators related
to the log-prior and likelihood constraint, which we have
not yet considered in further detail. In principle computing
proximal operators involves solving a minimisation problem,
although in many scenarios this can be solved analytically or
otherwiseefÔ¨Åcientiterativealgorithmscanbeused.Inthefol-
lowing section we consider explicit forms of proximal nested
sampling for common forms of the prior and likelihood, out-
lining explicitly how the required proximal operators can be
computed.
Algorithm 4 Proximal nested sampling algorithm
Initialization: Data Y. Set Z = 0, Œæ0 = 1 and i = 0. Using Algorithm 3
to draw Nlive samples {xn}Nlive
n=1 from the prior distribution œÄ(x) in the
prior space .
Output: Evidence Z and posterior probabilities {pi}.
for i = 1, . . . , until the stopping criterion reached
- Find the lowest likelihood, say Li, in the set of live samples.
- Compute weight wi = (Œæi‚àí1 ‚àíŒæi+1)/2, where Œæi = exp(‚àíi/Nlive).
- Update evidence by Z = Z + Liwi.
- Randomly select a sample, say x(0), from the set of live samples.
-
Use
Algorithm
2
to
draw
a
new
sample
xnew
=
ProxSampleDraw(x(0), Li) from the prior distribution œÄ(x) in the
restricted parameter space Li , and replace the individual sample xi,low
by the newly drawn sample xnew.
end for
Update the evidence by Z = Z + Nlive
n=1 L(xn)wi+1/Nlive.
Compute the posterior probability for each individual sample pi =
Liwi/Z.
Before concluding this section, we note that the proposed
proximal nested sampling method summarised in Algorithm
4 seeks to provide a Bayesian model selection strategy that
is computationally efÔ¨Åcient, simple, robust, and easy to
deploy, as opposed to a strategy that seeks to deliver opti-
mal performance by using adaptive methods or by leveraging
model-speciÔ¨Åc properties. For example, for some models
with favourable factorisation properties, better results would
be obtained by replacing ULA by a Gibbs sampler (see
e.g. Lucka 2016). Similarly, for models that are close to
isotropic, one could replace ULA with a proximal Markov
kernel derived from the underdamped Langevin SDE, which
includes a Hamiltonian term (see e.g. Melidonis et al. 20223).
Such methods scale more efÔ¨Åciently to large models than
the overdamped Langevin method used in this paper, but they
are less robust to anisotropy, which is a common feature in
Bayesian inverse problems. Moreover, one could also con-
sider using an adaptive MALA kernel with a matrix-valued
step-size taking into account second-order properties of the
posterior distribution (Pereyra et al. 2016). Lastly, because
the proposed proximal nested sampling method has been
speciÔ¨Åcally designed for large models that are log-concave, it
is not equipped with mechanisms to handle multi-modality.
For problems involving multi-modality, we would recom-
mend modifying the Markov kernel either by using some
formofannealing(Neal2001),orbyusinganadaptiveimpor-
tance sampling scheme (Martino et al. 2017). However, as
mentioned previously, performing model selection for mod-
els that are both large and multi-modal is very difÔ¨Åcult and
remains an important perspective for future work.
5 Explicit forms of proximal nested sampling
In the general proximal nested sampling framework pre-
sented in Sect. 4 we considered arbitrary log-concave terms
for the prior and likelihood and did not consider further how
to compute the proximal operators related to those terms.
We now exemplify our proposed proximal nested sampling
framework with explicit forms for common priors and likeli-
hoods used in high-dimensional signal and image processing
problems. In particular, we outline explicitly how to compute
the required proximal operators.
For illustration, we focus on sparsity-promoting priors
corresponding to f (x) = Œº‚à•‚Ä†x‚à•1, where ‚Ä† ‚ààCp√ód
represents a sparsifying transform, and Gaussian likelihoods
corresponding to g(x) = ‚à•y ‚àíx‚à•2
2/2œÉ 2, where y ‚ààCm
denotes measured data, x ‚ààRd the underlying parameters,
and  ‚ààCm√ód the measurement operator (model), although
other common priors are also considered. For simplicity,
3 Note that we focus on proximal MCMC kernels since purely gradient-
based MCMC methods based on the Langevin or Hamiltonian dynamics
are not directly applicable to the non-smooth models considered in this
paper. They might fail to be geometrically ergodic, in which case the
nested sampling scheme would also behave poorly (see Betancourt 2011
for an example of a nested sampling method based on Hamiltonian
dynamics).
123
87
Page 12 of 22
Statistics and Computing (2022) 32 :87
although not essential, we assume  is an orthonormal trans-
formation, i.e., ‚Ä† = ‚Ä† = I.
From the iterative forms given in (36), (38) and (41), on
which our proximal nested sampling framework is based, it
is necessary to compute two proximal operators: proxŒª
f (x)
and proxœáBœÑ (x), related to the prior and likelihood, respec-
tively (recall that the deÔ¨Ånition of œáBœÑ is related to likelihood
function g; see (30)). In the following we calculate these two
proximal operators for explicit expressions of f (x) and g(x)
and show the corresponding explicit forms of the iterative
formulas of (36), (38) and (41).
5.1 Proximal operator for the prior
When f (x) represents a Ô¨Çat prior or f (x) = Œº‚à•‚Ä†x‚à•2
2
(Gaussian prior) it is differentiable with gradient
‚àáf (x) = 0 or ‚àáf (x) = 2Œº‚Ä†x = 2Œºx,
(42)
respectively (here we use ‚Ä† = I). Obviously, there is no
need to use the Moreau-Yosida envelope ‚àáf Œª(x) to approx-
imate ‚àáf (x) when f (x) is differentiable.
When f (x) represents a sparsity-promoting Laplacian-
type prior f (x) = Œº‚à•‚Ä†x‚à•1, ‚àÄx‚Ä≤ ‚ààRd, we have
proxŒª
f (x‚Ä≤) = argmin
x‚ààRd

Œº‚à•‚Ä†x‚à•1 + ‚à•x ‚àíx‚Ä≤‚à•2
2/2Œª

= x‚Ä≤ + 

proxŒªŒº
‚à•¬∑‚à•1(‚Ä†x‚Ä≤) ‚àí‚Ä†x‚Ä≤	
= x‚Ä≤ + 

softŒªŒº(‚Ä†x‚Ä≤) ‚àí‚Ä†x‚Ä≤	
,
(43)
where the second line follows by standard properties of
the proximal operator (Combettes and Pesquet 2011) and
where softŒª(x) = (softŒª(x1), softŒª(x2), ¬∑ ¬∑ ¬∑ ) is the soft-
thresholding operator deÔ¨Åned by
softŒª(xi) =

0,
|xi| < Œª,
xi(|xi| ‚àíŒª)/|xi|, otherwise.
(44)
5.2 Proximal operator for the likelihood
Consider the Gaussian likelihood corresponding to g(x) =
‚à•y‚àíx‚à•2
2/2œÉ 2.RecallthatœáBœÑ (x) = 0if x ‚àà{x | g(x) < œÑ}
and otherwise œáBœÑ (x) = +‚àû. We are to solve
proxŒª
œáBœÑ (x‚Ä≤) = argmin
x‚ààRd

œáBœÑ (x) + ‚à•x ‚àíx‚Ä≤‚à•2
2/2Œª

= argmin
x‚ààRd

œáBœÑ (x) + ‚à•x ‚àíx‚Ä≤‚à•2
2

= projœáBœÑ (x‚Ä≤),
(45)
which is a projection onto set BœÑ.
For the case where the measurement operator is the iden-
tity,  = I, (e.g. denoising problems) then problem (45) is
the projection onto the ‚Ñì2 ball with radius
‚àö
2œÑœÉ 2. In this case
the proximal (projection) operator has closed-form solution
projœáBœÑ (x) =

x,
if x ‚ààBœÑ,
x‚àíy
‚à•x‚àíy‚à•2
‚àö
2œÑœÉ 2 + y, otherwise.
(46)
For the case where the measurement operator is not the
identity,  Ã∏= I, problem (45) is equivalent to Ô¨Ånding an
x ‚ààRd satisfying
min
x‚ààRd

œáB‚Ä≤
œÑ‚Ä≤(u) + ‚à•x ‚àíx‚Ä≤‚à•2
2/2

,
s.t. u = x,
(47)
where B‚Ä≤
œÑ
:= {z | ‚à•y ‚àíz‚à•2
2 < œÑ} and œÑ ‚Ä≤ = 2œÑœÉ 2.
Minimisation problem (47) can be solved by a variety of dif-
ferent optimisation methods, e.g. by the alternating direction
method of multipliers (ADMM) and primal-dual algorithms
(see, e.g., Parikh and Boyd 2013 and references therein for
further details). In the following we present detailed proce-
dures for using the ADMM and primal-dual algorithms to
solve problem (47).
5.2.1 Computation using ADMM method
Firstly, the augmented Lagrangian of the minimisation prob-
lem (47) can be represented as
(x, u, z) := œáB‚Ä≤
œÑ‚Ä≤(u) + 1
2‚à•x ‚àíx‚Ä≤‚à•2
2 + Œ≤z‚Ä†(u ‚àíx)
+Œ≤
2 ‚à•u ‚àíx‚à•2
2,
(48)
for dual variable z and penalty parameter Œ≤ > 0. Starting
from an initialisation x(0), z(0), the augmented Lagrangian
of (48) can be minimised with respect to variables u and x
alternatively, while updating the dual value z using the dual
ascent method to ensure the constraint u = x is satisÔ¨Åed
for the Ô¨Ånal solution, i.e.
u(i) = argmin
u‚ààCm (x(i), u, z(i)),
(49)
x(i+1) = argmin
x‚ààRd (x, u(i), z(i)),
(50)
z(i+1) = z(i) + u(i) ‚àíx(i+1),
(51)
123
Statistics and Computing (2022) 32 :87
Page 13 of 22
87
which can be rewritten as the following explicit iterative
scheme
u(i) = argmin
u‚ààCM

œáB‚Ä≤
œÑ‚Ä≤(u) + Œ≤
2 ‚à•u ‚àíx(i) + z(i)‚à•2
2

,
(52)
x(i+1) = argmin
x‚ààRD
1
2‚à•x ‚àíx‚Ä≤‚à•2
2 + Œ≤
2 ‚à•u(i) ‚àíx + z(i)‚à•2
2

,
(53)
z(i+1) = z(i) + u(i) ‚àíx(i+1).
(54)
The solution to problem (52) has a closed-form expression
since it is the projection onto a scaled and shifted ‚Ñì2 ball, i.e.,
u(i) =

x(i)‚àíz(i),
if x(i)‚àíz(i) ‚ààB‚Ä≤
œÑ ‚Ä≤,
x(i)‚àíz(i)‚àíY
‚à•x(i)‚àíz(i)‚àíY‚à•2
‚àö
2œÑœÉ 2 + Y, otherwise.
(55)
Problem (53) is differentiable and so can be solved by gra-
dient descent. It is straightforward to show that this problem
is equivalent to solving the linear system w.r.t. x
(Œ≤‚Ä† + I)x = x‚Ä≤ + Œ≤‚Ä†(u(i) + z(i)),
(56)
which can be solved by using iterative methods, with
(Œ≤‚Ä† + I) positive deÔ¨Ånite.
The pseudo code to compute the proximal operator,
proxœáBœÑ (x), using ADMM is summarised in Algorithm 5.
Various stopping criteria can be considered, such as a max-
imum iteration number or the relative error of solutions at
two consecutive iterations, i.e., ‚à•x(i+1) ‚àíx(i)‚à•2/‚à•x(i)‚à•2.
Algorithm 5 ADMM for proximal operator associated with
the likelihood
Initialization: x(0), z(0).
Input: x, L‚àó
Output: x‚àó(the value of proxœáBœÑ (x)).
Compute œÑ = ‚àílog L‚àó, and form œáBœÑ .
for i = 0, . . ., until the stopping criterion reached
- Compute u(i) by (55);
- Compute x(i+1) by solving (56);
- Update z(i+1) by (54).
end for
Set x‚àó= x(i+1).
5.2.2 Computation using primal-dual method
Alternatively, problem (45) can be solved using a primal-dual
method. Note that the problem can be rewritten as
min
x‚ààRd

œáB‚Ä≤
œÑ‚Ä≤(x) + ‚à•x ‚àíx‚Ä≤‚à•2
2/2

,
(57)
which is equivalent to the saddle-point problem
min
x‚ààRd max
z‚ààCK

z‚Ä†x ‚àíœá‚àó
B‚Ä≤
œÑ‚Ä≤(z) + ‚à•x ‚àíx‚Ä≤‚à•2
2/2

,
(58)
where œá‚àó
B‚Ä≤
œÑ‚Ä≤ is the convex conjugate of œáB‚Ä≤
œÑ‚Ä≤. The saddle-
point problem (58) can be solved by alternatively optimising
with respect to the primal variable x and the dual variable
z. Considering a proximal forward-background step for each
alternate optimisation, Ô¨Årst for the dual variable z followed by
the primal variable x, leads to the following iterative scheme
z(i+1) = proxœá‚àó
B‚Ä≤
œÑ‚Ä≤
(z(i) + Œ¥1¬Øx(i)),
(59)
x(i+1) = proxh(x(i) ‚àíŒ¥2‚Ä†z(i+1)),
(60)
¬Øx(i+1) = x(i+1) + Œ¥3(x(i+1) ‚àíx(i)),
(61)
where h(x) = ‚à•x ‚àíx‚Ä≤‚à•2
2/2, and Œ¥k, for k = 1, 2, 3, are algo-
rithm step size parameters. We next consider how to solve
problem (59) and (60) explicitly.
Problem (59) can be solved by
z(i+1) = proxœá‚àó
B‚Ä≤
œÑ‚Ä≤
(z(i) + Œ¥1¬Øx(i))
= z(i) + Œ¥1¬Øx(i) ‚àíproxœáB‚Ä≤
œÑ‚Ä≤ (z(i) + Œ¥1¬Øx(i)),
(62)
where we have noted the relationship between the proximal
operator of the convex conjugate of a function given by (9).
Since B‚Ä≤
œÑ ‚Ä≤ is an ‚Ñì2 ball, the proximal operator in (62) has the
closed-form expression
proxœáB‚Ä≤
œÑ‚Ä≤ (z)=projB‚Ä≤
œÑ‚Ä≤ (z)=

z,
if z ‚ààB‚Ä≤
œÑ ‚Ä≤,
z‚àíy
‚à•z‚àíy‚à•2
‚àö
2œÑœÉ 2 + y, otherwise.
(63)
Problem (60) is to solve
x(i+1) =argmin
x‚ààRd

‚à•x ‚àíx‚Ä≤‚à•2
2+‚à•x‚àí(x(i)‚àíŒ¥2‚Ä†z(i+1))‚à•2
2

,
(64)
which involves a differentiable objective function and so can
be solved analytically, yielding the closed-form solution
x(i+1) = (x‚Ä≤ + x(i) ‚àíŒ¥2‚Ä†z(i+1))/2.
(65)
The pseudo code to compute the proximal operator,
proxŒª
œáBœÑ (x), using the primal-dual method is summarised in
Algorithm 6. The same stopping criterion as for ADMM in
Algorithm 5 can also be used for Algorithm 6.
Note that the main difference between the primal-dual
method and ADMM is that the primal-dual method does not
123
87
Page 14 of 22
Statistics and Computing (2022) 32 :87
need to solve the linear system in (56). Therefore, the primal-
dual method is typically more efÔ¨Åcient computationally and
is the approach used in the numerical experiments that follow.
However, there are speciÔ¨Åc problems for which the linear
system in (56) admits a computationally efÔ¨Åcient solution
and where the ADMM method might be more appropriate.
Algorithm 6 Primal-dual method for proximal operator asso-
ciated with the likelihood
Initialization: x(0), ¬Øx(0), z(0).
Input: x, L‚àó
Output: x‚àó(the value of proxœáBœÑ (x)).
Compute œÑ = ‚àílog L‚àó, and form œáBœÑ .
for i = 0, . . ., until the stopping criterion reached
- Compute z(i+1) by (62);
- Compute x(i+1) by solving (65);
- Update ¬Øx(i+1) by (61).
end for
Set x‚àó= x(i+1).
5.3 Explicit iterative formula for drawing samples
We are now in a position to outline the explicit iterative for-
mulas to draw samples for a variety of common priors using
our proximal nested sampling method.
The explicit representations of the iterative equations (36)
(for differentiable f (x)) and (38) (for non-differentiable
f (x)), which are used in Algorithm 2 to draw an individual
samplefromthepriorunderthehardlikelihoodconstraint,for
uniform, Gaussian and Laplacian priors, i.e. f (x) constant,
f (x) = Œº‚à•‚Ä†x‚à•2
2 and f (x) = Œº‚à•‚Ä†x‚à•1, respectively, are
x(k+1) = (1 ‚àíŒ¥
2Œª)x(k) + Œ¥
2Œªx‚àó(k) +
‚àö
Œ¥w(k+1),
(66)
x(k+1) = (1 ‚àíŒ¥
2Œª ‚àíŒ¥Œº)x(k) + Œ¥
2Œªx‚àó(k) +
‚àö
Œ¥w(k+1), (67)
x(k+1) = (1 ‚àíŒ¥
2Œª)x(k) + Œ¥
2Œª

softŒªŒº(‚Ä†x(k))
‚àí‚Ä†x(k)
+ Œ¥
2Œªx‚àó(k) +
‚àö
Œ¥w(k+1),
(68)
where x‚àó(k) = proxœáBœÑ (x(k)) is obtained using Algorithm 5
or 6.
Correspondingly, the explicit representations of equation
(41), which is used in Algorithm 3 to draw Nlive initial live
samples from the prior distribution œÄ(x) in the prior space
, are, respectively,
x(k+1) = x(k) +
‚àö
Œ¥w(k+1),
(69)
x(k+1) = (1 ‚àíŒ¥Œº)x(k) +
‚àö
Œ¥w(k+1),
(70)
x(k+1) =x(k)+ Œ¥
2Œª

softŒªŒº(‚Ä†x(k))‚àí‚Ä†x(k)
+
‚àö
Œ¥w(k+1).
(71)
We conclude this section with a brief discussion of the
types of priors that the proposed proximal nested sampling
method supports. While any prior that is log-concave could
be addressed by using proximal nested sampling, we only
recommend using the method for priors with proximal oper-
ators that are easy to evaluate or to approximate numerically.
This is the case for many models used in applied high-
dimensional statistics, where inference is often conducted
by using convex optimisation algorithms that also require
computing proximal operators. For more details about how
to evaluate proximal operators, their properties, and lists of
functions with known mappings please see Bauschke and
Combettes (2011), Combettes and Pesquet (2011) and Parikh
and Boyd (2013, Ch. 6). A library with MATLAB imple-
mentations of frequently used proximity mappings is also
available online4.
Moreover, since the proposed proximal nested sampling
approach was speciÔ¨Åcally designed for models that are log-
concave and with Bayesian imaging applications in mind,
we anticipate that it will be mostly used with informative
priors designed to regularise and stabilise high-dimensional
estimation problems. As explained in Llorente et al. (2022),
the marginal likelihood can be very sensitive to the choice
of the prior. Therefore, it is important that the parameters
of the prior are chosen carefully. In particular, we expect
that proximal nested sampling will be used in combination
with empirical Bayesian strategies that automatically adjust
the parameters of the prior by maximum marginal likelihood
estimation (see e.g. Vidal et al. 2020).
Furthermore, high-dimensional Bayesian models that are
log-concave often result from a careful trade-off between
modelling accuracy and computational tractability, and thus
they are inherently misspeciÔ¨Åed (e.g., in the case of Bayesian
imaging applications, one would not expect the prior to
deÔ¨Åne a realistic generative model). Consequently, when
using proximal nested sampling in this context one is inher-
ently operating in an M-open Bayesian modelling paradigm,
where none of the models under consideration are formally
‚Äútrue‚Äù. We refer the reader to Llorente et al. (2022) for more
details about performing model selection in this context, as
well as for details about prior sensitivity, objectivity, and the
use of data-driven priors in Bayesian model selection.
4 https://github.com/cvxgrp/proximal.
123
Statistics and Computing (2022) 32 :87
Page 15 of 22
87
6 Numerical experiments
In this section we validate our proposed proximal nested
sampling method and demonstrate its utility on a range of
illustrative problems.
We Ô¨Årst validate our method on a problem with a Gaussian
likelihood and Gaussian prior where the value of the marginal
likelihood(Bayesianevidence)canbecomputedanalytically.
The dimensions of the problem considered range from low
to very high, i.e. 2 to 106 dimensions.
Following on from this, we demonstrate the effective-
ness of the proximal nested sampling method by applying
it to two canonical imaging inverse problems, namely image
denoising and image reconstruction. In particular, we demon-
strate the use of proximal nested sampling for the principled
Bayesian model selection of the sparsifying dictionary, the
regularisation parameter (i.e. the Œº parameter of the prior)
and the appropriate measurement operator when it may be
misspeciÔ¨Åed. Furthermore, as mentioned already, as a by-
product the samples obtained by nested sampling approaches
can also be used to perform posterior inferences. This is
critical in imaging problems in order to recover point esti-
mates, e.g. restored images. Moreover, alternative forms of
uncertainty quantiÔ¨Åcation can also be considered from other
posterior inferences, e.g. variance estimates and posterior
credible regions (see, e.g., Cai et al. 2018).
6.1 Implementation and computational resources
To perform the numerical experiments presented subse-
quently, the proximal nested sampling algorithms developed
in this article were implemented in MATLAB.5 The numer-
ical experiments performed to compute the marginal like-
lihood for low-dimensional problems (i.e., dimensions less
than 200) were run on a Macbook laptop with an i7 Intel
CPU and memory of 16 GB. A high-performance worksta-
tion, with 24 CPU cores, x86 64 architecture and 256 GB
memory, was used for high-dimensional problems.
6.2 Validation in high dimensions
We Ô¨Årst consider the validation of the proximal nested sam-
pling method. For ease of validation, we consider the prior
potential f (x) = Œº‚à•‚Ä†x‚à•2
2, with Œº = 1/2,  = I, and the
likelihood potential g(x) = ‚à•y ‚àíx‚à•2
2/2œÉ 2, with œÉ = 1,
 = I. For this setting, we have a closed-form solution of the
marginal likelihood value (see Appendix for further details).
5 A Python version of the proxnest code implementing the proximal
nestedsamplingframeworkproposedinthisarticlehassincebeendevel-
oped and is available at https://github.com/astro-informatics/proxnest.
Fig. 1 Validation of our proximal nested sampling technique (for
dimensions 2‚Äì200) to compute the marginal likelihood (Bayesian evi-
dence) for a scenario where a closed-form solution is accessible. The
logarithm of the unnormalised prior volume (V ) times the marginal
likelihood value (Z) is plotted against the dimensions of the problem
considered. The blue-circle line, red-asterisk line and the black-solid
line show the results of MC integration, proximal nested sampling and
the ground truth, respectively. We can clearly see that the results com-
puted by proximal nested sampling agree with the ground truth well,
whereas the result computed by MC integration with 105 samples can
only achieve acceptable results when the dimension is below ‚àº20. The
computation time for the problem with dimension 200 is approximately
one minute.
Test data y ‚ààRd are generated by
y = x + w,
(72)
where x is an d-dimensional vector of uniformly distributed
random numbers in [0, 1]d, and w is an d-dimensional vec-
tor of normally distributed random numbers. Note that the
underlying model used to generate the mock data does not
match the prior œÄ used here, but that is Ô¨Åne for validation of
the calculation of the marginal likelihood. Also, in imaging
setting the prior is never perfectly speciÔ¨Åed. In the following,
we consider increasing dimensions from d = 2 to d = 106.
Weseparatethetestintothreeparts:i)smallmodelsofdimen-
sion from d = 2 to d = 200, ii) moderately large models of
dimension from d = 2 to d = 105, and iii) high dimensional
models with d = 106.
We Ô¨Årst test our method for low-dimensional models (i.e.,
d < 200). For our proximal nested sampling method, we use
Nlive = 2√ó102 live samples and N = 3√ó103 dead samples,
with a thinning factor of 10. We also compare our result with
vanilla Monte Carlo (MC) integration where a uniform prior
with integrand f ¬∑ g is utilised, with the number of samples
set to 105. Figure 1 presents the results. Our proximal nested
sampling method agrees well with the ground truth, whereas
simple MC integration can only achieve acceptable results
when the dimension is small, say d < 20. The computation
time for the problem with dimension 200 is approximately
one minute.
We now test our proximal nested sampling method for
high-dimensional cases. Results for dimensions of y up to
123
87
Page 16 of 22
Statistics and Computing (2022) 32 :87
Fig. 2 Validation of our proximal nested sampling technique (for
dimensions up to 105) to compute the marginal likelihood (Bayesian
evidence) for a scenario where a closed-form solution is accessible.
The logarithm of the unnormalised prior volume (V ) times the marginal
likelihood value (Z) is plotted against the dimensions of the prob-
lem considered. The red-asterisk line and the black-solid line show
the results of proximal nested sampling and the ground truth, respec-
tively. We can clearly see that the results computed by proximal nested
sampling agrees with the ground truth well. The computation time for
the problem with dimension 105 is approximately 10 minutes
105 are given in Fig. 2, where we set the number of the
live samples Nlive = 103 and the number of dead samples
N = 104, with thinning factor 10 (we do not consider direct
MC integration any further since it is already shown to fail
for dimensions above ‚àº20). These results again show that
our proximal nested sampling method can achieve results
in close agreement with the ground truth. The computation
time for the problem with dimension 105 is approximately
10 minutes.
Finally, we consider dimension 106 as an example to show
that our proximal nested sampling method can be pushed to
dimensions much higher than 105. With the same parame-
ters as that used for dimension 105, ten runs were performed
for a 106 dimensional setting of the same problem. The
logarithm of the ground truth value was calculated to be
2.3850 √ó 105. The mean of ten runs of proximal nested
sampling was computed be to 2.3851 √ó 105, with standard
deviation 0.0002 √ó 105. The result computed by proximal
nested sampling is in excellent agreement with the ground
truth. The computation time for each run of the problem with
dimension 106 is approximately 30 minutes.
6.3 Model selection in image processing
We now illustrate the application of proximal nested sam-
pling for Bayesian model selection in imaging problems.
In particular, we focus on two canonical problems, image
denoising and image reconstruction, with different like-
lihoods and priors. We emphasise that Bayesian model
selection for these imaging problems is not well addressed
by existing techniques due to the high dimensions considered
(i.e., higher than 105) and the use of general log-concave pri-
ors (e.g., like the sparsity promoting Laplace-type priors that
include ‚Ñì1 terms).
The three images in Fig. 3 are used in the experiments
that follow: Cameraman image, the W28 supernova rem-
nant, and the HI region of the M31 galaxy, all with size of
256 √ó 256 pixels and with intensities in the range [0, 255].
Sparsity-promoting priors (which are not smooth) and Gaus-
sian likelihoods are consider in the following experiments,
formed as f (x) = Œº‚à•‚Ä†x‚à•1 and g(x) = ‚à•y ‚àíx‚à•2
2/2œÉ 2,
respectively, where Œº,  and  are set to different forms for
model selection purposes.
6.3.1 Prior model selection in image denoising: dictionary
selection
For a standard denoising problem we apply proximal nested
sampling to select the dictionary  used for the sparsify-
ing transform. The noisy image y is generated by y =
x + w, where x is the ground truth clean image and w
Fig.3 Images used to showcase the use of proximal nested sampling for
Bayesian model selection in high-dimensional image processing prob-
lems. Panel (a): Cameraman grey-scale image; Panels (b)‚Äì(c): W28 and
M31 radio galaxies normalised to [0, 1] and then shown in log10 scale
(i.e. the numeric labels on the colour bar are the logarithms of the image
intensity), respectively
123
Statistics and Computing (2022) 32 :87
Page 17 of 22
87
Fig. 4 Dictionary selection for
an image denoising problem
solved by proximal nested
sampling (test image is
cameraman). First row shows
the clean image and noisy
image. Second row shows the
posterior mean images
recovered by proximal nested
sampling for priors with
(sparsifying) transforms
 = I, DB2 and DB8,
respectively, where the log-prior
reads f (x) = Œº‚à•‚Ä†x‚à•1. By
eye, both DB2 and DB8
wavelets provide superior
reconstruction Ô¨Ådelity compared
to  = I. The model  = DB2
may also be judged to provide
slightly superior performance to
 = DB8
Table 1 Marginal likelihood (Bayesian evidence) values computed by
proximal nested sampling for Bayesian model selection of the spar-
sifying dictionary for an image denoising problem (see Fig. 4 for
corresponding reconstructed images)
Prior
log Z
RMSE
 = I
‚àí6.54 √ó 104¬±0.08
41.07
 = DB2
‚àí3.06 √ó 104¬±0.09
14.29
 = DB8
‚àí3.09 √ó 104¬±0.09
14.51
Sparsity-promoting (non-differentiable) priors are considered with
(sparsifying) transforms  = I, DB2 and DB8. Comparing models,
Bayesian model selection afforded by proximal nested sampling sug-
gests the model with the DB2 dictionary is superior, followed by DB8,
both of which are far superior to the case where  = I, which agrees
with the RMSE (root mean square error) values and assessment per-
formed by eye, which require the ground truth to be known
is Gaussian noise with zero mean and standard deviation
œÉ = ‚à•x‚à•‚àû10‚àíSNR/20, where ‚à•¬∑ ‚à•‚àûis the inÔ¨Ånity norm,
and the input signal-to-noise ratio (SNR) is set to 20. Set
 = I in the likelihood g(x) = ‚à•y ‚àíx‚à•2
2/2œÉ 2 (i.e.,
g(x) = ‚à•y ‚àíx‚à•2
2/2œÉ 2). We then investigate the inÔ¨Çuence of
different choices for  in the prior term f (x) = Œº‚à•‚Ä†x‚à•1,
with Œº = 105. SpeciÔ¨Åcally, three forms of  are consid-
ered, namely the identity (I), Daubechies 2 wavelets (DB2),
and Daubechies 8 wavelets (DB8). For the proximal nested
sampling method, the number of the live samples Nlive and
dead samples N is respectively set to 2 √ó 103, and 4 √ó 104
with thinning factor 102, which is sufÔ¨Åcient to ensure con-
vergence.
Figure 4 presents the posterior means recovered (i.e. the
reconstructed images) for the three dictionaries considered,
i.e. for  = {I, DB2, DB8}. It is clear that the reconstructed
imagescorrespondingto = DB2andDB8aresigniÔ¨Åcantly
better than that for  = I. Moreover, while the difference
between the reconstructed images of the models for  =
DB2 and DB8 is small, by eye the image recovered with
DB2 may be judged slightly superior.
Table 1 presents the calculated marginal likelihood val-
ues6 for the different sparsifying transforms  selected for
the prior. The root mean square error (RMSE) is also given,
where the RMSE gauges the difference between the posterior
mean image and the ground truth image. Note that the RMSE
cannot normally be computed in practical problems since the
ground truth is not known. Since for these experiments we
know the ground truth the RMSE is a useful measure for
comparison purposes.
Table 1 shows that the model with  = I possesses the
smallest marginal likelihood value. This implies that for this
denoising problem the model with  = I is inferior to mod-
els where  is set to DB2 and DB8. Moreover, the marginal
likelihood difference between models where  is set to DB2
or DB8 is not dramatic, nevertheless this implies that DB2 is
preferred. These Ô¨Ånding inferred by Bayesian model selec-
tion agree with the RSME values computed for each model,
where the model with  = DB2 is slightly preferred over
DB8, and both models with DB2 and DB8 are highly pre-
ferred over the model with  = I (recall that in practice
it is not possible to compute the RMSE since it requires
knowledge of the underlying ground truth). Furthermore, the
6 The value of the log marginal likelihoods computed is low (in other
words, its absolute value is very high) since the problems we consider
are extremely high-dimensional.
123
87
Page 18 of 22
Statistics and Computing (2022) 32 :87
Fig. 5 Regularisation parameter selection for an image reconstruction
problem solved by proximal nested sampling (test image is W28 radio
galaxy). Images from left to right are the posterior mean images recov-
ered by proximal nested sampling for Œº in the prior deÔ¨Ånition set to
106, 107 and 108, respectively. The data y are generated by measuring
30% of noisy Fourier coefÔ¨Åcients of the test image. On close inspection
it may be noticed that reconstruction for model with Œº = 106 is superior
to the one with Œº = 107, which is superior to the one with Œº = 108
model preferences inferred by proximal nested sampling also
agree with the assessment of reconstructed image quality
by-eye discussed above. The results obtained are consistent
with common knowledge that it is typically more effective
to denoise a natural image using a prior that promotes spar-
sity in some (sparsifying) transform domain (e.g. a wavelet
domain) rather than in the image domain itself. The compu-
tation time for the problem with  = I is approximately
10 minutes, and for the problem with  = DB2 or DB8 is
approximately 60 minutes.
In high-dimensional settings note that Bayes factors can
be very large due to the concentration of probability in high-
dimensions, hence it is not meaningful to consider traditional
scales for assessing model comparisons such as the Jeffery‚Äôs
scale (Nesseris and Garc√≠a-Bellido 2013). Instead, we rec-
ommend comparing marginal likelihood values directly.
6.3.2 Prior model selection in image reconstruction:
regularisation parameter selection
We now apply proximal nested sampling to a standard recon-
struction problem and, Ô¨Årstly, consider the selection of the
regularisationparameterŒºdeÔ¨Åningthewidthoftheprior.Itis
typically very challenging to optimally set the regularisation
parameter Œº, which controls the strength of prior knowledge
and plays a key role in reconstruction quality. Consider noisy
observations (noisy measurements)
y = x + w,
(73)
where w again denotes Gaussian noise with zero mean and
œÉ = ‚à•x‚à•‚àû10‚àíSNR/20 (standard deviation), with SNR set to
30,andm andd arerespectivelythedimensionof y andimage
x. Consider the prior f (x) = Œº‚à•‚Ä†x‚à•1, with  = DB8, and
Table 2 Marginal likelihood (Bayesian evidence) values computed by
proximal nested sampling for Bayesian model selection of the regulari-
sation parameter Œº for an image reconstruction problem (see Fig. 5 for
corresponding reconstructed images)
Œº
log Z
RMSE
106
‚àí2.61 √ó 104¬±0.09
1.82
107
‚àí5.39 √ó 104¬±0.09
2.81
108
‚àí2.90 √ó 105¬±0.09
6.70
Prior deÔ¨Ånition with Œº set to 106, 107 and 108, respectively, are con-
sidered. Comparing models, Bayesian model selection afforded by
proximal nested sampling suggests the model with Œº = 106 is superior
to the one with Œº = 107, which is superior to the one with Œº = 108,
whichagreeswiththe RMSE (root meansquare error)valuesandassess-
ment performed by eye, which require the ground truth to be known
likelihood g(x) = ‚à•y ‚àíx‚à•2
2/2œÉ 2. For the reconstruction
scenario,  represents the sensing (measurement) operator.
In particular, we consider a measurement model comprising
incomplete Fourier measurements (common in radio inter-
ferometric and magnetic resonance imaging) deÔ¨Åned by the
sensing operator  = M F, constructed from the Fourier
transform F followed by a selection mask M which is gener-
ated randomly through the variable density sampling proÔ¨Åle
(Puy et al. 2011). We consider the scenario where only 30%
of Fourier coefÔ¨Åcients are measured, i.e. m = 0.3d. Note
that different forms of the mask M result in different sensing
operators .
Figure 5 presents the posterior means recovered by prox-
imal nested sampling (i.e. the reconstructed images) for
models with Œº set to 106, 107 and 108. It is difÔ¨Åcult to assess
the effectiveness of different regularisation parameters by
eye, but on close inspection it may be noticed that the model
with Œº = 106 is superior to the one with Œº = 107, which is
123
Statistics and Computing (2022) 32 :87
Page 19 of 22
87
superior to the one with Œº = 108. The computation time for
each problem is approximately 150 minutes.
Table 2 presents the marginal likelihood and RMSE values
computed for the models with different regularisation param-
eters Œº. The computed marginal likelihood for the model
with Œº = 106 is larger that the value for the model with
Œº = 107, which is larger than the model with Œº = 108,
suggesting the model with Œº = 106 is preferred. The com-
puted marginal likelihoods are consistent with the model
preferences obtained by comparing the RMSE of each model
and by visual inspection. Recall that both RMSE and visual
comparisons can only be performed here where the ground
truth is available and cannot be used for model compari-
son in practice. In summary, this example demonstrates that
our proximal nested sampling method is capable of select-
ing superior regularisation parameters for models stemming
from high-dimensional inverse problems.
6.3.3 Measurement model selection in image
reconstruction
We now apply proximal nested sampling to the same recon-
struction problem considered above (i.e. image reconstruc-
tion from noisy and incomplete Fourier measurements) but
focus on the problem of misspeciÔ¨Åcation of the measure-
ment model . Noisy observations Y are generated by (73),
measuring 10% of Fourier coefÔ¨Åcients, i.e. with m = 0.1d.
We use the ground truth model Mtruth to simulate obser-
vation data y. However, when solving the resulting inverse
problem we consider a number of different measurement
models, not only the ground truth model Mtruth but also mis-
speciÔ¨Åed models MŒ≥ , where Œ≥ > 0 encodes the level of
misspeciÔ¨Åcation.
The method by which the model is misspeciÔ¨Åed in
motivated by radio interferometric imaging. In radio interfer-
ometry, the coordinates of the Fourier coefÔ¨Åcients acquired
by the telescope are measured in units of (radio) wavelength.
If the wavelength at which observations are made is misspec-
iÔ¨Åed, the coordinates of the Fourier coefÔ¨Åcients acquired will
bescaled.WemodelpreciselythistypeofmisspeciÔ¨Åedmodel
here to represent the case where the instrument wavelength
is not calibrated accurately.
An incorrectly speciÔ¨Åed wavelength then simply acts to
modify the mask of the ground truth measurement model
Mtruth. The misspeciÔ¨Åed model corresponding to mask MŒ≥ ,
for misspeciÔ¨Åcation parameter Œ≥ , is generated by extend-
ing every measured position in Mtruth radially. SpeciÔ¨Åcally,
every measured position is extended radially along the line
connecting it to the origin to a length of Œ≥ d j, j ‚ààmask,
where Œ≥ is the misspeciÔ¨Åcation scaling factor, d j is the dis-
tance from the original measured position j to the origin in
Mtruth, and mask is the set which contains all the measured
positions. It is worth mentioning that the larger the scaling
factor Œ≥ , the larger the distortion of MŒ≥ from the ground
truth Mtruth. Note also that Œ≥ = 0 corresponds to a correctly
speciÔ¨Åed model, i.e. MŒ≥ =0 = Mtruth.
For proximal nested sampling, the number of the live sam-
ples Nlive and dead samples N is respectively set to 2 √ó 103
and 3 √ó 104 with thinning factor 102, which is sufÔ¨Åcient to
ensure convergence. Regularisation parameter Œº = 108 is
used for these experiments.
Figure 6 presents the posterior means recovered (i.e. the
reconstruction images) for models with Œ≥ = MŒ≥ F and
 = MtruthF,. Here misspeciÔ¨Åed models M0.12, M0.09,
M0.06 and M0.03 are generated for misspeciÔ¨Åcation scaling
factors Œ≥ with values of 0.12, 0.09, 0.06 and 0.03, respec-
tively. It is apparent by eye that the posterior mean image
recovered with the ground truth model is the best and that the
quality of the recovered posterior mean image degrades as
the size of the misspeciÔ¨Åcation scale parameter Œ≥ increases.
The computation time for each problem is approximately 150
minutes.
Table 3 presents the marginal likelihood and RMSE val-
ues computed for the different models considered. The
computed marginal likelihood is largest when the correct
ground truth model is adopted in the likelihood. As the
misspeciÔ¨Åcation parameter Œ≥ is increased (corresponding
to greater misspeciÔ¨Åcation and less accurate models), the
corresponding computed marginal likelihood values mono-
tonically decrease (become more negative). For Bayesian
model comparison, the model with the lowest misspeci-
Ô¨Åcation parameter Œ≥ is always preferred. The computed
marginal likelihoods are consistent with the model prefer-
ences obtained by comparing the RMSE of each model and
by visual inspection (although recall that such tests cannot
be used for model comparison in practice when the ground
truth is not known).
7 Conclusions
Nested sampling provides an efÔ¨Åcient computational frame-
work to estimate the marginal likelihood (Bayesian evidence)
for Bayesian model selection. It effectively re-parameterises
the marginal likelihood into a one-dimensional integral of
the likelihood with respect to the enclosed prior volume. The
challenge of nested sampling is to sample from the prior dis-
tribution subject to a hard likelihood constraint. A variety of
successful techniques have been developed to perform such
sampling in low and moderate dimensional problems. How-
ever, existing approaches are not directly useful for imaging
applications because they scale poorly to large problems and
struggle to support models that are not smooth.
In this article we presented the proximal nested sampling
method that is speciÔ¨Åcally designed for Bayesian models that
are log-concave, potentially very high-dimensional (d = 106
123
87
Page 20 of 22
Statistics and Computing (2022) 32 :87
Fig. 6 Measurement model misspeciÔ¨Åcation for an image reconstruc-
tion problem solved by proximal nested sampling (test image is M31
radio galaxy). Panel (a): dirty (back-projected) image ‚Ä†Y; Panels (b)‚Äì
(f): posterior mean images recovered by proximal nested sampling
for misspeciÔ¨Åed models MŒ≥ , where increasing Œ≥ > 0 corresponds
to increasing levels of misspeciÔ¨Åcation (and Œ≥ = 0 corresponds to the
ground truth model). It is apparent by eye that the posterior mean image
recovered with the ground truth model is the best and that the quality
of the recovered posterior mean image degrades as the size of the mis-
speciÔ¨Åcation scale parameter Œ≥ increases
Table 3 Marginal likelihood (Bayesian evidence values computed by
proximal nested sampling for Bayesian model selection for measure-
ment model misspeciÔ¨Åcation for an image reconstruction problem (see
Fig. 6 for corresponding reconstructed images)
Likelihood
log Z
RMSE
 = MtruthF
‚àí4.47 √ó 103¬±0.08
3.40
 = M0.03F
‚àí4.88 √ó 103¬±0.08
7.85
 = M0.06F
‚àí5.63 √ó 103¬±0.08
12.01
 = M0.09F
‚àí9.21 √ó 103¬±0.07
15.71
 = M0.12 F
‚àí1.44 √ó 104¬±0.08
18.08
MisspeciÔ¨Åed models are denoted MŒ≥ , where increasing Œ≥ > 0 corre-
sponds to increasing levels of misspeciÔ¨Åcation (and Œ≥ = 0 corresponds
to the ground truth model). Comparing models, Bayesian model selec-
tion afforded by proximal nested sampling suggests the model with the
lowest misspeciÔ¨Åcation parameter Œ≥ is always preferred, which also
agrees with the RMSE (root mean square error) values and assessment
performed by eye, which require the ground truth to be known
and beyond), and potentially not smooth. This is achieved by
exploiting tools from proximal calculus and Moreau-Yosida
regularisation to efÔ¨Åciently sample from the prior subject to
the hard likelihood constraint through a proximal MCMC
approach. The resulting Markov chain iterations combine a
gradient step that approximates a Langevin SDE that scales
efÔ¨Åciently to large problems, with a projection term that acts
to push the Markov chain back into the likelihood constraint
set if it wanders outside of it, and a Metropolis-Hastings
correction step to ensure the hard likelihood constraint is
satisÔ¨Åed.
The proposed proximal nested sampling framework was
implemented and validated on a Gaussian model for which
the marginal likelihood could be calculated in closed-form,
showing excellent agreement between values computed ana-
lytical and by proximal nested sampling, even in very high
dimensions. The use of proximal nested sampling for prin-
cipled Bayesian model selection was then showcased on
a variety of imaging problems with non-smooth sparsity-
promoting prior distributions. In particular, model selection
problems were considered related to dictionary selection, and
selection of the appropriate measurement model when it may
be misspeciÔ¨Åed.
Proximal nested sampling allows Bayesian model selec-
tion to be performed at a much higher dimension than that
123
Statistics and Computing (2022) 32 :87
Page 21 of 22
87
was previously possible, while also supporting non-smooth
priors that are widely used in imaging. It is our hope that
proximal nested sampling will thus Ô¨Ånd widespread use for
high-dimensional Bayesian model selection, particularly in
the imaging sciences.
Important perspectives for future work include: a detailed
theoretical analysis of the convergence properties of prox-
imal nested sampling; an extension to (biased) accelerated
proximal methods (Vargas et al. 2020); and an analysis of
the properties of marginal maximum likelihood estimation
for the class of models considered in this paper, such as
estimator consistency for model selection in an M-closed
setting and concentration in an M-open setting (Llorente
et al. 2022). Moreover, it would be interesting to apply prox-
imalnestedsamplingtoothertypesofmodels,suchasmodels
with likelihood-based priors (Llorente et al. 2022), which
can be handled straightforwardly by proximal nested sam-
pling when the likelihood is log-concave. It would also be
interesting to modify proximal nested sampling to tackle
high-dimensional models that are multi-modal, particularly
models with data-driven priors encoded by neural networks
(see e.g. Mukherjee et al. 2022, Section 5).
Acknowledgements This work was supported by the Leverhulme Trust
and by EPSRC grants EP/T007346/1 and EP/W007673/1. The authors
would like to thank the editor and two anonymous reviewers for their
valuable suggestions to improve the manuscript. The authors are also
grateful to Abdul-Lateef Haji-Ali for helpful comments. For the pur-
pose of open access, the authors have applied a Creative Commons
Attribution (CC BY) licence to any Author Accepted Manuscript ver-
sion arising.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing, adap-
tation, distribution and reproduction in any medium or format, as
long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons licence, and indi-
cate if changes were made. The images or other third party material
in this article are included in the article‚Äôs Creative Commons licence,
unless indicated otherwise in a credit line to the material. If material
is not included in the article‚Äôs Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the
permitteduse,youwillneedtoobtainpermissiondirectlyfromthecopy-
right holder. To view a copy of this licence, visit http://creativecomm
ons.org/licenses/by/4.0/.
Appendix A
The volume of the prior f (x) = Œº‚à•‚Ä†x‚à•2
2 with  = I is
V =
 ‚àû
‚àí‚àû
exp

‚àíŒº‚à•x‚à•2
2
	
dx
=
 ‚àû
‚àí‚àû
exp

‚àí1
22Œºx‚ä§x

dx
=

(2œÄ)d
(2Œº)d .
(A1)
For the prior f (x) = Œº‚à•‚Ä†x‚à•2
2 with  = I and the
likelihood g(x) = ‚à•y‚àíx‚à•2
2/2œÉ 2 with  = I, the Bayesian
evidence value has the following closed-form representation:
1
V
 ‚àû
‚àí‚àû
exp(‚àíŒº‚à•x‚à•2
2) exp(‚àí‚à•y ‚àíx‚à•2
2/2œÉ 2)dx
= 1
V
 ‚àû
‚àí‚àû
exp

‚àíŒº‚à•x‚à•2
2 ‚àí‚à•y ‚àíx‚à•2
2/2œÉ 2	
dx
= 1
V
 ‚àû
‚àí‚àû
exp

‚àí(Œº + 1/2œÉ 2)x‚ä§x + y‚ä§x/œÉ 2 ‚àíy‚ä§y/2œÉ 2	
dx
= 1
V exp

‚àíy‚ä§y
2œÉ 2
  ‚àû
‚àí‚àû
exp

‚àí1
2(2Œº+1/œÉ 2)x‚ä§x+y‚ä§x/œÉ 2

dx
= 1
V

(2œÄ)d
(2Œº+1/œÉ 2)d exp

‚àíy‚ä§y
2œÉ 2

exp
1
2
1
2Œº+1/œÉ 2
y‚ä§y
œÉ 4

,
(A2)
whose logarithmic value is
log

(2œÄ)d
(2Œº + 1/œÉ 2)d +

‚àíy‚ä§y
2œÉ 2

+
1
2
1
2Œº + 1/œÉ 2
y‚ä§y
œÉ 4

‚àílog V .
(A3)
References
Bauschke, H.H., Combettes, P.L.: Convex Analysis and Mono-
tone Operator Theory in Hilbert Spaces. Springer-Verlag,
New York (2011). https://link.springer.com/book/10.1007/978-1-
4419-9467-7
Betancourt, M.: Nested sampling with constrained Hamiltonian Monte
Carlo. AIP Conference Proceedings 1305, 165 (2011). https://doi.
org/10.1063/1.3573613
Brewer, B.J., P√°rtay, L.B., Cs√°nyi, G.: Diffusive nested sampling. Stat.
Comput. 21, 649‚Äì656 (2011)
Brosse, N., Durmus, A., √âric Moulines, et al.: Sampling from a
log-concave distribution with compact support with proximal
Langevin Monte Carlo. In: Kale, S., Shamir, O. (eds) Proceed-
ings of the 2017 Conference on Learning Theory, Proceedings of
Machine Learning Research, vol 65. PMLR, Amsterdam, Nether-
lands, pp. 319‚Äì342 (2017)
Cai, X., Pereyra, M., McEwen, J.D.: Uncertainty quantiÔ¨Åcation for radio
interferometric imaging I: proximal-MCMC methods. Mon. Not.
R. Astron. Soc. (MNRAS) 480(3), 4154‚Äì4169 (2018)
Cai, X., Pratley, L., McEwen, J.D.: Online radio interferometric imag-
ing: assimilating and discarding visibilities on arrival. Mon. Not.
R. Astron. Soc. (MNRAS) 485(4), 4559‚Äì4572 (2019)
Casella, G., Berger, R.L.: Statistical Inference. Duxbury - Thomson
Learning,
Boston
(2002).
https://books.google.co.uk/books/
about/Statistical_Inference.html?id=ZpkPPwAACAAJ&redir_
esc=y
Chib, S.: Marginal likelihood from the Gibbs output. J. Am. Stat. Assoc.
90, 1313‚Äì1321 (1995)
Chib, S., Jeliazkov, I.: Marginal likelihood from the Metropolis-
Hastings output. J. Am. Stat. Assoc. 96, 270‚Äì281 (2001)
Chopin, N., Robert, C.P.: Properties of nested sampling. Biometrika
97(3), 741‚Äì755 (2010)
123
87
Page 22 of 22
Statistics and Computing (2022) 32 :87
Clyde, M.A., Berger, J.O., Bullard, F., et al.: Current challenges in
Bayesian model choice. In: Statistical Challenges in Modern
Astronomy IV ASP Conference Series, vol. 371, pp. 224‚Äì240
(2007)
Combettes, P., Pesquet, J.C.: Proximal Splitting Methods in Signal Pro-
cessing. Springer, New York (2011)
Durmus, A., Moulines, E., Pereyra, M.: EfÔ¨Åcient Bayesian computation
by proximal Markov chain Monte Carlo: when Langevin meets
Moreau. SIAM J. Imaging Sci. 1(1), 473‚Äì506 (2018)
Feroz, F., Skilling, J.: Exploring multi-modal distributions with nested
sampling. In: AIP Conference Proceedings, vol. 1553,pp. 106‚Äì113
(2013)
Feroz, F., Hobson, M.P.: Multimodal nested sampling: an efÔ¨Åcient and
robust alternative to MCMC methods for astronomical data analy-
sis. Mon. Not. R. Astron. Soc. (MNRAS) 384(2), 449‚Äì463 (2008)
Feroz, F., Hobson, M.P., Bridges, M.: MULTINEST: an efÔ¨Åcient and
robust Bayesian inference tool for cosmology and particle physics.
Mon. Not. R. Astron. Soc. (MNRAS) 398(4), 1601‚Äì1614 (2009)
Friel, N., Wyse, J.: Estimating the evidence - a review. Stat. Neerl. 66(3),
288‚Äì308 (2012)
Green, P.J.: Reversible jump markov chain monte carlo computation and
bayesian model determinatio. Biometrika 82(4), 711‚Äì732 (1995)
Green, P.J., ≈Åatuszy¬¥nski, K., Pereyra, M., et al.: Bayesian computa-
tion: a summary of the current state, and samples backwards and
forwards. Stat. Comput. 25(4), 835‚Äì862 (2015)
Handley, W.J., Hobson, M.P., Lasenby, A.N.: POLYCHORD: nested
sampling for cosmology. Mon. Not. R. Astron. Soc. Lett. 450,
L61‚ÄìL65 (2015)
Harroue, B.: Approche bay√©sienne pour la s√©lection de mod√®les : appli-
cation √° la restauration d‚Äôimage. PhD thesis, http://www.theses.fr/
2020BORD0127 (2020)
Kaipio, J., Somersalo, E.: Statistical and Computational Inverse Prob-
lems. Springer, New-York (2005)
Kamary, K., Mengersen, K., Robert, C.P., et al.: Testing hypotheses via
a mixture estimation model. arXiv: 1412.2044 (2018)
Llorente, F., Martino, L., Curbelo, E., et al.: On the safe use of prior den-
sities for Bayesian model selection. arXiv:2206.05210v1 (2022)
Llorente, F., Martino, L., Delgado, D., et al.: Marginal likelihood com-
putation for model selection and hypothesis testing: an extensive
review. arXiv: 2005.08334 (2020)
Lucka, F.: Fast gibbs sampling for high-dimensional bayesian inversion.
Inverse Probl. 32(11), 115019 (2016)
Lunz, S., Hauptmann, A., Tarvainen, T., et al.: On learned operator
correction in inverse problems. SIAM J. Imaging Sci. 14(1), 92‚Äì
127 (2021)
Martino, L., Elvira, V., et al.: Layered adaptive importance sampling.
Stat. Comput. 27, 599‚Äì623 (2017)
McEwen, J.D., Wallis, C.G.R., Price, M.A. et al.: Machine learning
assisted Bayesian model comparison: the learnt harmonic mean
estimator. Stat. Comput. arXiv: 2111.12720 (2022)
Melidonis, S., Dobson, P., Altmann, Y., et al.: EfÔ¨Åcient Bayesian com-
putation for low-photon imaging problems. arXiv: 2206.05350
(2022)
Mukherjee, S., Hauptmann, A., √ñktem, O., et al.: Learned reconstruc-
tion methods with convergence guarantees. arXiv: 2206.05431
(2022)
Mukherjee,P.,Parkinson,D.,Liddle,A.R.:Anestedsamplingalgorithm
for cosmological model selection. Astrophys. J. 638, L51‚ÄìL54
(2006)
Neal, R.: Annealed importance sampling. Stat. Comput. 11, 125‚Äì139
(2001)
Nesseris, S., Garc√≠a-Bellido, J.: Is the Jeffreys‚Äô scale a reliable tool for
Bayesian model comparison in cosmology? J. Cosmol. Astropart.
Phys. 2013, 036 (2013)
Newton, M.A., Raftery, A.E.: Approximate Bayesian inference with the
weighted likelihood bootstrap. J. R. Stat. Soc. 56, 3‚Äì48 (1994)
O‚ÄôRuanaidh, J., Fitzgerald, W.J.: Numerical Bayesian Methods Applied
to Signal Processing. Springer-Verlag, New York (1996)
Parikh, N., Boyd, S.: Proximal algorithms. Found. Trends Optim. 1,
123‚Äì231 (2013)
Pereyra, M., McLaughlin, S.:Comparing bayesian models in the
absence of ground truth. In: 2016 24th European Signal Processing
Conference (EUSIPCO), pp. 528‚Äì532 (2016)
Pereyra, M.: Proximal Markov chain Monte Carlo algorithms. Stat.
Comput. 26, 745‚Äì760 (2016)
Pereyra, M., Schniter, P., Chouzenoux, E., et al.: A survey of stochastic
simulation and optimization methods in signal processing. IEEE
J. Sel. Top. Signal Process. 10(2), 224‚Äì241 (2016)
Puy, G., Vandergheynst, P., Wiaux, Y.: On variable density compressive
sampling. IEEE Signal Process. Lett. 18, 595‚Äì598 (2011)
Robert, C.P.: The Bayesian Choice. Springer-Verlag, New York (2007)
Robert, C.P., Casella, G.: Monte Carlo Statistical Methods. Springer-
Verlag, New York (2004)
Sivia, D., Skilling, J.: Data Analysis: A Bayesian Tutorial. Oxford Sci-
ence Publications, Oxford (2006)
Skilling, J.: Nested sampling for general Bayesian computation.
Bayesian Anal. 1, 833‚Äì859 (2006)
Tierney, L., Kadane, J.B.: Accurate approximations for posterior
moments and marginal densities. J. Am. Stat. Assoc. 81, 82‚Äì86
(1986)
Trotta, R.: Applications of Bayesian model selection to cosmologi-
cal parameters. Mon. Not. R. Astron. Soc. (MNRAS) 378, 72‚Äì82
(2007)
Vargas, L., Pereyra, M., Zygalakis, K.C.: Accelerating proximal markov
chain monte carlo by using an explicit stabilised method. SIAM J.
Imaging Sci., in press, arXiv: 1908.08845 (2020)
Vidal, A.F., Bortoli, V.D., Pereyra, M., et al.: Maximum likelihood esti-
mation of regularization parameters in high-dimensional inverse
problems: an empirical bayesian approach part i: methodology
and experiments. SIAM J. Imaging Sci. 13(4), 1945‚Äì1989 (2020).
https://doi.org/10.1137/20m1339829
Zhou, Q., Yu, T., Zhang, X., et al.: Bayesian inference and uncertainty
quantiÔ¨Åcation for medical image reconstruction with poisson data.
SIAM J. Imaging Sci. 13(1), 29‚Äì52 (2020)
Publisher‚Äôs Note Springer Nature remains neutral with regard to juris-
dictional claims in published maps and institutional afÔ¨Åliations.
123
