of the N spatial features vectors, i.e., {vi}N
i=1, where each vi represents a distinct frame of the input video
clip, see Figure 6. Afterwards, the ViT block outputs a final representation z, which is then fed into the
softmax layer to classify the action in the video. In detail, the Transformer encoder is designed to process
a sequence of vectors, each representing one frame, and aggregate information into a single vector for
classification.
In the proposed ViT-only model in Figure 7 for the purpose of comparison, each vector represents
a distinct patch. These vectors are first linearly projected into a high-dimensional space, facilitating the
model’s ability to learn complex patterns within the data. To ensure the model captures the sequential
nature of the input, positional encodings are added to these embeddings. The core of the ViT consists of
two layers, each comprising a multi-head self-attention mechanism and a feed-forward network. The self-
attention mechanism allows the model to weigh the importance of different patches relative to each other,
while the feed-forward network, utilizing an exponential linear unit (ELU) activation function, processes each
position independently to capture global context. The ViT is designed to aggregate the information from all
vectors and positional encodings into a single [CLS] token, which is prepended to the input sequence. The
18
Table 6: Capability comparison between Transformer-based, CNN-based, and hybrid models in HARs.
Criteria
ViT-based
CNN-based
Hybrid Models
Advantages
Excel at capturing global dependencies
✓
✓
Scalable with data
✓
✓
Flexible in processing various input sizes
✓
✓
Adept at extracting local features
✓
✓
Computationally efficient
✓
Robust to input variations
✓
✓
Efficient and versatile
✓
Adapts well to a range of tasks
✓
Disadvantages
Computationally intensive
✓
✓
Requires substantial training data
✓
✓
Limited global contextual understanding
✓
Prone to overfitting
✓
Limited in modeling long-range dependencies
✓
Architectural complexity
✓
Higher resource demands
✓
Need for careful tuning
✓
Balancing contributions of both components can be challenging
✓
output vector associated with this [CLS] token, after propagation through the Transformer layers, serves as
a comprehensive representation of the entire input, suitable for downstream classification tasks.
ViT
Class
Temporal Component
Spatial Component
Video Frames TimeDistributed
Layer
Spatial
Features
Softmax
Layer
v1
v2
...
vN−2
vN−1
vN
z
Fig. 6: The hybrid CNN-ViT architecture for HARs.
19
ViT
Class
Position+
Patch Embedding
Patches
Frames
Softmax Layer
0
1
N
...
...
...
...
Fig. 7: The ViT-only architecture for HARs.
4.1 Experiments
The goal of the presented experiments is not necessarily to produce a model that outperforms the state-of-
the-art models in the HAR field. Rather, the aim is to conduct a comparison among the CNN, ViT-only,
and hybrid models to give further insights.
The Royal Institute of Technology in 2004 unveiled the KTH dataset, a significant and publicly accessible
dataset for action recognition (Schuldt et al. 2004). The KTH dataset was chosen here for its balanced
representation of spatial and temporal features. Renowned as a benchmark dataset, it encompasses six
types of actions: walking, jogging, running, boxing, hand-waving, and hand-clapping. The dataset features
performances by 25 different individuals, introducing a diversity in execution. Additionally, the environment
for each participant’s actions was deliberately altered, including settings such as outdoors, outdoors with
scale changes, outdoors with clothing variations, and indoors. The KTH dataset comprises 2,391 video
sequences, all recorded at 25 frames per second using a stationary camera against uniform backgrounds.
Six experiments were conducted, with each of the aforementioned models trained on three different
lengths of frame sequences. Care was taken to avoid pre-training in order to ensure the neutrality of the
results. The TransNet model by Alomar and Cai (2023) was adopted to represent the CNN model, and
the ViT model was depicted in Figure 7. For the spatial component of the hybrid model, we employed the
spatial component of TransNet; and for the temporal component, we employed the same ViT model that we
used in the ViT-only model. We constructed our model utilizing Python 3.6, incorporating the Keras deep
learning framework, OpenCV for image processing, matplotlib, and the scikit-learn library. The training
and test were performed on a computer equipped with an Intel Core i7 processor, an NVidia RTX 2070
graphics card, and 64GB of RAM.
4.1.1 Results and Discussion
Table 7: Experimental results of different models on the KTH
Dataset using three different context lengths. In particular, the
hybrid model was trained without pre-training whereas Hybridpre
is for the hybrid model pre-trained on ImageNet.
Context length
CNN-based
ViT-only
Hybrid
Hybridpre
12 frames
94.35
92.44
94.12
96.34
18 frames
93.91
92.82
94.56
97.13
24 frames
93.49
93.69
95.78
97.89
20
Table 7 presents the quantitative results of the three distinct models, i.e., CNN, ViT-only, and a hybrid
model on the KTH dataset, focusing on three different context lengths, i.e., short (12 frames), medium (18
frames), and long (24 frames). The results from these experiments provide insightful revelations into the
efficacy of each model under different temporal contexts. More details are given below.
The CNN model exhibited a decrease in accuracy as the frame length increased, recording 94.35% for
12 frames, 93.91% for 18 frames, and 93.49% for 24 frames. This descending trend suggests that CNN
may struggle with processing longer sequences where temporal dynamics become more complex, potentially
leading to challenges such as overfitting or difficulties in temporal feature retention over extended durations.
In contrast, the ViT model demonstrated an improvement in performance with longer sequences, achiev-
ing accuracy of 92.44% for 12 frames, 92.82% for 18 frames, and 93.69% for 24 frames. This ascending
pattern supports the notion that ViT architectures, with their inherent self-attention mechanisms, are well-
suited to managing longer sequences. The ability of ViTs to assign varying degrees of importance to different
parts of the sequence likely contributes to their enhanced performance on longer input frames.
The hybrid CNN-ViT model showcased the highest and continuously improving accuracy rates across
all frame lengths: 94.12% for 12 frames, 94.56% for 18 frames, and an impressive 95.78% for 24 frames.
Moreover, the pre-trained hybrid model showcased the same trend, with the best accuracy achieved. This
type of model synergistically combines CNN’s robust spatial feature extraction capabilities with ViT’s
efficient handling of temporal relationships via self-attention. The results from this model indicate that such
a hybrid approach is particularly effective in capturing the complexities of action recognition tasks in video
sequences, especially as the sequence length increases.
These findings underscore the potential advantages of hybrid neural network architectures in video-based
action recognition tasks, particularly for handling longer sequences with complex interactions. The superior
performance of the hybrid CNN-ViT model suggests that integrating the spatial acuity of CNNs with the
temporal finesse of ViTs can lead to more accurate and reliable recognition systems. Future work could
explore the scalability of these models to other datasets, their computational efficiency, and their robustness
against variations in video quality and scene dynamics. Additionally, further research might investigate the
optimal balance of CNN and ViT components within hybrid models to maximize both performance and
efficiency.
Table 8: Comparison of the proposed hybrid model with
the state-of-the-art models on the KTH dataset.
Methods
Venue
Accuracy
Geng and Song (2016)
ICCSAE ’16
92.49
Arunnehru et al. (2018)
RoSMa ’18
94.90
Abdelbaky and Aly (2020)
ITCE ’20
87.52
Jaouedi et al. (2020)
KSUCI journal ’20
96.30
Liu et al. (2020)
JAIHC ’20
91.93
Sahoo et al. (2020)
TETCI ’20
97.67
Lee et al. (2021)
CVF ’21
89.40
Basha et al. (2022)
MTA journal ’22
96.53
Ye and Bilodeau (2023)
CVF ’23
90.90
Ours
-
97.89
To complete the comparison, Table 8 shows that the impressive 97.89% accuracy achieved by the pre-
sented CNN-ViT hybrid model on the KTH dataset places it prominently among state-of-the-art models for
HAR. This performance is notably superior when compared to earlier benchmarks reported in the literature
such as Geng and Song (2016) with 92.49% and Arunnehru et al. (2018) with 94.90%. Our model utilizes
an ImageNet-pre-trained MobileNet (Howard et al. 2017) as the CNN backbone in the spatial component,
which enhances its robust feature extraction capabilities. Combined with the dynamic attention mechanisms
of ViT, it can thereby enhance both the spatial and temporal processing of video sequences. Furthermore,
our hybrid model not only surpasses other contemporary approaches like Liu et al. (2020) (91.93%) and
Lee et al. (2021) (89.40%), but also shows competitive/superior performance against some of the highest
accuracy in the field, such as Jaouedi et al. (2020) (96.30%) and Basha et al. (2022) (96.53%). Even in
comparison to the high benchmark set by Sahoo et al. (2020) (97.67%), our hybrid model demonstrates a
marginal but significant improvement, underscoring the efficacy of integrating CNN with ViT. This integra-
tion not only facilitates more nuanced feature extraction across both spatial and sequential dimensions but
also adapts more dynamically to the varied contexts inherent in video data, making it a potent solution for
realistic action recognition scenarios.
On the whole, the integration of CNN with ViT is particularly advantageous for enhancing feature
extraction capabilities and focusing on relevant segments dynamically through the attention mechanisms of
ViTs. This not only helps in improving accuracy but also in making the model more adaptable to varied
21
video contexts, a key requirement for action recognition in realistic scenarios. This comparative advantage
suggests that hybrid models are paving the way for future explorations in HAR, combining the best of
convolutional and ViT-based architectures for improved performance and efficiency.
5 Challenges and Future Directions
The field of HAR faces several formidable challenges that stem from the inherent complexity of interpreting
human movements within diverse and dynamic environments. One of the primary obstacles is the variability
in human actions themselves, which can differ significantly in speed, scale, and execution from one individ-
ual to another (Pareek and Thakkar 2021). This variability necessitates the development of sophisticated
models capable of generalizing across a wide range of actions without sacrificing accuracy (Nayak et al.
2021). Additionally, the presence of complex backgrounds and environments further complicates the task
of HAR. Systems must be adept at isolating and recognizing human actions against a backdrop of poten-
tially distracting or obstructive elements, which can vary from the bustling activity of a city street to the
unpredictable conditions of outdoor settings (Wang and Schmid 2013; He et al. 2016).
HAR systems furthermore must navigate the fine line between inter-class similarity and intra-class
variability, where actions that are similar to each other (such as running versus jogging) require nuanced
differentiation, while the same action can appear markedly different when performed by different individ-
uals or under varying circumstances (Gong et al. 2020; Zhu and Yang 2018). The challenge of temporal
segmentation adds another layer of complexity, as accurately determining the start and end of an action
within a continuous video stream is crucial for effective recognition (Zolfaghari et al. 2018). Coupled with
the need for computational efficiency to process video data in real-time and the difficulties associated with
obtaining large, accurately annotated datasets, these challenges underscore the multifaceted nature of HAR
(Caba Heilbron et al. 2015). Addressing these issues is critical for advancing the field and enhancing the prac-
tical applicability of HAR systems in real-world applications, from surveillance and security to healthcare
and entertainment.
The motivation behind this work has been driven by the compelling need to bridge the existing gaps
between the spatial feature extraction capabilities inherent in CNNs and the dynamic temporal processing
strengths found in ViTs (Arnab et al. 2021). Through the introduction of a novel hybrid model, an attempt
has been made to leverage the synergistic potential of these technologies, thereby enhancing the accuracy
and efficiency of HAR systems in capturing the complex spatial-temporal dynamics of human actions.
Looking forward, a promising future for HAR is envisioned, particularly through the development of
hybrid and integrated models. It is believed that the potential of these models extends beyond immediate
performance improvements, inspiring new directions for research within the field. It is anticipated that
future studies will focus on optimizing these hybrid architectures, aiming to make them more scalable and
adaptable to real-world applications across various domains such as surveillance, healthcare, and interactive
media. Furthermore, the exploration of self-attention mechanisms and the adaptation of large-scale pre-
training strategies from ViTs are seen as exciting prospects for HAR. These approaches are expected to lead
to the development of more sophisticated models capable of understanding and interpreting human actions
with unprecedented accuracy and nuance.
The integration of CNNs and ViTs into hybrid CNN-ViT models presents a promising avenue for over-
coming the challenges faced by HAR systems. These hybrid models capitalize on the strengths of both
architectures: the local feature extraction capabilities of CNNs and the global context understanding of
ViTs. Future developments could focus on enhancing model adaptability to generalize across diverse actions,
improving the isolation of human actions from complex backgrounds through advanced attention mecha-
nisms, and developing nuanced differentiation techniques for closely related actions (Carion et al. 2020).
Innovations in model architecture, alongside the application of transfer learning and few-shot learning
techniques, could significantly reduce the variability challenge in human actions.
Moreover, addressing the temporal segmentation challenge requires the integration of specialized tempo-
ral modules and sequence-to-sequence models to accurately determine the start and end of an action within
continuous video streams. Computational efficiency remains paramount for real-time processing, necessitat-
ing ongoing efforts in model optimization and the exploration of synthetic data generation to mitigate the
difficulties associated with obtaining large and accurately annotated datasets. Customizable hybrid CNN-
ViT models that can be tailored for specific applications, from surveillance to healthcare, will ensure that
these advancements not only push the boundaries of academic research but also enhance practical applica-
bility in real-world scenarios. Through these concerted efforts, hybrid CNN-ViT models are poised to make
significant contributions to the field of HAR, offering innovative solutions to its multifaceted challenges.
This work has highlighted the importance of continued innovation and cross-disciplinary collaboration in
the advancement of HAR technologies. By integrating insights from computer vision, machine learning, and
domain-specific knowledge, it is hoped that HAR systems will not only become more efficient and accurate
22
but also more responsive to the complexities and variances of human behavior in natural environments. As
the field moves forward, the focus is set on pushing the boundaries of what is possible in HAR, with the aim
of creating systems that enhance human-computer interaction and contribute positively to society through
various applications.
6 Conclusions
This survey provides a comprehensive overview of the current state of HAR by examining the roles and
advancements of CNNs, RNNs, and ViTs. It delves into the evolution of these architectures, emphasizing
their individual contributions to the field. The introduction of a hybrid model that combines the spatial
processing capabilities of CNNs with the temporal understanding of ViTs represents a methodological
advancement in HAR. This model aims to address the limitations of each architecture when used in isolation,
proposing a unified approach that potentially enhances the accuracy and efficiency of action recognition
tasks. The paper identifies key challenges and opportunities within HAR, such as the need for models that
can effectively integrate spatial and temporal information from video data. The exploration of hybrid models,
as suggested, offers a pathway for future research, particularly in improving model performance on complex
video datasets. The discussion encourages further investigation into optimizing these hybrid architectures
and exploring their applicability across various domains. This work sets a foundation for future studies to
build upon, aiming to push the boundaries of what is currently achievable in HAR and to explore new
applications of these technologies in real-world scenarios.
Acknowledgements.
K.A. and H.I.A. are thankful for the support from The Ministry of Education in
Saudi Arabia and the Republic of Turkiye Ministry of National Education, respectively.
Author contributions.
Conceptualisation, K.A. and X.C.; methodology, K.A.; software, K.A.; vali-
dation,, all authors.; investigation, all authors; resources, K.A. and H.I.A.; data curation, K.A.; writ-
ing—original draft preparation, all authors; writing—review and editing, all authors; visualisation, K.A.
and H.I.A.; supervision, X.C. All authors have read and agreed to the published version of the manuscript.
Declarations
Competing interests The authors declare no competing interests.
References
Abdelbaky, A. and S. Aly 2020.
Human action recognition based on simple deep convolution network
pcanet. In 2020 international conference on innovative trends in communication and computer engineering
(ITCE), pp. 257–262. IEEE.
Ahmadabadi, H., O.N. Manzari, and A. Ayatollahi 2023. Distilling knowledge from cnn-transformer mod-
els for enhanced human action recognition. In 2023 13th International Conference on Computer and
Knowledge Engineering (ICCKE), pp. 180–184. IEEE.
Alomar, K. and X. Cai 2023. Transnet: A transfer learning-based network for human action recognition. In
2023 International Conference on Machine Learning and Applications (ICMLA), pp. 1825–1832. IEEE.
Arnab, A., M. Dehghani, G. Heigold, C. Sun, M. Luˇci´c, and C. Schmid 2021.
Vivit: A video vision
transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6836–6846.
Arunnehru, J., G. Chamundeeswari, and S.P. Bharathi. 2018. Human action recognition using 3d convo-
lutional neural networks with 3d motion cuboids in surveillance videos. Procedia computer science 133:
471–477 .
Bahdanau, D., K. Cho, and Y. Bengio. 2014. Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473 .
Barron, J.L., D.J. Fleet, and S.S. Beauchemin. 1994. Performance of optical flow techniques. International
journal of computer vision 12: 43–77 .
Basha, S.S., V. Pulabaigari, and S. Mukherjee. 2022. An information-rich sampling technique over spatio-
temporal cnn for classification of human actions in videos. Multimedia Tools and Applications 81(28):
40431–40449 .
23
Bengio, Y., P. Simard, and P. Frasconi. 1994. Learning long-term dependencies with gradient descent is
difficult. IEEE transactions on neural networks 5(2): 157–166 .
Bertasius, G., H. Wang, and L. Torresani 2021. Is space-time attention all you need for video understanding?
In ICML, Volume 2, pp. 4.
Brauwers, G. and F. Frasincar. 2021. A general survey on attention mechanisms in deep learning. IEEE
Transactions on Knowledge and Data Engineering 35(4): 3279–3298 .
Brown, T., B. Mann, N. Ryder, M. Subbiah, J.D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing
systems 33: 1877–1901 .
Caba Heilbron, F., V. Escorcia, B. Ghanem, and J. Carlos Niebles 2015. Activitynet: A large-scale video
benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision
and pattern recognition, pp. 961–970.
Carion, N., F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko 2020. End-to-end object
detection with transformers. In European conference on computer vision, pp. 213–229. Springer.
Carreira, J. and A. Zisserman 2017. Quo vadis, action recognition? a new model and the kinetics dataset.
In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299–6308.
Chen, J. and C.M. Ho 2022. Mm-vit: Multi-modal video transformer for compressed video action recognition.
In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 1910–1921.
Cho, K., B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. 2014.
Learning phrase representations using rnn encoder-decoder for statistical machine translation.
arXiv
preprint arXiv:1406.1078 .
Chung, J., C. Gulcehre, K. Cho, and Y. Bengio. 2014.
Empirical evaluation of gated recurrent neural
networks on sequence modeling. arXiv preprint arXiv:1412.3555 .
Cortes, C. and V. Vapnik. 1995. Support-vector networks. Machine learning 20: 273–297 .
Cosmin Duta, I., B. Ionescu, K. Aizawa, and N. Sebe 2017. Spatio-temporal vector of locally max pooled
features for action recognition in videos. In Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition, pp. 3097–3106.
Dalal, N. and B. Triggs 2005. Histograms of oriented gradients for human detection. In 2005 IEEE computer
society conference on computer vision and pattern recognition (CVPR’05), Volume 1, pp. 886–893. Ieee.
Dar, G., M. Geva, A. Gupta, and J. Berant. 2022. Analyzing transformers in embedding space. arXiv
preprint arXiv:2209.02535 .
Deng, J., W. Dong, R. Socher, L.J. Li, K. Li, and L. Fei-Fei 2009. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee.
Devlin, J., M.W. Chang, K. Lee, and K. Toutanova. 2018.
Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 .
Devlin, J., M.W. Chang, K. Lee, and K. Toutanova 2019. Bert: Pre-training of deep bidirectional transform-
ers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pp. 4171–4186.
Diba, A., M. Fayyaz, V. Sharma, A.H. Karami, M.M. Arzani, R. Yousefzadeh, and L. Van Gool. 2017.
Temporal 3d convnets: New architecture and transfer learning for video classification. arXiv preprint
arXiv:1711.08200 .
Djenouri, Y. and A. Belbachir 2022.
A hybrid visual transformer for efficient deep human activity
recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision.
24
Donahue, J., L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell
2015. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 2625–2634.
Dosovitskiy, A., L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-
derer, G. Heigold, S. Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition
at scale. arXiv preprint arXiv:2010.11929 .
Fan, H., B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer 2021. Multiscale vision
transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6824–
6835.
Feichtenhofer, C. 2020. X3d: Expanding architectures for efficient video recognition. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pp. 203–213.
Feichtenhofer, C., H. Fan, J. Malik, and K. He 2019. Slowfast networks for video recognition. In Proceedings
of the IEEE/CVF international conference on computer vision, pp. 6202–6211.
Feichtenhofer, C., A. Pinz, and A. Zisserman 2016. Convolutional two-stream network fusion for video
action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 1933–1941.
Fukushima, K. 1980. Neocognitron: A self-organizing neural network model for a mechanism of pattern
recognition unaffected by shift in position. Biological cybernetics 36(4): 193–202 .
Geng, C. and J. Song 2016.
Human action recognition based on convolutional neural networks with a
convolutional auto-encoder. In 2015 5th International Conference on Computer Sciences and Automation
Engineering (ICCSAE 2015), pp. 933–938. Atlantis Press.
Gers, F.A., J. Schmidhuber, and F. Cummins. 2000. Learning to forget: Continual prediction with lstm.
Neural computation 12(10): 2451–2471 .
Ghadiyaram, D., D. Tran, and D. Mahajan 2019. Large-scale weakly-supervised pre-training for video action
recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.
12046–12055.
Gong, G., X. Wang, Y. Mu, and Q. Tian 2020. Learning temporal co-attention models for unsupervised
video action localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 9819–9828.
Graves, A. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 .
Graves, A., A.r. Mohamed, and G. Hinton 2013. Speech recognition with deep recurrent neural networks.
In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6645–6649. Ieee.
Han, K., Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu, et al. 2022. A survey
on vision transformer. IEEE transactions on pattern analysis and machine intelligence 45(1): 87–110 .
Hara, K., H. Kataoka, and Y. Satoh 2018. Can spatiotemporal 3d cnns retrace the history of 2d cnns
and imagenet? In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp.
6546–6555.
He, K., X. Zhang, S. Ren, and J. Sun 2016. Deep residual learning for image recognition. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 770–778.
Hochreiter, S. and J. Schmidhuber. 1997. Long short-term memory. Neural computation 9(8): 1735–1780 .
Howard, A.G., M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam.
2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861 .
Hu, Y., Y. Wong, W. Wei, Y. Du, M. Kankanhalli, and W. Geng. 2018. A novel attention-based hybrid
cnn-rnn architecture for semg-based gesture recognition. PloS one 13(10): e0206049 .
25
Jaouedi, N., N. Boujnah, and M.S. Bouhlel. 2020. A new hybrid deep learning model for human action
recognition. Journal of King Saud University-Computer and Information Sciences 32(4): 447–453 .
Jegham, I. et al. 2022. Multi-view vision transformer for driver action recognition. SpringerLink .
Ji, S., W. Xu, M. Yang, and K. Yu. 2012. 3d convolutional neural networks for human action recognition.
IEEE transactions on pattern analysis and machine intelligence 35(1): 221–231 .
Jordan, M. 1986. Serial order: a parallel distributed processing approach. technical report, june 1985-march
1986. Technical report, California Univ., San Diego, La Jolla (USA). Inst. for Cognitive Science.
Kalchbrenner, N. and P. Blunsom 2013. Recurrent continuous translation models. In Proceedings of the
2013 conference on empirical methods in natural language processing, pp. 1700–1709.
Kalfaoglu, M. et al. 2022. Human action recognition with transformers. SpringerLink .
Karpathy, A., G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei 2014. Large-scale video
classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition, pp. 1725–1732.
Khan, S., M. Naseer, M. Hayat, S.W. Zamir, F.S. Khan, and M. Shah. 2022. Transformers in vision: A
survey. ACM computing surveys (CSUR) 54(10s): 1–41 .
Kong, Y. and Y. Fu. 2022. Human action recognition and prediction: A survey. International Journal of
Computer Vision 130(5): 1366–1401 .
Koot, R., M. Hennerbichler, and H. Lu. 2021. Evaluating transformers for lightweight action recognition.
arXiv preprint arXiv:2111.09641 .
Kopuklu, O., N. Kose, A. Gunduz, and G. Rigoll 2019. Resource efficient 3d convolutional neural networks.
In Proceedings of the IEEE/CVF international conference on computer vision workshops, pp. 0–0.
Krizhevsky, A., I. Sutskever, and G.E. Hinton. 2012. Imagenet classification with deep convolutional neural
networks. Advances in neural information processing systems 25 .
Kuehne, H., H. Jhuang, E. Garrote, T. Poggio, and T. Serre 2011. Hmdb: a large video database for human
motion recognition. In 2011 International conference on computer vision, pp. 2556–2563. IEEE.
LeCun, Y., Y. Bengio, and G. Hinton. 2015. Deep learning. nature 521(7553): 436–444 .
LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document
recognition. Proceedings of the IEEE 86(11): 2278–2324 .
Lee, S., H.G. Kim, D.H. Choi, H.I. Kim, and Y.M. Ro 2021. Video prediction recalling long-term motion
context via memory alignment learning. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 3054–3063.
Li, J., X. Liu, M. Zhang, and D. Wang. 2020. Spatio-temporal deformable 3d convnets with attention for
action recognition. Pattern Recognition 98: 107037 .
Li, Y., C. Lan, J. Xing, W. Zeng, C. Yuan, and J. Liu 2016. Online human action detection using joint
classification-regression recurrent neural networks.
In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14, pp. 203–220.
Springer.
Li, Z., K. Gavrilyuk, E. Gavves, M. Jain, and C.G. Snoek. 2018. Videolstm convolves, attends and flows for
action recognition. Computer Vision and Image Understanding 166: 41–50 .
Lin, T., Y. Wang, X. Liu, and X. Qiu. 2022. A survey of transformers. AI Open .
Liu, X., D.y. Qi, and H.b. Xiao. 2020. Construction and evaluation of the human behavior recognition model
in kinematics under deep learning. Journal of Ambient Intelligence and Humanized Computing: 1–9 .
26
Liu, Z., J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu 2022. Video swin transformer. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3202–3211.
Luong, M.T., H. Pham, and C.D. Manning. 2015. Effective approaches to attention-based neural machine
translation. arXiv preprint arXiv:1508.04025 .
Malik, N.u.R., S.A.R. Abu-Bakar, U.U. Sheikh, A. Channa, and N. Popescu. 2023. Cascading pose features
with cnn-lstm for multiview human action recognition. Signals 4(1): 40–55 .
Muhammad, K., A. Ullah, A.S. Imran, M. Sajjad, M.S. Kiran, G. Sannino, V.H.C. de Albuquerque, et al.
2021. Human action recognition using attention based lstm network with dilated cnn features. Future
Generation Computer Systems 125: 820–830 .
Mutegeki, R. and D.S. Han 2020. A cnn-lstm approach to human activity recognition. In 2020 international
conference on artificial intelligence in information and communication (ICAIIC), pp. 362–366. IEEE.
Nayak, R., U.C. Pati, and S.K. Das. 2021. A comprehensive review on deep learning-based methods for
video anomaly detection. Image and Vision Computing 106: 104078 .
Pareek, P. and A. Thakkar. 2021.
A survey on video-based human action recognition: recent updates,
datasets, challenges, and applications. Artificial Intelligence Review 54: 2259–2322 .
Peng, Y., Y. Zhao, and J. Zhang. 2018. Two-stream collaborative learning with spatial-temporal attention
for video classification. IEEE Transactions on Circuits and Systems for Video Technology 29(3): 773–786
.
Qiu, Z., T. Yao, and T. Mei 2017. Learning spatio-temporal representation with pseudo-3d residual networks.
In proceedings of the IEEE International Conference on Computer Vision, pp. 5533–5541.
Rumelhart, D.E., G.E. Hinton, and R.J. Williams 1985.
Learning internal representations by error
propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science.
Sahoo, S.P., S. Ari, K. Mahapatra, and S.P. Mohanty. 2020. Har-depth: a novel framework for human action
recognition using sequential learning and depth estimated history images. IEEE transactions on emerging
topics in computational intelligence 5(5): 813–825 .
Schuldt, C., I. Laptev, and B. Caputo 2004. Recognizing human actions: a local svm approach. In Proceedings
of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., Volume 3, pp. 32–36.
IEEE.
Sharir, G., A. Noy, and L. Zelnik-Manor. 2021. An image is worth 16x16 words, what is a video worth?
arXiv preprint arXiv:2103.13915 .
Simonyan, K. and A. Zisserman. 2014a. Two-stream convolutional networks for action recognition in videos.
Advances in neural information processing systems 27 .
Simonyan, K. and A. Zisserman. 2014b. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 .
Soomro, K., A.R. Zamir, and M. Shah. 2012. Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402 .
Srivastava, N., E. Mansimov, and R. Salakhudinov 2015. Unsupervised learning of video representations
using lstms. In International conference on machine learning, pp. 843–852. PMLR.
Sun, Z., Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, and J. Liu. 2022. Human action recognition from
various data modalities: A review. IEEE transactions on pattern analysis and machine intelligence .
Surek, G., L. Seman, S. Stefenon, V. Mariani, and L. Coelho. 2023. Video-based human activity recognition
using deep learning approaches. Sensors 23(14): 6384 .
Sutskever, I., O. Vinyals, and Q.V. Le. 2014. Sequence to sequence learning with neural networks. Advances
in neural information processing systems 27 .
27
Szegedy, C., W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich
2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1–9.
Touvron, H., M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J´egou 2021. Training data-efficient
image transformers & distillation through attention. In International conference on machine learning,
pp. 10347–10357. PMLR.
Tran, D., L. Bourdev, R. Fergus, L. Torresani, and M. Paluri 2015. Learning spatiotemporal features with
3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pp.
4489–4497.
Tran, D., H. Wang, L. Torresani, and M. Feiszli 2019. Video classification with channel-separated convo-
lutional networks. In Proceedings of the IEEE/CVF international conference on computer vision, pp.
5552–5561.
Tran, D., H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri 2018. A closer look at spatiotemporal
convolutions for action recognition.
In Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition, pp. 6450–6459.
Ulhaq, A., N. Akhtar, G. Pogrebna, and A. Mian. 2022. Vision transformers for action recognition: A survey.
arXiv preprint arXiv:2209.05700 .
Ullah, A., J. Ahmad, K. Muhammad, M. Sajjad, and S.W. Baik. 2017. Action recognition in video sequences
using deep bi-directional lstm with cnn features. IEEE access 6: 1155–1166 .
Varol, G., I. Laptev, and C. Schmid. 2017. Long-term temporal convolutions for action recognition. IEEE
transactions on pattern analysis and machine intelligence 40(6): 1510–1517 .
Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez,  L. Kaiser, and I. Polosukhin. 2017.
Attention is all you need. Advances in neural information processing systems 30 .
Wang, H. and C. Schmid 2013. Action recognition with improved trajectories. In Proceedings of the IEEE
international conference on computer vision, pp. 3551–3558.
Wang, L., Z. Tong, B. Ji, and G. Wu 2021. Tdn: Temporal difference networks for efficient action recognition.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1895–1904.
Wang, L., Y. Xiong, Z. Wang, and Y. Qiao. 2015. Towards good practices for very deep two-stream convnets.
arXiv preprint arXiv:1507.02159 .
Wang, L., Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool 2016. Temporal segment networks:
Towards good practices for deep action recognition.
In European conference on computer vision, pp.
20–36. Springer.
Wang, L., Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool. 2018. Temporal segment networks
for action recognition in videos. IEEE transactions on pattern analysis and machine intelligence 41(11):
2740–2755 .
Wang, M., J. Xing, and Y. Liu. 2021. Actionclip: A new paradigm for video action recognition. arXiv
preprint arXiv:2109.08472 .
Wang, X., R. Girshick, A. Gupta, and K. He 2018. Non-local neural networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7794–7803.
Wang, Y., M. Long, J. Wang, and P.S. Yu 2017.
Spatiotemporal pyramid network for video action
recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp.
1529–1538.
Wu, Z., X. Wang, Y.G. Jiang, H. Ye, and X. Xue 2015. Modeling spatial-temporal clues in a hybrid deep
learning framework for video classification. In Proceedings of the 23rd ACM international conference on
Multimedia, pp. 461–470.
28
Xia, K., J. Huang, and H. Wang. 2020. Lstm-cnn architecture for human activity recognition. IEEE Access 8:
56855–56866 .
Xie, S., C. Sun, J. Huang, Z. Tu, and K. Murphy 2018. Rethinking spatiotemporal feature learning: Speed-
accuracy trade-offs in video classification. In Proceedings of the European conference on computer vision
(ECCV), pp. 305–321.
Xing, Z., Q. Dai, H. Hu, J. Chen, Z. Wu, and Y.G. Jiang 2023. Svformer: Semi-supervised video transformer
for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 18816–18826.
Ye, X. and G.A. Bilodeau 2023. A unified model for continuous conditional video prediction. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3603–3612.
Yu, W. et al. 2023. Swin-fusion: Swin-transformer with feature fusion for human action recognition. Neural
Processing Letters .
Yue-Hei Ng, J., M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici 2015. Beyond
short snippets: Deep networks for video classification. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 4694–4702.
Zhang, X. et al. 2021. A two-stream hybrid cnn-transformer network for skeleton-based human interaction
recognition. arXiv preprint arXiv:2105.02087 .
Zhu, L. and Y. Yang 2018. Compound memory networks for few-shot video classification. In Proceedings
of the European Conference on Computer Vision (ECCV), pp. 751–766.
Zolfaghari, M., K. Singh, and T. Brox 2018.
Eco: Efficient convolutional network for online video
understanding. In Proceedings of the European conference on computer vision (ECCV), pp. 695–712.
29
