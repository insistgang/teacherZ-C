CNNs, RNNs and Transformers in Human Action Recognition:
A Survey and a Hybrid Model
Khaled Alomar1*, Halil Ibrahim Aysel1 and Xiaohao Cai1
1Electronics and Computer Science, University of Southampton, Southampton, SO17 1BJ,
United Kingdom.
*Corresponding author(s). E-mail(s): kaa1u20@soton.ac.uk;
Contributing authors: hia1v20@soton.ac.uk; x.cai@soton.ac.uk;
Abstract
Human action recognition (HAR) encompasses the task of monitoring human activities across various
domains, including but not limited to medical, educational, entertainment, visual surveillance, video
retrieval, and the identification of anomalous activities. Over the past decade, the field of HAR has wit-
nessed substantial progress by leveraging convolutional neural networks (CNNs) and recurrent neural
networks (RNNs) to effectively extract and comprehend intricate information, thereby enhancing the
overall performance of HAR systems. Recently, the domain of computer vision has witnessed the emer-
gence of Vision Transformers (ViTs) as a potent solution. The efficacy of Transformer architecture has
been validated beyond the confines of image analysis, extending their applicability to diverse video-
related tasks. Notably, within this landscape, the research community has shown keen interest in HAR,
acknowledging its manifold utility and widespread adoption across various domains. This article aims to
present an encompassing survey that focuses on CNNs and the evolution of RNNs to ViTs given their
importance in the domain of HAR. By conducting a thorough examination of existing literature and
exploring emerging trends, this study undertakes a critical analysis and synthesis of the accumulated
knowledge in this field. Additionally, it investigates the ongoing efforts to develop hybrid approaches.
Following this direction, this article presents a novel hybrid model that seeks to integrate the inherent
strengths of CNNs and ViTs.
Keywords: Human action recognition · Convolutional neural networks · Recurrent neural networks · Vision
Transformers · Deep learning · Video classification
1 Introduction
Human action recognition (HAR) focuses on the classification of the specific actions exhibited within a given
video. On the other hand, action detection and segmentation focus on the precise localization or extraction
of individual instances of actions from video content (Ulhaq et al. 2022). The capacity of deep learning
models to effectively capture the spatial and temporal complexities inherent in video representations plays
a vital role in the recognition and understanding of actions.
Over the preceding decade, a considerable amount of research has been dedicated to the thorough
investigation of action recognition, resulting in an extensive collection of review articles and survey papers
addressing the topic (Pareek and Thakkar 2021; Sun et al. 2022; Kong and Fu 2022). However, it is worth
noting that a predominant focus of these scholarly works has been placed on the examination and evaluation
of convolutional neural networks (CNNs) and traditional machine learning models within the realm of action
recognition.
The advent of Transformer architecture (Vaswani et al. 2017) has sparked a paradigm shift in deep
learning. By employing a multi-head self-attention layer, the Transformer model computes sequence repre-
sentations by effectively aligning words within the sequence with other words in the same sequence (Ulhaq
et al. 2022). This approach outperforms traditional convolutional and recursive operations in terms of
representation quality while utilizing fewer computational resources. As a consequence, the Transformer
1
arXiv:2407.06162v2  [cs.CV]  15 Aug 2024
architecture diverges from conventional convolutional and recursive methods, favoring a more focused uti-
lization of multiple processing nodes. The incorporation of multi-head attention allows the Transformer
model to collectively learn a range of representations from diverse perspectives through the collaboration
of multiple attention layers. Inspired by Transformers, many natural language processing (NLP) tasks have
achieved remarkable performance, reaching human-level capabilities, as exemplified by models such as GPT
(Brown et al. 2020) and BERT (Devlin et al. 2018).
The remarkable achievements of Transformers in handling sequential data, particularly in the domain of
NLP, have prompted the exploration and advancement of Vision Transformer (ViT) (Dosovitskiy et al. 2020)
(a special Transformer for computer vision tasks). ViTs have demonstrated comparable or even superior
performance compared to CNNs in the context of image recognition tasks, especially when operating on vast
datasets such as ImageNet (Han et al. 2022; Lin et al. 2022; Khan et al. 2022). This observation signifies
a noteworthy shift in the field, wherein ViTs possess the potential to supplant the established dominance
of CNNs in computer vision, mirroring the displacement witnessed in the case of recurrent neural networks
(RNNs) (Ulhaq et al. 2022). The achievements of Transformer models have engendered considerable scholarly
interest within the computer vision research community, prompting rigorous exploration of their efficacy in
pure computer vision tasks.
The natural progression in the advancement of ViTs has led to the logical exploration of video recognition
tasks. Unlike image recognition, video recognition focuses on the complex challenge of identifying and
understanding events within video sequences, including the recognition of human actions. Consequently,
there is a compelling need for a recent review that comprehensively examines the state-of-the-art research
including ViTs and hybrid models in addition to CNNs and RNNs for HAR. Such a review would serve as
a crucial guiding resource to shape the future research directions with Transformer and CNN-Transformer
hybrid architectures beside CNNs which previously were seen as unique and influential models for HAR.
The main contributions of this paper is as follows.
• We present a thorough review of the CNNs, RNNs and ViTs. This review examines the evolution from
traditional methods to the latest advancements in neural network architectures.
• We present an extensive examination of existing literature related to HAR.
• We propose a novel hybrid model integrating the strengths of CNNs and ViTs. In addition, we provide
a detailed performance comparison of the proposed hybrid model against existing models. The analysis
highlights the model’s efficacy in handling complex HAR tasks with improved accuracy and efficiency.
• We also discuss emerging trends and the future direction of HAR technologies, emphasizing the importance
of hybrid models in enhancing the interpretability and robustness of HAR systems.
These contributions enrich the understanding of the current state and future prospects of HAR, proposing
innovative approaches and highlighting the importance of integrating different neural network architectures
to advance the field.
The paper is structured as follows. Section 2 delves into the background, covering foundational concepts
and technologies crucial to HAR, including CNNs, RNNs and ViTs, highlighting the chronological evolution
of HAR deep learning technologies. Section 3 thoroughly reviews related HAR works with a brief discussion.
A novel hybrid model combining CNNs and ViTs is proposed in Section 4, including the details of the
experimental setup and the results. Section 5 discusses the challenges and their implications for future
directions in HAR. Finally, Section 6 concludes the paper.
2 Background
This section provides a chronological and technical overview of three fundamental types of neural networks:
CNNs, RNNs, and Transformers. CNNs, introduced in the late 1980s, revolutionized image processing by
leveraging local connectivity and shared weights to efficiently detect spatial hierarchies in data. As the field
progressed, RNNs emerged in the 1990s, addressing the need for modeling sequential data through their
ability to maintain temporal dependencies across sequences. The advent of Transformers in 2017 marked a
paradigm shift by utilizing self-attention mechanisms to capture global relationships in data more effectively,
thereby enhancing performance in a wide array of tasks beyond sequential data. This background section
will delve into the technical intricacies and evolutionary trajectory of these architectures, highlighting their
contributions and transitions in the realm of deep learning.
2.1 CNNs
The evolution of CNNs has been remarkable since their introduction in the 1980s. Originally, CNNs were
designed to process static images, primarily focusing on spatial recognition tasks such as object and pattern
recognition. The initial idea was to build layers of convolutional filters that would apply various operations
2
to the image to extract features like edges, textures, and shapes. This structure proved highly effective for
tasks like image classification, object detection, image segmentation and more in computer vision.
The Neocognitron (Fukushima 1980), developed by Kunihiko Fukushima, presented an early example
of neural networks incorporating convolutional operations for image processing, setting the foundations
for subsequent progress. Shortly after, Yann LeCun and collaborators introduced LeNet-5 (LeCun et al.
1998), a key architecture designed for handwritten digit recognition, showcasing the effectiveness of convo-
lutional layers in pattern recognition tasks. The progress of CNNs reached a turning point in the mid-2010s
with the introduction of models like AlexNet (Krizhevsky et al. 2012), showcasing their potential in image
classification tasks. Alongside architectural innovations, this milestone was achieved thanks to access to
large datasets, notably, ImageNet (Deng et al. 2009), and computational improvements, including the rise
of graphics processing units (GPUs) for parallel computing. Large-scale datasets provided the diversity
and complexity necessary for training deep networks, while enhanced computational power accelerated the
training of sophisticated CNN architectures.
The architectural enhancements, large datasets, and increased computational capabilities helped CNNs
to be a cornerstone in deep learning methodologies, extending their applications beyond image processing to
various domains. Notable architectures like VGGNet (Simonyan and Zisserman 2014a), distinguished by its
uniform design and small convolutional filters, GoogLeNet (Szegedy et al. 2015), with its inception modules
for capturing features at different scales efficiently, and ResNet (He et al. 2016), which introduced residual
learning for training very deep networks, have further enriched the landscape of CNNs.
2.1.1 Spatio-Temporal CNNs
As CNNs excelled in spatial tasks, researchers began exploring their potential in handling temporal data,
such as video and time-series analysis. The challenge was to incorporate the dimension of time into the
inherently spatial architecture of CNNs. To address this task, spatio-temporal CNNs were developed.
These networks extend traditional CNN architectures by adding a temporal component to analyze dynamic
behaviors across time frames. Several approaches have been utilized and main types are as follows.
3D convolution involves extending the 2D kernels to 3D, allowing the network to perform convolution
across both spatial and temporal dimensions. This approach is directly applied to video data where the
third dimension represents time (Hara et al. 2018; Tran et al. 2015). The two-stream CNNs involve run-
ning two parallel CNN streams: one for spatial processing of individual frames and another for temporal
processing, usually of optical flow, which captures motion between frames (Simonyan and Zisserman 2014a;
Feichtenhofer et al. 2016). RNNs with CNNs aim to combine CNNs for spatial processing with RNNs like
long short-term memory (LSTM) or gated recurrent unit (GRU) to handle temporal dependencies. This
hybrid model leverages CNNs’ ability to extract spatial features and RNNs’ capacity to manage temporal
sequences effectively (Yue-Hei Ng et al. 2015; Donahue et al. 2015).
2.2 From Vanilla RNN to Attention-Based Transformers
This section explores the evolution from RNNs to the Transformers, highlighting the progression in handling
time series and sequence data. Initially, RNNs were the go-to deep learning technique for managing temporal
tasks, effectively capturing sequential dependencies. However, the development of Transformers marked a
significant leap forward, driven by a series of iterative improvements and optimizations that built upon the
limitations of RNNs. Transformers, with their focus on NLP, introduced a novel attention mechanism that
allows for more efficient and scalable processing of sequential data. By examining the foundational RNN
techniques and the subsequent enhancements leading to the Transformer architecture, this section elucidates
the transformative journey from traditional RNN models to the sophisticated attention-based frameworks
that now dominate the field.
We firstly establish common notations for RNN architectures including vanilla RNNs, LSTM and GRU
to streamline discussions in subsequent sections. In these architectures, each iteration involves a cell that
sequentially processes an input embedding xt ∈Rnx and retains information from the previous sequence
through the hidden state ht−1 ∈Rnh using weight matrices W ∈Rnh×nh and U ∈Rnh×nx. The W -like
matrices encompass all weights related to hidden-to-hidden connections, while U-like matrices encompass
all weight matrices related to input-to-hidden connections. Additionally, bias terms are represented by b-like
vectors. Each cell produces a new hidden state ht ∈Rnh as its output.
2.2.1 Vanilla RNNs
Vanilla RNNs (Rumelhart et al. 1985; Jordan 1986) lack the presence of a cell state, relying solely on the
hidden states as the primary means of memory retention within the RNN framework. The hidden state ht
is subsequently updated and propagated to the subsequent cell, or alternatively, depending on the specific
3
task at hand, it can be employed to generate a prediction. Figure 1a illustrates the internal mechanisms of
an RNN and a mathematical description of it given as
ht = tanh(W ht−1 + Uxt + b),
(1)
where tanh is the activation function.
Vanilla RNNs effectively incorporate short-term dependencies of temporal order and past inputs in a
meaningful manner. However, they are characterized by certain limitations. Firstly, due to their intrinsic
sequential nature, RNNs pose challenges in parallelized computations (Graves et al. 2013). Consequently,
this limitation can impose restrictions on the overall speed and scalability of the network. Secondly, when
processing lengthy sequences, the issue of exploding or vanishing gradients may arise, thereby impeding the
stable training of the network (Bengio et al. 1994).
tanh
ht−1
xt
ht
ht
(a) Vanilla RNN
tanh
tanh
σ
σ
σ
Forget
Input
Output
ct−1
ht−1
xt
ct
ht
ft
it
˜ct
ot
(b) LSTM
tanh
σ
σ
Reset
Update
ht−1
ht
xt
rt zt
1−
˜ht
(c) GRU
Fig. 1: Various types of RNN cells.
2.2.2 LSTM
Hochreiter and Schmidhuber (1997) introduced the LSTM cell as a solution to address the issue of long-term
dependencies and to mitigate the challenge of interdependencies among successive steps (Hochreiter and
Schmidhuber 1997). LSTM architecture incorporates a distinct component known as the cell state ct ∈Rnh,
illustrated in Figure 1b. Analogous to a freeway, this cell state facilitates the smooth flow of information,
ensuring that it can readily traverse without undergoing significant alterations.
Gers et al. (2000) made modifications to the initial LSTM architecture by incorporating a forget gate
within the cell structure. The mathematical expressions describing this modified LSTM cell are derived
from its inner connections. Hence, the LSTM cell can be formally represented based on the depicted
interconnections as follows.
• Forget gate decides what information should be thrown away or kept from the cell state with the equation
f t = σ(W fht−1 + U fxt + bf),
(2)
where f t ∈Rnh is the output of the forget gate and σ is the sigmoid activation function.
• Input gate determines which new information is added to the cell state with two activation functions
defined as
it = σ(W iht−1 + U ixt + bi),
(3)
where it ∈Rnh is the output of the sigmoid activation function; and
˜ct = tanh(W ˜cht−1 + U ˜cxt + b˜c),
(4)
where ˜ct ∈Rnh is known as candidate value. After obtaining it and ˜ct, we can update the cell state with
ct = f t ⊙ct−1 + it ⊙˜ct,
(5)
where ct−1 ∈Rnh is the previous cell state and ⊙is the Hadamard operator.
4
• Output gate determines the next hidden state based on the cell state and output gate’s activity
ot = σ(W oht−1 + U oxt + bo),
(6)
where ot ∈Rnh is the output of the output gate. Finally the updated hidden state,
ht = tanh(ct) ⊙ot
(7)
is fed to the next iteration.
To enable selective information retention, LSTM employs three distinct gates. The first gate, known as
the forget gate, examines the previous hidden state ht−1 and the current input xt. It generates a vector
f t containing values between 0 and 1, determining the portion of information to discard from the previous
cell state ct−1. The second gate, referred to as the input gate, follows a similar process to the forget gate.
However, instead of discarding information, it utilizes the output it to determine the new information to
be stored in the cell state based on a candidate cell state ˜ct. Lastly, the output gate employs the output
ot to filter the updated cell state ct, thereby transforming it into the new hidden state ht. The LSTM cell
exhibits superior performance in retaining both long-term and short-term memory compared to the vanilla
RNN cell. However, this advantage comes at the expense of increased complexity.
2.2.3 GRU
The LSTM cell surpasses the learning capability of the conventional recurrent cell, yet the additional number
of parameters escalates the computational load. Consequently, to address this concern, Chung et al. (2014)
introduced the GRU, see Figure 1c. GRU demonstrates comparable performance to LSTM while offering
a more computationally efficient design with fewer weights. This is achieved by merging the cell state and
the hidden state into “reset state” resulting in a simplified architecture. Furthermore, GRU combines the
forget and input gates into an “update gate”, contributing to a more streamlined computational process.
For further elaboration, GRU cell incorporates two essential gates. The first gate is the reset gate, which
examines the previous hidden state ht−1 and the current input xt. It generates a vector rt containing values
between 0 and 1, determining the extent to which past information in ht−1 should be disregarded. The
second gate is the update gate, which governs the selection of information to either retain or discard when
updating the new hidden state ht, based on the value of rt.
Based on the depicted information in Figure 1c, the mathematical expressions governing the behavior
of the GRU cell can be expressed as follows.
• Update gate decides how much of the past information needs to be passed along with
zt = σ(W zht−1 + U zxt + bz),
(8)
where zt ∈Rnh is the output of the update gate. The output of the reset gate rt ∈Rnh is obtained by
rt = σ(W rht−1 + U rxt + br).
(9)
A candidate activation for the subsequent step is
˜ht = tanh(W ˜h(rt ⊙ht−1) + U ˜hxt + b˜h)
(10)
where ˜ht ∈Rnh.
• The final activation is a blend of the previous hidden state and the candidate activation, weighted by the
update gate, i.e.,
ht = zt ⊙˜ht + (1 −zt) ⊙ht−1
(11)
where ht ∈Rnh is the updated hidden state. This mechanism allows the GRU to effectively retain or
replace old information with new information.
5
(a) One-to-one
h0
y1
x1
(b) One-to-many
h0
x1
y1
y2
yt
...
(c) Many-to-many
h0
x1
x2
xt
y1
y2
yt
...
(d) Many-to-one
h0
yt
x1
x2
xt
...
Fig. 2: Types of RNN structures based on input-output pairs.
2.2.4 Types of RNNs
RNNs were created with an internal memory mechanism that allows them to store and use information from
previous outputs. This unique trait enables RNNs to retain important contextual information over time,
enabling reasoned decision-making based on past results. There are four types of popular RNN variants that
each serve different purposes across a variety of applications, see Figure 2.
The one-to-one is considered the simplest form of RNNs, where a single input corresponds to a single
output. It operates with fixed input and output sizes, functioning similarly to a standard neural network.
One-to-many represents a specific category of RNNs that is characterized by its ability to produce multiple
outputs based on a single input provided to the model. This type of RNN is particularly useful in applications
like image captioning, where a fixed input size results in a series of data outputs. Many-to-one RNNs merge
a sequence of inputs into a single output through a series of hidden layers that learn relevant features. An
illustrative instance of this RNN type is sentiment analysis, where the model analyzes a sequence of text
inputs and produces a single output indicating the sentiment expressed in the text.
Many-to-many RNNs are employed to generate a sequence of output data from a sequence of input units.
It can be categorized into two subcategories: equal size and unequal size. In the equal size subcategory, the
input and output layers have the same size, see many-to-many architecture in Figure 2c. Several research
efforts have emerged to tackle the limitation of the fixed-size input-output sequences in machine translation
tasks, as they fail to adequately represent real-world requirements. The unequal size subcategory can handle
different sizes of inputs and outputs. A practical application of the unequal size subcategory can be observed
in machine translation. In this scenario, the model generates a sequence of translated text outputs based
on a sequence of input sentences. Unequal size subcategory employs an encoder-decoder architecture, where
the encoder adopts the many-to-one architecture, and the decoder adopts the one-to-many architecture.
One notable contribution in this area was made by Kalchbrenner and Blunsom (2013), who pioneered the
approach of mapping the entire input sentence to a vector. This work is related to the study conducted by
Cho et al. (2014), although the latter was specifically utilized to refine hypotheses generated by a phrase-
based system (Sutskever et al. 2014). In this architecture, the encoder component plays a crucial role in
transforming the inputs into a singular vector, commonly referred to as the context. This context vector,
typically with a length of 256, 512 or 1024, encapsulates all the pertinent information detected by the encoder
from the input sentence, which serves as the translation target, see Figure 3a. Subsequently, this vector is
passed on to the decoder, which generates the corresponding output sequence. It is important to note that
both the encoder and decoder components in this architecture are RNNs. Different from Figure 3a, Figure
3b gives the encoder-decoder architecture with attention which will be introduced in the next section.
h0
x0
RNN
x1
RNN
xt
RNN
ht
sos
RNN
↓
RNN
eos
RNN
(a) Without attention
x0
RNN
↓
x1
RNN
xt
RNN
ht
h0
h0
α0
h1
α1
ht
αt
→
sos
eos
RNN
↓
RNN
RNN
P
(b) With attention
Fig. 3: Sequence-to-sequence RNN with and without the attention mechanism.
6
2.2.5 Attention
The evolution of attention mechanisms in neural networks represents a significant advancement in the field
of deep learning, particularly in tasks related to NLP and machine translation. Initially introduced by
Graves (2013), the concept of attention mechanisms was designed to enhance the model’s ability to focus on
specific parts of the input sequence when generating an output, mimicking the human ability to concentrate
on particular aspects of a task. This foundational work laid the groundwork for subsequent developments
in attention mechanisms, providing a mechanism for models to dynamically assign importance to different
parts of the input data.
Building on Graves’ initial concept, Bahdanau et al. (2014) introduced the additive attention mechanism,
which was specifically designed to improve machine translation. This approach computes the attention
weights through a feed-forward neural network, allowing the model to consider the entire input sequence and
determine the relevance of each part when translating a segment. This additive form of attention significantly
improved the performance of sequence-to-sequence models by enabling a more nuanced understanding and
alignment between the input and output sequences (Sutskever et al. 2014). Following this, Luong et al. (2015)
proposed the multiplicative attention mechanism, also known as dot-product attention, which simplifies
the computation of attention weights by calculating the dot product between the query and all keys. This
method not only streamlined the attention mechanism but also offered improvements in computational
efficiency and performance in various NLP tasks, marking a pivotal moment in the evolution of attention
mechanisms from their inception to more sophisticated and efficient variants.
The central idea of the attention mechanism is to shift focus from the task of learning a single vector rep-
resentation for each sentence. Instead, it adopts a strategy of selectively attending to particular input vectors
in the input sequence, guided by assigned attention weights. This strategy enables the model to dynami-
cally allocate its attention resources to the most pertinent segments of the sequence, thereby improving its
capacity to process and comprehend the information more efficiently (Brauwers and Frasincar 2021).
One possible explanation for the improvement is that the attention layer created memories associated
with the context pattern rather than memories associated with the input itself, relieving pressure on the
RNN model structure’s weights and causing the model memory to be devoted to remembering the input
rather than the context pattern (Hu et al. 2018).
(a) Transformer
(b) Self-Attention
Fig. 4: Transformer architecture and its self-attention mechanism (adapted from Vaswani et al. (2017)).
7
2.2.6 Self-Attention
To this point, attention mechanisms in sequence-transformation models have primarily relied on complex
RNNs, featuring an encoder and a decoder, the most successful models in language translation yet. How-
ever, Vaswani et al. (2017) introduced a simple network architecture known as the Transformer, see Figure
4, which exclusively utilized attention mechanism, eliminating the need for RNNs. They introduced a novel
attention mechanism called self-attention, which is also known as KQV-attention (Key, Query, and Value).
This attention mechanism subsequently gained prominence as a central component within the Transformer
architecture. The attention mechanism stands out due to its ability to provide Transformers with an exten-
sive long-term memory. In the Transformer model, it becomes possible to focus on all previously generated
tokens.
The embedding layer in a Transformer model is the initial stage where input tokens are transformed
into dense vectors, capturing semantic information about each token’s meaning and context within the text.
These embeddings serve as the foundation for subsequent layers to process and understand the relationships
between words in the input sequence (Dar et al. 2022).
Self-attention is a mechanism that allows an input sequence to process itself in a way that each position
in the sequence can attend to all positions within the same sequence. This mechanism is a cornerstone of the
Transformer architecture, which has revolutionized NLP and beyond by enabling models to efficiently handle
sequences of data with complex dependencies. The purpose of self-attention is to compute a representation of
each element in a sequence by considering the entire sequence, thereby capturing the contextual relationships
between elements regardless of their positional distance from each other. This ability to capture both local
and global dependencies makes self-attention particularly powerful for tasks such as machine translation,
text summarization, and sequence prediction, where understanding the context and the relationship between
words or elements in a sequence is crucial (Vaswani et al. 2017).
The mathematical formulation of self-attention involves several key steps. First, a set of query vectors
Q = XW Q, a set of key vectors K = XW K, and a set of value vectors V = XW V are calculated through
linear transformations of the input sequence, where X is the input matrix representing embeddings of tokens
in a sequence, and W Q,W K, and W V are weight matrices for queries, keys, and values, respectively. The
attention scores are then calculated by taking the dot product of the query vector with all key vectors,
followed by scaling the result by the inverse square root of the dimension of the keys (say √dk) to avoid
overly large values. These scores are then passed through a softmax function to obtain the attention weights,
which represent the importance of each element’s contribution to the output. Finally, the output say A is
computed as a weighted sum of the value vectors, i.e.,
A(Q, K, V ) = softmax(QK⊤
√dk
)V .
(12)
This process allows the model to dynamically focus on different parts of the input sequence, enabling the
extraction of rich contextual information from the sequence.
2.2.7 Multi-Head-Attention
Multi-head attention is an extension of the self-attention mechanism designed to allow the model to jointly
attend the information from different representation subspaces at different positions (Vaswani et al. 2017).
Instead of performing a single attention function, it runs the attention mechanism multiple times in parallel.
The outputs of these independent attention computations are then concatenated and linearly transformed
into the expected dimension. The mathematical formulation of the multi-head attention can be described
in the following steps. First, for the i-th self-attention head, find
Qi = XW Q
i ,
Ki = XW K
i ,
V i = XW V
i ,
(13)
and then compute
Ai(Qi, Ki, V i) = softmax
 
QiK⊤
i
√dk
!
V i.
(14)
The multi-head attention is obtained by concatenating all Ai(Qi, Ki, V i).
The multi-head attention mechanism enables the model to capture different types of information from
different positions of the input sequence. By processing the sequence through multiple attention “heads”, the
model can focus on different aspects of the sequence, such as syntactic and semantic features, simultaneously.
This capability enhances the model’s ability to understand and represent complex data, making multi-head
attention a powerful component of Transformer-based architectures (Devlin et al. 2019).
8
2.3 From Transformer to Vision Transformer
The journey from the inception of the Transformer model to the development of the ViT marks a pivotal
advancement in deep learning, showcasing the adaptability of models initially designed for sequence data
processing to the realm of image analysis. This transition underscores a significant shift in approach, from
conventional image processing techniques to more sophisticated sequence-based methodologies.
Introduced by Vaswani et al. (2017) through the seminal paper “Attention Is All You Need”, the Trans-
former model revolutionized NLP by leveraging self-attention mechanisms. This innovation allowed for the
processing of sequences of data without the reliance on recurrent layers, facilitating unprecedented paral-
lelization and significantly reducing training times for large datasets. The Transformer’s success in NLP
sparked curiosity about its potential applicability across different types of data, including images, setting
the stage for a transformative adaptation.
The adaptation of Transformers for image data pivoted on a novel concept: treating images not as
traditional 2D arrays of pixels but as sequences of smaller and discrete image patches. This approach,
however, faced computational challenges due to the self-attention mechanism’s quadratic complexity with
respect to input length. The breakthrough came with the introduction of the ViT by Dosovitskiy et al.
(2020), which applied the Transformer architecture directly to images, see Figure 5. By dividing an image
into fixed-size patches and processing these patches as if they were tokens in a text sequence, ViT was able
to capture complex relationships between different parts of an image using the Transformer’s encoder.
The operational mechanics of ViT begin with the division of an input image into fixed-size patches, each
of which is flattened and linearly transformed into a vector, effectively converting the 2D image into a 1D
sequence of embeddings. To account for the lack of inherent positional awareness within the Transformer
architecture, positional embeddings are added to these patch embeddings, ensuring the model retains spatial
information. The sequence of embeddings is then processed through the Transformer encoder, which consists
of layers of multi-head self-attention and feed-forward neural networks, allowing the model to dynamically
weigh the importance of each patch relative to others for a given task.
For tasks like image classification, the output from the Transformer encoder is passed through a classi-
fication head, often utilizing a learnable “class token” appended to the sequence of patch embeddings for
this purpose. The model is trained on large datasets using backpropagation and, during inference, processes
images through these steps to predict their classes.
The ViT not only demonstrates exceptional performance on image classification tasks, often surpassing
CNNs when trained on extensive datasets, but also highlights the Transformer architecture’s capacity to
capture the global context within images. Despite its advantages, ViT’s reliance on substantial computational
resources for training and its need for large datasets to achieve optimal performance present challenges.
Nonetheless, the development of ViT signifies a significant milestone in the application of sequence processing
models to the field of computer vision, opening new avenues for research and practical applications.
Fig. 5: The ViT architecture (adapted from Dosovitskiy et al. (2020)).
The original ViT, designed for static image processing, divides images into patches and interprets these
as sequences, leveraging the Transformer’s self-attention mechanism to understand complex spatial relation-
ships. Extending this model to action recognition involves adapting it to analyze video frames sequentially to
capture both spatial and temporal relationships. Several works attempted to adapt ViT in action recognition
task using different methods as below.
9
Temporal dimension integration. The integration of the temporal dimension is a fundamental step in
adapting ViT for action recognition. Traditional ViT models process images as a series of patches, treating
them essentially as sequences for the self-attention mechanism to analyze spatial relationships. By extending
this concept to include the temporal dimension, the models can now treat videos as sequences of frame
patches over time. This allows the models to capture the evolution of actions across frames. The work by
Bertasius et al. (2021) highlights the potential of incorporating temporal information into Transformers,
marking a significant advancement in video analysis capabilities.
Spatio-temporal embeddings. To effectively capture the dynamics of actions within videos, adapted ViT
models generate spatio-temporal embeddings. This involves extending the traditional positional embed-
dings used in ViTs to also include temporal positions, thereby creating embeddings that account for both
spatial and temporal information within video sequences. The discussion by Arnab et al. (2021) on the cre-
ation of these spatio-temporal embeddings showcases the method’s effectiveness in enhancing the model’s
understanding of action dynamics across both space and time.
Multi-head self-attention across time. The extension of self-attention mechanisms to analyze relationships
between patches not just within individual frames but also across different frames is crucial for recognizing
actions over time. This approach enables the model to identify relevant features and changes across the video
sequences, facilitating a deeper understanding of motion and the progression of actions. The exploration by
Bertasius et al. (2021) of this concept demonstrates how Transformers can be effectively adapted to capture
the temporal dynamics of actions, a key aspect of video analysis.
3 Literature Review
This section briefly recalls the most commonly used deep learning-based HAR approaches.
3.1 CNN-Based Approaches in HAR
This section recalls the most prominent CNN-based approaches in HAR based on the model type (i.e., the
two-stream CNN, 3D CNN, and RNNs with CNNs), organized chronologically.
Deep learning was still in its early stages in 2012, and CNNs or RNNs had not yet gained significant
popularity in the field of HAR. The focus was primarily on traditional machine learning approaches, such as
support vector machines (Cortes and Vapnik 1995), and handcrafted features, such as histogram of oriented
gradients (Dalal and Triggs 2005) and histogram of optical flow (Barron et al. 1994). A few studies did,
nevertheless, start looking into neural networks for action recognition.
In 2014, the use of CNNs in action recognition was at a pivotal stage, marking a shift from hand-crafted
feature-based methods to deep learning approaches. The key points of the use of CNNs in action recognition
at that period of time are the following. (I) Emergence of deep learning: deep learning, particularly CNNs,
had started to dominate image classification tasks, thanks to their ability to learn feature representations
directly from raw pixel data. This success in static images paved the way for applying CNNs to video
data for action recognition. (II) Challenges in video data: unlike 2D images, videos incorporate a third
dimension which represents the temporal patterns, making action recognition more complex. CNNs had to
be adapted to not only recognize spatial patterns but also capture motion information over time dimension.
(III) Datasets and benchmarks: the adoption of large-scale video datasets like UCF-101 (Soomro et al. 2012)
and HMDB-51 (Kuehne et al. 2011) became more common. These datasets provided diverse sets of actions
and were large enough to train deep networks. The performance on these benchmarks has been becoming
a key measure of progress for action recognition models. (IV) Transfer learning: due to the computational
expense of training CNNs from scratch and the relatively smaller size of video datasets compared to image
datasets, transfer learning became a popular strategy. Networks pre-trained on large image datasets like
ImageNet (Deng et al. 2009) were fine-tuned on video frames for action recognition tasks. (V) Computational
constraints: despite the promise of CNNs, computational constraints were a significant challenge. Training
deep networks required significant GPU power, and processing video data with CNNs was resource-intensive.
This limited the complexity of the models that could be trained and the size of the datasets that could be
used.
3.1.1 Two-Stream CNNs
Simonyan and Zisserman (2014a) presented an innovative approach to recognize actions in video sequences by
using a two-stream CNN architecture. This approach divides the task into two distinct problems: recognizing
spatial features from single frames and capturing temporal features across frames. The spatial stream CNN
processes static visual information, while the temporal stream CNN handles motion by analyzing optical flow.
The model was tested on benchmark datasets like UCF-101 and HMDB-51, where it achieved state-of-the-art
results, showcasing the effectiveness of this two-stream method. The novelty of this work lies in the separation
10
of motion and appearance features, which allows for more specialized networks that can better capture the
complexities of video-based action recognition. The success of this model has made a significant impact
on the field, influencing many future research directions in video understanding. Consequently, numerous
methods have been proposed to enhance the the two-stream model (Wang et al. 2015; Feichtenhofer et al.
2016; Wang et al. 2016; Peng et al. 2018; Wang et al. 2017).
In 2016, building on the the two-stream CNN, Feichtenhofer et al. (2016) focused on improving the two-
stream CNN by exploring various fusion strategies for combining spatial and temporal streams, resulting
in better performance on the UCF-101 and HMDB-51 datasets. By enhancing fusion techniques, this work
addressed the limitations of the initial two-stream model, leading to more effective integration of spatial
and temporal information. Wang et al. (2016) introduced temporal segment networks (TSN). This work
aimed to capture long-range temporal structures for action recognition, achieving significant improvements
on the UCF-101 and HMDB-51 datasets by dividing videos into segments for comprehensive analysis. The
introduction of TSN extended the temporal analysis capabilities of the two-stream CNN, enabling the
capture of long-range dependencies.
In 2017, derived from the two-stream CNN, Cosmin Duta et al. (2017) proposed a three-stream method
by using spatio-temporal vectors, with locally max-pooled features to enhance performance. Tested on
the UCF-101 and HMDB-51 datasets, the approach demonstrated improved recognition accuracy by effi-
ciently capturing spatio-temporal dynamics. In 2018, the efficient convolutional network for online video
understanding (ECO) was introduced by Zolfaghari et al. (2018), combining the two-stream CNN approach
with lightweight 3D CNNs, and focusing on efficiency and real-time processing, with high efficiency and
competitive accuracy demonstrated on the Kinetics and UCF-101 datasets.
Feichtenhofer et al. (2019) introduced the SlowFast network which processes video data at varying
frame rates to capture both spatial semantics and motion dynamics, achieving state-of-the-art results on
