Less but Better: Parameter-Efficient Fine-Tuning of
Large Language Models for Personality Detection
Lingzhi Shen
University of Southampton
Southampton, United Kingdom
l.shen@soton.ac.uk
Yunfei Long
Queen Mary University of London
London, United Kingdom
yl20051@essex.ac.uk
Xiaohao Cai
University of Southampton
Southampton, United Kingdom
x.cai@soton.ac.uk
Guanming Chen
University of Southampton
Southampton, United Kingdom
gc3n21@soton.ac.uk
Imran Razzak
Mohamed bin Zayed University of Artificial Intelligence
Abu Dhabi, United Arab Emirates
imran.razzak@mbzuai.ac.ae
Shoaib Jameel
University of Southampton
Southampton, United Kingdom
M.S.Jameel@southampton.ac.uk
Abstract—Personality detection automatically identifies an in-
dividual’s personality from various data sources, such as social
media texts. However, as the parameter scale of language models
continues to grow, the computational cost becomes increasingly
difficult to manage. Fine-tuning also grows more complex, mak-
ing it harder to justify the effort and reliably predict outcomes.
We introduce a novel parameter-efficient fine-tuning framework,
PersLLM, to address these challenges. In PersLLM, a large
language model (LLM) extracts high-dimensional representations
from raw data and stores them in a dynamic memory layer.
PersLLM then updates the downstream layers with a replaceable
output network, enabling flexible adaptation to various person-
ality detection scenarios. By storing the features in the memory
layer, we eliminate the need for repeated complex computations
by the LLM. Meanwhile, the lightweight output network serves as
a proxy for evaluating the overall effectiveness of the framework,
improving the predictability of results. Experimental results
on key benchmark datasets like Kaggle and Pandora show
that PersLLM significantly reduces computational cost while
maintaining competitive performance and strong adaptability.
Index Terms—Personality Detection, Large Language Models,
Parameter-Efficient Fine-Tuning, Text Classification
I. INTRODUCTION
Personality refers to the stable traits in an individual’s
emotions, thoughts, and behaviours that shape how they per-
ceive, interpret, and interact with the world [1], [2]. It is
a complex and multifaceted construct that combines various
characteristics to form a person’s unique identity. The Myers-
Briggs Type Indicator (MBTI) [3] is a widely used personality
assessment tool based on Carl Jung’s theory of psychological
types. It categorizes individuals into 16 distinct personality
types by evaluating them across four dimensions: Extraversion
(E) vs. Introversion (I), Sensing (S) vs. Intuition (N), Thinking
(T) vs. Feeling (F), and Judging (J) vs. Perceiving (P). In
recent years, automated personality detection using machine
learning has gained widespread attention in fields such as
This work was supported by the Alan Turing Institute and DSO National
Laboratories under a grant on improving multimodal misinformation detection
through affective analysis. Additional support was provided by the Interdis-
ciplinary Research Pump-Priming Fund, University of Southampton.
personalized marketing [4], mental health assessment [5], and
employee recruitment [6].
As consistently demonstrated [7], [8], the ability of language
models to capture complex linguistic patterns and contextual
dependencies is crucial for understanding personality traits
from textual data [9]. These models are often extended through
continued pre-training [10] and full fine-tuning [11] to adapt to
specific domains or optimize for particular tasks. LLMs with
billions or even trillions of parameters, represent a qualitative
leap over traditional models. They have become increasingly
popular in tasks involving complex reasoning [12] and deep
semantic analysis [13]. However, as the number of parameters
increases, LLMs face significant challenges in fine-tuning.
These difficulties include enormous computational resource
demands, longer training durations, and increased memory
consumption, all of which severely impact the ability to
comprehensively evaluate an LLM’s performance on specific
tasks [14]. Additionally, the inability to quickly receive feed-
back and validate improvements during the development phase
further complicates the process.
Parameter-efficient fine-tuning (PEFT) [15] is a practical
and effective knowledge transfer strategy that enables large
pre-trained models to adapt to new tasks while significantly
reducing computational and memory costs. Rather than up-
dating all parameters, PEFT typically optimizes only a small
subset. For example, by applying low-rank reparameterization
techniques such as LoRA [16], introducing lightweight mod-
ules such as adapters [17], or incorporating soft prompts such
as prefix tuning [18] and prompt tuning [19]. Recent work
like AdapterHub [20] has unified these techniques under a
generalized “adapter method” abstraction, further enhancing
parameter efficiency and modular composability. This makes
it particularly suitable for deployment in resource-constrained
environments. PEFT has been widely adopted in natural lan-
guage processing, where LLMs can be efficiently customized
for specialized applications such as medical diagnosis, legal
research, and customer service.
In this work, we develop a novel PEFT framework for
arXiv:2504.05411v1  [cs.CL]  7 Apr 2025
personality detection tasks called PersLLM (Personality Large
Language Model). Our intuition is that in traditional end-to-
end models, each time a new input is provided to the LLM, the
model must repeat complex computations [21], such as feature
extraction, forward propagation, and backpropagation. These
steps involve extensive matrix operations and multi-layer com-
putations, such as the self-attention mechanism in transformer
architectures and gradient calculations, all requiring access
to the vast parameters of the LLM, making it a highly
computationally intensive process [22]. To address this issue,
we introduce a novel dynamic memory layer after feature
extraction. This layer leverages an indexing mechanism, a
querying mechanism, and cache updates to store extracted
features, eliminating the need for costly LLM computations
during each inference. Besides the feature memory capability,
another novelty lies in our choice of a lightweight neural
network for the output layer. Compared to the LLM, these
networks have fewer parameters and lower computational
complexity, significantly reducing the computational cost of
training. Unlike training from scratch, the downstream network
only needs to learn task-specific relationships and patterns
based on the features stored in the memory layer, further
accelerating the training process. Additionally, since these
output layers are replaceable, the framework gains adaptability.
For different personality detection scenarios, we do not need
to retrain the LLM. Instead, we can adjust the output network,
selecting the appropriate one based on the performance of the
individual output layers.
Main Contributions: The PersLLM framework not only ef-
fectively mitigates the computational overhead and time costs
associated with LLMs through PEFT, but also delivers more
effective quantitative results. This introduces a novel strategy
for fine-tuning LLMs. The key technical contribution is the
use of a dynamic memory layer to store and update features,
optimizing computational efficiency, while a flexible output
network can quickly adapt to various personality detection
scenarios. PersLLM has also fully demonstrated the immense
potential of fine-tuning LLMs for this specific task. PersLLM
outperforms the current state-of-the-art models on two bench-
mark datasets, setting a new standard for performance.
II. RELATED WORK
Classic Personality Detection: Some commonly used ma-
chine learning models such as SVM [23] and XGBoost [24]
have shown excellent performance in classifying personality
traits from social media data. Various deep neural networks
also have also demonstrated their strengths in text-based per-
sonality detection tasks. A study using BiLSTM with an atten-
tion mechanism achieved notable results in detecting psycho-
pathic traits in social media text [25]. Graph techniques capture
the complex relationships between text and psychological traits
by constructing graph structures. For example, both D-DGCN
[26] and Semi-PerGCN [27] leverage graph convolutional
networks to model the structure of user-generated content, with
D-DGCN focusing on dynamic relationships between posts,
and Semi-PerGCN integrating psycholinguistic features with
unsupervised learning. However, most of the above methods
often struggle to capture deeper and contextual relationships
within unstructured text.
Language Models for Personality Detection: BERT [28],
known for capturing nuanced language patterns, has been
fine-tuned for personality recognition [29], such as a study
combined sentiment lexicons with the BERT model for multi-
label personality detection [30]. Another study used a hybrid
strategy to fine-tune RoBERTa for predicting continuous val-
ues of personality traits [31]. Recently, researchers have begun
applying LLMs to personality classification tasks. PsyCoT [32]
enhances the LLM by using psychological questionnaires as
chain-of-thought steps, guiding the model to assess personality
traits through a multi-turn dialogue process based on textual
input; TAE [33] enhances smaller models by extracting knowl-
edge from the LLM through text augmentation, improving
post representation and labelling information by combining
contrastive learning with semantic, emotional, and linguistic
analysis. However, as the size of language models grows, fine-
tuning becomes increasingly complex, leading researchers to
favour prompt-based methods to reduce training costs. Fine-
tuning LLMs for personality detection is even rarer [34], which
may result in overlooking the most optimal approaches for this
task.
Our PersLLM differs from previous approaches as it in-
troduces a PEFT method tailored for personality detection
using LLMs. The LLM extracts rich semantic representations,
which are stored in a dynamically updated memory layer,
while training and prediction are focused on a lightweight and
flexible output network. This approach significantly reduces
training costs and enhances adaptability to tasks, making LLM
fine-tuning feasible for personality detection.
III. METHODOLOGY
PersLLM is a novel PEFT framework (in Fig. 1) for person-
ality detection tasks. Leveraging the powerful representation
capabilities of the LLM, rich semantic information is extracted
from text and stored in a memory layer for repeated access
and dynamic updates. The subsequent training and inference
processes are handled by a lightweight output network. This
design eliminates the computational burden of repeatedly
invoking the LLM’s large number of parameters, significantly
reducing computational costs while maintaining performance.
A. Feature Extraction Layer
Under the PersLLM framework, we leverage Llama 3.1
[35] to extract high-dimensional representations from user-
generated text. Each user ui in the dataset has a collection
of multiple posts Pi = {pi1, pi2, . . . , pim}. Since a user may
have a large number of posts, they are processed in batches
rather than as a single sequence. Each batch Pib ⊆Pi consists
of a subset of the user’s posts, where multiple batches are
processed sequentially.
Llama processes each batch Pib and generates a correspond-
ing set of feature embeddings Eib = {eib1, eib2, . . . , eibk},
where each eibj represents the extracted embedding of the j-th
Fig. 1. The framework diagram of PersLLM. First, the LLM is utilized for feature extraction from posts, generating high-dimensional representations that
are stored in a dynamic memory layer. Subsequent training is focused entirely on an output network for classification.
post in batch Pib. Consequently, instead of a single embedding
set per user, we obtain multiple embeddings across batches,
i.e., Ei = {Ei1, Ei2, . . . , EiB}, where B is the total number
of batches for user ui.
Llama incorporates the Grouped Query Attention (GQA)
mechanism to improve efficiency when processing multiple
user-generated inputs. GQA reduces the number of indepen-
dent query heads by a factor of g, while keeping the full set
of key and value heads. Specifically, the query representation
is modified as Q′ ∈Rs×(dk/g), where Q′ = W ′
Q ·Eib, s is the
total input size, dk is the original query dimensionality, and
g is the number of query groups. The key and value matrices
remain unchanged, with K ∈Rs×dk and V ∈Rs×dk. The
attention computation becomes:
Attention(Q′, K, V ) = softmax
 
Q′K⊤
p
dk/g
!
V.
By reducing the dimensionality of queries while maintain-
ing the expressiveness of key-value pairs, GQA lowers the
computational overhead without compromising the ability to
model long-range dependencies. This allows Llama to effi-
ciently extract contextual representations from multiple posts,
facilitating high-performance downstream processing at scale.
After passing through multiple layers of GQA and feed-
forward neural networks, each batch Eib is transformed into
its high-dimensional feature representation Hib = GQA(Eib).
Consequently, instead of a single output representation, we
obtain a sequence of feature representations across batches,
denoted as Hi
= {Hi1, Hi2, . . . , HiB}, where each Hib
encapsulates the extracted features of the corresponding batch.
B. Dynamic Memory Layer
The dynamic memory layer consists of three core compo-
nents: an indexing mechanism, a querying mechanism, and
a cache update mechanism. Instead of recomputing Llama-
extracted representations for every input, the Memory Layer
continuously updates its stored features based on recent in-
puts, leveraging locality-sensitive hashing (LSH) for efficient
retrieval.
Specifically, for each extracted feature batch Hib, the Mem-
ory Layer applies a LSH transformation to project it into a
lower-dimensional space while preserving similarity relation-
ships. LSH approximates nearest neighbor search by mapping
similar feature embeddings into the same hash buckets, where
the transformation is defined as hhash = sign(WLSHHib), with
WLSH as a random projection matrix that preserves locality
properties. The resulting binary vector hhash serves as the
indexing key for fast retrieval.
Using this hashed representation, the Memory Layer dy-
namically searches for the closest stored feature batch H∗by
minimizing a predefined distance metric:
H∗= arg min
Hj∈M d(Hib, Hj),
where M is the memory store and d(·, ·) is a similarity
function such as cosine similarity:
S(Hib, Hj) =
Hib · Hj
||Hib|| · ||Hj||.
To ensure adaptive retrieval, the Memory Layer dynamically
determines whether H∗can be reused based on a similarity
threshold θ, ensuring that if S(Hib, H∗) ≥θ, the stored
representation is directly used. Otherwise, Llama recomputes
the feature embeddings for the batch, and the updated repre-
sentation is stored for future retrieval.
Given limited storage capacity, the Memory Layer adopts
an adaptive Least Recently Used (LRU) strategy, prioritizing
frequently accessed embeddings while discarding the least
recently used ones when memory is full:
M =
(
M ∪{Hib},
if |M| < M;
(M \ Holdest) ∪{Hib},
if |M| ≥M.
Here, M represents the memory capacity, and Holdest is the
least recently accessed feature batch.
By continuously adapting to new data, the Dynamic Mem-
ory Layer optimizes retrieval efficiency while maintaining
relevance. Cached feature embeddings evolve dynamically
based on recent queries, ensuring that frequently accessed
personality representations remain available while outdated
ones are discarded.
C. Replaceable Output Layer
Since the memory layer stores and retrieves embeddings at
the batch-level, the output network efficiently transforms the
retrieved feature embeddings into compact user-level represen-
tations without redundant recomputation.
Given
the
batch-level
feature
embeddings
Hi
=
{Hi1, Hi2, . . . , HiB} retrieved from the Memory
Layer, the output network aggregates them into a final
representation suitable for classification. To achieve this
efficiently, we adopt a lightweight structure that processes
the embeddings in sequence while maintaining adaptability
to different architectures.
A computationally efficient sequential model, such as a
Gated Recurrent Unit (GRU), serves as an example to illustrate
this process. The model iteratively refines an internal repre-
sentation ht at each step t using the current batch embedding
Hit and the prior state ht−1, ensuring that only necessary
computations are performed. After processing all batches, the
final output representation, denoted as hfinal, encapsulates the
aggregated information across all stored embeddings.
To maintain efficiency, the Replaceable Output Layer is
designed to be lightweight, making it feasible to deploy
alternative architectures such as Transformer-based encoders
or convolutional networks without significant computational
cost. The simplicity of this layer ensures seamless adaptability,
enabling it to operate on top of cached feature embeddings
without introducing additional complexity.
D. Personality Classification
The final feature representation, denoted as hfinal, obtained
from the Replaceable Output Layer, serves as the input to the
classification module. Since the output network is replaceable,
the classification process remains independent of its specific
architecture, ensuring adaptability to different model designs.
The classification module performs both binary classification
for the four MBTI dimensions and a 16-class classification for
predicting the complete personality type.
For the binary classification task, each MBTI dimension
d ∈{E/I, N/S, T/F, J/P} is treated as an independent clas-
sification problem. The model predicts a categorical outcome
for each dimension, determining an individual’s preference
along the four personality axes, where ˆy = (ˆyE, ˆyN, ˆyT , ˆyJ)
and each ˆyd represents the predicted trait for the corresponding
dimension.
In addition to binary classification, the model also pre-
dicts the complete MBTI personality type as a single cat-
egorical variable. This is formulated as a 16-class clas-
sification problem, where each class corresponds to one
of the 16 MBTI personality types. The model assigns a
probability distribution over all 16 personality types and
selects the most probable one as the final prediction,
where ˆy16 ∈{INTJ, INTP, ENTJ, ENTP, INFJ, INFP, ENFJ,
ENFP, ISTJ, ISFJ, ESTJ, ESFJ, ISTP, ISFP, ESTP, ESFP}.
IV. EXPERIMENTS
A. Experimental Setup
Datasets: To ensure a fair comparison with previous work,
we selected the same two datasets – Kaggle and Pandora. The
Kaggle dataset is sourced from the PersonalityCafe forum, an
online community focused on discussions about personality
types. This dataset contains posts from 8,675 users, with
each user contributing approximately 45 to 50 posts. The
posts cover a variety of topics, including psychology, personal
experiences, and everyday discussions. Pandora is a larger
corpus from the Reddit platform, which includes MBTI labels
for 9,084 users. The number of posts per user varies from
dozens to hundreds, and due to the diversity of the Reddit
community, the content covers a broader range of topics
compared to the Kaggle dataset.
As shown in Table I, both datasets feature large-scale inputs,
user-level aggregation, and significant class imbalance, partic-
ularly in the S/N and E/I dimensions. These factors increase
the computational burden during training and inference and
make it more challenging to capture long-range dependencies
and coherent global structures across multiple user posts.
TABLE I
STATISTICS OF THE KAGGLE AND PANDORA DATASETS IN TERMS OF THE
SET DIVISION AND CLASS DISTRIBUTION.
Dataset
Types
Train
Validation
Test
Kaggle
E/I
1194 / 4011
409 / 1326
396 / 1339
S/N
610 / 4478
222 / 1513
248 / 1487
T/F
2410 / 2795
791 / 944
780 / 955
J/P
2109 / 3096
672 / 1063
653 / 1082
Posts
246794
82642
82152
Pandora
E/I
1162 / 4278
386 / 1427
377 / 1437
S/N
727 / 4830
208 / 1605
210 / 1604
T/F
3549 / 1891
1120 / 693
1182 / 632
J/P
2229 / 3211
770 / 1043
758 / 1056
Posts
523534
173005
174080
Implementation Details: We leveraged the “Llama-3.1-8B-
Instruct” model. For the output network, we primarily utilized
GRU and flexibly replaced it with other neural networks based
on task requirements. The feature vector length extracted by
Llama and the input vector length of the output network
were both standardized to 4096. To maintain lightweight
computation, the GRU consists of only three hidden layers
by default, and dropout regularization with a probability of
0.2 was applied to prevent overfitting. The hidden dimension
of GRU output at each time step is 512. The model uses the
Adam optimizer with a learning rate of 1 × 10−3. The loss
function used is cross-entropy loss. To prevent information
leakage, words that directly matched personality labels were
removed during data preprocessing. The datasets were split
TABLE II
COMPARISON OF PERSLLM WITH STATE-OF-THE-ART LANGUAGE MODEL-BASED BASELINES ON THE KAGGLE AND PANDORA DATASETS IN TERMS OF
THE MACRO-F1 (%) SCORES ACROSS THE FOUR DIMENSIONS AND THEIR OVERALL AVERAGE (AVG).
Methods
Kaggle
Pandora
E/I
S/N
T/F
J/P
Avg
E/I
S/N
T/F
J/P
Avg
BERT
64.65
57.12
77.95
65.25
66.24
54.22
48.71
64.70
56.07
56.56
RoBERTa
61.89
57.59
78.69
70.07
67.06
54.80
55.12
63.78
55.94
57.41
Transformer-MD
66.08
69.10
79.19
67.50
70.47
55.26
58.77
69.26
60.90
61.05
PQ-Net
68.94
67.65
79.12
69.57
71.32
57.07
55.26
65.64
58.74
59.18
GPT-4 Turbo (Zero-shot)
68.86
54.69
80.10
66.93
67.65
57.38
51.47
71.75
62.29
60.72
GPT-4 Turbo (CoT)
66.63
61.85
77.23
60.80
66.62
60.97
57.14
66.39
56.92
60.36
DeepSeek-V3 (Zero-shot)
69.76
58.61
75.71
64.37
67.11
61.39
54.31
68.05
58.17
60.48
DeepSeek-V3 (CoT)
68.59
61.47
74.56
61.21
66.46
58.23
56.15
67.94
55.01
59.33
PsyCoT
66.56
61.70
74.80
57.83
65.22
60.91
57.12
66.45
53.34
59.45
TAE
70.90
66.21
81.17
70.20
72.07
62.57
61.01
69.28
59.34
63.05
PersLLM
76.71
75.55
85.11
75.96
78.33
68.89
66.72
73.70
68.55
69.47
into training, validation, and test sets using a 60%/20%/20%
ratio. The results reported are averages of ten runs.
Evaluation Metrics: Following previous work, we used
Macro-F1 score as the primary evaluation metric, along with
accuracy (ACC), precision (P), and recall (R) in the newly
added scenarios for comprehensive evaluation.
Baselines: We selected several widely-used pretrained lan-
guage models for text classification, including fine-tuned
BERT [36] and RoBERTa [37], as well as advanced LLMs
like
GPT-4
[38]
and
DeepSeek-V3
[39].
We
also
in-
cluded more complex architectural designs based on differ-
ent language models. The Transformer-MD [40] was chosen
for its memory-augmented Transformer-XL architecture and
dimension-specific attention, enabling effective integration of
multi-post inputs for personality detection. The PQ-Net [41]
model was selected for its dual-stream design that combines
text and psychological questionnaires via cross-attention to ex-
tract key personality cues. The PsyCoT [32] was included for
its chain-of-thought prompting based on questionnaire items,
guiding the LLM through multi-turn reasoning to score each
trait. The TAE [33] was chosen for leveraging LLM-generated
text augmentations and label explanations with contrastive
learning to capture psycholinguistic features.
B. Overall Results
Macro F1: The results comparing PersLLM with baselines
in Macro-F1 scores are shown in Table II. PersLLM achieves
the top performance across all four dimensions and the overall
average. On the Kaggle dataset, the average score is 78.33%,
surpassing the best existing scheme, TAE, by 6.26%. On
the Pandora dataset, PersLLM achieves an average score of
69.47%, outperforming TAE by 6.42%. PersLLM also delivers
outstanding results across all four dimensions. Compared to
the baselines, PersLLM shows significant improvements in
dimensions with severe class imbalance, particularly narrow-
ing the performance gap between T/F and the other three
dimensions.
Compared to previously reported Transformer-based lan-
guage models such as BERT and RoBERTa, the improvements
observed in PersLLM indicate that increasing the number
of parameters significantly enhances the model’s ability to
capture richer semantic representations and handle deeper
linguistic nuances. We also compared PersLLM with several
contemporary LLM-based approaches and found that Per-
sLLM consistently outperforms methods that rely on prompt-
ing, such as zero-shot. Surprisingly, chain-of-thought (CoT)
reasoning with step-by-step inference produced results that
were even slightly worse than zero-shot performance. This
finding suggests that the model’s capacity constraints limit
its ability to further push performance boundaries, and the
complex decomposition process in CoT can even introduce
hallucinations and noise. When the model’s discriminative
power is sufficiently strong, adjusting its parameters should
be prioritized to break through its performance ceiling.
TABLE III
THE PERFORMANCE COMPARISON OF THE MODELS WITHIN THE 16-CLASS
EVALUATION FRAMEWORK.
Dataset
Method
Acc
P
R
F1
Kaggle
BERT
34.64
16.29
14.43
15.87
RoBERTa
38.56
25.26
21.25
22.72
Llama 3.1
43.16
37.53
32.37
34.79
Llama 3.1 (LoRA)
47.24
44.15
36.62
39.43
PersLLM
46.54
45.54
39.60
41.61
PersLLM (GCN)
51.14
48.60
46.57
44.21
PersLLM (LSTM)
58.43
50.86
45.09
47.15
Pandora
BERT
27.40
5.03
8.49
5.65
RoBERTa
30.49
10.65
9.63
9.98
Llama 3.1
37.88
45.30
24.99
30.40
Llama 3.1 (LoRA)
39.51
46.38
28.71
32.88
PersLLM
42.67
60.62
31.18
37.25
PersLLM (GCN)
43.72
55.52
31.73
36.86
PersLLM (LSTM)
45.59
69.67
30.49
38.75
16-class Evaluation: We also developed a new 16-class
evaluation framework designed to classify the 16 personality
types of the MBTI. Unlike trait-based theories, such as the Big
Five personality model [42] that describes personality through
continuous trait dimensions, MBTI is grounded in a discrete
typology theory. Due to MBTI’s emphasis on type distinctions
in personality assessment, a specialized classification approach
is required to address the challenges in practical applications
and optimize costs.
As shown in Table III, we selected BERT and RoBERTa
Fig. 2.
The paired bar charts in a logarithmic scale for the computational resource usage comparison between our PersLLM, Llama 3.1 and its LoRA
fine-tuning on the Kaggle dataset.
as our baselines, as they are widely used language models
known for their strong performance on various classification
tasks. However, both models performed poorly in the 16-
class classification scenario, even achieving near single-digit
scores on some metrics for the Pandora dataset. This is
primarily due to their lack of task-specific tuning for the 16-
class scenario. However, such adjustments often come at a
high cost, especially for language models that require full-
parameter updates or rely heavily on large-scale labeled data.
We compared PersLLM with Llama 3.1 fine-tuned using Low-
Rank Adaptation (LoRA) [43]. Although LoRA is also an effi-
cient fine-tuning method, its performance is significantly worse
across all metrics compared to the various configurations of
PersLLM. In contrast, PersLLM can achieve better results
by swapping the output network according to the scenario’s
requirements. For instance, using LSTM on large and complex
datasets helps learn more detailed features; or in more sensitive
applications, such as medical diagnostics [44], using GCN can
improve the detection of positive samples. Either replacing
LSTM or GCN yields better results than the initial PersLLM,
making it more suited for the 16-class classification task. This
highlights the flexibility of the PersLLM framework.
Computational Costs Comparison: As shown in Fig. 2,
we use paired bar charts to compare the performance of
Llama 3.1, LoRA fine-tuning, and our PersLLM, incorporating
different output networks such as CNN and GCN, across
multiple computational dimensions on the Kaggle dataset,
including FLOPs, training time, inference time, model pa-
rameters, GPU memory usage, and throughput. Due to the
large scale of model parameters, we used a logarithmic scale
for resource values. PersLLM demonstrates significant ad-
vantages in computational efficiency compared to Llama 3.1
and LoRA fine-tuning, with a notable reduction in FLOPs
Fig. 3. The line chart reflects the correlation between the performance of the
output neural network and the overall performance of PersLLM. The test was
conducted on the Pandora dataset.
and GPU memory usage. Additionally, PersLLM requires less
training time while maintaining competitive inference times.
Interestingly, despite its more compact architecture, PersLLM
achieves higher throughput (samples per second), highlighting
its excellent ability to balance computational efficiency and
performance, effectively addressing the challenges of fine-
tuning large models.
As previously mentioned, a key advantage of the PersLLM
framework is its ability to assess overall model performance
through the output network, thereby improving efficiency
and reducing computational costs. As shown in Fig. 3, the
independent performance of output layers such as GRU, RNN,
and CNN on the Pandora dataset is almost identical to their
performance when integrated into the PersLLM framework
as output networks. This consistency indicates that output
networks can reliably serve as proxies to predict PersLLM’s
final results, allowing us to make stable performance estimates
Fig. 4. The heatmap for ablation study. It illustrates the performance of various key components of PersLLM on the Kaggle (left) and Pandora (right) datasets.
Performance improves as the colour deepens.
without running the full model. Therefore, in optimization
scenarios, we can first experiment with the output network to
predict PersLLM’s outcomes, ensuring that we can accurately
identify necessary adjustments before performing expensive
and time-consuming full-scale model tuning.
C. Ablation Study
In the ablation study, we observed changes in the overall
performance of PersLLM by replacing its key components.
The results of the ablation analysis are visualized using a
heatmap for a more intuitive representation, as shown in Fig. 4.
The heatmap shows that both PersLLM and its variants with
replaced components exhibit a significant performance drop on
the Pandora dataset compared to the Kaggle dataset (indicated
by lighter colours). Similarly, the performance of output mod-
ules with different layer configurations declines across all di-
mensions. This drop may be attributed to inherent differences
between the two datasets, such as greater variability or more
complex textual patterns in the Pandora dataset. The heatmap
highlights the challenge of extending models optimized for
one dataset to another without sacrificing performance. Due to
the flexibility of PersLLM’s PEFT, as previously mentioned,
it can quickly adapt to the Pandora dataset by replacing the
output layer with other neural networks, such as GRU, without
needing to retrain the entire model.
Next, we removed the output layer and used fine-tuned
Llama 3.1 with a simple linear layer for direct classification.
The average F1 score drops by 11.41% on Kaggle and 9.59%
on Pandora. The standalone GRU exhibits a more significant
decline. Among the four specific dimensions, the largest
decline is observed in the S/N dimension. This demonstrates
that PEFT significantly enhances the model’s adaptability and
its ability to capture task-specific nuances.
To further investigate the impact of different backbone
models, we replaced Llama with BERT and GPT-4. BERT
performed significantly worse than Llama on both datasets.
In contrast, GPT-4 outperformed Llama in most dimensions,
demonstrating its strong generalization capability on MBTI
personality data.
We also experimented with the output network of varying
depths. Our observations show that stacking up to three hidden
layers in the GRU maximizes the overall performance of
PersLLM. However, adding a fourth layer leads to overfitting,
causing a performance drop, highlighting the trade-off between
model depth and generalization. Interestingly, the four-layer
and three-layer models perform better on different dimensions,
suggesting that neural networks of varying depths enable the
framework to learn more subtle or specific features on certain
dimensions. This points to a clear direction for optimizing
model performance: rather than solely increasing depth, se-
lectively adjusting the architecture for specific dimensions
can better meet particular needs. This ability to rapidly tune
parameters is a key advantage of our PersLLM architecture.
This flexibility, combined with targeted fine-tuning, provides
a more efficient approach to improving model performance
without incurring the computational cost of retraining the
entire model.
V. CONCLUSIONS
We proposed PersLLM, a novel PEFT framework that lever-
ages LLMs for personality detection tasks. The LLM extracts
rich representations, which are stored in a dynamic memory
layer, avoiding the need for repeated feature computation.
The subsequent training and prediction processes focus on a
low-parameter output network, which can be replaced based
on task requirements. This approach allows for flexible and
lightweight fine-tuning of the LLM, significantly reducing
computational costs. Experimental results demonstrate that
PersLLM outperforms existing state-of-the-art methods by a
large margin, proving its effectiveness and efficiency. Future
work will incorporate multimodal data, such as images and
audio, to enhance the framework’s scalability. Additionally, we
will develop adaptive fine-tuning techniques that dynamically
adjust the training effort based on task requirements, further
improving the framework’s efficiency.
REFERENCES
[1] D. Cervone and L. A. Pervin, Personality: Theory and research.
John
Wiley & Sons, 2022.
[2] T. Teng, H. Li, Y. Fang, and L. Shen, “Understanding the differen-
tial effectiveness of marketer versus user-generated advertisements in
closed social networking sites: An empirical study of wechat,” Internet
Research, vol. 32, no. 6, pp. 1910–1929, 2022.
[3] E. J. Choong and K. D. Varathan, “Predicting judging-perceiving of
myers-briggs type indicator (mbti) in online social forum,” PeerJ, vol. 9,
p. e11382, 2021.
[4] G. Jenifa, P. Sevanthi, K. Karthika, V. S. Pandi, D. Arumugam et al., “An
effective personality recognition model design using generative artificial
intelligence based learning principles,” in 2024 International Conference
on Computing and Data Science (ICCDS).
IEEE, 2024, pp. 1–6.
[5] N. K. Iyortsuun, S.-H. Kim, M. Jhon, H.-J. Yang, and S. Pant, “A review
of machine learning and deep learning approaches on mental health
diagnosis,” in Healthcare, vol. 11, no. 3.
MDPI, 2023, p. 285.
[6] M. Nirmala, B. Rajalakshmi, S. M. Sarvepally, S. Mandava, and
M. Megana, “Personality detection for recruitment using machine
learning,” in International Conference on Communications and Cyber
Physical Engineering 2018.
Springer, 2024, pp. 399–406.
[7] Q. Zhao, Y. Xia, Y. Long, G. Xu, and J. Wang, “Leveraging sensory
knowledge into text-to-text transfer transformer for enhanced emotion
analysis,” Information Processing & Management, vol. 62, no. 1, p.
103876, 2025.
[8] L. Shen, Y. Long, X. Cai, I. Razzak, G. Chen, K. Liu, and S. Jameel,
“Gamed: Knowledge adaptive multi-experts decoupling for multimodal
fake news detection,” in Proceedings of the Eighteenth ACM Interna-
tional Conference on Web Search and Data Mining, 2025, pp. 586–595.
[9] J. Killian Jr and R. Sun, “Detecting big-5 personality dimensions from
text based on large language models,” in International Conference on
Deep Learning Theory and Applications.
Springer, 2024, pp. 264–278.
[10] Q. Lu, X. Sun, Y. Long, Z. Gao, J. Feng, and T. Sun, “Sentiment anal-
ysis: Comprehensive reviews, recent advances, and open challenges,”
IEEE Transactions on Neural Networks and Learning Systems, 2023.
[11] Q. Lu, X. Sun, Y. Long, X. Zhao, W. Zou, J. Feng, and X. Wang,
“Multimodal dual perception fusion framework for multimodal affective
analysis,” Information Fusion, vol. 115, p. 102747, 2025.
[12] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Am-
atriain, and J. Gao, “Large language models: A survey,” arXiv preprint
arXiv:2402.06196, 2024.
[13] Y. Zhu, X. Shen, and R. Xia, “Personality-aware human-centric mul-
timodal reasoning: A new task, dataset and baselines,” arXiv preprint
arXiv:2304.02313, 2023.
[14] H. Li, C. Zhu, Y. Zhang, Y. Sun, Z. Shui, W. Kuang, S. Zheng, and
L. Yang, “Task-specific fine-tuning via variational information bottle-
neck for weakly-supervised pathology whole slide image classification,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023, pp. 7454–7463.
[15] T. Hui, Z. Zhang, S. Wang, W. Xu, Y. Sun, and H. Wu, “Hft: Half fine-
tuning for large language models,” arXiv preprint arXiv:2404.18466,
2024.
[16] Y. Wu, Y. Xiang, S. Huo, Y. Gong, and P. Liang, “Lora-sp: streamlined
partial parameter adaptation for resource efficient fine-tuning of large
language models,” in Third International Conference on Algorithms,
Microchips, and Network Applications (AMNA 2024), vol. 13171. SPIE,
2024, pp. 488–496.
[17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, “Towards
a unified view of parameter-efficient transfer learning,” arXiv preprint
arXiv:2110.04366, 2021.
[18] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts
for generation,” arXiv preprint arXiv:2101.00190, 2021.
[19] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan,
and S.-N. Lim, “Visual prompt tuning,” in European Conference on
Computer Vision.
Springer, 2022, pp. 709–727.
[20] C. Poth, H. Sterz, I. Paul, S. Purkayastha, L. Engl¨ander, T. Imhof,
I. Vuli´c, S. Ruder, I. Gurevych, and J. Pfeiffer, “Adapters: A unified
library for parameter-efficient and modular transfer learning,” arXiv
preprint arXiv:2311.11077, 2023.
[21] T. N. Sainath, Y. He, A. Narayanan, R. Botros, R. Pang, D. Rybach,
C. Allauzen, E. Variani, J. Qin, Q.-N. Le-The et al., “An efficient
streaming non-recurrent on-device end-to-end model with improvements
to rare-word modeling.” in Interspeech, vol. 8, 2021, pp. 1777–1781.
[22] S. Dave, R. Baghdadi, T. Nowatzki, S. Avancha, A. Shrivastava, and
B. Li, “Hardware acceleration of sparse and irregular tensor computa-
tions of ml models: A survey and insights,” Proceedings of the IEEE,
vol. 109, no. 10, pp. 1706–1752, 2021.
[23] R. Krishna, R. Teja, N. Neelima, and N. Peddi, “Advanced machine
learning models for depression level categorization using dsm 5 and
personality traits,” Procedia Computer Science, 2024.
[24] S. A. Salasa and W. Maharani, “Personality detection of twitter social
media users using the support vector machine method,” Jurnal Sistem
Komputer dan Informatika (JSON), 2022.
[25] J. Asghar, S. Akbar, M. Z. Asghar, B. Ahmad, M. S. Al-Rakhami, and
A. H. Gumaei, “Detection and classification of psychopathic personality
trait from social media text using deep learning model,” Comput. Math.
Methods Medicine, vol. 2021, pp. 5 512 241:1–5 512 241:10, 2021.
[26] T. Yang, J. Deng, X. Quan, and Q. Wang, “Orders are unwanted:
dynamic deep graph convolutional network for personality detection,” in
Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37,
no. 11, 2023, pp. 13 896–13 904.
[27] Y. Zhu, Y. Xia, M. Li, T. Zhang, and B. Wu, “Data augmented graph
neural networks for personality detection,” in AAAI Conference on
Artificial Intelligence, 2024.
[28] A. Alsuhaibani, H. Zogan, I. Razzak, S. Jameel, and G. Xu, “Idofew:
intermediate training using dual-clustering in language models for few
labels text classification,” in Proceedings of the 17th ACM International
Conference on Web Search and Data Mining, 2024, pp. 18–27.
[29] D. Jain, A. Kumar, and R. Beniwal, “Personality bert: A transformer-
based model for personality detection from textual data,” in Proceedings
of international conference on computing and communication networks:
ICCCN 2021.
Springer, 2022, pp. 515–522.
[30] Z. Ren, Q. Shen, X. Diao, and H. Xu, “A sentiment-aware deep learning
approach for personality detection from text,” Inf. Process. Manag.,
vol. 58, p. 102532, 2021.
[31] R. Wang and K. Sun, “Continuous output personality detection models
via mixed strategy training,” ArXiv, vol. abs/2406.16223, 2024.
[32] T. Yang, T. Shi, F. Wan, X. Quan, Q. Wang, B. Wu, and J. Wu, “Psycot:
Psychological questionnaire as powerful chain-of-thought for personality
detection,” arXiv preprint arXiv:2310.20256, 2023.
[33] L. Hu, H. He, D. Wang, Z. Zhao, Y. Shao, and L. Nie, “Llm vs
small model? large language model based text augmentation enhanced
personality detection model,” in Proceedings of the AAAI Conference
on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 18 234–18 242.
[34] L. Shen, Y. Long, X. Cai, G. Chen, Y. Wang, I. Razzak, and S. Jameel,
“Ll4g: Self-supervised dynamic optimization for graph-based personality
detection,” arXiv preprint arXiv:2504.02146, 2025.
[35] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,
A. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of
models,” arXiv preprint arXiv:2407.21783, 2024.
[36] S. S. Keh, I. Cheng et al., “Myers-briggs personality classification
and personality-specific language generation using pre-trained language
models,” arXiv preprint arXiv:1907.06333, 2019.
[37] Y. Liu, “Roberta: A robustly optimized bert pretraining approach,” arXiv
preprint arXiv:1907.11692, 2019.
[38] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,
D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4
technical report,” arXiv preprint arXiv:2303.08774, 2023.
[39] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng,
C. Zhang, C. Ruan et al., “Deepseek-v3 technical report,” arXiv preprint
arXiv:2412.19437, 2024.
[40] F. Yang, X. Quan, Y. Yang, and J. Yu, “Multi-document transformer
for personality detection,” in Proceedings of the AAAI conference on
artificial intelligence, vol. 35, no. 16, 2021, pp. 14 221–14 229.
[41] F. Yang, T. Yang, X. Quan, and Q. Su, “Learning to answer psy-
chological questionnaire for personality detection,” in Findings of the
Association for Computational Linguistics: EMNLP 2021, 2021, pp.
1131–1142.
[42] B. De Raad, The big five personality factors: the psycholexical approach
to personality.
Hogrefe & Huber Publishers, 2000.
[43] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, “Lora: Low-rank adaptation of large language models,”
arXiv preprint arXiv:2106.09685, 2021.
[44] J. G. D. Ochoa and F. E. Mustafa, “Graph neural network modelling as
a potentially effective method for predicting and analyzing procedures
based on patients’ diagnoses,” Artificial Intelligence in Medicine, vol.
131, p. 102359, 2022.
