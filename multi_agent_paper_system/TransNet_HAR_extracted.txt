TransNet: A Transfer Learning-Based Network for
Human Action Recognition
Khaled Alomar
School of Electronics and Computer Science
University of Southampton, SO17 1BJ, Southampton, UK
Email: k.a.alomar@soton.ac.uk
Xiaohao Cai
School of Electronics and Computer Science
University of Southampton, SO17 1BJ, Southampton, UK
Email: x.cai@soton.ac.uk
Abstract—Human action recognition (HAR) is a high-level and
significant research area in computer vision due to its ubiquitous
applications. The main limitations of the current HAR models are
their complex structures and lengthy training time. In this paper,
we propose a simple yet versatile and effective end-to-end deep
learning architecture, coined as TransNet, for HAR. TransNet
decomposes the complex 3D-CNNs into 2D- and 1D-CNNs, where
the 2D- and 1D-CNN components extract spatial features and
temporal patterns in videos, respectively. Benefiting from its
concise architecture, TransNet is ideally compatible with any
pretrained state-of-the-art 2D-CNN models in other fields, being
transferred to serve the HAR task. In other words, it naturally
leverages the power and success of transfer learning for HAR,
bringing huge advantages in terms of efficiency and effectiveness.
Extensive experimental results and the comparison with the
state-of-the-art models demonstrate the superior performance of
the proposed TransNet in HAR in terms of flexibility, model
complexity, training speed and classification accuracy.
I. INTRODUCTION
The computer vision community has studied video analysis
for decades, including action recognition [1] and activity
understanding [2]. Human action recognition (HAR) analyses
and detects actions from unknown video sequences. Due to the
rising demand for automated behaviour interpretation, HAR
has gained dramatic attention from academics and industry
and is crucial for many applications [3].
Good action recognition requires extracting spatial features
from the sequenced frames (images) of a video and then
establishing the temporal correlation (i.e., temporal features)
between these spatial features. Thus, action recognition models
analyse two types of features, establish their relationship, and
classify complex patterns. This makes these models vulnerable
to a number of significant challenges, including i) the limited
ability to transfer learning exploiting advanced models from
other fields in computer vision, ii) the need for large volumes
of data due to the model complexity, iii) the need for accurate
temporal analysis of spatial features, and iv) the overlap of
moving object data with cluttered background data [4].
The improvement process across generations of these mod-
els is inconsistent [5]. This results in a wide range of works
that may face difficulty of transferring learning ability between
generations, especially when these models are constructed
differently and/or developed in different fields for extracting
specific spatial features in HAR.
Temporal modeling presents a big challenge in action recog-
nition. To address this, researchers often employ 3D-CNN
models, which excel at interpreting spatio-temporal charac-
teristics but suffer from much larger size compared to 2D-
CNN models [6]. Moreover, optimising 3D networks becomes
difficult when dealing with insufficient data [7], since training
a 3D convolutional filter necessitates a substantial dataset
encompassing diverse video content and action categories
[8]. Unlike recurrent neural networks (RNNs) that emphasise
temporal patterns [9], 3D networks analyse videos as 3D
images, potentially compromising the sequential analysis of
temporal data. Both 3D-CNNs and RNNs are challenged by
the increased model size and lengthy training time [10].
The presence of cluttered backgrounds presents another
challenge in HAR. Indoor environments with static and
constant backgrounds are typically assumed to yield high
performance for HAR models, whereas performance could
significantly diminish in outdoor contexts [11], [12]. Cluttered
backgrounds introduce interruptions and background noise,
encoding problematic information in the extraction of global
features and leading to a notable decline in performance. To
address this challenge, a practical approach is to design models
that focus on the human object rather than the background.
Scholarly literature consistently indicates that incorporating
multiple input modalities, including optical flow and body part
segmentation, shows promise in enhancing HAR performance.
This conclusion is substantiated by a range of survey studies
conducted in the field of action recognition, providing robust
evidence for the effectiveness of leveraging diverse input
modalities [13], [14], [15].
However, there are several issues with these types of models,
including their various modelling steps, preprocessing stages,
lengthy training time, and significant demands on resources
such as memory and processing power. These models are also
difficult to implement in real-world applications.
In this paper, we propose an end-to-end deep learning
architecture called TransNet for HAR, see Figure 1. Rather
than using complex 3D-CNNs, TransNet consists of 2D- and
1D-CNNs that extract spatial features and temporal patterns
in videos, respectively. TransNet offers the following multiple
benefits: i) a single network stream using only RGB frames;
ii) transfer learning ability and flexibility because of its com-
patibility with any pretrained state-of-the-art 2D-CNN models
arXiv:2309.06951v1  [cs.CV]  13 Sep 2023
Video frames
Time-distributed
layer
Extracted
spatial features
1D-CNN
layer
Spatio-temporal
features
1D-CNN
layer
Spatio-temporal
features
SoftMax
layer
Action
prediction
1D Component
2D Component
z1
z2
...
zn−2
zn−1
zn
h1
h2
...
hn−2
hn−1
v
Fig. 1. TransNet architecture for HAR. The given video frames are input into the time-distributed layer, which employs a 2D-CNN model (e.g., MobileNet,
MobileNetV2, VGG16, or VGG19) several times based on the number of video frames, allowing the architecture to analyse multiple frames without expanding
in size. Then the spatial features corresponding to the individual input frames are generated, which are subsequently analysed by the 1D-CNN layers, extracting
the spatio-temporal features. The SoftMax layer finally classifies the action according to the spatio-temporal pattern.
Fig. 2.
An illustration of TransNet+ for HAR. TransNet+ inherits the
architecture of TransNet. It uses the autoencoder’s encoder to form the
TransNet’s 2D component.
for spatial feature extraction; iii) a customisable and simpler
architecture compared to existing 3D-CNN and RNN models;
and iv) fast learning speed and state-of-the-art performance in
HAR. These benefits allow TransNet to leverage the power
and success of transfer learning for HAR, bringing huge
advantages in terms of efficiency and effectiveness.
An additional contribution of this paper is that we in-
troduce the strategy of utilising autoencoders to form the
TransNet’s 2D component, i.e., named TransNet+, see Fig-
ure 2. TransNet+ employs the encoder part of the autoen-
coder trained on computer vision tasks like human semantic
segmentation (HSS) to conduct HAR. Extensive experimental
results and the comparison with the state-of-the-art mod-
els demonstrate the superior performance of the proposed
TransNet/TransNet+ in HAR.
II. RELATED WORK
A. HAR with background subtraction
Most research on HAR focuses on human detection and mo-
tion tracking [16]. Background subtraction has been suggested
in a number of methods and proven to be viable for HAR. For
example, a background updating model based on a dynamic
optimisation threshold method was developed in [17] to detect
more complete features of the moving object. The work in [18]
introduced a basic framework for detecting and recognising
moving objects in outdoor CCTV video data using background
subtraction and CNNs. Jaouedi et al. [16] employed a Gaussian
mixture model and Kalman filter [19] techniques to detect
human motion by subtracting the background.
B. HAR with multimodality
Since video comprehension requires motion information,
researchers have integrated several input modalities in addition
to RGB frames to capture the correlation between frames in
an effort to enhance model performance.
Optical flow. Optical flow [20], which effectively describes
object or scene motion flow, is one of the earliest attempts to
capture temporal patterns in videos. In comparison to RGB
images, optical flow may successfully remove the static back-
ground from scenes, resulting in a simpler learning problem
than using RGB images as input [21], [22]. Simonyan et al.
[23] began the trend of using multiple input modalities, includ-
ing optical flow, with CNNs. However, when compared to the
latest deep learning techniques, optical flow has a number of
disadvantages, including being computationally complex and
highly noise-sensitive [24], [25], which make its use in real-
time applications less feasible.
Semantic segmentation. Semantic segmentation is a tech-
nique that may be used to separate either the entire body
or particular body parts from the background [26]. It is a
pixel-wise labelling of a 2D image, offering spatial features
describing the shape of the object of interest [27]. Zolfaghari
et al. [28] presented a chained multi-stream model that pre-
computes and integrates appearance, optical flow, and human
body part segmentation to achieve better action recognition
and localisation. Benitez et al. [29] offered an alternative to the
costly optical flow estimates used in multimodal hand gesture
Fig. 3. Data samples. First row: samples of UCF101 actions (left) and HMDB51 actions (right); second row: samples of the supervisely person dataset (left)
and a frame sequence of the action class “walking” from the KTH dataset (right).
recognition methods. It was built using RGB frames and hand
segmentation masks, with better results achieved.
Although semantic segmentation approaches have shown
promising outcomes in action recognition, the majority of
them are computationally demanding. In fact, real-world action
recognition methods involving semantic segmentation of video
content are still in their infancy phase [30].
In sum, most of the aforementioned research focused on
creating synthetic images that reflect different input modalities
and then analysing them using action recognition models. Pre-
computing multiple input modalities such as optical flow, body
part segmentation, and semantic segmentation can be compu-
tationally and storage-intensive, making them unsuitable for
large-scale training and real-time deployment. Since research
in the subject of semantic segmentation may still be in its
early stage, one of the objectives of this study is to enhance
its potential in HAR.
C. 3D-CNNs decomposition
Video can be conceptually simplified by viewing it as a 3D
tensor with two spatial dimensions and one time dimension.
As a result, 3D-CNNs are adopted to model the spatial and
temporal data in video as a processing unit [31], [32], [33].
Ji et al. [32] proposed the pioneer work in the application
of 3D-CNNs in action recognition. Although the model’s
performance is encouraging, the network’s depth is insufficient
to demonstrate its potential. Tran et al. [1] extended the work
in [32] to a 3D network with more depth, called C3D. C3D
adopts the modular architecture, which can be viewed as a 3D
version of the VGG16 network.
It is worth noting that training a sufficiently deep 3D-CNN
from scratch will result in much higher computational cost
and memory requirements compared to 2D-CNNs. Further-
more, 3D networks are complex and difficult to optimise [7];
therefore, a big dataset with diverse video data and activity
categories is required to train a 3D-CNN effectively. In addi-
tion, it is not straightforward for 3D-CNNs to transfer learning
from state-of-the-art pretrained 2D-CNN models since kernel
shapes are completely different. Carreira et al. [34] proposed
I3D, a 3D-CNN architecture that circumvents the dilemma
that 3D-CNNs must be trained from scratch. A strategy was
employed to transform the weights of pretrained 2D models,
e.g. on ImageNet, to their 3D counterparts. To understand this
intuitively, they repeated the weights of the trained 2D kernels
along the time dimension of the 3D kernels. Although I3D
was successful in overcoming the challenge of spatial transfer-
learning, its 3D kernels require enormous quantities of action
recognition datasets to capture temporal features. Moreover,
the way that I3D stretches 2D-CNN models into 3D-CNNs
remains computationally expensive.
P3D [35] and R2+1D [36] investigate the concept of decom-
posing the 3D CNN’s kernels into 2D and 1D kernels. They
differ in their arrangement of the two factorised operations
and their formulation of each residual block. This kind of
approach to 3D network decomposition acts at the kernel
level. The notion of kernel-level factorisation restricts the
ability to switch models (e.g., ResNet50 and VGG16) based
on implementation requirements and hinders transfer learning
from the current state-of-the-art models.
III. PROPOSED TRANSNET
In this section, we first present our motivations and then
introduce the proposed TransNet and its variants.
A. Preliminary
Video data analysis in deep learning commonly involves two
types of approaches: 2D-CNN-RNN [37], [38], [39], [40] and
3D-CNN [41], [42], [43]. The CNN-RNN approach comprises
a spatial component based on 2D-CNN and a temporal com-
ponent based on RNN, offering customisation in the 2D-CNN
part. However, it often requires longer training time due to
the complexity of RNN compared to CNN [44]. On the other
hand, 3D-CNN is faster and simpler to implement but struggles
with convergence and generalisation when dealing with limited
datasets [45]. Alternatively, the implementation of 1D-CNN
in temporal data analysis holds promise for developing more
accurate and efficient models [46], [47].
The other main motivation is transfer learning, applying
well-designed and well-trained models learnt from one task
(i.e., the source task, generally with large data available)
to another (i.e., the target task, generally with limited data
available) for performance enhancement [48]. The underlying
essential assumption is that the source and target tasks are
sufficiently similar [48], [49]. In the data scarcity scenario,
models may be prone to overfitting, and data augmentation
may not be enough to resolve the issue [50]. Therefore,
transfer learning could play a key role in this regard.
Since HAR requires 3D data analysis, obtaining an op-
timised model requires training on a large amount of data
compared to 2D data [51], [8]. This calls for the use of
transfer learning, e.g., pre-training state-of-the-art models first
to classify 2D images using large datasets such as ImageNet.
However, it is important to study and verify the assumption
that the HAR task shares sufficient similarities with the image
classification task. Previous research in [52] has shown dispar-
ities between CNNs trained on ImageNet and human observers
in terms of shape and texture cues, with CNNs exhibiting a
strong preference for texture over shape. Additionally, several
studies suggest that object shape representations hold greater
importance in action recognition tasks [53], [54], [55], [56].
B. Methodology
TransNet. We propose to construct a paradigm of utilising
the synergy of 2D- and 1D-CNNs, see Figure 1 for the end-to-
end TransNet architecture. TransNet provides flexibility to the
2D-CNN component in terms of model customisability (i.e.,
using different state-of-the-art 2D-CNN models) and transfer-
ability (i.e., involving transfer learning); moreover, it benefits
from the 1D-CNN component supporting the development of
faster and less complex action recognition models.
TransNet includes the time-distributed layer wrapping the
2D-CNN model. In particular, the 2D component is customis-
able, and any sought-after 2D-CNN models (e.g., MobileNet,
MobileNetV2, VGG16 or VGG19) can be utilised. The time-
distributed layer is followed by three 1D convolutional layers
for spatio-temporal analysis. In detail, the first one’s kernels
process the feature map vectors over (n −1) steps, where
each kernel has a size of 2, capturing the correlations between
a frame and its neighbour, and n is the number of frames in
a video clip; the second one’s kernels have a size of (n −1),
analysing all feature vectors in one step to capture the whole
temporal pattern of the frame-sequence; and the third one uses
the SoftMax function for the final classification, followed by
the flatten layer. More details are given below.
We first define the symbols used for semantic segmentation.
Let X represent the input image, and z = pθ(X) ∈RL be
the output vector (i.e., latent representation) of the encoder
function pθ (e.g. MobileNet or VGG16) with parameters θ.
The decoder function is defined analogously. The formed
autoencoder can then be trained with the ground truth images.
Let X be a collection of n frames X = {Xi}n
i=1, which
is fed into the 2D component (spatial component) of the
TransNet architecture in Figure 1. The trained encoder pθ
is then used n times to process X frame by frame and
create a set of n spatial feature vectors Z = {zi}n
i=1, where
zi = pθ(Xi). Let {wj,1, wj,2}K
j=1 be a set of weights, where
wj,1, wj,2 ∈RL. The first of the three 1D layers (i.e., the
temporal component) processes every two adjacent spatial
vectors of Z, i.e., {zi, zi+1}, to generate the corresponding
spatio-temporal feature vectors hi = (hi
1, · · · , hi
K) ∈RK, i =
1, . . . , n −1, where
hi
j = f(
L
X
l=1
i+1
X
k=i
zk
l wj,k−i+1
l
+ bj
i),
j = 1, . . . , K,
bj
i
are
the
biases
and
f
is
the
activation
function
(i.e.,
Relu
f(x)
=
max(0, x)
is
used
here).
Let
{ ˆwj,1, ˆwj,2, · · · , ˆwj,n−1}C
j=1 be another set of weights, with
ˆwj,k ∈RK, k = 1, . . . , n −1. The second 1D layer processes
the set of spatio-temporal vectors {hi}n−1
i=1 to generate a single
spatio-temporal vector v = (v1, · · · , vC) ∈RC, where
vj = f(
K
X
l=1
n−1
X
k=1
hk
l ˆwj,k
l
+ ˆbj),
j = 1, . . . , C,
and ˆbj are the biases. Finally, the Softmax layer is used on v
to classify action classes.
TransNet+. Except for using a sought-after 2D-CNN for
TransNet’s 2D component, below we present a way of lever-
aging transfer learning for it. To do so, we construct an
autoencoder where TransNet’s 2D component serves as its en-
coder. The autoencoder is then trained on a specific computer
vision task such as HSS to extract specific features such as
human shape, e.g., see the left of the second row in Figure 3.
After training, the encoder’s parameters become saturated with
weights that are capable of describing the features of the
required task, such as HAR, see Figure 2. In this way, the
features like object shape that TransNet’s 2D component needs
to learn can be predetermined by training the autoencoder. We
name this way of executing TransNet as TransNet+.
Note that autoencoders have been used in action recognition
challenges e.g. [28]. However, there are a number of disadvan-
tages in their use of autoencoders, including the multiplicity of
modelling steps, the need for a large amount of memory, and
the lengthy training time due to the high computational cost
of training the autoencoder network and action recognition
network.
In contrast, TransNet+ is a huge step further in contributing
to the development of an end-to-end HAR model with potential
in real-time implementation, since it simplifies the process
by just integrating the trained encoder rather than the entire
autoencoder in TransNet, with the premise that the trained en-
coder carries weights capable of describing important features
(see Figure 2).
On the whole, the traditional method of using autoencoders
in HAR differs from TransNet+ in that the traditional method
uses the entire autoencoder and its output as the next stage’s
input, whereas TransNet+ just employs the trained encoder of
the autoencoder for spatial feature extraction.
Model complexity. The proposed TransNet model is cus-
tomisable, and thus its size varies depending on the 2D-CNN
model being used in the spatial component. In particular, it
is quite cost-effective since it uses a time-distributed layer,
allowing the 2D-CNN to be used repeatedly without expanding
in size. Table I gives the number of parameters regarding
different choices of the 2D-CNN models.
IV. DATA
In our study, we use two primary groups of benchmark
datasets. The first consists of ImageNet and the supervisely
person dataset used for transfer learning, while the second
TABLE I
TRANSNET’S MODEL COMPLEXITY. THE LAST COLUMN GIVES THE TOTAL
NUMBER OF PARAMETERS FOR EACH SETTING. THE 2D COMPONENT
COLUMN REFLECTS THE MODEL SIZE OF THE TIME-DISTRIBUTED LAYER,
WHICH IS INVARIANT AGAINST THE NUMBER OF FRAMES IN THE INPUT
VIDEO CLIP. THE TWO VALUES IN THE 1D COMPONENT COLUMN SHOW
THE NUMBER OF KERNELS IN THE FIRST AND SECOND 1D-CNN LAYERS,
RESPECTIVELY. IN OUR EXPERIMENTS, WE EQUIPPED TRANSNET WITH
MOBILENETV1 AND 64 FILTERS.
2D model
2D component
1D component
Total
MobileNetV1
6,444,288
64; 6
6,449,416
9,655,616
128; 6
9,673,992
MobileNetV2
6,277,248
64; 6
6,282,376
10,291,392
128; 6
10,309,768
VGG16
16,322,432
64; 6
16,327,560
17,928,128
128; 6
17,946,504
VGG19
21,632,128
64; 6
21,637,256
23,237,824
128; 6
23,256,200
TABLE II
RESULTS OF TRANSNET WITH DIFFERENT BACKBONES AND DIFFERENT
PRETRAINING STRATEGIES ON THE KTH DATASET.
TransNet backbone
Pre-training
Acc.
MobileNet
None
94.35 ± 0.33
ImageNet
100.00 ± 0.00
HSS
100.00 ± 0.00
MobileNetV2
None
88.31 ± 0.54
ImageNet
95.86 ± 0.41
HSS
96.40 ± 0.34
VGG16
None
90.12 ± 0.38
ImageNet
96.25 ± 0.43
HSS
98.01 ± 0.48
VGG19
None
80.06 ± 0.72
ImageNet
88.26 ± 0.51
HSS
94.39 ± 0.26
consists of the KTH, HMDB51 and UCF101 datasets used
for method evaluation (with a split ratio of 80% and 20% for
training and test, respectively); see below Figure 3 for a brief
description and for some samples from these datasets.
A. Transfer leaning datasets
ImageNet. ImageNet [57] is a famous database consisting
of 14,197,122 images with 1000 categories. Since 2010, it has
been used in the ImageNet Large Scale Visual Recognition
Challenge (ILSVRC).
Supervisely person dataset. This dataset [58] is publicly
available for human semantic segmentation, containing 5,711
images and 6,884 high-quality annotated human instances. It
is available for use in academic research with the purpose of
training machines to segment human bodies.
B. Human action recognition datasets
KTH. In 2004, the Royal Institute of Technology introduced
KTH, a non-trivial and publicly available dataset for action
recognition [59]. It is one of the most standard datasets, includ-
ing six actions (i.e., walking, jogging, running, boxing, hand-
waving, and hand-clapping). Twenty-five different people did
each activity, allowing for variation in performance; moreover,
the setting was systematically changed for each actor’s action,
i.e., outdoors, outdoors with scale variation, outdoors with
varied clothing, and indoors. KTH includes 2,391 sequences.
All sequences were captured using a stationary camera with
25 fps over homogeneous backgrounds.
UCF101. In 2012, UCF101 [60] was introduced as a follow-
up to the earlier UCF50 dataset. It is a realistic (not staged
by actors) HAR dataset, containing 13,320 YouTube videos
representing 101 human actions. It provides a high level of
diversity in terms of object appearance, significant variations
in camera viewpoint, object scale, illumination conditions, a
cluttered background, etc. These video clips are, in total, over
27 hours in duration. All videos have a fixed frame rate of 25
fps at a resolution of 320 × 240.
HMDB51. HMDB51 [61] was released in 2011 as a realistic
HAR dataset. It was primarily gathered from movies, with a
small portion coming from public sources such as YouTube
and Google videos. It comprises 6,849 videos organised into
51 action categories, each with at least 101 videos.
V. EXPERIMENTAL RESULTS
A. Settings
Our model is built using Python 3.6 with the deep learning
library Keras, the image processing library OpenCV, mat-
plotlib, and the scikit-learn library. A computer with an Intel
Core i7 processor, an NVidia RTX 2070, and 64GB of RAM
is used for training and test.
Four CNN models with small sizes (i.e., MobileNet, Mo-
bileNetV2, VGG16, and VGG19) are selected as the back-
bones of TransNet/TransNet+, with parameter numbers of
3,228,864, 2,258,984, 14,714,688, and 20,024,388 (without the
classification layers), respectively.
TransNet with each different backbone is implemented
in three different ways: i) untrained; ii) being trained on
ImageNet; and iii) being trained on HSS using the supervisely
person datasetas as encoders. Note that the last way is de-
scribed in TransNet+. For ease of reference, we drop the ‘+’
sign in the following. The number of 200 epochs is used to
train all autoencoders, with a batch size of 24. The models are
first trained and evaluated on the KTH dataset. Then the one
with the best performance is selected to be evaluated on all the
datasets, and compared with the current state-of-the-art HAR
models. Each video clip consists of a sequence of 12 frames,
and the input modality is RGB with a size of 224 × 224 × 3.
B. Results and discussion
In a nutshell, we conduct experiments below with three
main objectives: i) determining whether or not the proposed
TransNet architecture can offer a reliable mechanism by
leveraging transfer learning; ii) evaluating if the HSS-trained
TransNet provides superior spatio-temporal characteristics for
HAR than the ImageNet-trained TransNet; and iii) validating
if the performance of the TransNet architecture can achieve
state-of-the-art performance in comparison to current state-of-
the-art methods in HAR.
Initially, we subject TransNet to an evaluation using the
KTH dataset, which serves as a suitable choice due to its
TABLE III
RESULTS COMPARISON BETWEEN TRANSNET AND THE
STATE-OF-THE-ART METHODS ON THE KTH DATASET.
Method
Venue
Acc.
Grushin et al. [62]
IJCNN ’13
90.70
Shu et al. [63]
IJCNN ’14
92.30
Geng et al. [64]
ICCSAE ’15
92.49
Veeriah et al. [65]
ICCV ’15
93.96
Zhang et al. [66]
ISMEMS ’17
95.00
Arunnehru et al. [67]
RoSMa ’18
94.90
Abdelbaky et al. [68]
ITCE ’20
87.52
Jaouedi et al. [16]
KSUCI journal ’20
96.30
Liu et al. [69]
JAIHC ’20
91.93
HAR-Depth [70]
TETCI ’20
97.67
Ramya et al. [71]
MTA journal ’21
91.40
Lee et al. [72]
CVF ’21
89.40
Basha et al. [73]
MTA journal ’22
96.53
Picco et al. [74]
NN journal ’23
90.83
Ye et al. [75]
CVF ’23
90.90
TransNet
-
100.00
TABLE IV
RESULTS COMPARISON BETWEEN TRANSNET AND THE
STATE-OF-THE-ART METHODS ON THE UCF101 DATASET.
Model
Pre-training
Backbone
Venue
Acc.
DeepVideo [76]
ImageNet
AlexNet
CVPR ’14
65.40
Two-stream [5]
ImageNet
CNN-M
NeurIPS ’14
88.00
LRCN [77]
ImageNet
CNN-M
CVPR ’15
82.30
C3D [1]
ImageNet
VGG16-like
ICCV ’15
82.30
Fusion [78]
ImageNet
VGG16
CVPR ’16
92.50
TSN [77]
ImageNet
BN-Inception
ECCV ’16
94.00
I3D [34]
ImageNet
BN-Inception-like
CVPR ’17
95.60
Kinetics400
P3D [77]
Sports1M
ResNet50-like
ICCV ’17
88.60
ResNet3D [77]
Kinetics400
ResNeXt101-like
CVPR ’18
94.50
HAR-Depth [70]
-
BiLSTM+AlexNet
TETCI ’20
92.97
MemDPC [77]
Kinetics400
R-2D3D
ECCV ’20
86.10
TEA [79]
Imagenet
ResNet50-like
CVPR ’20
96.90
CVRL [77]
Kinetics600
R3D-50
CVPR ’21
93.40
TDN [80]
Kinetics400
ResNet
CVF ’21
97.40
ImageNet
Multi-Domain [81]
-
MDL
IEICE ’22
94.82
MEST [82]
Imagenet
2D-CNN
Sensors ’22
96.80
STA-TSN [83]
Imagenet
ResNet-LSTM
PloS One ’22
92.80
FSAN [84]
Imagenet
2D-CNN
Sensors ’23
95.68
TransNet
ImageNet
MobileNet V1
-
98.32
primary emphasis on human action detection while excluding
the presence of additional objects in the background, in
contrast to the UCF101 and HMDB51 datasets. The purpose
of this evaluation is to validate the viability of employing HSS
as a means of pretraining to improve the performance of the
model in similar tasks.
The results presented in Table II demonstrate the supe-
rior performance of the TransNet model which was trained
using HSS in comparison to its untrained and ImageNet-
trained counterparts. Specifically, the untrained MobileNet,
MobileNetV2, VGG16, and VGG19-based TransNet models
achieved an average accuracy of 88.21%, and the ImageNet-
trained models achieved an average accuracy of 95.09%. In
contrast, the HSS-trained TransNet models achieved an aver-
age accuracy of 97.20%, indicating a significant improvement
of ∼8.99% and ∼2.11% over the untrained and ImageNet-
trained models, respectively. These findings underscore the
TABLE V
RESULTS COMPARISON BETWEEN TRANSNET AND THE
STATE-OF-THE-ART METHODS ON THE HMDB51 DATASET.
Model
Pre-training
Backbone
Venue
Acc.
C3D [1]
ImageNet
VGG16-like
ICCV ’15
56.80
CBT [77]
ImageNet
S3D
arXiv ’19
44.60
DPC [77]
Kinetics400
R-2D3D
ICCVW ’19
35.70
SpeedNet
Kinetics400
S3D-G
CVPR ’20
48.80
MemDPC [77]
Kinetics400
R-2D3D
ECCV ’20
54.50
CoCLR [77]
Kinetics400
S3D
NeurIPS ’20
54.60
HAR-Depth [70]
-
BiLSTM+AlexNet
TETCI ’20
69.74
STA-TSN [85]
Imagenet
ResNet50-like
ECCV ’20
77.40
TEA [79]
Imagenet
ResNet50-like
CVPR ’20
73.30
TDN [80]
Kinetics400
ResNet
CVF ’21
76.30
ImageNet
Multi-Domain [81]
-
MDL
IEICE ’22
71.57
MEST [82]
Imagenet
2D-CNN
Sensors ’22
73.40
STA-TSN [83]
Imagenet
ResNet-LSTM
PloS One ’22
68.60
FSAN [84]
Imagenet
2D-CNN
Sensors ’23
72.60
TransNet
ImageNet
MobileNet V1
-
97.93
effectiveness of the pretraining strategy employing autoen-
coders in enhancing the performance of the TransNet model.
Additionally, the findings show the significance of incorporat-
ing transfer learning as a means of enhancing performance,
thereby bestowing a substantial advantage to the 2D-1D-CNN
architecture and enabling us to leverage transfer learning
within the 2D-CNN component.
Tables III, IV and V present the quantitative comparisons
between TransNet and the current state-of-the-art methods
being applied to the HAR datasets, i.e., KTH, UCF101 and
HMDB51. In these comparisons, a MobileNet-based TransNet
pretrained on ImageNet is used. The findings demonstrate the
exceptional performance achieved by the proposed TransNet,
surpassing the existing state-of-the-art results by a consider-
able margin. Additionally, these findings solidify the 2D-1D-
CNN architecture as a highly effective approach for HAR.
VI. CONCLUSION
In this paper, we proposed TransNet, a versatile and highly
efficient end-to-end deep learning architecture designed specif-
ically for HAR. TransNet employs a concise architecture, com-
bining 2D- and 1D-CNNs to effectively capture spatial features
and temporal patterns in video data, respectively. It demon-
strates excellent compatibility and flexibility by seamlessly
integrating with various pretrained state-of-the-art 2D-CNN
models, thereby facilitating straightforward transfer learning
for HAR tasks and significantly improving performance in
terms of both efficiency and effectiveness. Through extensive
experiments conducted on diverse HAR benchmark datasets
utilising different backbones and pre-training strategies, the
proposed TransNet consistently outperforms state-of-the-art
HAR models. These experiments validated the superior per-
formance of TransNet in HAR, further establishing its efficacy
and robustness in the field. The application of transfer learning
using autoencoder in the context of HSS has emerged as a
promising approach, offering a potential avenue for directing
the model to learn task-specific features in accordance with
the requirements of the given task. Future work may focus
on detailed ablation study of the architecture of TransNet
and its combination with other advanced architectures like
transformers for HAR.
REFERENCES
[1] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
spatiotemporal features with 3d convolutional networks,” in Proceedings
of the IEEE international conference on computer vision, 2015, pp.
4489–4497.
[2] K. M. Kitani, B. D. Ziebart, J. A. Bagnell, and M. Hebert, “Activity
forecasting,” in European conference on computer vision.
Springer,
2012, pp. 201–214.
[3] S. N. Paul and Y. J. Singh, “Survey on video analysis of human walking
motion,” International Journal of Signal Processing, Image Processing
and Pattern Recognition, vol. 7, no. 3, pp. 99–122, 2014.
[4] I. Jegham, A. B. Khalifa, I. Alouani, and M. A. Mahjoub, “Vision-based
human action recognition: An overview and real world challenges,”
Forensic Science International: Digital Investigation, vol. 32, p. 200901,
2020.
[5] K. Simonyan and A. Zisserman, “Two-stream convolutional networks for
action recognition in videos,” Advances in neural information processing
systems, vol. 27, 2014.
[6] H. Yang, C. Yuan, B. Li, Y. Du, J. Xing, W. Hu, and S. J. Maybank,
“Asymmetric 3d convolutional neural networks for action recognition,”
Pattern recognition, vol. 85, pp. 1–12, 2019.
[7] Y. Kong, Y. Wang, and A. Li, “Spatiotemporal saliency representation
learning for video action recognition,” IEEE Transactions on Multime-
dia, vol. 24, pp. 1515–1528, 2021.
[8] Z.-p. Hu, R.-x. Zhang, Y. Qiu, M.-y. Zhao, and Z. Sun, “3d convo-
lutional networks with multi-layer-pooling selection fusion for video
classification,” Multimedia Tools and Applications, vol. 80, no. 24, pp.
33 179–33 192, 2021.
[9] S. Narang, E. Elsen, G. Diamos, and S. Sengupta, “Exploring sparsity
in recurrent neural networks,” arXiv preprint arXiv:1704.05119, 2017.
[10] A. Stamoulakatos, J. Cardona, C. Michie, I. Andonovic, P. Lazaridis,
X. Bellekens, R. Atkinson, M. M. Hossain, and C. Tachtatzis, “A
comparison of the performance of 2d and 3d convolutional neural
networks for subsea survey video classification,” in OCEANS 2021: San
Diego–Porto.
IEEE, 2021, pp. 1–10.
[11] A.-A. Liu, N. Xu, Y.-T. Su, H. Lin, T. Hao, and Z.-X. Yang,
“Single/multi-view human action recognition via regularized multi-task
learning,” Neurocomputing, vol. 151, pp. 544–553, 2015.
[12] S. Wu, O. Oreifej, and M. Shah, “Action recognition in videos acquired
by a moving camera using motion decomposition of lagrangian particle
trajectories,” in 2011 International conference on computer vision.
IEEE, 2011, pp. 1419–1426.
[13] D. R. Beddiar, B. Nini, M. Sabokrou, and A. Hadid, “Vision-based hu-
man activity recognition: a survey,” Multimedia Tools and Applications,
vol. 79, no. 41, pp. 30 509–30 555, 2020.
[14] Y. Kong and Y. Fu, “Human action recognition and prediction: A
survey,” International Journal of Computer Vision, vol. 130, no. 5, pp.
1366–1401, 2022.
[15] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, and J. Liu,
“Human action recognition from various data modalities: A review,”
IEEE transactions on pattern analysis and machine intelligence, 2022.
[16] N. Jaouedi, N. Boujnah, and M. S. Bouhlel, “A new hybrid deep learning
model for human action recognition,” Journal of King Saud University-
Computer and Information Sciences, vol. 32, no. 4, pp. 447–453, 2020.
[17] L. Zhang and Y. Liang, “Motion human detection based on background
subtraction,” in 2010 Second International workshop on Education
Technology and computer science, vol. 1.
IEEE, 2010, pp. 284–287.
[18] C. Kim, J. Lee, T. Han, and Y.-M. Kim, “A hybrid framework combining
background subtraction and deep neural networks for rapid person
detection,” Journal of Big Data, vol. 5, no. 1, pp. 1–24, 2018.
[19] G. Liu, X. Tang, J. Huang, J. Liu, and D. Sun, “Hierarchical model-based
human motion tracking via unscented kalman filter,” in 2007 IEEE 11th
International Conference on Computer Vision.
IEEE, 2007, pp. 1–8.
[20] B. K. Horn, “Bg schunck determining optical flow,” Artificial intelli-
gence, vol. 17, no. 1-3, pp. 185–203, 1981.
[21] S. Diamantas and K. Alexis, “Optical flow based background subtraction
with a moving camera: Application to autonomous driving,” in Interna-
tional Symposium on Visual Computing.
Springer, 2020, pp. 398–409.
[22] L. Wang, Y. Guo, Z. Lin, X. Deng, and W. An, “Learning for video
super-resolution through hr optical flow estimation,” in Asian Conference
on Computer Vision.
Springer, 2018, pp. 514–529.
[23] M. S. Ryoo, A. Piergiovanni, M. Tan, and A. Angelova, “Assemblenet:
Searching for multi-stream neural connectivity in video architectures,”
arXiv preprint arXiv:1905.13209, 2019.
[24] A. C. Bovik, The essential guide to video processing.
Academic Press,
2009.
[25] H. Li and J. Wang, “A neural network model for optical flow compu-
tation,” in Neural Networks and Pattern Recognition.
Elsevier, 1998,
pp. 57–76.
[26] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and
D. Terzopoulos, “Image segmentation using deep learning: A survey,”
IEEE transactions on pattern analysis and machine intelligence, 2021.
[27] I. Ulku and E. Akag¨und¨uz, “A survey on deep learning-based archi-
tectures for semantic segmentation on 2d images,” Applied Artificial
Intelligence, pp. 1–45, 2022.
[28] M. Zolfaghari, G. L. Oliveira, N. Sedaghat, and T. Brox, “Chained multi-
stream networks exploiting pose, motion, and appearance for action
classification and detection,” in Proceedings of the IEEE International
Conference on Computer Vision, 2017, pp. 2904–2913.
[29] G.
Benitez-Garcia,
L.
Prudente-Tixteco,
L.
C.
Castro-Madrid,
R. Toscano-Medina, J. Olivares-Mercado, G. Sanchez-Perez, and L. J. G.
Villalba, “Improving real-time hand gesture recognition with semantic
segmentation,” Sensors, vol. 21, no. 2, p. 356, 2021.
[30] H.-B. Zhang, Y.-X. Zhang, B. Zhong, Q. Lei, L. Yang, J.-X. Du, and
D.-S. Chen, “A comprehensive survey of vision-based human action
recognition methods,” Sensors, vol. 19, no. 5, p. 1005, 2019.
[31] G. Yao, T. Lei, and J. Zhong, “A review of convolutional-neural-network-
based action recognition,” Pattern Recognition Letters, vol. 118, pp. 14–
22, 2019.
[32] S. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural networks
for human action recognition,” IEEE transactions on pattern analysis
and machine intelligence, vol. 35, no. 1, pp. 221–231, 2012.
[33] M. Kalfaoglu, S. Kalkan, and A. A. Alatan, “Late temporal modeling
in 3d cnn architectures with bert for action recognition,” in European
Conference on Computer Vision.
Springer, 2020, pp. 731–747.
[34] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new
model and the kinetics dataset,” in proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2017, pp. 6299–6308.
[35] Z. Qiu, T. Yao, and T. Mei, “Learning spatio-temporal representation
with pseudo-3d residual networks,” in proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2017, pp. 5533–5541.
[36] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, “A
closer look at spatiotemporal convolutions for action recognition,” in
Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition, 2018, pp. 6450–6459.
[37] R. Yang, S. K. Singh, M. Tavakkoli, N. Amiri, Y. Yang, M. A. Karami,
and R. Rai, “Cnn-lstm deep learning architecture for computer vision-
based modal frequency detection,” Mechanical Systems and signal
processing, vol. 144, p. 106885, 2020.
[38] Y. Fan, X. Lu, D. Li, and Y. Liu, “Video-based emotion recognition
using cnn-rnn and c3d hybrid networks,” in Proceedings of the 18th ACM
international conference on multimodal interaction, 2016, pp. 445–450.
[39] M. Abdullah, M. Ahmad, and D. Han, “Facial expression recognition in
videos: An cnn-lstm based model for video classification,” in 2020 In-
ternational Conference on Electronics, Information, and Communication
(ICEIC).
IEEE, 2020, pp. 1–3.
[40] K. Rangasamy, M. A. As’ari, N. A. Rahmad, N. F. Ghazali, and S. Is-
mail, “Deep learning in sport video analysis: a review,” TELKOMNIKA
(Telecommunication Computing Electronics and Control), vol. 18, no. 4,
pp. 1926–1933, 2020.
[41] A. Diba, A. M. Pazandeh, and L. Van Gool, “Efficient two-stream
motion and appearance 3d cnns for video classification,” arXiv preprint
arXiv:1608.08851, 2016.
[42] K. Hegde, R. Agrawal, Y. Yao, and C. W. Fletcher, “Morph: Flexible
acceleration for 3d cnn-based video understanding,” in 2018 51st Annual
IEEE/ACM International Symposium on Microarchitecture (MICRO).
IEEE, 2018, pp. 933–946.
[43] R. Hou, C. Chen, R. Sukthankar, and M. Shah, “An efficient 3d cnn for
action/object segmentation in video,” arXiv preprint arXiv:1907.08895,
2019.
[44] D. V. Prokhorov, L. Feldkarnp, and I. Y. Tyukin, “Adaptive behavior
with fixed weights in rnn: an overview,” in Proceedings of the 2002
International Joint Conference on Neural Networks. IJCNN’02 (Cat.
No. 02CH37290), vol. 3.
IEEE, 2002, pp. 2018–2022.
[45] Z. Wang, Z. Yu, Y. Wang, H. Zhang, Y. Luo, L. Shi, Y. Wang, and
C. Guo, “3d compressed convolutional neural network differentiates
neuromyelitis optical spectrum disorders from multiple sclerosis using
automated white matter hyperintensities segmentations,” Frontiers in
Physiology, vol. 11, p. 612928, 2020.
[46] P.-E. Martin, J. Benois-Pineau, R. P´eteri, and J. Morlier, “Three-stream
3d/1d cnn for fine-grained action classification and segmentation in table
tennis,” in Proceedings of the 4th International Workshop on Multimedia
Content Analysis in Sports, 2021, pp. 35–41.
[47] S. Liu, S. You, C. Zeng, H. Yin, Z. Lin, Y. Dong, W. Qiu, W. Yao,
and Y. Liu, “Data source authentication of synchrophasor measurement
devices based on 1d-cnn and gru,” Electric Power Systems Research,
vol. 196, p. 107207, 2021.
[48] W. Zhang, Y. Fang, and Z. Ma, “The effect of task similarity on deep
transfer learning,” in International Conference on Neural Information
Processing.
Springer, 2017, pp. 256–265.
[49] M. E. Taylor and P. Stone, “Transfer learning for reinforcement learning
domains: A survey.” Journal of Machine Learning Research, vol. 10,
no. 7, 2009.
[50] Y. Zhang, L. Wang, X. Wang, C. Zhang, J. Ge, J. Tang, A. Su, and
H. Duan, “Data augmentation and transfer learning strategies for reaction
prediction in low chemical data regimes,” Organic Chemistry Frontiers,
vol. 8, no. 7, pp. 1415–1423, 2021.
[51] I. Habibie, W. Xu, D. Mehta, G. Pons-Moll, and C. Theobalt, “In the
wild human pose estimation using explicit 2d features and intermediate
3d representations,” in Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 2019, pp. 10 905–10 914.
[52] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann,
and W. Brendel, “Imagenet-trained cnns are biased towards texture;
increasing shape bias improves accuracy and robustness,” arXiv preprint
arXiv:1811.12231, 2018.
[53] K. Hirota and T. Komuro, “Grasping action recognition in vr envi-
ronment using object shape and position information,” in 2021 IEEE
International Conference on Consumer Electronics (ICCE).
IEEE,
2021, pp. 1–2.
[54] X.-Y. Zhang, Y.-P. Huang, Y. Mi, Y.-T. Pei, Q. Zou, and S. Wang, “Video
sketch: A middle-level representation for action recognition,” Applied
Intelligence, vol. 51, no. 4, pp. 2589–2608, 2021.
[55] C. Dhiman and D. K. Vishwakarma, “View-invariant deep architecture
for human action recognition using two-stream motion and shape
temporal dynamics,” IEEE Transactions on Image Processing, vol. 29,
pp. 3835–3844, 2020.
[56] H. El-Ghaish, M. E. Hussien, A. Shoukry, and R. Onai, “Human action
recognition based on integrating body pose, part shape, and motion,”
IEEE Access, vol. 6, pp. 49 040–49 055, 2018.
[57] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE conference on
computer vision and pattern recognition.
Ieee, 2009, pp. 248–255.
[58] supervise.ly, “Supervisely person dataset,” https://supervise.ly, 2018.
[59] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a
local svm approach,” in Proceedings of the 17th International Confer-
ence on Pattern Recognition, 2004. ICPR 2004., vol. 3.
IEEE, 2004,
pp. 32–36.
[60] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human
actions classes from videos in the wild,” arXiv preprint arXiv:1212.0402,
2012.
[61] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “Hmdb: a
large video database for human motion recognition,” in 2011 Interna-
tional conference on computer vision.
IEEE, 2011, pp. 2556–2563.
[62] A. Grushin, D. D. Monner, J. A. Reggia, and A. Mishra, “Robust
human action recognition via long short-term memory,” in The 2013
International Joint Conference on Neural Networks (IJCNN).
IEEE,
2013, pp. 1–8.
[63] N. Shu, Q. Tang, and H. Liu, “A bio-inspired approach modeling spiking
neural networks of visual cortex for human action recognition,” in 2014
international joint conference on neural networks (IJCNN). IEEE, 2014,
pp. 3450–3457.
[64] C. Geng and J. Song, “Human action recognition based on convolutional
neural networks with a convolutional auto-encoder,” in 2015 5th Inter-
national Conference on Computer Sciences and Automation Engineering
(ICCSAE 2015).
Atlantis Press, 2016, pp. 933–938.
[65] V. Veeriah, N. Zhuang, and G.-J. Qi, “Differential recurrent neural net-
works for action recognition,” in Proceedings of the IEEE international
conference on computer vision, 2015, pp. 4041–4049.
[66] N. Zhang, Z. Hu, S. Lee, and E. Lee, “Human action recognition based
on global silhouette and local optical flow,” in International Symposium
on Mechanical Engineering and Material Science (ISMEMS 2017).
Atlantis Press, 2017, pp. 1–5.
[67] J. Arunnehru, G. Chamundeeswari, and S. P. Bharathi, “Human action
recognition using 3d convolutional neural networks with 3d motion
cuboids in surveillance videos,” Procedia computer science, vol. 133,
pp. 471–477, 2018.
[68] A. Abdelbaky and S. Aly, “Human action recognition based on simple
deep convolution network pcanet,” in 2020 International Conference on
Innovative Trends in Communication and Computer Engineering (ITCE).
IEEE, 2020, pp. 257–262.
[69] X. Liu, D.-y. Qi, and H.-b. Xiao, “Construction and evaluation of the
human behavior recognition model in kinematics under deep learning,”
Journal of Ambient Intelligence and Humanized Computing, pp. 1–9,
2020.
[70] S. P. Sahoo, S. Ari, K. Mahapatra, and S. P. Mohanty, “Har-depth: a
novel framework for human action recognition using sequential learning
and depth estimated history images,” IEEE transactions on emerging
topics in computational intelligence, vol. 5, no. 5, pp. 813–825, 2020.
[71] P. Ramya and R. Rajeswari, “Human action recognition using distance
transform and entropy based features,” Multimedia Tools and Applica-
tions, vol. 80, pp. 8147–8173, 2021.
[72] S. Lee, H. G. Kim, D. H. Choi, H.-I. Kim, and Y. M. Ro, “Video
prediction recalling long-term motion context via memory alignment
learning,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2021, pp. 3054–3063.
[73] S. S. Basha, V. Pulabaigari, and S. Mukherjee, “An information-rich
sampling technique over spatio-temporal cnn for classification of human
actions in videos,” Multimedia Tools and Applications, vol. 81, no. 28,
pp. 40 431–40 449, 2022.
[74] E. Picco, P. Antonik, and S. Massar, “High speed human action recog-
nition using a photonic reservoir computer,” Neural Networks, 2023.
[75] X. Ye and G.-A. Bilodeau, “A unified model for continuous conditional
video prediction,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2023, pp. 3603–3612.
[76] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and
L. Fei-Fei, “Large-scale video classification with convolutional neural
networks,” in Proceedings of the IEEE conference on Computer Vision
and Pattern Recognition, 2014, pp. 1725–1732.
[77] Y. Zhu, X. Li, C. Liu, M. Zolfaghari, Y. Xiong, C. Wu, Z. Zhang,
J. Tighe, R. Manmatha, and M. Li, “A comprehensive study of deep
video action recognition,” arXiv preprint arXiv:2012.06567, 2020.
[78] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Convolutional two-stream
network fusion for video action recognition,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 1933–
1941.
[79] Y. Li, B. Ji, X. Shi, J. Zhang, B. Kang, and L. Wang, “Tea: Temporal
excitation and aggregation for action recognition,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition,
2020, pp. 909–918.
[80] L. Wang, Z. Tong, B. Ji, and G. Wu, “Tdn: Temporal difference networks
for efficient action recognition,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2021, pp.
1895–1904.
[81] K. Omi, J. Kimata, and T. Tamaki, “Model-agnostic multi-domain
learning with domain-specific adapters for action recognition,” IEICE
TRANSACTIONS on Information and Systems, vol. 105, no. 12, pp.
2119–2126, 2022.
[82] Y. Zhang, “Mest: An action recognition network with motion encoder
and spatio-temporal module,” Sensors, vol. 22, no. 17, p. 6595, 2022.
[83] G. Yang, Y. Yang, Z. Lu, J. Yang, D. Liu, C. Zhou, and Z. Fan,
“Sta-tsn: Spatial-temporal attention temporal segment network for action
recognition in video,” PloS one, vol. 17, no. 3, p. e0265115, 2022.
[84] B. Chen, F. Meng, H. Tang, and G. Tong, “Two-level attention module
based on spurious-3d residual networks for human action recognition,”
Sensors, vol. 23, no. 3, p. 1707, 2023.
[85] H. Kwon, M. Kim, S. Kwak, and M. Cho, “Motionsqueeze: Neural
motion feature learning for video understanding,” in Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XVI 16.
Springer, 2020, pp. 345–362.
