# Attention Is All You Need

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **æ ‡é¢˜**: Attention Is All You Need
- **ä½œè€…**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, Illia Polosukhin
- **æœºæ„**: Google Brain
- **å‘è¡¨ä¼šè®®/æœŸåˆŠ**: NeurIPS 2017
- **å¹´ä»½**: 2017
- **è®ºæ–‡é“¾æ¥**: https://arxiv.org/abs/1706.03762
- **ä»£ç é“¾æ¥**: https://github.com/tensorflow/tensor2tensor
- **é˜…è¯»æ—¥æœŸ**: 2024-01-15
- **é˜…è¯»çŠ¶æ€**: âœ… å·²è¯»

---

## ğŸ¯ ä¸€å¥è¯æ€»ç»“
> æå‡ºTransformeræ¶æ„ï¼Œä»…ç”¨æ³¨æ„åŠ›æœºåˆ¶å®ç°SOTAæœºå™¨ç¿»è¯‘ï¼Œå¼€å¯å¤§æ¨¡å‹æ—¶ä»£

---

## 1ï¸âƒ£ ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº

### 1.1 é—®é¢˜å®šä¹‰
ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ï¼šç»™å®šæºè¯­è¨€å¥å­ï¼Œç”Ÿæˆç›®æ ‡è¯­è¨€å¥å­

### 1.2 ç°æœ‰æ–¹æ³•çš„å±€é™
| æ–¹æ³• | å±€é™æ€§ |
|------|--------|
| RNN/LSTM | é¡ºåºè®¡ç®—æ— æ³•å¹¶è¡Œï¼Œè®­ç»ƒæ…¢ |
| CNN (å¦‚ConvS2S) | éœ€è¦å¤šå±‚æ‰èƒ½æ•è·é•¿è·ç¦»ä¾èµ– |
| RNN+Attention | ä»å—é™äºRNNçš„åºåˆ—ç‰¹æ€§ |

### 1.3 æœ¬æ–‡åŠ¨æœº
- èƒ½å¦å®Œå…¨æŠ›å¼ƒRNN/CNNï¼Œä»…ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ
- æ³¨æ„åŠ›çš„O(1)è·¯å¾„é•¿åº¦å¯æ›´å¥½å»ºæ¨¡é•¿è·ç¦»ä¾èµ–
- å¹¶è¡Œè®¡ç®—å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡

---

## 2ï¸âƒ£ æ ¸å¿ƒæ–¹æ³•

### 2.1 æ–¹æ³•æ¦‚è¿°
æå‡ºTransformeræ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸ä½¿ç”¨ä»»ä½•å¾ªç¯æˆ–å·ç§¯ç»“æ„ã€‚æ ¸å¿ƒæ˜¯Self-Attentionå’ŒMulti-Head Attentionã€‚

### 2.2 æŠ€æœ¯æ¡†æ¶
```
Encoder (x6)              Decoder (x6)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input     â”‚          â”‚    Output       â”‚
â”‚  Embedding  â”‚          â”‚   Embedding     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Positional â”‚          â”‚   Positional    â”‚
â”‚  Encoding   â”‚          â”‚    Encoding     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Multi-Head  â”‚          â”‚  Masked Multi-  â”‚
â”‚  Attention  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Head Attn    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Feed      â”‚          â”‚  Multi-Head     â”‚
â”‚  Forward    â”‚          â”‚   Attention     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ...     â”‚          â”‚   Feed Forward  â”‚
â”‚   (Ã—6å±‚)    â”‚          â”‚      ...        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    (Ã—6å±‚)       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 æ•°å­¦æ¨¡å‹

#### Scaled Dot-Product Attention
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

#### Multi-Head Attention
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
$$
$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

#### ä½ç½®ç¼–ç 
$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})
$$
$$
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})
$$

### 2.4 ç®—æ³•æµç¨‹
```
ç®—æ³•1: Transformer Encoderå±‚
è¾“å…¥: x (åºåˆ—åµŒå…¥)
è¾“å‡º: x' (ç¼–ç åè¡¨ç¤º)

1. // Multi-Head Self-Attention
2. Q, K, V â† Linear(x), Linear(x), Linear(x)
3. attn â† Softmax(QK^T / âˆšd_k) V
4. mha â† Concat(attn_heads) W_O
5. x â† LayerNorm(x + mha)
6. // Feed Forward
7. ff â† ReLU(Linear(x)) W_2 + b_2
8. x' â† LayerNorm(x + ff)
9. return x'
```

### 2.5 å…³é”®æŠ€æœ¯ç»†èŠ‚
- **ç¼©æ”¾å› å­âˆšd_k**: é˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´softmaxæ¢¯åº¦æ¶ˆå¤±
- **Masked Attention**: Decoderä¸­é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯
- **Warmupå­¦ä¹ ç‡ç­–ç•¥**: å…ˆçº¿æ€§å¢ï¼ŒåæŒ‰âˆšstepè¡°å‡

---

## 3ï¸âƒ£ å®éªŒéƒ¨åˆ†

### 3.1 å®éªŒè®¾ç½®
- **æ•°æ®é›†**: WMT 2014 English-German, English-French
- **è¯„ä»·æŒ‡æ ‡**: BLEU
- **åŸºçº¿æ–¹æ³•**: GNMT, ConvS2Sç­‰
- **å®ç°ç»†èŠ‚**: 8 P100 GPU, è®­ç»ƒ12å°æ—¶(Base)/3.5å¤©(Big)

### 3.2 ä¸»å®éªŒç»“æœ
| æ–¹æ³• | En-De BLEU | En-Fr BLEU | è®­ç»ƒæ—¶é—´ |
|------|------------|------------|----------|
| GNMT | 24.6 | 38.95 | - |
| ConvS2S | 26.10 | 38.21 | - |
| **Transformer-Base** | 27.3 | 38.1 | 12h |
| **Transformer-Big** | **28.4** | **41.8** | 3.5å¤© |

### 3.3 æ¶ˆèå®éªŒ
| ç»„ä»¶ | En-De BLEU | è¯´æ˜ |
|------|------------|------|
| å®Œæ•´æ¨¡å‹ | 27.3 | |
| å»é™¤Multi-Head (h=1) | 26.4 | å¤šå¤´æ³¨æ„åŠ›é‡è¦ |
| å»é™¤ä½ç½®ç¼–ç  | 26.0 | ä½ç½®ä¿¡æ¯å¿…è¦ |
| å¯å­¦ä¹ ä½ç½®ç¼–ç  | 26.6 | æ­£å¼¦ç¼–ç æ›´ä¼˜ |
| h=2, heads | 26.9 | å¤´æ•°å½±å“å¤§ |

### 3.4 å‚æ•°æ•æ„Ÿæ€§åˆ†æ
- å¤´æ•°h: 8å¤´æœ€ä¼˜ï¼Œå¤ªå¤šå¤ªå°‘éƒ½ä¸å¥½
- æ¨¡å‹ç»´åº¦d_model: 512-2048å‡å¯
- å±‚æ•°: 6å±‚å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡

### 3.5 å¯è§†åŒ–åˆ†æ
- æ³¨æ„åŠ›æƒé‡æˆåŠŸæ•è·è¯­æ³•å…³ç³»
- ä¸åŒå¤´å…³æ³¨ä¸åŒç±»å‹çš„ä¿¡æ¯

---

## 4ï¸âƒ£ åˆ›æ–°ç‚¹æ€»ç»“

| # | åˆ›æ–°ç‚¹ | å…·ä½“æè¿° | é‡è¦æ€§ |
|---|--------|----------|--------|
| 1 | çº¯æ³¨æ„åŠ›æ¶æ„ | å®Œå…¨æŠ›å¼ƒRNN/CNN | â­â­â­ |
| 2 | Multi-Head Attention | å¤šå­ç©ºé—´å¹¶è¡Œè®¡ç®— | â­â­â­ |
| 3 | ä½ç½®ç¼–ç  | sin/coså‡½æ•°ç¼–ç ä½ç½® | â­â­ |
| 4 | å¹¶è¡Œè®­ç»ƒ | å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ | â­â­ |

---

## 5ï¸âƒ£ å±€é™æ€§åˆ†æ

### 5.1 ä½œè€…æåˆ°çš„å±€é™
- è®¡ç®—å¤æ‚åº¦O(nÂ²)ï¼Œé•¿åºåˆ—å¼€é”€å¤§
- ä½ç½®ç¼–ç å¤–æ¨èƒ½åŠ›æœ‰é™

### 5.2 ä¸ªäººå‘ç°çš„å±€é™
- å¯¹å°æ•°æ®é›†å¯èƒ½è¿‡æ‹Ÿåˆ
- éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒ
- æ¨ç†æ—¶ä»éœ€é€è¯ç”Ÿæˆ

### 5.3 å¯èƒ½çš„æ”¹è¿›æ–¹å‘
- ç¨€ç–æ³¨æ„åŠ›é™ä½å¤æ‚åº¦
- æ›´å¥½çš„ä½ç½®ç¼–ç æ–¹æ¡ˆ
- éè‡ªå›å½’ç”ŸæˆåŠ é€Ÿæ¨ç†

---

## 6ï¸âƒ£ ç›¸å…³å·¥ä½œ

### 6.1 èƒŒæ™¯çŸ¥è¯†
- Seq2Seqæ¡†æ¶
- Attentionæœºåˆ¶
- Layer Normalization

### 6.2 ç›¸å…³è®ºæ–‡
| è®ºæ–‡ | å…³ç³» | å…³é”®å·®å¼‚ |
|------|------|----------|
| Bahdanau Attention (2015) | ä¸Šæ¸¸ | åŠ æ€§æ³¨æ„åŠ› |
| ConvS2S (2017) | åŒæœŸ | åŸºäºCNN |
| BERT (2018) | ä¸‹æ¸¸ | ä»…Encoderï¼Œé¢„è®­ç»ƒ |
| GPT (2018) | ä¸‹æ¸¸ | ä»…Decoderï¼Œç”Ÿæˆå¼ |

---

## 7ï¸âƒ£ ä¸ªäººç¬”è®°

### 7.1 å…³é”®æ´è§
- Self-Attentionçš„æœ¬è´¨æ˜¯åºåˆ—ä¸­æ¯ä¸ªä½ç½®ä¸å…¶ä»–æ‰€æœ‰ä½ç½®äº¤äº’
- O(1)è·¯å¾„é•¿åº¦ vs RNNçš„O(n)æ˜¯å…³é”®ä¼˜åŠ¿
- æ¶æ„ç®€æ´ä½†æ•ˆæœå¼ºå¤§ï¼Œä½“ç°äº†"Less is More"

### 7.2 ç–‘é—®ä¸å¾…è§£å†³
- [ ] ä¸ºä»€ä¹ˆç¼©æ”¾å› å­æ˜¯âˆšd_kè€Œä¸æ˜¯å…¶ä»–ï¼Ÿ
- [ ] ä¸åŒå±‚çš„æ³¨æ„åŠ›æ¨¡å¼æœ‰ä»€ä¹ˆè§„å¾‹ï¼Ÿ

### 7.3 å¯å€Ÿé‰´ä¹‹å¤„
- æ¶æ„è®¾è®¡æ€è·¯ï¼šç”¨æœ€ç®€å•çš„æœºåˆ¶å®ç°æœ€å¼ºçš„æ•ˆæœ
- æ¶ˆèå®éªŒçš„æ–¹æ³•è®ºå€¼å¾—å­¦ä¹ 

### 7.4 è¿›ä¸€æ­¥é˜…è¯»
- [x] BERTè®ºæ–‡
- [x] GPTç³»åˆ—è®ºæ–‡
- [ ] Efficient Transformersç»¼è¿°

---

## ğŸ“ é™„å½•

### A. é‡è¦å¼•ç”¨
> "The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution." â€” Section 1

### B. æœ¯è¯­è¡¨
| æœ¯è¯­ | å«ä¹‰ |
|------|------|
| Self-Attention | åºåˆ—å†…éƒ¨å„ä½ç½®çš„æ³¨æ„åŠ› |
| Multi-Head | å¤šä¸ªå¹¶è¡Œçš„æ³¨æ„åŠ›å¤´ |
| Transduction | åºåˆ—åˆ°åºåˆ—è½¬æ¢ |

### C. å¤ç°èµ„æº
- ä»£ç : https://github.com/tensorflow/tensor2tensor
- PyTorchå®ç°: https://github.com/pytorch/examples/tree/main/word_language_model
- HuggingFace: transformersåº“
