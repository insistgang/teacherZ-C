# Attention Is All You Need

## 📋 基本信息
- **标题**: Attention Is All You Need
- **作者**: Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. (Google Brain)
- **发表会议/期刊**: NeurIPS 2017
- **年份**: 2017
- **论文链接**: https://arxiv.org/abs/1706.03762
- **代码链接**: https://github.com/tensorflow/tensor2tensor
- **阅读日期**: 2024-01-15

## 🎯 一句话总结
> 提出Transformer，纯注意力机制实现SOTA机器翻译

## ❓ 解决的问题
- RNN/LSTM的序列计算无法并行，训练慢
- 长距离依赖建模能力有限
- 需要一种更高效、更强表达力的架构

## 💡 核心方法
- 完全放弃RNN和CNN，仅使用自注意力机制
- 引入Multi-Head Attention，多子空间并行计算
- 使用位置编码注入序列顺序信息
- Encoder-Decoder各由6层相同结构堆叠

## 🔑 关键创新
- [x] Self-Attention机制替代循环结构
- [x] Multi-Head Attention增强表达能力
- [x] 位置编码保留序列信息
- [x] Layer Normalization + Residual Connection

## 📊 实验结果
| 数据集 | 指标 | 基线(BEST) | 本文 | 提升 |
|--------|------|-----------|------|------|
| WMT En-De | BLEU | 26.10 | 28.40 | +2.30 |
| WMT En-Fr | BLEU | 38.21 | 41.80 | +3.59 |

## 🔗 相关论文
- **上游**（本文基于）: 
  - Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau Attention, 2015)
  - Effective Approaches to Attention-based Neural Machine Translation (Luong Attention, 2015)
- **下游**（本文影响）:
  - BERT (2018)
  - GPT系列 (2018-)
  - Vision Transformer (2020)

## 📌 个人笔记
- Q/K/V的设计很优雅，本质是query-key-value的检索
- 位置编码用sin/cos函数很巧妙，可以外推到任意长度
- 并行计算是Transformer的核心优势之一
- 后续几乎所有大模型都基于此架构
