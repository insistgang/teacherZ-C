# PersLLM: äººæ ¼æ£€æµ‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•

> **è¶…ç²¾è¯»ç¬”è®°** | 5-Agentè¾©è®ºåˆ†æç³»ç»Ÿ
> åˆ†ææ—¶é—´ï¼š2026-02-16
> ä½œè€…ï¼šXiaohao Cai, Lingzhi Shen, Shuotian Bai, et al.
> æ¥æºï¼šarXiv:2508.12345 (2025)

---

## ğŸ“„ è®ºæ–‡å…ƒä¿¡æ¯

| å±æ€§ | ä¿¡æ¯ |
|------|------|
| **æ ‡é¢˜** | Less but Better: Parameter-Efficient Fine-Tuning for Personality Detection |
| **ä½œè€…** | Xiaohao Cai, Lingzhi Shen, Shuotian Bai, et al. |
| **å¹´ä»½** | 2025 |
| **arXiv ID** | 2508.12345 |
| **æœºæ„** | å¤æ—¦å¤§å­¦ã€å—å®‰æ™®é¡¿å¤§å­¦ç­‰ |
| **é¢†åŸŸ** | NLPã€äººæ ¼è®¡ç®—ã€å‚æ•°é«˜æ•ˆå¾®è°ƒ |

### ğŸ“ æ‘˜è¦ç¿»è¯‘

PersLLMæ˜¯ä¸€ç§ç”¨äºäººæ ¼æ£€æµ‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ¡†æ¶ã€‚ä¼ ç»Ÿå…¨å‚æ•°å¾®è°ƒåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸Šè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè€Œç°æœ‰PEFTæ–¹æ³•åœ¨äººæ ¼æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°æ¬¡ä¼˜ã€‚PersLLMé‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰é€‚é…å™¨ã€åŠ¨æ€è®°å¿†å±‚å’Œå¯æ›¿æ¢è¾“å‡ºç½‘ç»œï¼Œå®ç°ä»…0.8%å‚æ•°å¯è°ƒçš„æƒ…å†µä¸‹è¾¾åˆ°SOTAæ€§èƒ½ã€‚åœ¨Kaggleå’ŒPandoraæ•°æ®é›†ä¸ŠF1åˆ†æ•°åˆ†åˆ«è¾¾åˆ°78.33%å’Œ69.47%ã€‚

**å…³é”®è¯**: äººæ ¼æ£€æµ‹ã€å‚æ•°é«˜æ•ˆå¾®è°ƒã€GQAã€åŠ¨æ€è®°å¿†ã€LLM

---

## ğŸ¯ ä¸€å¥è¯æ€»ç»“

é€šè¿‡GQAé€‚é…å™¨ã€åŠ¨æ€è®°å¿†å±‚å’Œå¯æ›¿æ¢è¾“å‡ºç½‘ç»œï¼Œå®ç°ä»…å¾®è°ƒ0.8%å‚æ•°å³å¯è¾¾åˆ°SOTAçš„äººæ ¼æ£€æµ‹æ€§èƒ½ã€‚

---

## ğŸ”‘ æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **GQAé€‚é…å™¨**ï¼šåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦
2. **åŠ¨æ€è®°å¿†å±‚**ï¼šæ•æ‰äººæ ¼ç›¸å…³çš„é•¿æœŸä¾èµ–
3. **å¯æ›¿æ¢è¾“å‡ºç½‘ç»œ**ï¼šçµæ´»é€‚åº”ä¸åŒäººæ ¼åˆ†ç±»ä½“ç³»
4. **é«˜æ•ˆå‚æ•°æ¯”**ï¼šä»…0.8%å‚æ•°å¯è°ƒè¾¾åˆ°SOTA

---

## ğŸ“Š èƒŒæ™¯ä¸åŠ¨æœº

### äººæ ¼æ£€æµ‹çš„æŒ‘æˆ˜

**æ•°æ®ç¨€ç¼º**ï¼š
- é«˜è´¨é‡äººæ ¼æ ‡æ³¨æ•°æ®æœ‰é™
- å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæˆæœ¬é«˜æ˜‚

**å…¨å‚æ•°å¾®è°ƒé—®é¢˜**ï¼š
```
å‚æ•°é‡: LLaMA 7B â†’ 7Bå‚æ•°å¯è®­ç»ƒ
æ˜¾å­˜éœ€æ±‚: >100GB
è®­ç»ƒæ—¶é—´: æ•°å¤©
```

**ç°æœ‰PEFTå±€é™**ï¼š
- LoRAåœ¨äººæ ¼æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°æ¬¡ä¼˜
- é€‚é…å™¨æ–¹æ³•éš¾ä»¥æ•æ‰äººæ ¼çš„é•¿æœŸä¾èµ–

### äººæ ¼çš„è¯­è¨€å­¦ç‰¹å¾

**Big Fiveäººæ ¼ç»´åº¦**ï¼š
```
å¼€æ”¾æ€§ (Openness)    â†’ åˆ›é€ æ€§ã€å¥½å¥‡è¯æ±‡
å°½è´£æ€§ (Conscientiousness) â†’ ç»“æ„åŒ–ã€ç²¾ç¡®è¡¨è¾¾
å¤–å‘æ€§ (Extraversion)   â†’ ç¤¾äº¤ã€ç§¯ææƒ…æ„Ÿè¯
å®œäººæ€§ (Agreeableness)  â†’ å’Œè°ã€åˆä½œè¯­è¨€
ç¥ç»è´¨ (Neuroticism)    â†’ ç„¦è™‘ã€è´Ÿé¢æƒ…ç»ªè¯
```

---

## ğŸ’¡ æ–¹æ³•è¯¦è§£ï¼ˆå«å…¬å¼æ¨å¯¼ï¼‰

### 3.1 æ•´ä½“æ¶æ„

```
è¾“å…¥æ–‡æœ¬
    â”‚
    â–¼
å†»ç»“LLM Backbone (LLaMA/Qwen)
    â”‚
    â”œâ”€â†’ GQAé€‚é…å™¨ (åœ¨æ¯å±‚)
    â”‚   â””â”€â†’ åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›
    â”‚
    â”œâ”€â†’ åŠ¨æ€è®°å¿†å±‚
    â”‚   â””â”€â†’ äººæ ¼ç›¸å…³è®°å¿†æ›´æ–°
    â”‚
    â””â”€â†’ å¯æ›¿æ¢è¾“å‡ºç½‘ç»œ
        â”œâ”€â†’ GRUé€‰é¡¹
        â”œâ”€â†’ MLPé€‰é¡¹
        â””â”€â†’ Transformeré€‰é¡¹
            â”‚
            â–¼
        äººæ ¼é¢„æµ‹
```

### 3.2 åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰é€‚é…å™¨

**æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›**ï¼š
```
Attention(Q, K, V) = softmax(QK^T/âˆšd)V
```

**GQAå˜ä½“**ï¼šå¤šç»„æŸ¥è¯¢å…±äº«Key-Value
```
Q: hä¸ªå¤´
K, V: gç»„ (g << h)
```

**GQAé€‚é…å™¨æ³¨å…¥**ï¼š
```
h'_l = h_l + GQAAdapter_l(h_l)
```

å…¶ä¸­ï¼š
- `h_l`ï¼šç¬¬lå±‚éšè—çŠ¶æ€
- `GQAAdapter_l`ï¼šç¬¬lå±‚çš„GQAé€‚é…å™¨

**å‚æ•°æ•ˆç‡**ï¼š
```
æ ‡å‡†MHA: O(h Ã— dÂ²)
GQA:     O(g Ã— dÂ² + h Ã— d)
å½“g=4, h=32æ—¶ï¼Œå‚æ•°å‡å°‘çº¦87.5%
```

### 3.3 åŠ¨æ€è®°å¿†å±‚

**è®°å¿†çŠ¶æ€æ›´æ–°**ï¼š
```
m_t = Update(m_{t-1}, h_L, c)
```

å…¶ä¸­ï¼š
- `m_t`ï¼šæ—¶åˆ»tçš„è®°å¿†çŠ¶æ€
- `h_L`ï¼šLLMæœ€åä¸€å±‚è¾“å‡º
- `c`ï¼šå½“å‰è¾“å…¥çš„ä¸Šä¸‹æ–‡

**é—¨æ§æ›´æ–°æœºåˆ¶**ï¼š
```
g_t = Ïƒ(W_g [h_L; m_{t-1}])
m_t = g_t âŠ™ m_{t-1} + (1-g_t) âŠ™ h_L
```

**äººæ ¼ç›¸å…³è®°å¿†æ£€ç´¢**ï¼š
```
p_read = ReadPersonalityMemory(m_t, personality_query)
```

### 3.4 å¯æ›¿æ¢è¾“å‡ºç½‘ç»œ

**GRUé€‰é¡¹**ï¼š
```python
h_gru, _ = nn.GRU(h_L, hidden_dim)
logits = output_layer(h_gru[:, -1, :])
```

**MLPé€‰é¡¹**ï¼š
```python
h_mlp = nn.ReLU()(W1 @ h_L + b1)
logits = W2 @ h_mlp + b2
```

**Transformeré€‰é¡¹**ï¼š
```python
h_trans = transformer_encoder(h_L)
logits = output_layer(h_trans[:, 0, :])
```

### 3.5 è®­ç»ƒç›®æ ‡

**å¤šä»»åŠ¡æŸå¤±**ï¼š
```
L_total = L_pers + Î± L_aux
```

å…¶ä¸­ï¼š
- `L_pers`ï¼šäººæ ¼åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
- `L_aux`ï¼šè¾…åŠ©ä»»åŠ¡æŸå¤±ï¼ˆæƒ…æ„Ÿã€å¥æ³•ç­‰ï¼‰
- `Î±`ï¼šè¾…åŠ©ä»»åŠ¡æƒé‡

**äººæ ¼åˆ†ç±»æŸå¤±**ï¼š
```
L_pers = -Î£_{i=1}^{N} Î£_{c=1}^{C} y_{ic} log(p(Å·_{ic}|x_i))
```

---

## ğŸ§ª å®éªŒä¸ç»“æœ

### æ•°æ®é›†

| æ•°æ®é›† | æ ·æœ¬æ•° | äººæ ¼ä½“ç³» | æ¥æº |
|--------|--------|----------|------|
| Kaggle | 15,000 | Big Five | ç¤¾äº¤åª’ä½“ |
| Pandora | 10,000 | MBTI | è®ºå›å¸–å­ |
| Essays | 2,467 | Big Five | å­¦æœ¯å†™ä½œ |

### æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | Kaggle F1 | Pandora F1 | å¯è°ƒå‚æ•° |
|------|-----------|------------|----------|
| Full Fine-tuning | 76.21% | 67.83% | 100% |
| LoRA | 73.45% | 65.12% | 0.5% |
| AdapterHub | 72.88% | 64.56% | 3.2% |
| **PersLLM** | **78.33%** | **69.47%** | **0.8%** |

### æ¶ˆèå®éªŒ

| å˜ä½“ | Kaggle F1 | é™å¹… |
|------|-----------|------|
| PersLLMå®Œæ•´ | 78.33% | - |
| w/o GQAé€‚é…å™¨ | 75.21% | -3.12% |
| w/o åŠ¨æ€è®°å¿† | 76.54% | -1.79% |
| w/o å¯æ›¿æ¢è¾“å‡º | 77.02% | -1.31% |

### è¾“å‡ºç½‘ç»œå¯¹æ¯”

| è¾“å‡ºç½‘ç»œ | Kaggle F1 | å‚æ•°é‡ |
|----------|-----------|--------|
| GRU | 78.33% | 2.1M |
| MLP | 77.89% | 1.8M |
| Transformer | 78.01% | 3.2M |

---

## ğŸ“ˆ æŠ€æœ¯æ¼”è¿›è„‰ç»œ

```
ä¼ ç»Ÿäººæ ¼æ£€æµ‹
  â†“ æ‰‹å·¥ç‰¹å¾ (LIWC)
  â†“ ç»Ÿè®¡åˆ†ç±»å™¨ (SVM)
æ·±åº¦å­¦ä¹ æ—¶ä»£
  â†“ CNN/LSTMæ–‡æœ¬ç¼–ç 
  â†“ é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
2025: PersLLM (æœ¬æ–‡)
  â†“ GQAé€‚é…å™¨
  â†“ åŠ¨æ€è®°å¿†å±‚
  â†“ å¯æ›¿æ¢è¾“å‡ºç½‘ç»œ
æœªæ¥æ–¹å‘
  â†“ å¤šæ¨¡æ€äººæ ¼å»ºæ¨¡
  â†“ å› æœæ¨æ–­å¼•å…¥
  â†“ è”é‚¦å­¦ä¹ ä¿æŠ¤éšç§
```

---

## ğŸ”— ä¸Šä¸‹æ¸¸å…³ç³»

### ä¸Šæ¸¸ä¾èµ–

- **å¤§è¯­è¨€æ¨¡å‹**ï¼šLLaMAã€Qwenç­‰ä½œä¸ºbackbone
- **PEFTæ–¹æ³•**ï¼šLoRAã€Adapterç­‰æŠ€æœ¯åŸºç¡€
- **äººæ ¼ç†è®º**ï¼šBig Fiveã€MBTIåˆ†ç±»ä½“ç³»

### ä¸‹æ¸¸å½±å“

- æ¨åŠ¨å‚æ•°é«˜æ•ˆäººæ ¼æ£€æµ‹æ–¹æ³•å‘å±•
- ä¸ºå…¶ä»–å¿ƒç†ç‰¹è´¨æ£€æµ‹æä¾›æ–°æ€è·¯

---

## âš™ï¸ å¯å¤ç°æ€§åˆ†æ

### è®¡ç®—å¤æ‚åº¦

| ç»„ä»¶ | å¤æ‚åº¦ | è¯´æ˜ |
|------|--------|------|
| GQAé€‚é…å™¨ | O(gÃ—dÂ² + hÃ—d) | gä¸ºç»„æ•°ï¼Œhä¸ºå¤´æ•° |
| åŠ¨æ€è®°å¿† | O(dÂ²) | ä¸åºåˆ—é•¿åº¦æ— å…³ |
| è¾“å‡ºç½‘ç»œ | O(dÂ²) | å–å†³äºå…·ä½“é€‰é¡¹ |

### è¶…å‚æ•°é…ç½®

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| num_groups (GQA) | 4 | æŸ¥è¯¢åˆ†ç»„æ•° |
| memory_dim | 256 | åŠ¨æ€è®°å¿†ç»´åº¦ |
| output_net_type | gru | è¾“å‡ºç½‘ç»œç±»å‹ |
| lr | 5e-5 | å­¦ä¹ ç‡ |
| batch_size | 16 | æ‰¹å¤§å° |

### è®­ç»ƒèµ„æº

```
GPU: 1Ã— A100 (40GB)
æ—¶é—´: Kaggle ~4å°æ—¶, Pandora ~3å°æ—¶
æ˜¾å­˜å³°å€¼: ~24GB
```

---

## ğŸ“š å…³é”®å‚è€ƒæ–‡çŒ®

1. Hu et al. "LoRA: Low-Rank Adaptation of Large Language Models." ICLR 2022.
2. Pfeiffer et al. "AdapterFusion: Non-Destructive Task Composition for Transfer Learning." ICLR 2021.
3. Ainsworth et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." 2023.

---

## ğŸ’» ä»£ç å®ç°è¦ç‚¹

```python
import torch
import torch.nn as nn
from transformers import LlamaForCausalLM, LlamaModel

class PersLLM(nn.Module):
    """å‚æ•°é«˜æ•ˆäººæ ¼æ£€æµ‹æ¨¡å‹"""

    def __init__(self, llm_name="llama-3.1-8B",
                 output_net_type="gru",
                 num_classes=16,
                 num_groups=4,
                 memory_dim=256):
        super().__init__()

        # å†»ç»“LLM backbone
        self.llm = LlamaForCausalLM.from_pretrained(llm_name)
        for param in self.llm.parameters():
            param.requires_grad = False

        hidden_size = self.llm.config.hidden_size
        num_heads = self.llm.config.num_attention_heads

        # GQAé€‚é…å™¨
        self.gqa_adapters = nn.ModuleList([
            GQAAdapter(hidden_size, num_heads, num_groups)
            for _ in range(self.llm.config.num_hidden_layers)
        ])

        # åŠ¨æ€è®°å¿†å±‚
        self.memory_layer = DynamicMemoryLayer(
            hidden_size, memory_dim
        )

        # å¯æ›¿æ¢è¾“å‡ºç½‘ç»œ
        self.output_net = OutputNetwork(
            output_net_type,
            hidden_size,
            memory_dim,
            num_classes
        )

    def forward(self, input_ids, attention_mask):
        # è·å–LLMå„å±‚è¾“å‡º
        hidden_states = self.llm.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True
        ).hidden_states

        # åº”ç”¨GQAé€‚é…å™¨
        adapted_states = []
        for i, state in enumerate(hidden_states[1:], 1):
            adapted = self.gqa_adapters[i](state)
            adapted_states.append(adapted)

        # æœ€ç»ˆéšè—çŠ¶æ€
        final_hidden = adapted_states[-1]

        # åŠ¨æ€è®°å¿†
        memory_output = self.memory_layer(
            final_hidden,
            attention_mask
        )

        # è¾“å‡ºé¢„æµ‹
        logits = self.output_net(
            final_hidden,
            memory_output
        )

        return logits


class GQAAdapter(nn.Module):
    """åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›é€‚é…å™¨"""

    def __init__(self, hidden_size, num_heads, num_groups):
        super().__init__()
        self.num_heads = num_heads
        self.num_groups = num_groups
        self.head_dim = hidden_size // num_heads
        self.group_dim = hidden_size // num_groups

        # æŸ¥è¯¢æŠ•å½± (å¤šå¤´)
        self.q_proj = nn.Linear(hidden_size, hidden_size)

        # é”®å€¼æŠ•å½± (åˆ†ç»„)
        self.kv_proj = nn.Linear(hidden_size,
                                 self.group_dim * 2)

        self.out_proj = nn.Linear(hidden_size, hidden_size)

        self.gate = nn.Parameter(torch.ones(1))

    def forward(self, x):
        B, L, D = x.shape

        # å¤šå¤´æŸ¥è¯¢
        Q = self.q_proj(x).reshape(B, L, self.num_heads, -1)

        # åˆ†ç»„é”®å€¼
        KV = self.kv_proj(x)
        K = KV[:, :, :self.group_dim].reshape(B, L, self.num_groups, -1)
        V = KV[:, :, self.group_dim:].reshape(B, L, self.num_groups, -1)

        # åˆ†ç»„è®¡ç®—æ³¨æ„åŠ›
        output = self._grouped_attention(Q, K, V)

        return x + self.gate * self.out_proj(output)

    def _grouped_attention(self, Q, K, V):
        """è®¡ç®—åˆ†ç»„æ³¨æ„åŠ›"""
        B, L, H, D = Q.shape
        G = K.shape[2]

        # å°†å¤´æ˜ å°„åˆ°ç»„
        heads_per_group = H // G

        outputs = []
        for g in range(G):
            h_start = g * heads_per_group
            h_end = (g + 1) * heads_per_group

            Q_g = Q[:, :, h_start:h_end, :]
            K_g = K[:, :, g:g+1, :].expand(-1, -1, heads_per_group, -1)
            V_g = V[:, :, g:g+1, :].expand(-1, -1, heads_per_group, -1)

            attn = torch.einsum('blhd,bhkd->blhk', Q_g, K_g)
            attn = attn / (D ** 0.5)
            attn = F.softmax(attn, dim=-2)

            out_g = torch.einsum('blhk,bhkd->blhd', attn, V_g)
            outputs.append(out_g)

        output = torch.cat(outputs, dim=2)
        return output.reshape(B, L, H * D)


class DynamicMemoryLayer(nn.Module):
    """åŠ¨æ€è®°å¿†å±‚"""

    def __init__(self, hidden_size, memory_dim):
        super().__init__()
        self.memory_dim = memory_dim

        # é—¨æ§ç½‘ç»œ
        self.gate_net = nn.Linear(hidden_size * 2, hidden_size)

        # è®°å¿†æŠ•å½±
        self.memory_proj = nn.Linear(hidden_size, memory_dim)

        # è®°å¿†æ›´æ–°
        self.update_net = nn.GRUCell(memory_dim, memory_dim)

    def forward(self, hidden_states, attention_mask):
        B, L, D = hidden_states.shape

        # å–æœ€åä¸€ä¸ªæœ‰æ•ˆtoken
        if attention_mask is not None:
            lengths = attention_mask.sum(dim=1) - 1
            indices = lengths.unsqueeze(1).unsqueeze(2)
            indices = indices.expand(-1, 1, D)
            last_hidden = hidden_states.gather(1, indices).squeeze(1)
        else:
            last_hidden = hidden_states[:, -1, :]

        # åˆå§‹è®°å¿†
        memory = torch.zeros(B, self.memory_dim,
                            device=hidden_states.device)

        # æ›´æ–°è®°å¿†
        proj_memory = self.memory_proj(last_hidden)
        memory = self.update_net(proj_memory, memory)

        return memory


class OutputNetwork(nn.Module):
    """å¯æ›¿æ¢è¾“å‡ºç½‘ç»œ"""

    def __init__(self, net_type, hidden_size, memory_dim, num_classes):
        super().__init__()
        self.net_type = net_type

        input_dim = hidden_size + memory_dim

        if net_type == "gru":
            self.network = nn.GRU(input_dim, hidden_size // 2,
                                 batch_first=True)
            self.output = nn.Linear(hidden_size // 2, num_classes)

        elif net_type == "mlp":
            self.network = nn.Sequential(
                nn.Linear(input_dim, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU()
            )
            self.output = nn.Linear(hidden_size // 2, num_classes)

        elif net_type == "transformer":
            self.network = nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    d_model=input_dim,
                    nhead=8,
                    dim_feedforward=input_dim * 4
                ),
                num_layers=2
            )
            self.output = nn.Linear(input_dim, num_classes)

    def forward(self, hidden_states, memory):
        B, L, D = hidden_states.shape

        # æ‹¼æ¥è®°å¿†
        memory_expanded = memory.unsqueeze(1).expand(-1, L, -1)
        combined = torch.cat([hidden_states, memory_expanded], dim=-1)

        if self.net_type == "gru":
            _, h_n = self.network(combined)
            logits = self.output(h_n)

        elif self.net_type == "mlp":
            features = self.network(combined[:, -1, :])
            logits = self.output(features)

        elif self.net_type == "transformer":
            output = self.network(combined.transpose(0, 1))
            logits = self.output(output[0])

        return logits
```

---

## ğŸŒŸ åº”ç”¨ä¸å½±å“

### åº”ç”¨åœºæ™¯

1. **ç¤¾äº¤åª’ä½“åˆ†æ**
   - ç”¨æˆ·ç”»åƒæ„å»º
   - å†…å®¹æ¨èä¼˜åŒ–
   - ç¤¾äº¤è¡Œä¸ºé¢„æµ‹

2. **å¿ƒç†å¥åº·**
   - å¿ƒç†çŠ¶æ€è¯„ä¼°
   - æƒ…ç»ªéšœç¢è¾…åŠ©è¯Šæ–­

3. **äººåŠ›èµ„æº**
   - å€™é€‰äººæ€§æ ¼è¯„ä¼°
   - å›¢é˜ŸåŒ¹é…ä¼˜åŒ–

### å•†ä¸šæ½œåŠ›

- **è¥é”€é¢†åŸŸ**ï¼šç²¾å‡†ç”¨æˆ·ç”»åƒ
- **æ‹›è˜å¹³å°**ï¼šæ€§æ ¼åŒ¹é…æ¨è
- **æ•™è‚²ç§‘æŠ€**ï¼šä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„

---

## â“ æœªè§£é—®é¢˜ä¸å±•æœ›

### å±€é™æ€§

1. **å•æ¨¡æ€é™åˆ¶**ï¼šä»…ä½¿ç”¨æ–‡æœ¬ï¼Œæœªåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯
2. **æ–‡åŒ–åå·®**ï¼šè®­ç»ƒæ•°æ®ä¸»è¦æ¥è‡ªè‹±è¯­ï¼Œè·¨æ–‡åŒ–æ³›åŒ–æœªçŸ¥
3. **éšç§é£é™©**ï¼šäººæ ¼æ¨æ–­æ¶‰åŠæ•æ„Ÿä¿¡æ¯

### æœªæ¥æ–¹å‘

1. **å¤šæ¨¡æ€æ‰©å±•**ï¼šç»“åˆè¯­éŸ³ã€è§†è§‰ç‰¹å¾
2. **å› æœæ¨æ–­**ï¼šå»ºç«‹äººæ ¼-è¡Œä¸ºçš„å› æœæ¨¡å‹
3. **è”é‚¦å­¦ä¹ **ï¼šéšç§ä¿æŠ¤çš„åˆ†å¸ƒå¼è®­ç»ƒ
4. **å°‘æ ·æœ¬å­¦ä¹ **ï¼šè¿›ä¸€æ­¥é™ä½æ•°æ®éœ€æ±‚

---

## ğŸ“ åˆ†æç¬”è®°

```
ä¸ªäººç†è§£ï¼š

1. æ ¸å¿ƒåˆ›æ–°ï¼š
   - GQAé€‚é…å™¨æœ‰æ•ˆé™ä½å‚æ•°é‡
   - åŠ¨æ€è®°å¿†æ•æ‰äººæ ¼é•¿æœŸä¾èµ–
   - å¯æ›¿æ¢è¾“å‡ºç½‘ç»œæä¾›çµæ´»æ€§

2. æŠ€æœ¯äº®ç‚¹ï¼š
   - ä»…0.8%å‚æ•°è¾¾åˆ°SOTA
   - F1æå‡2.12%ï¼ˆvs Full FTï¼‰
   - å·¥ç¨‹å®ç°æ¸…æ™°

3. å®ç”¨ä»·å€¼ï¼š
   - å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬
   - å¯éƒ¨ç½²åˆ°æ¶ˆè´¹çº§GPU
   - é€‚é…ä¸åŒäººæ ¼åˆ†ç±»ä½“ç³»

4. æ”¹è¿›ç©ºé—´ï¼š
   - å¤šæ¨¡æ€ä¿¡æ¯èåˆ
   - è·¨æ–‡åŒ–æ³›åŒ–éªŒè¯
   - éšç§ä¿æŠ¤æœºåˆ¶
```

---

## ç»¼åˆè¯„åˆ†

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| ç†è®ºæ·±åº¦ | â˜…â˜…â˜…â˜…â˜† | PEFTç†è®ºæ‰å® |
| æ–¹æ³•åˆ›æ–° | â˜…â˜…â˜…â˜…â˜… | GQA+åŠ¨æ€è®°å¿†æ–°é¢– |
| å®ç°éš¾åº¦ | â˜…â˜…â˜…â˜†â˜† | æ¨¡å—åŒ–è®¾è®¡ |
| åº”ç”¨ä»·å€¼ | â˜…â˜…â˜…â˜…â˜… | è®­ç»ƒæˆæœ¬é™ä½æ˜¾è‘— |
| è®ºæ–‡è´¨é‡ | â˜…â˜…â˜…â˜…â˜† | å®éªŒå……åˆ† |

**æ€»åˆ†ï¼šâ˜…â˜…â˜…â˜…â˜† (4.2/5.0)**

---

*æœ¬ç¬”è®°ç”±5-Agentè¾©è®ºåˆ†æç³»ç»Ÿç”Ÿæˆï¼Œç»“åˆäº†å¤šæ™ºèƒ½ä½“ç²¾è¯»æŠ¥å‘Šå†…å®¹ã€‚*
