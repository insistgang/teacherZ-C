# 深度学习进阶学习路径

> **创建日期**: 2026年2月9日
> **目标**: 从深度学习基础到最新大模型的完整学习路径
> **预计学习周期**: 16-20周

---

## 深度学习知识体系架构

```
深度学习
    ├── 基础架构
    │   ├── 卷积神经网络 (CNN)
    │   ├── 循环神经网络 (RNN)
    │   └── Transformer
    │
    ├── 高级架构
    │   ├── 神经架构搜索 (NAS)
    │   ├── 图神经网络 (GNN)
    │   └── 扩散模型 (Diffusion)
    │
    ├── 训练技术
    │   ├── 迁移学习
    │   ├── 小样本学习
    │   ├── 自监督学习
    │   └── 对比学习
    │
    ├── 大模型技术
    │   ├── 参数高效微调 (PEFT)
    │   ├── LoRA与变体
    │   └── 提示工程
    │
    └── 可解释AI
        ├── 概念级解释
        ├── 注意力可视化
        └── 评估指标
```

---

## 学习路线图

---

### 第一阶段: 深度学习基础架构 (第1-4周)

**学习目标**: 掌握CNN、RNN、Transformer三大基础架构

#### 第1周: 卷积神经网络 (CNN)

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| CNN基本概念 | [1-01] 深度学习架构综述 | LeCun et al. 1998 | ⭐⭐⭐⭐⭐ |
| 经典架构 (AlexNet/VGG/ResNet) | [1-01] | CS231n | ⭐⭐⭐⭐⭐ |
| 卷积操作 | [1-01] | 《深度学习》- Goodfellow | ⭐⭐⭐⭐⭐ |
| 池化与步长 | [1-01] | Goodfellow 第9章 | ⭐⭐⭐⭐ |
| 批归一化 | [1-01] | Ioffe Szegedy 2015 | ⭐⭐⭐⭐ |

**核心概念**:
- 卷积核与特征图
- 感受野
| 跳跃连接 (ResNet)

**实践练习**:
- 实现LeNet/AlexNet
- 理解ResNet的残差连接
- 使用PyTorch实现图像分类

---

#### 第2周: 循环神经网络 (RNN)

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| RNN基础 | [1-01] 架构综述 | Elman 1990 | ⭐⭐⭐⭐ |
| LSTM | [1-01] | Hochreiter Schmidhuber 1997 | ⭐⭐⭐⭐⭐ |
| GRU | [1-01] | Cho et al. 2014 | ⭐⭐⭐⭐ |
| 双向RNN | [1-01] | Schuster Paliwal 1997 | ⭐⭐⭐⭐ |
| Seq2Seq | [1-01] | Sutskever et al. 2014 | ⭐⭐⭐⭐ |

**核心概念**:
- 梯度消失/爆炸
- 门控机制
- 序列建模

**实践练习**:
- 实现LSTM单元
- 文本分类任务
| 序列预测任务

---

#### 第3周: Transformer架构

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 自注意力机制 | [1-01] | Vaswani et al. 2017 | ⭐⭐⭐⭐⭐ |
| 多头注意力 | [1-01] | Vaswani et al. 2017 | ⭐⭐⭐⭐⭐ |
| 位置编码 | [1-01] | Vaswani et al. 2017 | ⭐⭐⭐⭐ |
| Encoder-Decoder | [1-01] | Vaswani et al. 2017 | ⭐⭐⭐⭐⭐ |
| ViT (Vision Transformer) | [3-10] CNN-ViT | Dosovitskiy et al. 2021 | ⭐⭐⭐⭐ |

**核心概念**:
- Q/K/V计算
- 缩放点积注意力
| 位置编码

**实践练习**:
- 实现自注意力模块
- 理解ViT在图像中的应用
| 对比CNN与Transformer

---

#### 第4周: 动作识别中的架构应用

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 3D CNN | [3-09] TransNet | Tran et al. 2015 | ⭐⭐⭐⭐ |
| Two-Stream | [1-07] 动作识别综述 | Simonyan Zisserman 2014 | ⭐⭐⭐⭐ |
| 时序分割 | [3-09] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |
| CNN-Transformer融合 | [3-10] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |

**核心概念**:
| 视频数据表示
| 时序建模
| 跨模态融合

---

### 第二阶段: 高级架构 (第5-8周)

**学习目标**: 掌握NAS、GNN、扩散模型等高级架构

#### 第5周: 神经架构搜索 (NAS)

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| NAS基础 | [4-19] 平衡NAS | Zoph Le 2017 | ⭐⭐⭐⭐ |
| 搜索空间设计 | [4-19] | Liu et al. 2018 | ⭐⭐⭐⭐ |
| 权重共享 | [4-19] | Pham et al. 2018 | ⭐⭐⭐⭐⭐ |
| 效率优化 | [4-19] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |
| 可微分搜索 | [4-19] | Liu et al. 2019 | ⭐⭐⭐⭐ |

**核心概念**:
| 搜索策略 (RL/Evolution/Gradient)
| 评估加速
| 性能-效率权衡

---

#### 第6周: 图神经网络 (GNN)

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| GCN | [3-03] LL4G | Kipf Welling 2017 | ⭐⭐⭐⭐⭐ |
| GAT | [3-03] | Veličković et al. 2018 | ⭐⭐⭐⭐ |
| GraphSAGE | [3-03] | Hamilton et al. 2017 | ⭐⭐⭐⭐ |
| 自监督GNN | [3-03] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |
| 动态图 | [3-03] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |

**核心概念**:
| 消息传递
| 图采样
| 对比学习在图上的应用

**实践练习**:
- 使用PyTorch Geometric
- 实现GCN层
- 节点分类任务

---

#### 第7周: 扩散模型基础

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 扩散过程 | [2-21] 扩散模型脑MRI | Sohl-Dickstein et al. 2015 | ⭐⭐⭐⭐ |
| DDPM | [2-21] | Ho et al. 2020 | ⭐⭐⭐⭐⭐ |
| 去噪过程 | [2-21] | Ho et al. 2020 | ⭐⭐⭐⭐⭐ |
| 条件扩散 | [2-21] | Saharia et al. 2021 | ⭐⭐⭐⭐ |
| 差异扩散 | [2-21] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |

**核心概念**:
| 前向扩散 (加噪)
| 反向扩散 (去噪)
| 分数匹配

**实践练习**:
| 实现简单DDPM
| 理解医学图像中的应用

---

#### 第8周: 生成模型对比

| 模型 | 论文参考 | 核心特点 | 优先级 |
|------|----------|----------|--------|
| GAN | Goodfellow 2014 | 对抗训练 | ⭐⭐⭐⭐ |
| VAE | Kingma Welling 2014 | 变分推断 | ⭐⭐⭐⭐ |
| Flow | Rezende Mohamed 2015 | 可逆变换 | ⭐⭐⭐ |
| Diffusion | Ho et al. 2020 | 迭代去噪 | ⭐⭐⭐⭐⭐ |

**对比维度**:
| 训练稳定性
| 生成质量
| 采样速度
| 理论基础

---

### 第三阶段: 训练技术 (第9-12周)

**学习目标**: 掌握迁移学习、小样本学习、自监督学习、对比学习

#### 第9周: 迁移学习

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 迁移学习基础 | [3-09] TransNet | Pan Yang 2010 | ⭐⭐⭐⭐ |
| 预训练-微调 | [3-09] | Yosinski et al. 2014 | ⭐⭐⭐⭐⭐ |
| 域适应 | [2-13] [4-22] 跨域检测 | Ganin et al. 2016 | ⭐⭐⭐⭐ |
| TransNet | [3-09] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |

**核心概念**:
| 源域vs目标域
| 负迁移
| 域差异度量

---

#### 第10周: 小样本学习 ⭐ 重点加强

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 小样本学习设定 | [2-25] 医学小样本 | Lake et al. 2015 | ⭐⭐⭐⭐⭐ |
| 度量学习 | [2-25] | Koch et al. 2015 | ⭐⭐⭐⭐⭐ |
| 元学习 | [2-25] | Finn et al. 2017 (MAML) | ⭐⭐⭐⭐⭐ |
| 医学应用 | [2-25] | Cai MedIA 2021 ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 非负子空间 | [2-26] | Cai IEEE TMI 2022 ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 原型网络 | [2-25] | Snell et al. 2017 | ⭐⭐⭐⭐⭐ |
| 关系网络 | [2-25] | Sung et al. 2018 | ⭐⭐⭐⭐ |

**核心概念**:
- N-way K-shot: N类每类K个样本
- 支持集-查询集: 训练-测试分离
- Episode训练: 元学习范式
| 度量空间: 学习相似性函数

**非负子空间小样本学习** (IEEE TMI 2022 ⭐⭐⭐⭐⭐):
```
问题: 医学图像标注数据极其稀缺
方法: 非负子空间特征学习

核心思想:
1. 特征分解为非负子空间基
2. 支持集样本学习子空间投影
3. 查询集投影到子空间进行分类

优势:
- 非负约束增强可解释性
- 子空间建模降维抗噪
- 医学数据物理意义清晰
```

**实践练习**:
- 实现原型网络 (Prototypical Networks)
- 实现MAML一阶近似
- 医学图像1-shot/5-shot分类实验
- 非负子空间特征分解

---

#### 第11周: 自监督学习

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 自监督基础 | [3-03] 自监督GNN | LeCun 2022 | ⭐⭐⭐⭐ |
| 对比预测编码 | [3-03] | Oord et al. 2018 | ⭐⭐⭐⭐ |
| SimCLR | [3-03] | Chen et al. 2020 | ⭐⭐⭐⭐⭐ |
| MoCo | [3-03] | He et al. 2020 | ⭐⭐⭐⭐ |
| BYOL | [3-03] | Grill et al. 2020 | ⭐⭐⭐⭐ |

**核心概念**:
| 预训练任务设计
| 正负样本对构造
| 数据增强

---

#### 第12周: 对比学习

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 对比学习框架 | [3-06] Talk2Radar | Hadsell et al. 2006 | ⭐⭐⭐⭐ |
| InfoNCE损失 | [3-06] | Oord et al. 2018 | ⭐⭐⭐⭐⭐ |
| 跨模态对比 | [3-06] [3-07] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |
| 零样本泛化 | [3-06] | Radford et al. 2021 | ⭐⭐⭐⭐ |

**核心概念**:
| 对比学习预训练
| 跨模态对齐
| 零样本检索

**实践练习**:
| 实现InfoNCE损失
| 多模态特征对齐
| 跨模态检索

---

### 第四阶段: 大模型技术 (第13-16周)

**学习目标**: 掌握参数高效微调和大模型应用

#### 第13周: 大模型与PEFT基础

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 大模型基础 | [3-01] LLM微调 | Brown et al. 2020 | ⭐⭐⭐⭐⭐ |
| PEFT概述 | [3-01] | Hu et al. 2021 | ⭐⭐⭐⭐⭐ |
| Adapter | [3-01] | Houlsby et al. 2019 | ⭐⭐⭐⭐ |
| Prefix Tuning | [3-01] | Li Liang 2021 | ⭐⭐⭐ |
| Prompt Tuning | [3-01] | Lester et al. 2021 | ⭐⭐⭐⭐ |

**核心概念**:
| 预训练-微调范式
| 参数效率
| 任务适配

---

#### 第14周: LoRA系列

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| LoRA基础 | [3-02] tCURLoRA | Hu et al. 2021 | ⭐⭐⭐⭐⭐ |
| 低秩适应 | [3-02] | Hu et al. 2021 | ⭐⭐⭐⭐⭐ |
| tCURLoRA | [3-02] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |
| 张量分解 | [3-02] | Kolda Bader 2009 | ⭐⭐⭐⭐ |

**核心概念**:
| 矩阵低秩分解
| 可训练参数 vs 总参数
| 推理效率

**实践练习**:
| 实现LoRA层
| 图像分割任务微调
| 对比LoRA vs 全量微调

---

#### 第15周: 高效微调进阶

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| LoHA | [3-01] | Zhang et al. 2023 | ⭐⭐⭐ |
| LoKr | [3-01] | Liu et al. 2022 | ⭐⭐⭐ |
| AdapterFusion | [3-01] | Pfeiffer et al. 2021 | ⭐⭐⭐ |
| 多任务PEFT | [3-01] | He et al. 2021 | ⭐⭐⭐⭐ |

**核心概念**:
| PEFT方法对比
| 多任务学习
| 知识蒸馏

---

#### 第16周: 提示工程

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| Prompt基础 | [3-01] | Liu et al. 2023 | ⭐⭐⭐⭐ |
| In-Context Learning | [3-01] | Brown et al. 2020 | ⭐⭐⭐⭐⭐ |
| Chain-of-Thought | [3-01] | Wei et al. 2022 | ⭐⭐⭐⭐ |
| Prompt优化 | [3-01] | Zhou et al. 2023 | ⭐⭐⭐⭐ |

**核心概念**:
| 零样本/少样本提示
| 思维链
| 提示模板设计

---

### 第五阶段: 可解释AI (第17-20周)

**学习目标**: 掌握可解释AI的理论与方法

#### 第17周: XAI基础

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| XAI概述 | [1-06] XAI综述 | Arrieta et al. 2020 | ⭐⭐⭐⭐⭐ |
| 可解释性分类 | [1-06] | Gilpin et al. 2018 | ⭐⭐⭐⭐ |
| 显性vs隐性 | [1-06] | Lipton 2016 | ⭐⭐⭐ |
| 可信AI | [1-06] | Arrieta et al. 2020 | ⭐⭐⭐⭐ |

**核心概念**:
| 事后解释 vs 内在可解释
| 全局vs局部解释
| 模型无关vs特定方法

---

#### 第18周: 概念级XAI

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 概念瓶颈 | [3-11] 概念级XAI | Koh et al. 2020 | ⭐⭐⭐⭐⭐ |
| TCAV | [3-11] | Kim et al. 2018 | ⭐⭐⭐⭐⭐ |
| 概念保真度 | [3-11] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |
| XAI评估 | [3-11] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |

**核心概念**:
| 概念激活向量
| 线性可分性
| 人类可理解性

**实践练习**:
| 实现TCAV计算
| 概念保真度评估
| 可视化概念激活

---

#### 第19周: 注意力可视化与解释

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 注意力权重 | [3-12] 多层次XAI | Bahdanau et al. 2015 | ⭐⭐⭐⭐ |
| 可视化方法 | [3-12] | Vig 2019 | ⭐⭐⭐⭐ |
| 多层次解释 | [3-12] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |
| 视觉-语言解释 | [3-12] | Cai 论文核心 | ⭐⭐⭐⭐⭐ |

**核心概念**:
| 注意力图
| 梯度可视化
| 多模态解释

---

#### 第20周: XAI评估与应用

| 知识点 | 论文参考 | 推荐资源 | 优先级 |
|--------|----------|----------|--------|
| 评估指标 | [3-11] | Nguyen 2022 | ⭐⭐⭐⭐ |
| 用户研究 | [3-11] | Hohman et al. 2019 | ⭐⭐⭐ |
| 医学XAI | [2-28] [2-29] | Aerts et al. 2021 | ⭐⭐⭐⭐ |
| XAI工具 | [3-11] | Captum/LIME/SHAP | ⭐⭐⭐⭐ |

**核心概念**:
| 保真度-可理解性权衡
| 定量vs定性评估
| 领域特定解释

---

## 推荐资源

### 教科书

**深度学习基础**
- 《深度学习》- Ian Goodfellow, Yoshua Bengio, Aaron Courville
- 《动手学深度学习》- Aston Zhang, Zachary Lipton

**计算机视觉**
- 《计算机视觉: 算法与应用》- Richard Szeliski
- 《深度学习计算机视觉》- Rajalingappaa Shanmugamani

**自然语言处理**
- 《Speech and Language Processing》- Dan Jurafsky
- 《自然语言处理综论》- Daniel Jurafsky

### 在线课程

| 课程 | 平台 | 链接 |
|------|------|------|
| CS231n: CNNs | Stanford | http://cs231n.stanford.edu |
| CS224N: NLP | Stanford | http://cs224n.stanford.edu |
| CS224W: ML with Graphs | Stanford | http://cs224n.stanford.edu |
| Deep Learning Specialization | Coursera | Andrew Ng |

### 关键论文（必读）

**基础架构**
1. LeCun et al. (1998) - Gradient-based learning
2. Hochreiter & Schmidhuber (1997) - Long Short-Term Memory
3. Vaswani et al. (2017) - Attention is All You Need

**高级架构**
4. Zoph & Le (2017) - Neural Architecture Search
5. Kipf & Welling (2017) - Graph Convolutional Networks
6. Ho et al. (2020) - Denoising Diffusion Probabilistic Models

**训练技术**
7. Finn et al. (2017) - MAML
8. Chen et al. (2020) - SimCLR
9. He et al. (2020) - MoCo
10. Cai et al. (2021) - Medical Few-Shot Learning (MedIA) ⭐⭐⭐⭐⭐
11. Cai et al. (2022) - Non-negative Subspace Few-Shot (IEEE TMI) ⭐⭐⭐⭐⭐

**大模型与PEFT**
10. Hu et al. (2021) - LoRA
11. Cai et al. (2024) - tCURLoRA (ICML)
12. Brown et al. (2020) - Language Models are Few-Shot Learners

**可解释AI**
13. Kim et al. (2018) - TCAV
14. Cai et al. (2023) - Concept-based XAI (TPAMI)
15. Selvaraju et al. (2017) - Grad-CAM

---

## 学习检查清单

### 基础架构
- [ ] 理解CNN卷积操作原理
- [ ] 掌握LSTM门控机制
- [ ] 理解自注意力计算
- [ ] 能实现基础网络架构

### 高级架构
- [ ] 理解NAS搜索策略
- [ ] 掌握GNN消息传递
- [ ] 理解扩散模型去噪过程
- [ ] 能对比不同生成模型

### 训练技术
- [ ] 掌握迁移学习方法
- [ ] 理解小样本学习设定
- [ ] 掌握对比学习框架
- [ ] 能实现自监督预训练

### 大模型技术
- [ ] 理解PEFT原理
- [ ] 掌握LoRA实现
- [ ] 理解张量分解在PEFT中的应用
- [ ] 能进行提示工程

### 可解释AI
- [ ] 理解XAI分类体系
- [ ] 掌握概念级解释方法
- [ ] 能进行注意力可视化
- [ ] 理解XAI评估指标

---

## 学习建议

### 学习路径
1. **循序渐进**: CNN → RNN → Transformer → NAS/GNN/Diffusion → PEFT → XAI
2. **理论+实践**: 每个知识点配合代码实现
3. **论文研读**: 跟踪最新顶会论文 (NeurIPS/ICML/ICLR/CVPR)

### 时间分配
- 理论学习: 30%
- 论文阅读: 30%
- 代码实现: 40%

### 实践项目建议
1. **图像分类**: 使用ResNet/EfficientNet
2. **动作识别**: 实现TransNet
3. **小样本分类**: 实现原型网络
4. **参数高效微调**: 使用LoRA微调分割模型
5. **可解释性**: 实现TCAV分析

### 推荐工具
- PyTorch (深度学习框架)
- HuggingFace (预训练模型)
| PyTorch Geometric (图神经网络)
- Diffusers (扩散模型)
- Captum (可解释性)

---

*创建日期: 2026年2月9日*
*版本: 1.0*
*创建者: 论文学习小分队*
