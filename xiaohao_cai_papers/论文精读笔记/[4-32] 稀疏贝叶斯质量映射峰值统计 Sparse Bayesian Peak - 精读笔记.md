# [4-32] ç¨€ç–è´å¶æ–¯è´¨é‡æ˜ å°„å³°å€¼ç»Ÿè®¡ - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Sparse Bayesian Mass Mapping: Peak Statistics
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (é«˜)
> **é‡è¦æ€§**: â­â­â­â­ (å¤©ä½“ç»Ÿè®¡)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Sparse Bayesian Mass Mapping: Peak Statistics |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **åº”ç”¨é¢†åŸŸ** | å¤©ä½“ç‰©ç†å­¦ã€å®‡å®™å­¦ç»Ÿè®¡ |
| **å…³é”®è¯** | Peak Statistics, Mass Mapping, Cosmology, Random Field |
| **æ ¸å¿ƒä»·å€¼** | å³°å€¼ç»Ÿè®¡åœ¨å®‡å®™å­¦å‚æ•°ä¼°è®¡ä¸­çš„åº”ç”¨ |

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

### å³°å€¼ç»Ÿè®¡çš„é‡è¦æ€§

```
å³°å€¼ç»Ÿè®¡åœ¨å®‡å®™å­¦ä¸­çš„ä½œç”¨:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

èƒŒæ™¯:
  - è´¨é‡æ˜ å°„ä¸­çš„å³°å€¼å¯¹åº”æ˜Ÿç³»å›¢
  - å³°å€¼ç»Ÿè®¡åŒ…å«å®‡å®™å­¦ä¿¡æ¯
  - å¯ç”¨äºçº¦æŸæš—ç‰©è´¨å’Œæš—èƒ½é‡

å³°å€¼ç»Ÿè®¡é‡:
  1. å³°å€¼æ•°å¯†åº¦ (Peak Count)
  2. å³°å€¼é«˜åº¦åˆ†å¸ƒ (Peak Height Distribution)
  3. å³°å€¼-å³°å€¼å…³è” (Peak-Peak Correlation)
  4. ç©ºæ´ç»Ÿè®¡ (Void Statistics)

ç§‘å­¦ç›®æ ‡:
  - ä»è§‚æµ‹æ•°æ®æå–å®‡å®™å­¦å‚æ•°
  - æ£€éªŒÎ›CDMæ¨¡å‹
  - æ¢æµ‹åç¦»æ ‡å‡†æ¨¡å‹çš„ä¿¡å·
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### å³°å€¼ç»Ÿè®¡ vs ä¼ ç»Ÿç»Ÿè®¡

| ç»Ÿè®¡é‡ | ä¿¡æ¯å†…å®¹ | è®¡ç®—å¤æ‚åº¦ | å¯¹å™ªå£°é²æ£’æ€§ |
|:---|:---|:---:|:---:|
| **åŠŸç‡è°±** | äºŒé˜¶ç»Ÿè®¡ | ä½ | ä¸­ |
| **åŒè°±** | ä¸‰é˜¶ç»Ÿè®¡ | ä¸­ | ä½ |
| **å³°å€¼è®¡æ•°** | éé«˜æ–¯ä¿¡æ¯ | ä¸­ | **é«˜** |
| **Minkowskiæ³›å‡½** | å½¢æ€ç»Ÿè®¡ | é«˜ | ä¸­ |

---

## ğŸ”¬ æ–¹æ³•è®º

### å³°å€¼ç»Ÿè®¡æ¡†æ¶

```
å³°å€¼ç»Ÿè®¡åˆ†ææµç¨‹:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. è´¨é‡æ˜ å°„é‡å»º
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  - ç¨€ç–è´å¶æ–¯é‡å»º                    â”‚
   â”‚  - è·å¾—åéªŒæ ·æœ¬                      â”‚
   â”‚  - ä¸ç¡®å®šæ€§é‡åŒ–                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
2. å³°å€¼æ£€æµ‹
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  - å±€éƒ¨æå¤§å€¼è¯†åˆ«                    â”‚
   â”‚  - æ˜¾è‘—æ€§é˜ˆå€¼                        â”‚
   â”‚  - å™ªå£°å³°å€¼å‰”é™¤                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
3. å³°å€¼ç‰¹å¾æå–
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  - å³°å€¼é«˜åº¦                          â”‚
   â”‚  - å³°å€¼æ›²ç‡                          â”‚
   â”‚  - å³°å€¼ä½ç½®                          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
4. ç»Ÿè®¡é‡è®¡ç®—
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  - å³°å€¼æ•°å¯†åº¦                        â”‚
   â”‚  - é«˜åº¦åˆ†å¸ƒ                          â”‚
   â”‚  - å…³è”å‡½æ•°                          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
5. å®‡å®™å­¦æ¨æ–­
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  - ä¸ç†è®ºæ¨¡å‹æ¯”è¾ƒ                    â”‚
   â”‚  - å‚æ•°ä¼°è®¡                          â”‚
   â”‚  - æ¨¡å‹é€‰æ‹©                          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### æ ¸å¿ƒç»„ä»¶1: å³°å€¼æ£€æµ‹

```python
import numpy as np
from scipy import ndimage
from scipy.spatial import cKDTree

class PeakDetector:
    """
    å³°å€¼æ£€æµ‹å™¨

    åœ¨æ”¶æ•›åœºä¸­è¯†åˆ«æ˜¾è‘—å³°å€¼
    """

    def __init__(self, threshold=3.0, min_distance=5):
        """
        Args:
            threshold: ä¿¡å™ªæ¯”é˜ˆå€¼ (sigma)
            min_distance: å³°å€¼é—´æœ€å°è·ç¦» (åƒç´ )
        """
        self.threshold = threshold
        self.min_distance = min_distance

    def detect_peaks(self, kappa_map, noise_std=None):
        """
        æ£€æµ‹å³°å€¼

        Args:
            kappa_map: æ”¶æ•›åœº (2D array)
            noise_std: å™ªå£°æ ‡å‡†å·® (å¯é€‰)

        Returns:
            peaks: å³°å€¼åˆ—è¡¨ [{'position': (y, x), 'height': ..., 'snr': ...}, ...]
        """
        if noise_std is None:
            noise_std = self.estimate_noise(kappa_map)

        # å±€éƒ¨æå¤§å€¼
        local_max = self.find_local_maxima(kappa_map)

        # ä¿¡å™ªæ¯”é˜ˆå€¼
        snr = kappa_map / noise_std
        significant = snr > self.threshold

        # å³°å€¼ä½ç½®
        peak_mask = local_max & significant
        peak_positions = np.argwhere(peak_mask)

        # æ„å»ºå³°å€¼åˆ—è¡¨
        peaks = []
        for pos in peak_positions:
            y, x = pos
            height = kappa_map[y, x]

            peaks.append({
                'position': (y, x),
                'height': height,
                'snr': snr[y, x]
            })

        # æœ€å°è·ç¦»ç­›é€‰
        peaks = self.filter_by_distance(peaks)

        return peaks

    def find_local_maxima(self, data):
        """æ‰¾åˆ°å±€éƒ¨æå¤§å€¼"""
        # ä½¿ç”¨æœ€å¤§å€¼æ»¤æ³¢
        max_filtered = ndimage.maximum_filter(data, size=3)
        local_max = (data == max_filtered)

        # æ’é™¤è¾¹ç•Œ
        local_max[0, :] = False
        local_max[-1, :] = False
        local_max[:, 0] = False
        local_max[:, -1] = False

        return local_max

    def estimate_noise(self, kappa_map):
        """ä¼°è®¡å™ªå£°æ°´å¹³"""
        # ä½¿ç”¨è¾¹ç¼˜åŒºåŸŸä¼°è®¡å™ªå£°
        edge_mask = np.zeros_like(kappa_map, dtype=bool)
        edge_mask[:10, :] = True
        edge_mask[-10:, :] = True
        edge_mask[:, :10] = True
        edge_mask[:, -10:] = True

        return np.std(kappa_map[edge_mask])

    def filter_by_distance(self, peaks):
        """æŒ‰æœ€å°è·ç¦»ç­›é€‰å³°å€¼"""
        if len(peaks) <= 1:
            return peaks

        # æŒ‰é«˜åº¦æ’åº
        peaks_sorted = sorted(peaks, key=lambda p: p['height'], reverse=True)

        filtered = [peaks_sorted[0]]

        for peak in peaks_sorted[1:]:
            pos = np.array(peak['position'])

            # æ£€æŸ¥ä¸å·²é€‰å³°å€¼çš„è·ç¦»
            too_close = False
            for selected in filtered:
                selected_pos = np.array(selected['position'])
                dist = np.linalg.norm(pos - selected_pos)

                if dist < self.min_distance:
                    too_close = True
                    break

            if not too_close:
                filtered.append(peak)

        return filtered

    def compute_peak_curvature(self, kappa_map, peak):
        """
        è®¡ç®—å³°å€¼å¤„çš„æ›²ç‡

        ç”¨äºåŒºåˆ†çœŸå®å³°å€¼å’Œå™ªå£°
        """
        y, x = peak['position']

        # æå–å±€éƒ¨åŒºåŸŸ
        window = 2
        local = kappa_map[y-window:y+window+1, x-window:x+window+1]

        if local.shape != (2*window+1, 2*window+1):
            return None

        # è®¡ç®—HessiançŸ©é˜µ
        # äºŒé˜¶å¯¼æ•°
        dyy = (local[0, 1] - 2*local[1, 1] + local[2, 1])
        dxx = (local[1, 0] - 2*local[1, 1] + local[1, 2])
        dxy = (local[0, 0] - local[0, 2] - local[2, 0] + local[2, 2]) / 4

        hessian = np.array([[dxx, dxy], [dxy, dyy]])

        # ç‰¹å¾å€¼
        eigenvalues = np.linalg.eigvals(hessian)

        return {
            'eigenvalues': eigenvalues,
            'curvature': -np.sum(eigenvalues),  # æ‹‰æ™®æ‹‰æ–¯
            'anisotropy': abs(eigenvalues[0] - eigenvalues[1])
        }
```

---

### æ ¸å¿ƒç»„ä»¶2: å³°å€¼ç»Ÿè®¡é‡è®¡ç®—

```python
class PeakStatistics:
    """
    å³°å€¼ç»Ÿè®¡é‡è®¡ç®—
    """

    def __init__(self, peaks, field_size):
        """
        Args:
            peaks: å³°å€¼åˆ—è¡¨
            field_size: åœºå¤§å° (deg^2)
        """
        self.peaks = peaks
        self.field_size = field_size

    def peak_count_histogram(self, kappa_bins=None):
        """
        å³°å€¼é«˜åº¦åˆ†å¸ƒç›´æ–¹å›¾

        å®‡å®™å­¦æ•æ„Ÿç»Ÿè®¡é‡
        """
        if kappa_bins is None:
            heights = [p['height'] for p in self.peaks]
            kappa_bins = np.linspace(min(heights), max(heights), 10)

        counts, bin_edges = np.histogram(
            [p['height'] for p in self.peaks],
            bins=kappa_bins
        )

        # å½’ä¸€åŒ–åˆ°å•ä½é¢ç§¯
        density = counts / self.field_size

        return {
            'bin_edges': bin_edges,
            'counts': counts,
            'density': density
        }

    def peak_correlation_function(self, r_bins=None):
        """
        å³°å€¼-å³°å€¼å…³è”å‡½æ•°
        """
        if len(self.peaks) < 2:
            return None

        positions = np.array([p['position'] for p in self.peaks])

        # è®¡ç®—æ‰€æœ‰å³°å€¼å¯¹è·ç¦»
        from scipy.spatial.distance import pdist
        distances = pdist(positions)

        if r_bins is None:
            r_bins = np.linspace(0, np.max(distances), 20)

        # è·ç¦»ç›´æ–¹å›¾
        counts, bin_edges = np.histogram(distances, bins=r_bins)

        # å½’ä¸€åŒ– (é™¤ä»¥éšæœºåˆ†å¸ƒæœŸæœ›)
        area = np.pi * (bin_edges[1:]**2 - bin_edges[:-1]**2)
        density = len(self.peaks) / self.field_size
        expected = np.pi * density**2 * area

        correlation = counts / (expected + 1e-10) - 1

        return {
            'r_bins': (bin_edges[:-1] + bin_edges[1:]) / 2,
            'correlation': correlation
        }

    void_statistics(self, kappa_map, threshold=-0.5):
        """
        ç©ºæ´ç»Ÿè®¡

        åˆ†æä½å¯†åº¦åŒºåŸŸ
        """
        # äºŒå€¼åŒ–: ä½äºé˜ˆå€¼ä¸ºç©ºæ´
        void_mask = kappa_map < threshold

        # æ ‡è®°è¿é€šåŒºåŸŸ
        labeled, num_voids = ndimage.label(void_mask)

        # åˆ†ææ¯ä¸ªç©ºæ´
        voids = []
        for i in range(1, num_voids + 1):
            void_mask_i = labeled == i
            size = np.sum(void_mask_i)

            if size > 10:  # æœ€å°å°ºå¯¸
                voids.append({
                    'size': size,
                    'depth': np.abs(np.min(kappa_map[void_mask_i]))
                })

        return {
            'num_voids': len(voids),
            'mean_size': np.mean([v['size'] for v in voids]) if voids else 0,
            'size_distribution': [v['size'] for v in voids]
        }

    def minkowski_functionals(self, kappa_map, thresholds=None):
        """
        Minkowskiæ³›å‡½

        æè¿°åœºçš„å½¢æ€ç‰¹å¾
        """
        if thresholds is None:
            thresholds = np.linspace(-0.5, 1.0, 20)

        V0 = []  # é¢ç§¯
        V1 = []  # å‘¨é•¿
        V2 = []  # æ¬§æ‹‰ç¤ºæ€§æ•°

        for thresh in thresholds:
            binary = kappa_map > thresh

            # é¢ç§¯ (å½’ä¸€åŒ–)
            area = np.sum(binary) / kappa_map.size
            V0.append(area)

            # å‘¨é•¿ä¼°è®¡
            from scipy.ndimage import binary_erosion
            boundary = binary & ~binary_erosion(binary)
            perimeter = np.sum(boundary)
            V1.append(perimeter)

            # æ¬§æ‹‰ç¤ºæ€§æ•°
            labeled, num_features = ndimage.label(binary)
            holes = num_features - 1  # ç®€åŒ–ä¼°è®¡
            V2.append(num_features - holes)

        return {
            'thresholds': thresholds,
            'V0_area': np.array(V0),
            'V1_perimeter': np.array(V1),
            'V2_euler': np.array(V2)
        }
```

---

### æ ¸å¿ƒç»„ä»¶3: å®‡å®™å­¦æ¨æ–­

```python
class CosmologicalInference:
    """
    å®‡å®™å­¦å‚æ•°æ¨æ–­

    ä»å³°å€¼ç»Ÿè®¡çº¦æŸå®‡å®™å­¦å‚æ•°
    """

    def __init__(self, theory_model):
        """
        Args:
            theory_model: ç†è®ºæ¨¡å‹ (å¦‚HALOFIT + å³°å€¼ç†è®º)
        """
        self.theory = theory_model

    def compute_likelihood(self, data_stats, cosmological_params):
        """
        è®¡ç®—ä¼¼ç„¶å‡½æ•°

        P(data | params)
        """
        # ç†è®ºé¢„æµ‹
        theory_stats = self.theory.predict(cosmological_params)

        # é«˜æ–¯ä¼¼ç„¶ (ç®€åŒ–)
        diff = data_stats - theory_stats

        # åæ–¹å·®çŸ©é˜µ (éœ€è¦é¢„å…ˆè®¡ç®—)
        cov = self.load_covariance_matrix()

        chi2 = diff @ np.linalg.inv(cov) @ diff

        log_likelihood = -0.5 * chi2

        return log_likelihood

    def mcmc_inference(self, data_stats, initial_params, n_samples=10000):
        """
        MCMCå‚æ•°æ¨æ–­
        """
        import emcee

        n_params = len(initial_params)

        def log_probability(params):
            # å…ˆéªŒ
            if not self.check_prior(params):
                return -np.inf

            # ä¼¼ç„¶
            return self.compute_likelihood(data_stats, params)

        # åˆå§‹åŒ– walkers
        n_walkers = 4 * n_params
        pos = initial_params + 1e-4 * np.random.randn(n_walkers, n_params)

        sampler = emcee.EnsembleSampler(n_walkers, n_params, log_probability)
        sampler.run_mcmc(pos, n_samples, progress=True)

        return sampler

    def fisher_forecast(self, cosmological_params):
        """
        FisherçŸ©é˜µé¢„æµ‹

        é¢„æµ‹å‚æ•°çº¦æŸç²¾åº¦
        """
        n_params = len(cosmological_params)
        fisher_matrix = np.zeros((n_params, n_params))

        # æ•°å€¼è®¡ç®—å¯¼æ•°
        delta = 0.01

        for i in range(n_params):
            for j in range(n_params):
                # è®¡ç®—äºŒé˜¶å¯¼æ•°
                params_pp = cosmological_params.copy()
                params_pp[i] += delta
                params_pp[j] += delta

                params_pm = cosmological_params.copy()
                params_pm[i] += delta
                params_pm[j] -= delta

                params_mp = cosmological_params.copy()
                params_mp[i] -= delta
                params_mp[j] += delta

                params_mm = cosmological_params.copy()
                params_mm[i] -= delta
                params_mm[j] -= delta

                # ä¸­å¿ƒå·®åˆ†
                f_pp = self.theory.predict(params_pp)
                f_pm = self.theory.predict(params_pm)
                f_mp = self.theory.predict(params_mp)
                f_mm = self.theory.predict(params_mm)

                fisher_matrix[i, j] = np.sum(
                    (f_pp - f_pm - f_mp + f_mm) / (4 * delta**2)
                )

        # åæ–¹å·®çŸ©é˜µ
        covariance = np.linalg.inv(fisher_matrix)

        return {
            'fisher_matrix': fisher_matrix,
            'parameter_covariance': covariance,
            'parameter_constraints': np.sqrt(np.diag(covariance))
        }
```

---

## ğŸ“Š å®éªŒç»“æœ

### å³°å€¼ç»Ÿè®¡çš„å®‡å®™å­¦æ•æ„Ÿæ€§

| å‚æ•° | å³°å€¼è®¡æ•°æ•æ„Ÿæ€§ | åŠŸç‡è°±æ•æ„Ÿæ€§ |
|:---|:---:|:---:|
| **Î©_m** | é«˜ | é«˜ |
| **Ïƒ_8** | **å¾ˆé«˜** | é«˜ |
| **w** | ä¸­ | ä½ |
| **n_s** | ä¸­ | é«˜ |

### å‚æ•°çº¦æŸå¯¹æ¯”

| æ–¹æ³• | Ïƒ(Î©_m) | Ïƒ(Ïƒ_8) | é€€åŒ–ç¨‹åº¦ |
|:---|:---:|:---:|:---:|
| åŠŸç‡è°± | 0.03 | 0.04 | é«˜ |
| å³°å€¼è®¡æ•° | 0.025 | 0.03 | ä¸­ |
| **è”åˆåˆ†æ** | **0.02** | **0.025** | **ä½** |

---

## ğŸ’¡ å¯¹äº•ç›–æ£€æµ‹çš„å¯ç¤º

### å¼‚å¸¸æ¨¡å¼ç»Ÿè®¡

```python
class AnomalyPatternStatistics:
    """
    å¼‚å¸¸æ¨¡å¼ç»Ÿè®¡

    å€Ÿé‰´å³°å€¼ç»Ÿè®¡æ€æƒ³
    """

    def __init__(self):
        self.patterns = []

    def collect_patterns(self, detection_results):
        """
        æ”¶é›†æ£€æµ‹åˆ°çš„å¼‚å¸¸æ¨¡å¼

        ç±»ä¼¼å³°å€¼æ”¶é›†
        """
        for result in detection_results:
            if result['is_anomaly']:
                self.patterns.append({
                    'position': result['position'],
                    'severity': result['severity'],
                    'type': result['anomaly_type']
                })

    def pattern_statistics(self):
        """
        å¼‚å¸¸æ¨¡å¼ç»Ÿè®¡
        """
        # ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        severities = [p['severity'] for p in self.patterns]

        # ç±»å‹åˆ†å¸ƒ
        from collections import Counter
        type_counts = Counter([p['type'] for p in self.patterns])

        # ç©ºé—´åˆ†å¸ƒ
        positions = np.array([p['position'] for p in self.patterns])

        # èšç±»åˆ†æ
        from sklearn.cluster import DBSCAN
        clustering = DBSCAN(eps=50, min_samples=3).fit(positions)

        return {
            'total_anomalies': len(self.patterns),
            'severity_distribution': np.histogram(severities, bins=5),
            'type_distribution': type_counts,
            'spatial_clusters': len(set(clustering.labels_)) - 1
        }
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **å³°å€¼è®¡æ•°** | Peak Count | è¶…è¿‡é˜ˆå€¼çš„å³°å€¼æ•°é‡ |
| **å…³è”å‡½æ•°** | Correlation Function | ä¸¤ç‚¹ç©ºé—´å…³è” |
| **Minkowskiæ³›å‡½** | Minkowski Functionals | å½¢æ€æè¿°é‡ |
| **FisherçŸ©é˜µ** | Fisher Matrix | å‚æ•°çº¦æŸé¢„æµ‹ |
| **ç©ºæ´ç»Ÿè®¡** | Void Statistics | ä½å¯†åº¦åŒºåŸŸç»Ÿè®¡ |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å³°å€¼ç»Ÿè®¡çš„å®‡å®™å­¦æ„ä¹‰
- [ ] æŒæ¡å³°å€¼æ£€æµ‹æ–¹æ³•
- [ ] äº†è§£å³°å€¼ç»Ÿè®¡é‡è®¡ç®—
- [ ] ç†è§£å®‡å®™å­¦å‚æ•°æ¨æ–­æµç¨‹

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
