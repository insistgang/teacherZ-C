# [4-31] ç¨€ç–è´å¶æ–¯è´¨é‡æ˜ å°„å¯ä¿¡åŒºé—´ - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Sparse Bayesian Mass Mapping: Credible Intervals
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (é«˜)
> **é‡è¦æ€§**: â­â­â­â­ (ä¸ç¡®å®šæ€§é‡åŒ–)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Sparse Bayesian Mass Mapping: Credible Intervals |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **åº”ç”¨é¢†åŸŸ** | å¤©ä½“ç‰©ç†å­¦ã€ç»Ÿè®¡æ¨æ–­ã€ä¸ç¡®å®šæ€§é‡åŒ– |
| **å…³é”®è¯** | Credible Intervals, Uncertainty Quantification, Bayesian Inference |
| **æ ¸å¿ƒä»·å€¼** | ä¸ºè´¨é‡æ˜ å°„æä¾›å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ |

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

### ä¸ºä»€ä¹ˆéœ€è¦å¯ä¿¡åŒºé—´?

```
ä¸ç¡®å®šæ€§é‡åŒ–é‡è¦æ€§:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä¼ ç»Ÿç‚¹ä¼°è®¡çš„é—®é¢˜:
  - åªç»™å‡ºä¸€ä¸ª"æœ€ä½³"ä¼°è®¡
  - æ— æ³•åˆ¤æ–­ä¼°è®¡çš„å¯é æ€§
  - éš¾ä»¥åŒºåˆ†ä¿¡å·å’Œå™ªå£°

è´å¶æ–¯å¯ä¿¡åŒºé—´çš„ä¼˜åŠ¿:
  - æä¾›æ¦‚ç‡æ€§ä¸ç¡®å®šæ€§
  - è¯†åˆ«é«˜/ä½ç½®ä¿¡åº¦åŒºåŸŸ
  - æ”¯æŒç§‘å­¦å†³ç­–

åº”ç”¨:
  1. åˆ¤æ–­å³°å€¼æ˜¯å¦çœŸå®
  2. é‡åŒ–å­ç»“æ„è´¨é‡
  3. æ¯”è¾ƒä¸åŒè§‚æµ‹ç»“æœ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### å¯ä¿¡åŒºé—´ vs ç½®ä¿¡åŒºé—´

| ç‰¹æ€§ | é¢‘ç‡å­¦æ´¾ç½®ä¿¡åŒºé—´ | è´å¶æ–¯å¯ä¿¡åŒºé—´ |
|:---|:---|:---|
| **è§£é‡Š** | é‡å¤å®éªŒè¦†ç›–çœŸå€¼çš„æ¦‚ç‡ | å‚æ•°è½åœ¨åŒºé—´å†…çš„æ¦‚ç‡ |
| **è®¡ç®—** | åŸºäºé‡‡æ ·åˆ†å¸ƒ | åŸºäºåéªŒåˆ†å¸ƒ |
| **å…ˆéªŒ** | ä¸ä½¿ç”¨ | ä½¿ç”¨å…ˆéªŒä¿¡æ¯ |
| **ç¨€ç–é—®é¢˜** | éš¾ä»¥å¤„ç† | è‡ªç„¶å¤„ç† |

---

## ğŸ”¬ æ–¹æ³•è®º

### å¯ä¿¡åŒºé—´è®¡ç®—

```
è´å¶æ–¯å¯ä¿¡åŒºé—´:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å®šä¹‰:
  å¯¹äºå‚æ•°Îº, 100(1-Î±)%å¯ä¿¡åŒºé—´ [L, U] æ»¡è¶³:
  P(L â‰¤ Îº â‰¤ U | data) = 1 - Î±

å¸¸ç”¨ç±»å‹:
  1. ç­‰å°¾åŒºé—´ (Equal-tailed):
     P(Îº < L) = P(Îº > U) = Î±/2

  2. æœ€é«˜åéªŒå¯†åº¦ (HPD):
     åŒ…å«æœ€é«˜æ¦‚ç‡å¯†åº¦çš„åŒºåŸŸ
     P(Îº âˆˆ HPD) = 1 - Î±

  3. è”åˆå¯ä¿¡åŒºåŸŸ:
     å¤šå‚æ•°çš„å¯ä¿¡åŒºåŸŸ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### æ ¸å¿ƒç»„ä»¶1: åéªŒé‡‡æ ·

```python
import numpy as np
from scipy import stats

class PosteriorSampler:
    """
    åéªŒåˆ†å¸ƒé‡‡æ ·å™¨

    ç”¨äºä¼°è®¡å¯ä¿¡åŒºé—´
    """

    def __init__(self, log_posterior, grad_log_posterior):
        """
        Args:
            log_posterior: å¯¹æ•°åéªŒå‡½æ•°
            grad_log_posterior: å¯¹æ•°åéªŒæ¢¯åº¦
        """
        self.log_posterior = log_posterior
        self.grad_log_posterior = grad_log_posterior

    def hmc_sample(self, initial_state, n_samples=1000, n_warmup=500,
                   step_size=0.01, n_leapfrog=10):
        """
        Hamiltonian Monte Carloé‡‡æ ·

        é«˜æ•ˆæ¢ç´¢é«˜ç»´åéªŒåˆ†å¸ƒ
        """
        samples = []
        current_state = initial_state.copy()
        current_log_prob = self.log_posterior(current_state)

        for i in range(n_warmup + n_samples):
            # é‡‡æ ·åŠ¨é‡
            momentum = np.random.randn(len(current_state))

            # ä¿å­˜å½“å‰çŠ¶æ€
            old_state = current_state.copy()
            old_momentum = momentum.copy()
            old_log_prob = current_log_prob

            # Leapfrogç§¯åˆ†
            # åŠæ­¥åŠ¨é‡æ›´æ–°
            momentum = momentum + 0.5 * step_size * self.grad_log_posterior(current_state)

            # å®Œæ•´ä½ç½®æ›´æ–°
            for _ in range(n_leapfrog):
                current_state = current_state + step_size * momentum
                if _ < n_leapfrog - 1:
                    momentum = momentum + step_size * self.grad_log_posterior(current_state)

            # æœ€ååŠæ­¥åŠ¨é‡æ›´æ–°
            momentum = momentum + 0.5 * step_size * self.grad_log_posterior(current_state)

            # è®¡ç®—æ¥å—æ¦‚ç‡
            current_log_prob = self.log_posterior(current_state)

            # Metropolis-Hastingsæ¥å—
            log_accept_prob = (current_log_prob - old_log_prob -
                             0.5 * np.sum(momentum**2) +
                             0.5 * np.sum(old_momentum**2))

            if np.log(np.random.rand()) < log_accept_prob:
                # æ¥å—
                pass
            else:
                # æ‹’ç»,æ¢å¤æ—§çŠ¶æ€
                current_state = old_state
                current_log_prob = old_log_prob

            # ä¿å­˜æ ·æœ¬ (warmupå)
            if i >= n_warmup:
                samples.append(current_state.copy())

        return np.array(samples)

    def gibbs_sample(self, initial_state, n_samples=1000, n_warmup=500):
        """
        Gibbsé‡‡æ ·

        é€‚ç”¨äºæ¡ä»¶åˆ†å¸ƒæ˜“é‡‡æ ·çš„æƒ…å†µ
        """
        samples = []
        current = initial_state.copy()

        for i in range(n_warmup + n_samples):
            # å¯¹æ¯ä¸ªç»´åº¦ä¾æ¬¡é‡‡æ ·
            for d in range(len(current)):
                # ä»æ¡ä»¶åˆ†å¸ƒ P(Îº_d | Îº_{-d}, data) é‡‡æ ·
                current[d] = self.sample_conditional(d, current)

            if i >= n_warmup:
                samples.append(current.copy())

        return np.array(samples)

    def sample_conditional(self, dim, current_state):
        """é‡‡æ ·æ¡ä»¶åˆ†å¸ƒ (éœ€è¦å…·ä½“å®ç°)"""
        # ç®€åŒ–ä¸ºé«˜æ–¯æè®®
        current_val = current_state[dim]
        proposal = current_val + 0.1 * np.random.randn()

        # è®¡ç®—æ¥å—æ¦‚ç‡
        old_log_prob = self.log_posterior(current_state)

        new_state = current_state.copy()
        new_state[dim] = proposal
        new_log_prob = self.log_posterior(new_state)

        if np.log(np.random.rand()) < (new_log_prob - old_log_prob):
            return proposal
        else:
            return current_val

    def variational_posterior_sample(self, mean, var, n_samples=1000):
        """
        ä»å˜åˆ†åéªŒé‡‡æ ·

        å‡è®¾é«˜æ–¯å˜åˆ†åˆ†å¸ƒ
        """
        std = np.sqrt(var)
        samples = mean + std * np.random.randn(n_samples, len(mean))
        return samples
```

---

### æ ¸å¿ƒç»„ä»¶2: å¯ä¿¡åŒºé—´è®¡ç®—

```python
class CredibleIntervalCalculator:
    """
    å¯ä¿¡åŒºé—´è®¡ç®—å™¨
    """

    def __init__(self, samples):
        """
        Args:
            samples: åéªŒæ ·æœ¬ (n_samples, n_dims)
        """
        self.samples = samples

    def equal_tailed_interval(self, alpha=0.05):
        """
        ç­‰å°¾å¯ä¿¡åŒºé—´

        ä½¿ç”¨æ ·æœ¬åˆ†ä½æ•°
        """
        lower_percentile = 100 * alpha / 2
        upper_percentile = 100 * (1 - alpha / 2)

        lower = np.percentile(self.samples, lower_percentile, axis=0)
        upper = np.percentile(self.samples, upper_percentile, axis=0)

        return lower, upper

    def hpd_interval(self, alpha=0.05):
        """
        æœ€é«˜åéªŒå¯†åº¦åŒºé—´

        ä½¿ç”¨ç½‘æ ¼æœç´¢æ‰¾æœ€é«˜å¯†åº¦åŒºåŸŸ
        """
        n_dims = self.samples.shape[1]
        intervals = []

        for d in range(n_dims):
            samples_d = self.samples[:, d]

            # æ ¸å¯†åº¦ä¼°è®¡
            from scipy.stats import gaussian_kde
            kde = gaussian_kde(samples_d)

            # è¯„ä¼°å¯†åº¦
            x_range = np.linspace(samples_d.min(), samples_d.max(), 1000)
            density = kde(x_range)

            # æ‰¾HPDåŒºé—´
            sorted_indices = np.argsort(density)[::-1]
            cumulative_prob = np.cumsum(density[sorted_indices])
            cumulative_prob /= cumulative_prob[-1]

            # æ‰¾åˆ°è¦†ç›–1-alphaæ¦‚ç‡çš„é˜ˆå€¼
            threshold_idx = np.where(cumulative_prob >= 1 - alpha)[0][0]
            density_threshold = density[sorted_indices[threshold_idx]]

            # HPDåŒºé—´æ˜¯å¯†åº¦å¤§äºé˜ˆå€¼çš„åŒºåŸŸ
            hpd_mask = density >= density_threshold
            hpd_regions = x_range[hpd_mask]

            if len(hpd_regions) > 0:
                intervals.append((hpd_regions.min(), hpd_regions.max()))
            else:
                intervals.append((np.median(samples_d), np.median(samples_d)))

        lower = np.array([i[0] for i in intervals])
        upper = np.array([i[1] for i in intervals])

        return lower, upper

    def simultaneous_credible_region(self, alpha=0.05, method='bonferroni'):
        """
        è”åˆå¯ä¿¡åŒºåŸŸ

        è€ƒè™‘å¤šå‚æ•°çš„ç›¸å…³æ€§
        """
        n_dims = self.samples.shape[1]

        if method == 'bonferroni':
            # Bonferroniæ ¡æ­£
            alpha_adjusted = alpha / n_dims
            return self.equal_tailed_interval(alpha_adjusted)

        elif method == 'mvn':
            # å¤šå…ƒæ­£æ€è¿‘ä¼¼
            mean = np.mean(self.samples, axis=0)
            cov = np.cov(self.samples.T)

            from scipy.stats import chi2
            threshold = chi2.ppf(1 - alpha, df=n_dims)

            # é©¬æ°è·ç¦»æ¤­çƒ
            return mean, cov, threshold

    def credible_interval_width(self, interval_type='equal_tailed'):
        """è®¡ç®—å¯ä¿¡åŒºé—´å®½åº¦"""
        if interval_type == 'equal_tailed':
            lower, upper = self.equal_tailed_interval()
        else:
            lower, upper = self.hpd_interval()

        return upper - lower

    def coverage_probability(self, true_values):
        """
        è®¡ç®—è¦†ç›–æ¦‚ç‡ (éªŒè¯ç”¨)

        æ£€æŸ¥å¯ä¿¡åŒºé—´æ˜¯å¦åŒ…å«çœŸå€¼
        """
        lower, upper = self.equal_tailed_interval()

        coverage = np.mean((true_values >= lower) & (true_values <= upper))

        return coverage
```

---

### æ ¸å¿ƒç»„ä»¶3: ç©ºé—´å¯ä¿¡åŒºåŸŸ

```python
class SpatialCredibleRegion:
    """
    ç©ºé—´å¯ä¿¡åŒºåŸŸè®¡ç®—

    é’ˆå¯¹2D/3Dè´¨é‡æ˜ å°„
    """

    def __init__(self, kappa_samples):
        """
        Args:
            kappa_samples: åéªŒæ ·æœ¬ (n_samples, H, W)
        """
        self.samples = kappa_samples
        self.n_samples, self.H, self.W = kappa_samples.shape

    def pixelwise_credible_intervals(self, alpha=0.05):
        """
        é€åƒç´ å¯ä¿¡åŒºé—´
        """
        lower = np.percentile(self.samples, 100 * alpha / 2, axis=0)
        upper = np.percentile(self.samples, 100 * (1 - alpha / 2), axis=0)
        median = np.median(self.samples, axis=0)

        return {
            'median': median,
            'lower': lower,
            'upper': upper,
            'width': upper - lower
        }

    def significant_regions(self, threshold=0.95):
        """
        è¯†åˆ«æ˜¾è‘—åç¦»é›¶çš„åŒºåŸŸ

        è®¡ç®—P(Îº > 0 | data) æˆ– P(Îº < 0 | data)
        """
        prob_positive = np.mean(self.samples > 0, axis=0)
        prob_negative = np.mean(self.samples < 0, axis=0)

        # æ˜¾è‘—æ­£åŒºåŸŸ
        sig_positive = prob_positive > threshold

        # æ˜¾è‘—è´ŸåŒºåŸŸ
        sig_negative = prob_negative > threshold

        return {
            'prob_positive': prob_positive,
            'prob_negative': prob_negative,
            'significant_positive': sig_positive,
            'significant_negative': sig_negative
        }

    def cluster_credible_regions(self, min_cluster_size=10):
        """
        èšç±»å¯ä¿¡åŒºåŸŸ

        è¯†åˆ«è¿é€šçš„æ˜¾è‘—åŒºåŸŸ
        """
        from scipy import ndimage

        sig_regions = self.significant_regions()

        # æ ‡è®°è¿é€šåŒºåŸŸ
        labeled_pos, n_pos = ndimage.label(sig_regions['significant_positive'])
        labeled_neg, n_neg = ndimage.label(sig_regions['significant_negative'])

        clusters = []

        # åˆ†ææ­£åŒºåŸŸ
        for i in range(1, n_pos + 1):
            cluster_mask = labeled_pos == i
            size = np.sum(cluster_mask)

            if size >= min_cluster_size:
                cluster_samples = self.samples[:, cluster_mask]
                clusters.append({
                    'type': 'positive',
                    'size': size,
                    'mean_kappa': np.mean(cluster_samples),
                    'std_kappa': np.std(cluster_samples),
                    'mask': cluster_mask
                })

        # åˆ†æè´ŸåŒºåŸŸ
        for i in range(1, n_neg + 1):
            cluster_mask = labeled_neg == i
            size = np.sum(cluster_mask)

            if size >= min_cluster_size:
                cluster_samples = self.samples[:, cluster_mask]
                clusters.append({
                    'type': 'negative',
                    'size': size,
                    'mean_kappa': np.mean(cluster_samples),
                    'std_kappa': np.std(cluster_samples),
                    'mask': cluster_mask
                })

        return clusters

    def visualize_credible_intervals(self, save_path=None):
        """
        å¯è§†åŒ–å¯ä¿¡åŒºé—´
        """
        import matplotlib.pyplot as plt

        intervals = self.pixelwise_credible_intervals()
        sig_regions = self.significant_regions()

        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        # ä¸­ä½æ•°ä¼°è®¡
        im1 = axes[0, 0].imshow(intervals['median'], cmap='RdBu_r')
        axes[0, 0].set_title('Posterior Median')
        plt.colorbar(im1, ax=axes[0, 0])

        # ä¸‹ç•Œ
        im2 = axes[0, 1].imshow(intervals['lower'], cmap='RdBu_r')
        axes[0, 1].set_title('95% Credible Interval Lower')
        plt.colorbar(im2, ax=axes[0, 1])

        # ä¸Šç•Œ
        im3 = axes[0, 2].imshow(intervals['upper'], cmap='RdBu_r')
        axes[0, 2].set_title('95% Credible Interval Upper')
        plt.colorbar(im3, ax=axes[0, 2])

        # åŒºé—´å®½åº¦
        im4 = axes[1, 0].imshow(intervals['width'], cmap='viridis')
        axes[1, 0].set_title('Credible Interval Width')
        plt.colorbar(im4, ax=axes[1, 0])

        # æ­£æ˜¾è‘—æ€§æ¦‚ç‡
        im5 = axes[1, 1].imshow(sig_regions['prob_positive'], cmap='hot', vmin=0, vmax=1)
        axes[1, 1].set_title('P(Îº > 0 | data)')
        plt.colorbar(im5, ax=axes[1, 1])

        # è´Ÿæ˜¾è‘—æ€§æ¦‚ç‡
        im6 = axes[1, 2].imshow(sig_regions['prob_negative'], cmap='hot', vmin=0, vmax=1)
        axes[1, 2].set_title('P(Îº < 0 | data)')
        plt.colorbar(im6, ax=axes[1, 2])

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        else:
            plt.show()

        plt.close()
```

---

## ğŸ“Š å®éªŒç»“æœ

### å¯ä¿¡åŒºé—´è¦†ç›–éªŒè¯

| æ–¹æ³• | åä¹‰è¦†ç›– | å®é™…è¦†ç›– | å¹³å‡å®½åº¦ |
|:---|:---:|:---:|:---:|
| ç­‰å°¾åŒºé—´ | 95% | 94.2% | 0.42 |
| HPDåŒºé—´ | 95% | 95.1% | 0.38 |
| Bonferroni | 95% | 98.5% | 0.55 |

### å³°å€¼ä½ç½®ä¸ç¡®å®šæ€§

| å³°å€¼ç±»å‹ | ä½ç½®è¯¯å·® | è´¨é‡è¯¯å·® |
|:---|:---:|:---:|
| å­¤ç«‹å³°å€¼ | Â±0.5åƒç´  | Â±15% |
| é‡å å³°å€¼ | Â±1.2åƒç´  | Â±25% |
| å¼±ä¿¡å· | Â±2.0åƒç´  | Â±40% |

---

## ğŸ’¡ å¯¹äº•ç›–æ£€æµ‹çš„å¯ç¤º

### æ£€æµ‹ä¸ç¡®å®šæ€§é‡åŒ–

```python
class DetectionUncertaintyEstimator:
    """
    æ£€æµ‹ä¸ç¡®å®šæ€§ä¼°è®¡

    å€Ÿé‰´å¯ä¿¡åŒºé—´æ€æƒ³
    """

    def __init__(self, model):
        self.model = model

    def estimate_uncertainty(self, image, n_samples=100):
        """
        ä¼°è®¡æ£€æµ‹ç»“æœçš„ä¸ç¡®å®šæ€§

        ä½¿ç”¨MC Dropoutæˆ–é›†æˆæ–¹æ³•
        """
        predictions = []

        # MC Dropouté‡‡æ ·
        self.model.train()  # å¯ç”¨dropout
        for _ in range(n_samples):
            with torch.no_grad():
                pred = self.model(image)
                predictions.append(pred)

        predictions = torch.stack(predictions)

        # è®¡ç®—ç»Ÿè®¡é‡
        mean_pred = predictions.mean(dim=0)
        std_pred = predictions.std(dim=0)

        # å¯ä¿¡åŒºé—´
        lower = torch.quantile(predictions, 0.025, dim=0)
        upper = torch.quantile(predictions, 0.975, dim=0)

        return {
            'mean': mean_pred,
            'std': std_pred,
            'lower': lower,
            'upper': upper
        }

    def reliable_detection(self, image, confidence_threshold=0.95):
        """
        å¯é æ£€æµ‹

        åªåœ¨é«˜ç½®ä¿¡åº¦åŒºåŸŸæŠ¥å‘Šæ£€æµ‹ç»“æœ
        """
        uncertainty = self.estimate_uncertainty(image)

        # åŒºé—´å®½åº¦ä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
        interval_width = uncertainty['upper'] - uncertainty['lower']

        # é€‰æ‹©ä½ä¸ç¡®å®šæ€§åŒºåŸŸ
        reliable_mask = interval_width < (1 - confidence_threshold)

        return {
            'detection': uncertainty['mean'],
            'reliable_mask': reliable_mask,
            'uncertainty': interval_width
        }
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **å¯ä¿¡åŒºé—´** | Credible Interval | è´å¶æ–¯åéªŒæ¦‚ç‡åŒºé—´ |
| **HPD** | Highest Posterior Density | æœ€é«˜åéªŒå¯†åº¦ |
| **MCMC** | Markov Chain Monte Carlo | é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›é‡‡æ · |
| **HMC** | Hamiltonian Monte Carlo | å“ˆå¯†é¡¿è’™ç‰¹å¡æ´› |
| **è¦†ç›–æ¦‚ç‡** | Coverage Probability | åŒºé—´åŒ…å«çœŸå€¼çš„æ¦‚ç‡ |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å¯ä¿¡åŒºé—´çš„è´å¶æ–¯è§£é‡Š
- [ ] æŒæ¡ç­‰å°¾åŒºé—´å’ŒHPDåŒºé—´çš„è®¡ç®—
- [ ] äº†è§£MCMCé‡‡æ ·æ–¹æ³•
- [ ] ç†è§£ç©ºé—´å¯ä¿¡åŒºåŸŸçš„æ¦‚å¿µ

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
