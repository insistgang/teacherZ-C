# [2-21] æ‰©æ•£æ¨¡å‹è„‘MRIç—…å˜ Diffusion Brain MRI - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Diffusion Model for Brain MRI Lesion Segmentation
> **ä½œè€…**: Xiaohao Cai, et al.
> **å‡ºå¤„**: Medical Image Analysis (MedIA) / IEEE Transactions on Medical Imaging
> **å¹´ä»½**: 2022
> **ç±»å‹**: åŒ»å­¦å›¾åƒ + æ‰©æ•£æ¨¡å‹
> **ç²¾è¯»æ—¥æœŸ**: 2026å¹´2æœˆ9æ—¥

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

### å…ƒæ•°æ®
| é¡¹ç›® | å†…å®¹ |
|:---|:|
| **ç±»å‹** | æ–¹æ³•åˆ›æ–° (Method Innovation) |
| **é¢†åŸŸ** | åŒ»å­¦å›¾åƒ + æ‰©æ•£æ¨¡å‹ |
| **èŒƒå›´** | è„‘MRIç—…å˜åˆ†å‰² |
| **é‡è¦æ€§** | â˜…â˜…â˜…â˜…â˜† (æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒçš„åº”ç”¨) |
| **ç‰¹ç‚¹** | æ‰©æ•£è¿‡ç¨‹ã€ç—…å˜æ£€æµ‹ã€ä¸ç¡®å®šæ€§ä¼°è®¡ |

### å…³é”®è¯
- **Diffusion Model** - æ‰©æ•£æ¨¡å‹
- **Brain MRI** - è„‘éƒ¨æ ¸ç£å…±æŒ¯
- **Lesion Segmentation** - ç—…å˜åˆ†å‰²
- **Medical Image** - åŒ»å­¦å›¾åƒ
- **Probabilistic Segmentation** - æ¦‚ç‡åˆ†å‰²
- **Uncertainty Quantification** - ä¸ç¡®å®šæ€§é‡åŒ–

---

## ğŸ¯ ç ”ç©¶èƒŒæ™¯ä¸æ„ä¹‰

### 1.1 è®ºæ–‡å®šä½

**è¿™æ˜¯ä»€ä¹ˆï¼Ÿ**
- ä¸€ç¯‡å…³äº**è„‘MRIç—…å˜åˆ†å‰²**çš„åŒ»å­¦å›¾åƒè®ºæ–‡
- å°†**æ‰©æ•£æ¨¡å‹(Diffusion Model)**åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²
- æå‡ºæ¦‚ç‡åˆ†å‰²æ¡†æ¶,å¸¦ä¸ç¡®å®šæ€§ä¼°è®¡

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
```
è„‘MRIç—…å˜åˆ†å‰²æŒ‘æˆ˜:
â”œâ”€â”€ ç—…å˜å½¢çŠ¶ä¸è§„åˆ™
â”œâ”€â”€ è¾¹ç•Œæ¨¡ç³Š
â”œâ”€â”€ ä¸æ­£å¸¸ç»„ç»‡å¯¹æ¯”åº¦ä½
â”œâ”€â”€ å°ºå¯¸å’Œä½ç½®å˜åŒ–å¤§
â””â”€â”€ 3Dä½“ç§¯æ•°æ®è®¡ç®—å¤æ‚

æ‰©æ•£æ¨¡å‹ä¼˜åŠ¿:
â”œâ”€â”€ æ¦‚ç‡ç”Ÿæˆæ¨¡å‹
â”œâ”€â”€ ä¼¼ç„¶ä¼°è®¡è‡ªç„¶
â”œâ”€â”€ ä¸ç¡®å®šæ€§é‡åŒ–
â””â”€â”€ ç”Ÿæˆè´¨é‡é«˜
```

### 1.2 æ‰©æ•£æ¨¡å‹åŸºç¡€

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ‰©æ•£æ¨¡å‹ (Diffusion Model)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  æ ¸å¿ƒæ€æƒ³: é€šè¿‡é€æ­¥å»å™ªç”Ÿæˆæ•°æ®                          â”‚
â”‚                                                         â”‚
â”‚  å‰å‘è¿‡ç¨‹ (é‡‡æ ·):                                        â”‚
â”‚  x_T ~ N(0, I)  (çº¯å™ªå£°)                                  â”‚
â”‚       â†“                                                 â”‚
â”‚  x_{T-1} = denoise(x_T, T-1)                            â”‚
â”‚  x_{T-2} = denoise(x_{T-1}, T-2)                          â”‚
â”‚       â†“                                                 â”‚
â”‚  ...                                                     â”‚
â”‚  x_0 = denoise(x_1, 0)  (æ•°æ®æ ·æœ¬)                        â”‚
â”‚                                                         â”‚
â”‚  è®­ç»ƒç›®æ ‡: å­¦ä¹ å»å™ªç½‘ç»œ                                   â”‚
â”‚  Îµ_Î¸(x_t, t) = x_{t-1} + sqrt(1-Î²_t)Îµ                    â”‚
â”‚                                                         â”‚
â”‚  å…¶ä¸­:                                                  â”‚
â”‚  â”œâ”€â”€ Î²_t: å™ªå£°è°ƒåº¦                                      â”‚
â”‚  â”œâ”€â”€ Îµ: æ ‡å‡†å™ªå£°                                          â”‚
â”‚  â””â”€â”€ Î¸: ç½‘ç»œå‚æ•°                                         â”‚
â”‚                                                         â”‚
â”‚  åº”ç”¨åˆ°åˆ†å‰²:                                            â”‚
â”‚  â”œâ”€â”€ è¾“å…¥: å™ªå£°+æ¡ä»¶å›¾åƒ                                â”‚
â”‚  â”œâ”€â”€ è¾“å‡º: åˆ†å‰²æ©ç                                      â”‚
â”‚  â””â”€â”€ æ¡ä»¶: è¾“å…¥å›¾åƒä½œä¸ºæ¡ä»¶                             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ æ–¹æ³•è®ºæ¡†æ¶

### 2.1 æ ¸å¿ƒæ€æƒ³

#### æ¡ä»¶æ‰©æ•£æ¨¡å‹

```
æ ‡å‡†æ‰©æ•£æ¨¡å‹: æ— æ¡ä»¶ç”Ÿæˆ

æ¡ä»¶æ‰©æ•£æ¨¡å‹: p(x|c)
â”œâ”€â”€ x: è¦ç”Ÿæˆçš„æ•°æ® (åˆ†å‰²æ©ç )
â”œâ”€â”€ c: æ¡ä»¶ä¿¡æ¯ (è¾“å…¥å›¾åƒ)
â””â”€â”€ ç›®æ ‡: ç”Ÿæˆç»™å®šå›¾åƒçš„åˆ†å‰²

æ¡ä»¶æ–¹å¼:
â”œâ”€â”€ concat: æ¡ä»¶æ‹¼æ¥åˆ°å™ªå£°
â”œâ”€â”€ attention: æ³¨æ„åŠ›æœºåˆ¶èåˆæ¡ä»¶
â””â”€â”€ classifier-free: å¼•å¯¼æ‰©æ•£è¿‡ç¨‹
```

#### åŒ»å­¦å›¾åƒç‰¹æ®Šè€ƒè™‘

```
åŒ»å­¦å›¾åƒç‰¹ç‚¹:
â”œâ”€â”€ é«˜åˆ†è¾¨ç‡ (3Dä½“ç§¯)
â”œâ”€â”€ ç»“æ„å¤æ‚ (è„‘éƒ¨è§£å‰–)
â”œâ”€â”€ ç—…å˜å°ç›®æ ‡
â””â”€â”€ éœ€è¦ç²¾ç¡®è¾¹ç•Œ

æ‰©æ•£æ¨¡å‹é€‚é…:
â”œâ”€â”€ 3Då·ç§¯å¤„ç†ä½“ç§¯æ•°æ®
â”œâ”€â”€ å¤šå°ºåº¦ç‰¹å¾æå–
â”œâ”€â”€ è§£ç å™¨èåˆè§£å‰–å…ˆéªŒ
â””â”€â”€ æ»‘åŠ¨çª—å£æ¨ç†
```

### 2.2 ç½‘ç»œæ¶æ„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DiffusionUNet3D(nn.Module):
    """
    3Dæ‰©æ•£UNetç”¨äºè„‘MRIç—…å˜åˆ†å‰²
    """

    def __init__(
        self,
        in_channels=4,  # T1 + T2 + FLAIR + æ¡ä»¶ç¼–ç 
        out_channels=1,  # åˆ†å‰²æ©ç 
        base_channels=64,
        time_dim=256
    ):
        super().__init__()

        # æ—¶é—´æ­¥åµŒå…¥
        self.time_mlp = nn.Sequential(
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim)
        )

        # ç¼–ç å™¨
        self.enc1 = self._make_down_block(in_channels, base_channels)
        self.enc2 = self._make_down_block(base_channels, base_channels*2)
        self.enc3 = self._make_down_block(base_channels*2, base_channels*4)
        self.enc4 = self._make_down_block(base_channels*4, base_channels*8)

        # ç“¶é¢ˆ
        self.bottleneck = self._make_bottleneck(base_channels*8, time_dim)

        # è§£ç å™¨
        self.dec4 = self._make_up_block(base_channels*16, base_channels*4)
        self.dec3 = self._make_up_block(base_channels*8, base_channels*2)
        self.dec2 = self._make_up_block(base_channels*4, base_channels)
        self.dec1 = self._make_up_block(base_channels*2, out_channels)

    def _make_down_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv3d(in_channels, out_channels, 3, padding=1),
            nn.GroupNorm(8, out_channels),
            nn.SiLU(),
            nn.Conv3d(out_channels, out_channels, 3, padding=1),
            nn.GroupNorm(8, out_channels),
            nn.SiLU(),
            nn.Conv3d(out_channels, out_channels, 2, stride=2)
        )

    def _make_up_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose3d(in_channels, out_channels, 2, stride=2),
            nn.GroupNorm(8, out_channels),
            nn.SiLU(),
            nn.Conv3d(out_channels, out_channels, 3, padding=1),
            nn.GroupNorm(8, out_channels),
            nn.SiLU(),
            nn.Conv3d(out_channels, out_channels, 3, padding=1)
        )

    def _make_bottleneck(self, in_channels, time_dim):
        return nn.Sequential(
            nn.Conv3d(in_channels, in_channels, 3, padding=1),
            nn.GroupNorm(8, in_channels),
            nn.SiLU(),
            nn.Conv3d(in_channels, in_channels, 3, padding=1),
            nn.GroupNorm(8, in_channels)
        )

    def forward(self, x, t):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            x: è¾“å…¥ (B, C, D, H, W)
            t: æ—¶é—´æ­¥ (B,)
        """
        # æ—¶é—´åµŒå…¥
        t_emb = self.time_mlp(t)  # (B, time_dim)

        # ç¼–ç 
        e1 = self.enc1(x)
        e2 = self.enc2(e1)
        e3 = self.enc3(e2)
        e4 = self.enc4(e3)

        # ç“¶é¢ˆ (èå…¥æ—¶é—´ä¿¡æ¯)
        # ç®€åŒ–: ç›´æ¥åŠ æ³• (å®é™…éœ€è¦æ›´å¤æ‚çš„èåˆ)
        b = self.bottleneck(e4)

        # è§£ç 
        d4 = self.dec4(b)
        d3 = self.dec3(d4)
        d2 = self.dec2(d3)
        d1 = self.dec1(d2)

        return d1


class ConditionalDiffusionModel:
    """
    æ¡ä»¶æ‰©æ•£æ¨¡å‹
    """

    def __init__(
        self,
        unet,
        n_timesteps=1000,
        beta_start=0.0001,
        beta_end=0.02
    ):
        self.unet = unet
        self.n_timesteps = n_timesteps

        # å™ªå£°è°ƒåº¦
        self.betas = torch.linspace(beta_start, beta_end, n_timesteps)
        self.alphasas = 1 - self.betas
        self.alphasas_cumprod = torch.cumprod(self.alphasas, dim=0)

    def get_time_embedding(self, t, batch_size):
        """æ­£å¼¦æ—¶é—´åµŒå…¥"""
        # ä½ç½®ç¼–ç 
        half_dim = 256 // 2

        frequencies = torch.arange(
            half_dim, dtype=torch.float32
        ) / (10000 ** (torch.arange(0, half_dim, 2).float() / half_dim))

        args = t[:, None].float() * frequencies[None, :]
        embedding = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)

        return embedding

    def forward_diffusion(self, x0, t):
        """
        å‰å‘æ‰©æ•£è¿‡ç¨‹ (è®­ç»ƒæ—¶ä½¿ç”¨)

        å‚æ•°:
            x0: åˆå§‹æ•°æ® (B, C, D, H, W)
            t: æ—¶é—´æ­¥ (B,)
        """
        noise = torch.randn_like(x0)
        alpha = self.alphasas_cumprod[t]

        # åŠ å™ª
        xt = torch.sqrt(alpha)[:, None, None, None, None] * x0 + \
               torch.sqrt(1 - alpha)[:, None, None, None, None] * noise

        return xt

    def reverse_diffusion(self, xt, t, condition):
        """
        åå‘æ‰©æ•£è¿‡ç¨‹ (é‡‡æ ·æ—¶ä½¿ç”¨)

        å‚æ•°:
            xt: å½“å‰å™ªå£°æ•°æ® (B, C, D, H, W)
            t: æ—¶é—´æ­¥ (B,)
            condition: æ¡ä»¶å›¾åƒ (B, C_cond, D, H, W)
        """
        # æ‹¼æ¥æ¡ä»¶å’Œå™ªå£°æ•°æ®
        x_in = torch.cat([xt, condition], dim=1)

        # é¢„æµ‹å™ªå£°
        time_emb = self.get_time_embedding(t, xt.shape[0])

        # UNeté¢„æµ‹
        predicted_noise = self.unet(x_in, time_emb)

        return predicted_noise

    def sample(self, condition, n_samples=1):
        """
        ä»å™ªå£°é‡‡æ ·åˆ†å‰²

        å‚æ•°:
            condition: æ¡ä»¶å›¾åƒ (B, C_cond, D, H, W)
            n_samples: é‡‡æ ·æ•°é‡
        """
        device = condition.device
        batch_size = condition.shape[0]

        # ä»çº¯å™ªå£°å¼€å§‹
        xt = torch.randn(batch_size, 1, *condition.shape[2:]).to(device)

        # é€æ­¥å»å™ª
        for t in reversed(range(self.n_timesteps)):
            t_tensor = torch.full((batch_size,), t, device=device).long()

            # é¢„æµ‹å™ªå£°
            predicted_noise = self.reverse_diffusion(xt, t_tensor, condition)

            # å»å™ªæ­¥éª¤
            alpha = self.alphasas[t]
            alpha_prev = self.alphasas[t-1] if t > 0 else torch.tensor(1.0)

            xt = (xt - torch.sqrt(1 - alpha) * predicted_noise) / torch.sqrt(alpha)
            xt = torch.clamp(xt, -1, 1)

        return xt
```

### 2.3 è®­ç»ƒè¿‡ç¨‹

```python
class DiffusionTrainer:
    """
    æ‰©æ•£æ¨¡å‹è®­ç»ƒå™¨
    """

    def __init__(self, model, n_timesteps=1000, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.n_timesteps = n_timesteps

        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    def train_step(self, images, segmentations):
        """
        å•æ¬¡è®­ç»ƒæ­¥éª¤

        å‚æ•°:
            images: è¾“å…¥å›¾åƒ (B, C, D, H, W)
            segmentations: çœŸå®åˆ†å‰² (B, 1, D, H, W)
        """
        batch_size = images.shape[0]

        # éšæœºé‡‡æ ·æ—¶é—´æ­¥
        t = torch.randint(0, self.n_timesteps, (batch_size,), device=self.device)

        # åŠ å™ª
        noisy_seg = self.model.forward_diffusion(segmentations, t)

        # é¢„æµ‹å™ªå£°
        predicted_noise = self.model.reverse_diffusion(noisy_seg, t, images)

        # æŸå¤±: é¢„æµ‹å™ªå£°ä¸å®é™…å™ªå£°çš„å·®å¼‚
        # å®é™…å™ªå£° = åŠ å™ªåˆ†å‰² - çº¯å™ªå£°Ã—åˆ†å‰²
        alpha = self.model.alphasas_cumprod[t]
        actual_noise = (noisy_seg - torch.sqrt(alpha[:, None, None, None, None] * segmentations) / \
                     torch.sqrt(1 - alpha[:, None, None, None, None])

        loss = F.mse_loss(predicted_noise, actual_noise)

        return loss

    def train(self, dataloader, num_epochs=100):
        """å®Œæ•´è®­ç»ƒ"""
        for epoch in range(num_epochs):
            total_loss = 0

            for batch in dataloader:
                images = batch['image'].to(self.device)
                segs = batch['segmentation'].to(self.device)

                loss = self.train_step(images, segs)

                self.optimizer.zero_grad()
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch}: Loss = {avg_loss:.6f}")
```

---

## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°ä¸€: è§£å‰–å­¦æ„ŸçŸ¥æ‰©æ•£

```python
class AnatomyAwareDiffusion:
    """
    è§£å‰–å­¦æ„ŸçŸ¥çš„æ‰©æ•£æ¨¡å‹

    åˆ©ç”¨è„‘éƒ¨è§£å‰–ç»“æ„å¼•å¯¼åˆ†å‰²
    """

    def __init__(self, base_diffusion, atlas_path):
        """
        å‚æ•°:
            base_diffusion: åŸºç¡€æ‰©æ•£æ¨¡å‹
            atlas_path: è„‘è§£å‰–å›¾è°±
        """
        self.base_diffusion = base_diffusion
        self.atlas = self._load_atlas(atlas_path)

        # è§£å‰–åŒºåŸŸå®šä¹‰
        self.anatomy_regions = {
            'gray_matter': [0, 1],      # ç°è´¨
            'white_matter': [2, 3],      # ç™½è´¨
            'csf': [4],                   # è„‘è„Šæ¶²
            'ventricles': [5],             # è„‘å®¤
            'lesion': [6]                  # ç—…å˜ (ç›®æ ‡)
        }

    def _load_atlas(self, atlas_path):
        """åŠ è½½è„‘è§£å‰–å›¾è°±"""
        # ç®€åŒ–: è¿”å›åŒºåŸŸmask
        # å®é™…éœ€è¦åŠ è½½é¢„å®šä¹‰çš„å›¾è°±
        return None

    def forward_with_anatomy(self, condition, anatomy_prior):
        """
        ä½¿ç”¨è§£å‰–å…ˆéªŒçš„å‰å‘ä¼ æ’­
        """
        # èåˆè§£å‰–å…ˆéªŒ
        anatomy_features = self._extract_anatomy_features(anatomy_prior)

        # å°†è§£å‰–ä¿¡æ¯æ³¨å…¥æ¡ä»¶
        enhanced_condition = torch.cat([condition, anatomy_features], dim=1)

        # ä½¿ç”¨å¢å¼ºçš„æ¡ä»¶
        return self.base_diffusion(enhanced_condition)

    def _extract_anatomy_features(self, anatomy_prior):
        """
        ä»è§£å‰–å…ˆéªŒä¸­æå–ç‰¹å¾

        åŒ…æ‹¬:
        - ç»„ç»‡ç±»å‹æ¦‚ç‡å›¾
        - è§£å‰–ç»“æ„è¾¹ç•Œ
        - ç©ºé—´ä½ç½®å…ˆéªŒ
        """
        # ç®€åŒ–å®ç°
        return anatomy_prior
```

### åˆ›æ–°äºŒ: ä¸ç¡®å®šæ€§é‡åŒ–

```python
class UncertaintyQuantification:
    """
    ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—
    """

    @staticmethod
    def monte_carlo_dropout(model, condition, n_samples=10):
        """
        Monte Carlo Dropoutä¼°è®¡ä¸ç¡®å®šæ€§

        å‚æ•°:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            condition: æ¡ä»¶å›¾åƒ
            n_samples: é‡‡æ ·æ¬¡æ•°

        è¿”å›:
            mean_prediction: å¹³å‡é¢„æµ‹
            uncertainty: ä¸ç¡®å®šæ€§åœ°å›¾
        """
        model.eval()

        predictions = []

        with torch.no_grad():
            for _ in range(n_samples):
                model.train()  # å¯ç”¨dropout
                pred = model.sample(condition, n_samples=1)
                predictions.append(pred)

        # è®¡ç®—ç»Ÿè®¡é‡
        predictions = torch.cat(predictions, dim=0)
        mean = predictions.mean(dim=0)
        std = predictions.std(dim=0)

        # ä¸ç¡®å®šæ€§ = æ ‡å‡†å·®
        uncertainty = std

        return mean, uncertainty

    @staticmethod
    def ensemble_uncertainty(models, condition):
        """
        é›†æˆä¸ç¡®å®šæ€§
        """
        predictions = []

        for model in models:
            pred = model.sample(condition, n_samples=1)
            predictions.append(pred)

        predictions = torch.cat(predictions, dim=0)
        mean = predictions.mean(dim=0)
        uncertainty = predictions.std(dim=0)

        return mean, uncertainty
```

### åˆ›æ–°ä¸‰: å¤šå°ºåº¦å¤„ç†

```python
class MultiScaleDiffusion(nn.Module):
    """
    å¤šå°ºåº¦æ‰©æ•£æ¨¡å‹

    åœ¨å¤šä¸ªåˆ†è¾¨ç‡ä¸Šè¿›è¡Œåˆ†å‰²
    """

    def __init__(self, in_channels=4, scales=[0.5, 1.0, 2.0]):
        super().__init__()

        self.scales = scales
        self.in_channels = in_channels

        # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºUNet
        self.unets = nn.ModuleList([
            DiffusionUNet3D(
                in_channels=in_channels,
                out_channels=1,
                base_channels=32,
                time_dim=128
            )
            for _ in scales
        ])

        # èåˆä¸åŒå°ºåº¦ç»“æœ
        self.fusion = nn.Conv3d(len(scales), 1, 1)

    def forward(self, x, t):
        """
        å¤šå°ºåº¦å‰å‘ä¼ æ’­
        """
        results = []

        for i, (scale, unet) in enumerate(zip(self.scales, self.unets)):
            # ç¼©æ”¾åˆ°ç›®æ ‡å°ºåº¦
            if scale != 1.0:
                size = [int(x.shape[2] * scale),
                       int(x.shape[3] * scale)]
                x_scaled = F.interpolate(x, size=size, mode='trilinear')
            else:
                x_scaled = x

            # å¤„ç†
            result = unet(x_scaled, t)
            results.append(result)

        # ä¸Šé‡‡æ ·å¹¶èåˆ
        fused_results = []
        for i, result in enumerate(results):
            if i > 0:
                # ä¸Šé‡‡æ ·åˆ°åŸå§‹å°ºå¯¸
                result = F.interpolate(result, size=x.shape[2:],
                                         mode='trilinear')
            fused_results.append(result)

        # èåˆ
        stacked = torch.stack(fused_results, dim=2)
        output = self.fusion(stacked)

        return output
```

---

## ğŸ“Š å®éªŒä¸ç»“æœ

### æ•°æ®é›†

| æ•°æ®é›† | æ¨¡æ€ | åˆ†å‰²ç›®æ ‡ | æ¥æº |
|:---|:---|:---|:---|
| **BRATS** | T1/T2/FLAIR | è„‘è‚¿ç˜¤ | å…¬å¼€æ•°æ®é›† |
| **ATLAS** | T1 | å¤šç§ç»“æ„ | å…¬å¼€æ•°æ®é›† |
| **ISLES** | T2/T2-FLAIR | ç¼ºè¡€ç—…ç¶ | å…¬å¼€æ•°æ®é›† |
| **MS lesions** | å¤šæ¨¡æ€ | å¤šå‘ç¡¬åŒ– | å…¬å¼€æ•°æ®é›† |

### ä¸»è¦ç»“æœ

#### Diceç³»æ•°å¯¹æ¯”

| æ–¹æ³• | BRATS | ATLAS | ISLES | MS lesions |
|:---|:---:|:---:|:---:|:---:|
| U-Net | 0.78 | 0.82 | 0.71 | 0.75 |
| nnU-Net | 0.82 | 0.85 | 0.74 | 0.78 |
| Attention U-Net | 0.83 | 0.86 | 0.75 | 0.79 |
| **Diffusion Model** | **0.85** | **0.87** | **0.77** | **0.81** |

#### HD95æŒ‡æ ‡ (Hausdorffè·ç¦»95%)

| æ–¹æ³• | BRATS | ATLAS | ISLES | MS lesions |
|:---|:---:|:---:|:---:|:---:|
| U-Net | 6.2 | 4.8 | 8.1 | 7.5 |
| nnU-Net | 5.8 | 4.2 | 7.6 | 6.9 |
| **Diffusion Model** | **4.5** | **3.6** | **6.8** | **6.2** |

---

## ğŸ’» å¯å¤ç”¨ä»£ç ç»„ä»¶

### å®Œæ•´åº”ç”¨ç¤ºä¾‹

```python
class BrainMRISegmentationApp:
    """
    è„‘MRIç—…å˜åˆ†å‰²å®Œæ•´åº”ç”¨
    """

    def __init__(self, model_path, device='cuda'):
        import torch

        # åŠ è½½æ¨¡å‹
        checkpoint = torch.load(model_path, map_location=device)
        self.model = checkpoint['model']
        self.model.to(device)
        self.model.eval()
        self.device = device

        # é…ç½®
        self.config = checkpoint['config']

    def preprocess(self, mri_volume):
        """
        é¢„å¤„ç†MRIä½“ç§¯
        """
        import numpy as np

        # å½’ä¸€åŒ–
        volume = mri_volume.astype(np.float32)
        mean, std = volume.mean(), volume.std()
        normalized = (volume - mean) / (std + 1e-8)

        return torch.from_numpy(normalized).unsqueeze(0).unsqueeze(0)

    def postprocess(self, segmentation, original_shape):
        """
        åå¤„ç†åˆ†å‰²ç»“æœ
        """
        import numpy as np

        seg = segmentation.squeeze().cpu().numpy()
        seg = (seg > 0.5).astype(np.uint8)

        # å½¢æ€å­¦æ“ä½œ
        from scipy import ndimage

        # å»é™¤å°å™ªå£°
        seg = ndimage.binary_opening(seg, structure=np.ones((3,3,3)))
        seg = ndimage.binary_closing(seg, structure=np.ones((3,3,3)))

        # è¿é€šåŸŸåˆ†æ, ä¿ç•™æœ€å¤§è¿é€šåŒºåŸŸ
        labeled, num_features = ndimage.label(seg)
        if num_features > 1:
            sizes = ndimage.sum(seg == 1)
            seg = (seg == (sizes == sizes.max())).astype(np.uint8)

        return seg

    def segment_volume(self, mri_volume, patch_size=64, overlap=32):
        """
        åˆ†å‰²å¤§ä½“ç§¯æ•°æ®

        ä½¿ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥
        """
        import torch

        D, H, W = mri_volume.shape

        # è®¡ç®—patchä½ç½®
        stride = patch_size - overlap
        patches = []

        for d in range(0, D - patch_size + 1, stride):
            for h in range(0, H - patch_size + 1, stride):
                for w in range(0, W - patch_size + 1, stride):
                    patch = mri_volume[d:d+patch_size,
                                    h:h+patch_size,
                                    w:w+patch_size]
                    patches.append((d, h, w))

        # åˆ†æ‰¹å¤„ç†
        results = []

        with torch.no_grad():
            for d, h, w in patches:
                patch = mri_volume[d:d+patch_size,
                                    h:h+patch_size,
                                    w:w+patch_size]
                patch_tensor = torch.from_numpy(patch).unsqueeze(0).to(self.device)

                # é¢„å¤„ç†
                patch_processed = self.preprocess(patch)

                # é‡‡æ ·
                pred = self.model.sample(patch_processed)
                pred = pred.squeeze().cpu().numpy()

                results.append((d, h, w, pred))

        # é‡æ„å®Œæ•´ä½“ç§¯
        segmentation = np.zeros((D, H, W), dtype=np.float32)

        for d, h, w, pred in results:
            d_end, h_end, w_end = d+patch_size, h+patch_size, w+patch_size

            # å¤„ç†è¾¹ç•Œ
            d_start = min(d, D)
            h_start = min(h, H)
            w_start = min(w, W)
            d_end = min(d_end, D)
            h_end = min(h_end, H)
            w_end = min(w_end, W)

            # è£å‰ªpatch
            pred_patch = pred[
                :d_end-d_start,
                :h_end-h_start,
                :w_end-w_start
            ]

            # æ”¾å…¥ç»“æœ (é‡å åŒºåŸŸå–å¹³å‡)
            segmentation[d_start:d_end, h_start:h_end, w_start:w_end] += pred_patch

        # é‡å åŒºåŸŸå¹³å‡
        weights = np.zeros((D, H, W), dtype=np.float32)
        for d, h, w, _ in results:
            d_end, h_end, w_end = min(d+patch_size, D), \
                                     min(h+patch_size, H), \
                                     min(w+patch_size, W)

            weights[d:d_end, h:h_end, w:w_end] += 1

        segmentation = segmentation / (weights + 1e-8)

        return segmentation
```

---

## ğŸ”— ä¸å…¶ä»–å·¥ä½œçš„å…³ç³»

### ç ”ç©¶è„‰ç»œ

```
åŒ»å­¦å›¾åƒåˆ†å‰²æ¼”è¿›:

[2-20] æ”¾ç–—ç›´è‚ åˆ†å‰²
    â†“ ä¼ ç»Ÿå˜åˆ†æ³•
    â†“
[2-29] ä¸­å¿ƒä½“åˆ†å‰²ç½‘ç»œ
    â†“ æ·±åº¦å­¦ä¹ 
    â†“
[2-21] æ‰©æ•£æ¨¡å‹è„‘MRI â† æœ¬ç¯‡
    â†“ æ‰©æ•£æ¨¡å‹
    â†“
æœªæ¥: ç”Ÿæˆå¼åˆ†å‰²
```

---

## ğŸ“ ä¸ªäººæ€è€ƒä¸æ€»ç»“

### æ ¸å¿ƒæ”¶è·

- **æ‰©æ•£æ¨¡å‹ä¼˜åŠ¿**: æ¦‚ç‡æ¡†æ¶ã€ä¸ç¡®å®šæ€§é‡åŒ–
- **åŒ»å­¦å›¾åƒåº”ç”¨**: è„‘MRIç—…å˜åˆ†å‰²çš„ç‰¹æ®Šè€ƒè™‘
- **å¤šå°ºåº¦å¤„ç†**: å¤„ç†3Dä½“ç§¯æ•°æ®çš„ç­–ç•¥
- **å¯è§£é‡Šæ€§**: ä¸ç¡®å®šæ€§åˆ†æçš„ä¸´åºŠä»·å€¼

---

**ç²¾è¯»å®Œæˆæ—¶é—´**: 2026å¹´2æœˆ9æ—¥
**è®ºæ–‡ç±»å‹**: åŒ»å­¦å›¾åƒ + æ‰©æ•£æ¨¡å‹
**å…³è”è®ºæ–‡**: [2-29] ä¸­å¿ƒä½“åˆ†å‰²ç½‘ç»œ, [2-20] æ”¾ç–—ç›´è‚ åˆ†å‰²

---

*æœ¬ç²¾è¯»ç¬”è®°åŸºäºDiffusion Model for Brain MRI Lesion Segmentationè®ºæ–‡*
*é‡ç‚¹å…³æ³¨: æ‰©æ•£æ¨¡å‹ã€è„‘MRIåˆ†å‰²ã€ä¸ç¡®å®šæ€§é‡åŒ–*
