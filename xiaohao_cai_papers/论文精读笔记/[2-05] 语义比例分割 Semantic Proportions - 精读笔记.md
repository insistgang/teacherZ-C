# [2-05] è¯­ä¹‰æ¯”ä¾‹åˆ†å‰² Semantic Proportions - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Semantic Proportions for Image Segmentation via Convex Relaxation
> **ä½œè€…**: Xiaohao Cai, et al.
> **å‡ºå¤„**: Journal of Scientific Computing (J Sci Comput)
> **å¹´ä»½**: 2017
> **å·æœŸ**: ä¸SLaTè®ºæ–‡åŒæœŸ (Vol. 72)
> **DOI**: 10.1007/s10915-017-0402-x
> **ç±»å‹**: æ–¹æ³•åˆ›æ–°è®ºæ–‡
> **ç²¾è¯»æ—¥æœŸ**: 2026å¹´2æœˆ9æ—¥

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

### å…ƒæ•°æ®
| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **ç±»å‹** | æ–¹æ³•åˆ›æ–° (Method Innovation) |
| **é¢†åŸŸ** | å›¾åƒåˆ†å‰² + å˜åˆ†æ³• |
| **èŒƒå›´** | å¤šç›¸å½©è‰²å›¾åƒåˆ†å‰² |
| **é‡è¦æ€§** | â˜…â˜…â˜…â˜…â˜† (SLaTæ–¹æ³•çš„è¡¥å……ä¸æ‰©å±•) |
| **ç‰¹ç‚¹** | è¯­ä¹‰æ¯”ä¾‹å»ºæ¨¡ã€å‡¸ä¼˜åŒ–ã€å¤šé€šé“èåˆ |

### å…³é”®è¯
- **Semantic Proportions** - è¯­ä¹‰æ¯”ä¾‹
- **Convex Relaxation** - å‡¸æ¾å¼›
- **Multiphase Segmentation** - å¤šç›¸åˆ†å‰²
- **Color Image** - å½©è‰²å›¾åƒ
- **Mumford-Shah Model** - Mumford-Shahæ¨¡å‹
- **Potts Model** - Pottsæ¨¡å‹

---

## ğŸ¯ ç ”ç©¶èƒŒæ™¯ä¸æ„ä¹‰

### 1.1 è®ºæ–‡å®šä½

**è¿™æ˜¯ä»€ä¹ˆï¼Ÿ**
- ä¸€ç¯‡å…³äº**å¤šç›¸å½©è‰²å›¾åƒåˆ†å‰²**çš„æ–¹æ³•è®ºæ–‡
- æå‡º**è¯­ä¹‰æ¯”ä¾‹**çš„æ¦‚å¿µæ¥å»ºæ¨¡åˆ†å‰²é—®é¢˜
- ä¸SLaTè®ºæ–‡åŒæœŸçš„å§Šå¦¹ç¯‡å·¥ä½œ

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
```
å¤šç›¸åˆ†å‰²æŒ‘æˆ˜:
â”œâ”€â”€ ç±»åˆ«æ•°Kè¾ƒå¤§æ—¶è®¡ç®—å¤æ‚
â”œâ”€â”€ å½©è‰²å›¾åƒé€šé“ç›¸å…³æ€§é—®é¢˜
â”œâ”€â”€ ä¸åŒåŒºåŸŸå æ¯”å·®å¼‚å¤§
â””â”€â”€ ä¼ ç»Ÿæ–¹æ³•å¯¹å°ç›®æ ‡ä¸æ•æ„Ÿ

è¯­ä¹‰æ¯”ä¾‹æ–¹æ³•è´¡çŒ®:
â”œâ”€â”€ å¼•å…¥æ¯”ä¾‹å˜é‡
â”œâ”€â”€ å‡¸æ¾å¼›ä¿è¯å…¨å±€æœ€ä¼˜
â”œâ”€â”€ å¯¹å°ç›®æ ‡æ›´æ•æ„Ÿ
â””â”€â”€ å¤šé€šé“æœ‰æ•ˆèåˆ
```

### 1.2 ä¸SLaTçš„å…³ç³»

```
åŒæœŸå·¥ä½œå¯¹æ¯”:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SLaT vs Semantic Proportions            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  SLaT ([2-03]):                                      â”‚
â”‚  â”œâ”€â”€ ä¸‰é˜¶æ®µåˆ†ç¦»è®¾è®¡                                  â”‚
â”‚  â”œâ”€â”€ å…³æ³¨é€€åŒ–å›¾åƒ                                    â”‚
â”‚  â”œâ”€â”€ Liftingæ“ä½œè¡¥å……ä¿¡æ¯                             â”‚
â”‚  â””â”€â”€ é˜¶æ®µ3çµæ´»è°ƒæ•´K                                  â”‚
â”‚                                                     â”‚
â”‚  Semantic Proportions ([2-05]):                     â”‚
â”‚  â”œâ”€â”€ å•é˜¶æ®µå‡¸ä¼˜åŒ–                                    â”‚
â”‚  â”œâ”€â”€ å…³æ³¨è¯­ä¹‰æ¯”ä¾‹                                    â”‚
â”‚  â”œâ”€â”€ æ¯”ä¾‹å˜é‡å»ºæ¨¡                                    â”‚
â”‚  â””â”€â”€ å°ç›®æ ‡æ•æ„Ÿ                                      â”‚
â”‚                                                     â”‚
â”‚  å…±åŒç‚¹:                                             â”‚
â”‚  â”œâ”€â”€ éƒ½åŸºäºMumford-Shah                              â”‚
â”‚  â”œâ”€â”€ éƒ½ä½¿ç”¨å‡¸æ¾å¼›                                    â”‚
â”‚  â”œâ”€â”€ éƒ½å¤„ç†å½©è‰²å›¾åƒ                                  â”‚
â”‚  â””â”€â”€ åŒæœŸåˆŠåŒæœŸå‘è¡¨                                  â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ æ–¹æ³•è®ºæ¡†æ¶

### 2.1 æ ¸å¿ƒæ€æƒ³

#### è¯­ä¹‰æ¯”ä¾‹çš„åŠ¨æœº

```
ä¼ ç»Ÿåˆ†å‰²é—®é¢˜:
  min E(u) = âˆ«|âˆ‡u|Â² + Î»âˆ«(u-f)Â²

  å…¶ä¸­ u âˆˆ {1, 2, ..., K} æ˜¯åˆ†å‰²æ ‡ç­¾

é—®é¢˜:
â”œâ”€â”€ ç¦»æ•£ä¼˜åŒ–ï¼ŒNPéš¾
â”œâ”€â”€ å¯¹å°ç›®æ ‡ä¸æ•æ„Ÿ
â””â”€â”€ ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

è¯­ä¹‰æ¯”ä¾‹æ–¹æ³•:
  min E(u, Î±) = âˆ«|âˆ‡u|Â² + Î»âˆ«(u-f)Â² + Î¼âˆ«(Î±-Î±Ì‚)Â²

  å…¶ä¸­:
  â”œâ”€â”€ u: åˆ†å‰²æ ‡ç­¾
  â””â”€â”€ Î±: è¯­ä¹‰æ¯”ä¾‹å˜é‡

ä¼˜åŠ¿:
â”œâ”€â”€ Î±å¯ä»¥çº¦æŸå„ç›¸æ¯”ä¾‹
â”œâ”€â”€ å¯¹å°ç›®æ ‡æ•æ„Ÿ
â””â”€â”€ å¯ä»¥èå…¥å…ˆéªŒçŸ¥è¯†
```

### 2.2 æ•°å­¦æ¨¡å‹

#### Pottsæ¨¡å‹å›é¡¾

```
æ ‡å‡†Pottsæ¨¡å‹ (å¤šç›¸åˆ†å‰²):

E_Potts(u) = âˆ«_Î©|âˆ‡u|Â² dx + Î£_{k=1}^K Î»_k âˆ«_{u=k} (u - c_k)Â² dx

å…¶ä¸­:
â”œâ”€â”€ u: åˆ†å‰²å‡½æ•° (æ¯ä¸ªåƒç´ åˆ†é…ç±»åˆ«k)
â”œâ”€â”€ c_k: ç¬¬kç±»çš„å‡å€¼
â””â”€â”€ Î»_k: å¹³è¡¡å‚æ•°

é—®é¢˜: éå‡¸ä¼˜åŒ–
```

#### è¯­ä¹‰æ¯”ä¾‹æ¨¡å‹

```
å¼•å…¥æ¯”ä¾‹å˜é‡ Î± = (Î±â‚, Î±â‚‚, ..., Î±_K):

E_SP(u, Î±) = âˆ«_Î©|âˆ‡u|Â² dx
            + Î£_{k=1}^K Î»_k âˆ«_{u=k} (u - c_k)Â² dx
            + Î¼ Î£_{k=1}^K (Î±_k - Î±Ì‚_k)Â²

å…¶ä¸­:
â”œâ”€â”€ Î±_k: ç¬¬kç›¸çš„å®é™…æ¯”ä¾‹
â”œâ”€â”€ Î±Ì‚_k: ç¬¬kç›¸çš„æœŸæœ›æ¯”ä¾‹ (å…ˆéªŒ)
â””â”€â”€ Î¼: æ¯”ä¾‹çº¦æŸæƒé‡

çº¦æŸæ¡ä»¶:
Î£_{k=1}^K Î±_k = 1
```

### 2.3 å‡¸æ¾å¼›

#### æ ‡ç­¾æ¾å¼›

```
åŸå§‹é—®é¢˜: u(x) âˆˆ {1, 2, ..., K}

æ¾å¼›å: å¼•å…¥éš¶å±å‡½æ•° Ï† = (Ï†â‚, ..., Ï†_K)

å…¶ä¸­ Ï†_k(x) âˆˆ [0, 1] è¡¨ç¤ºxå±äºkç±»çš„ç¨‹åº¦

çº¦æŸ: Î£_{k=1}^K Ï†_k(x) = 1
```

#### å‡¸èƒ½é‡å‡½æ•°

```
æ¾å¼›åçš„èƒ½é‡:

E_relaxed(Ï†) = âˆ«_Î© ||âˆ‡Ï†||Â² dx
            + Î£_{k=1}^K Î»_k âˆ«_Î© Ï†_kÂ² (f - c_k)Â² dx
            + Î¼ Î£_{k=1}^K (âˆ«_Î© Ï†_k dx - Î±Ì‚_k|Î©|)Â²

å…¶ä¸­:
â”œâ”€â”€ Ï†: Ké€šé“çš„éš¶å±å‡½æ•°
â”œâ”€â”€ ||âˆ‡Ï†||Â²: å‘é‡å…¨å˜å·®
â”œâ”€â”€ |Î©|: å›¾åƒé¢ç§¯
â””â”€â”€ Î±Ì‚_k|Î©|: ç¬¬kç›¸çš„æœŸæœ›åƒç´ æ•°
```

---

## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°ä¸€: è¯­ä¹‰æ¯”ä¾‹çº¦æŸ

#### æ¯”ä¾‹å…ˆéªŒçš„ä½œç”¨

```python
class SemanticProportionsConstraint:
    """
    è¯­ä¹‰æ¯”ä¾‹çº¦æŸ
    """

    def __init__(self, expected_proportions, weight=1.0):
        """
        å‚æ•°:
            expected_proportions: æœŸæœ›æ¯”ä¾‹ [Î±Ì‚â‚, Î±Ì‚â‚‚, ..., Î±Ì‚_K]
                                  æ»¡è¶³ Î£Î±Ì‚_k = 1
            weight: çº¦æŸæƒé‡ Î¼
        """
        self.expected_proportions = np.array(expected_proportions)
        self.weight = weight

    def proportion_loss(self, membership_functions):
        """
        è®¡ç®—æ¯”ä¾‹æŸå¤±

        å‚æ•°:
            membership_functions: éš¶å±å‡½æ•° Ï† (K, H, W)

        è¿”å›:
            loss: æ¯”ä¾‹æŸå¤±
        """
        # è®¡ç®—å®é™…æ¯”ä¾‹
        actual_proportions = np.zeros(len(self.expected_proportions))
        total_pixels = membership_functions.shape[1] * membership_functions.shape[2]

        for k in range(len(self.expected_proportions)):
            # ç¬¬kç›¸çš„ç§¯åˆ† (æ±‚å’Œ)
            integral_k = np.sum(membership_functions[k])
            actual_proportions[k] = integral_k / total_pixels

        # æ¯”ä¾‹å·®å¼‚æŸå¤±
        loss = self.weight * np.sum(
            (actual_proportions - self.expected_proportions) ** 2
        )

        return loss

    def get_actual_proportions(self, membership_functions):
        """
        è·å–å®é™…æ¯”ä¾‹
        """
        K, H, W = membership_functions.shape
        actual_proportions = np.zeros(K)

        for k in range(K):
            integral_k = np.sum(membership_functions[k])
            actual_proportions[k] = integral_k / (H * W)

        return actual_proportions
```

### åˆ›æ–°äºŒ: å°ç›®æ ‡æ•æ„Ÿæ€§

```
é—®é¢˜: ä¼ ç»Ÿæ–¹æ³•å¯¹å°ç›®æ ‡ä¸æ•æ„Ÿ

åŸå› :
â”œâ”€â”€ èƒ½é‡å‡½æ•°æŒ‰åŒºåŸŸåŠ æƒ
â”œâ”€â”€ å°ç›®æ ‡è´¡çŒ®å°
â””â”€â”€ æ¢¯åº¦ä¿¡æ¯å¼±

è¯­ä¹‰æ¯”ä¾‹æ–¹æ³•è§£å†³æ–¹æ¡ˆ:
â”œâ”€â”€ é€šè¿‡æ¯”ä¾‹çº¦æŸæ”¾å¤§å°ç›®æ ‡å½±å“
â”œâ”€â”€ æœŸæœ›æ¯”ä¾‹Î±Ì‚_kå¯ä»¥å¼ºè°ƒå°ç›®æ ‡
â””â”€â”€ çº¦æŸé¡¹è¿«ä½¿æ¨¡å‹å…³æ³¨å°ç›®æ ‡

ç¤ºä¾‹:
å‡è®¾å›¾åƒä¸­:
- èƒŒæ™¯: 90%
- å°ç›®æ ‡A: 5%
- å°ç›®æ ‡B: 5%

è®¾ç½®æœŸæœ›æ¯”ä¾‹:
Î±Ì‚ = [0.85, 0.075, 0.075]

â†’ æ¨¡å‹ä¼šåŠªåŠ›åŒ¹é…è¿™äº›æ¯”ä¾‹
â†’ å°ç›®æ ‡ä¸ä¼šè¢«å¿½ç•¥
```

### åˆ›æ–°ä¸‰: å¤šé€šé“èåˆç­–ç•¥

```python
class MultiChannelSemanticSegmentation:
    """
    å¤šé€šé“è¯­ä¹‰æ¯”ä¾‹åˆ†å‰²
    """

    def __init__(
        self,
        n_classes,
        lambda_smooth=0.1,
        lambda_data=1.0,
        mu_proportion=0.5,
        expected_proportions=None
    ):
        """
        å‚æ•°:
            n_classes: åˆ†å‰²ç±»åˆ«æ•°
            lambda_smooth: å¹³æ»‘å‚æ•°
            lambda_data: æ•°æ®ä¿çœŸå‚æ•°
            mu_proportion: æ¯”ä¾‹çº¦æŸæƒé‡
            expected_proportions: æœŸæœ›æ¯”ä¾‹
        """
        self.n_classes = n_classes
        self.lambda_smooth = lambda_smooth
        self.lambda_data = lambda_data
        self.mu_proportion = mu_proportion

        if expected_proportions is None:
            # é»˜è®¤å‡åŒ€åˆ†å¸ƒ
            self.expected_proportions = np.ones(n_classes) / n_classes
        else:
            self.expected_proportions = np.array(expected_proportions)

    def compute_energy(self, phi, f, class_centers):
        """
        è®¡ç®—æ€»èƒ½é‡

        å‚æ•°:
            phi: éš¶å±å‡½æ•° (K, H, W)
            f: è¾“å…¥å›¾åƒ (3, H, W) æˆ– (H, W)
            class_centers: ç±»åˆ«ä¸­å¿ƒ (K,)

        è¿”å›:
            energy: æ€»èƒ½é‡
        """
        H, W = phi.shape[1:]

        # 1. å¹³æ»‘é¡¹ (å…¨å˜å·®)
        smoothness = 0
        for k in range(self.n_classes):
            grad_x = np.gradient(phi[k], axis=1)
            grad_y = np.gradient(phi[k], axis=0)
            smoothness += np.sum(grad_x**2 + grad_y**2)

        energy_smooth = self.lambda_smooth * smoothness

        # 2. æ•°æ®é¡¹
        data_fidelity = 0
        for k in range(self.n_classes):
            # (f - c_k)Â² åŠ æƒ by Ï†_kÂ²
            if f.ndim == 3:  # å½©è‰²å›¾åƒ
                diff = np.sum((f - class_centers[k][:, None, None])**2, axis=0)
            else:  # ç°åº¦å›¾
                diff = (f - class_centers[k])**2

            data_fidelity += np.sum(phi[k]**2 * diff)

        energy_data = self.lambda_data * data_fidelity

        # 3. æ¯”ä¾‹çº¦æŸé¡¹
        actual_proportions = np.zeros(self.n_classes)
        for k in range(self.n_classes):
            actual_proportions[k] = np.sum(phi[k]) / (H * W)

        proportion_penalty = self.mu_proportion * np.sum(
            (actual_proportions - self.expected_proportions)**2
        )

        # æ€»èƒ½é‡
        total_energy = energy_smooth + energy_data + proportion_penalty

        return {
            'total': total_energy,
            'smoothness': energy_smooth,
            'data': energy_data,
            'proportion': proportion_penalty,
            'actual_proportions': actual_proportions
        }

    def optimize(self, f, max_iter=1000, tol=1e-4):
        """
        ä¼˜åŒ–æ±‚è§£

        ä½¿ç”¨æ¢¯åº¦ä¸‹é™æˆ–Split Bregman
        """
        H, W = f.shape[:2] if f.ndim == 3 else f.shape
        C = f.shape[0] if f.ndim == 3 else 1

        # åˆå§‹åŒ–éš¶å±å‡½æ•°
        phi = np.random.rand(self.n_classes, H, W)
        phi = phi / np.sum(phi, axis=0, keepdims=True)  # å½’ä¸€åŒ–

        # åˆå§‹åŒ–ç±»åˆ«ä¸­å¿ƒ
        if C == 3:  # å½©è‰²
            class_centers = np.random.rand(self.n_classes, 3)
        else:  # ç°åº¦
            class_centers = np.random.rand(self.n_classes)

        # ä¼˜åŒ–å¾ªç¯
        energies = []
        for iteration in range(max_iter):
            # 1. æ›´æ–°ç±»åˆ«ä¸­å¿ƒ (å›ºå®šÏ†)
            for k in range(self.n_classes):
                weights = phi[k]**2
                if C == 3:
                    for c in range(3):
                        numerator = np.sum(weights * f[c])
                        denominator = np.sum(weights) + 1e-8
                        class_centers[k, c] = numerator / denominator
                else:
                    numerator = np.sum(weights * f)
                    denominator = np.sum(weights) + 1e-8
                    class_centers[k] = numerator / denominator

            # 2. æ›´æ–°éš¶å±å‡½æ•° (å›ºå®šç±»åˆ«ä¸­å¿ƒ)
            # ä½¿ç”¨æ¢¯åº¦ä¸‹é™
            for k in range(self.n_classes):
                # è®¡ç®—æ¢¯åº¦
                grad = self._compute_gradient(phi, f, class_centers, k)

                # æ¢¯åº¦ä¸‹é™æ›´æ–°
                phi[k] -= 0.01 * grad

            # 3. æŠ•å½±åˆ°çº¦æŸé›† (Î£Ï†_k = 1, Ï†_k â‰¥ 0)
            phi = np.maximum(phi, 0)
            phi = phi / np.sum(phi, axis=0, keepdims=True)

            # 4. è®¡ç®—èƒ½é‡
            energy_dict = self.compute_energy(phi, f, class_centers)
            energies.append(energy_dict['total'])

            # 5. æ£€æŸ¥æ”¶æ•›
            if iteration > 10:
                if abs(energies[-2] - energies[-1]) < tol:
                    break

        return phi, class_centers, energy_dict

    def _compute_gradient(self, phi, f, class_centers, k):
        """
        è®¡ç®—Ï†_kçš„æ¢¯åº¦
        """
        # å¹³æ»‘é¡¹æ¢¯åº¦ (æ‹‰æ™®æ‹‰æ–¯)
        laplacian = (
            np.roll(phi[k], 1, axis=0) +
            np.roll(phi[k], -1, axis=0) +
            np.roll(phi[k], 1, axis=1) +
            np.roll(phi[k], -1, axis=1) -
            4 * phi[k]
        )

        # æ•°æ®é¡¹æ¢¯åº¦
        if f.ndim == 3:
            diff = np.sum((f - class_centers[k][:, None, None])**2, axis=0)
        else:
            diff = (f - class_centers[k])**2

        grad_data = 2 * phi[k] * diff

        # æ¯”ä¾‹é¡¹æ¢¯åº¦
        H, W = phi.shape[1:]
        actual_prop = np.sum(phi[k]) / (H * W)
        grad_prop = 2 * self.mu_proportion * (actual_prop - self.expected_proportions[k]) / (H * W)

        # æ€»æ¢¯åº¦
        gradient = -2 * self.lambda_smooth * laplacian + \
                   self.lambda_data * grad_data + grad_prop

        return gradient

    def get_segmentation(self, phi):
        """
        ä»éš¶å±å‡½æ•°è·å–ç¡¬åˆ†å‰²
        """
        # æœ€å¤§éš¶å±åº¦
        segmentation = np.argmax(phi, axis=0)

        return segmentation
```

---

## ğŸ“Š å®éªŒä¸ç»“æœ

### å®éªŒè®¾ç½®

#### æ•°æ®é›†

| æ•°æ®é›† | å›¾åƒæ•° | ç±»åˆ«æ•° | ç‰¹ç‚¹ |
|:---|:---:|:---:|:---|
| **åˆæˆå›¾åƒ** | 50 | 4-8 | å¯æ§å®éªŒ |
| **Berkeley Segmentation** | 500 | 2-6 | è‡ªç„¶å›¾åƒ |
| **MSRC** | 591 | 21 | å¤æ‚åœºæ™¯ |

#### å¯¹æ¯”æ–¹æ³•

```
å¯¹æ¯”æ–¹æ³•:
â”œâ”€â”€ æ ‡å‡† Mumford-Shah
â”œâ”€â”€ Chan-Vese
â”œâ”€â”€ SLaT ([2-03])
â””â”€â”€ æœ¬æ–‡ Semantic Proportions
```

### ä¸»è¦ç»“æœ

#### åˆ†å‰²è´¨é‡å¯¹æ¯”

| æ–¹æ³• | åˆæˆå›¾åƒ | Berkeley | MSRC | å°ç›®æ ‡IoU |
|:---|:---:|:---:|:---:|:---:|
| Mumford-Shah | 0.85 | 0.76 | 0.68 | 0.52 |
| Chan-Vese | 0.87 | 0.78 | 0.70 | 0.55 |
| SLaT | 0.92 | 0.83 | 0.75 | 0.62 |
| **Semantic Prop.** | **0.93** | **0.84** | **0.77** | **0.71** |

**å…³é”®å‘ç°**:
- âœ“ æ•´ä½“æ€§èƒ½æœ€ä¼˜
- âœ“ å°ç›®æ ‡æ£€æµ‹æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•
- âœ“ æ¯”ä¾‹å…ˆéªŒæœ‰æ•ˆ

#### æ¯”ä¾‹çº¦æŸæœ‰æ•ˆæ€§

```
å®éªŒ: ä¸åŒçš„æ¯”ä¾‹è®¾ç½®

æœŸæœ›æ¯”ä¾‹          å®é™…æ¯”ä¾‹      IoU
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[0.5, 0.5]       [0.51, 0.49]  0.92
[0.7, 0.3]       [0.69, 0.31]  0.89
[0.9, 0.1]       [0.88, 0.12]  0.85
[0.95, 0.05]     [0.92, 0.08]  0.81

ç»“è®º: æ¯”ä¾‹çº¦æŸæœ‰æ•ˆï¼Œå³ä½¿æç«¯æ¯”ä¾‹ä¹Ÿèƒ½è¾ƒå¥½åŒ¹é…
```

---

## ğŸ’» å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å®Œæ•´å®ç°

```python
import numpy as np
from scipy import ndimage
from sklearn.cluster import KMeans

class SemanticProportionsSegmentation:
    """
    è¯­ä¹‰æ¯”ä¾‹åˆ†å‰²å®Œæ•´å®ç°
    """

    def __init__(
        self,
        n_classes,
        expected_proportions=None,
        lambda_smooth=0.1,
        lambda_data=1.0,
        mu_proportion=0.5,
        optimization='gradient_descent'
    ):
        """
        å‚æ•°:
            n_classes: åˆ†å‰²ç±»åˆ«æ•° K
            expected_proportions: æœŸæœ›æ¯”ä¾‹ (K,)
            lambda_smooth: å¹³æ»‘å‚æ•°
            lambda_data: æ•°æ®ä¿çœŸå‚æ•°
            mu_proportion: æ¯”ä¾‹çº¦æŸæƒé‡
            optimization: ä¼˜åŒ–æ–¹æ³•
        """
        self.n_classes = n_classes
        self.lambda_smooth = lambda_smooth
        self.lambda_data = lambda_data
        self.mu_proportion = mu_proportion
        self.optimization = optimization

        if expected_proportions is None:
            self.expected_proportions = np.ones(n_classes) / n_classes
        else:
            self.expected_proportions = np.array(expected_proportions)

        # å½’ä¸€åŒ–æœŸæœ›æ¯”ä¾‹
        self.expected_proportions /= self.expected_proportions.sum()

    def segment(self, image, max_iter=500):
        """
        æ‰§è¡Œåˆ†å‰²

        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (H, W) æˆ– (H, W, 3)
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°

        è¿”å›:
            segmentation: åˆ†å‰²ç»“æœ (H, W)
            phi: éš¶å±å‡½æ•° (K, H, W)
            info: é¢å¤–ä¿¡æ¯å­—å…¸
        """
        # é¢„å¤„ç†
        if image.ndim == 3:
            f = image.transpose(2, 0, 1) / 255.0  # (3, H, W)
        else:
            f = image.astype(np.float32) / 255.0
            f = f[np.newaxis, ...]  # (1, H, W)

        H, W = f.shape[1:]

        # åˆå§‹åŒ–
        phi = self._initialize_membership(H, W)
        class_centers = self._initialize_centers(f)

        # ä¼˜åŒ–
        if self.optimization == 'gradient_descent':
            phi, class_centers, energies = self._optimize_gd(
                phi, f, class_centers, max_iter
            )
        else:
            phi, class_centers, energies = self._optimize_split_bregman(
                phi, f, class_centers, max_iter
            )

        # è·å–ç¡¬åˆ†å‰²
        segmentation = np.argmax(phi, axis=0)

        # è®¡ç®—å®é™…æ¯”ä¾‹
        actual_proportions = np.array([
            np.sum(phi[k]) / (H * W) for k in range(self.n_classes)
        ])

        info = {
            'phi': phi,
            'class_centers': class_centers,
            'actual_proportions': actual_proportions,
            'expected_proportions': self.expected_proportions,
            'energies': energies
        }

        return segmentation, info

    def _initialize_membership(self, H, W):
        """åˆå§‹åŒ–éš¶å±å‡½æ•°"""
        # ä½¿ç”¨K-meansåˆå§‹åŒ–
        phi = np.random.rand(self.n_classes, H, W)
        phi = phi / np.sum(phi, axis=0, keepdims=True)
        return phi

    def _initialize_centers(self, f):
        """åˆå§‹åŒ–ç±»åˆ«ä¸­å¿ƒ"""
        C, H, W = f.shape

        # ä½¿ç”¨K-meansåœ¨åƒç´ ç©ºé—´åˆå§‹åŒ–
        pixels = f.reshape(C, -1).T  # (H*W, C)

        kmeans = KMeans(n_clusters=self.n_classes, random_state=42)
        labels = kmeans.fit_predict(pixels)

        centers = kmeans.cluster_centers_  # (K, C)

        return centers

    def _optimize_gd(self, phi, f, centers, max_iter):
        """æ¢¯åº¦ä¸‹é™ä¼˜åŒ–"""
        energies = []

        for iteration in range(max_iter):
            # 1. æ›´æ–°ç±»åˆ«ä¸­å¿ƒ
            centers = self._update_centers(phi, f)

            # 2. æ›´æ–°éš¶å±å‡½æ•°
            phi = self._update_membership_gd(phi, f, centers)

            # 3. è®¡ç®—èƒ½é‡
            energy = self._compute_energy(phi, f, centers)
            energies.append(energy)

            # æ‰“å°è¿›åº¦
            if iteration % 50 == 0:
                print(f"Iteration {iteration}, Energy: {energy:.4f}")

        return phi, centers, energies

    def _update_centers(self, phi, f):
        """æ›´æ–°ç±»åˆ«ä¸­å¿ƒ"""
        C, K = f.shape[0], self.n_classes
        new_centers = np.zeros((K, C))

        for k in range(K):
            weights = phi[k]**2
            total_weight = np.sum(weights) + 1e-8

            for c in range(C):
                new_centers[k, c] = np.sum(weights * f[c]) / total_weight

        return new_centers

    def _update_membership_gd(self, phi, f, centers, lr=0.01):
        """æ¢¯åº¦ä¸‹é™æ›´æ–°éš¶å±å‡½æ•°"""
        K, H, W = phi.shape
        C = f.shape[0]

        # è®¡ç®—æ¢¯åº¦
        grad = np.zeros_like(phi)

        for k in range(K):
            # å¹³æ»‘é¡¹: -2*Î»*Î”Ï†
            laplacian = (
                np.roll(phi[k], 1, axis=0) +
                np.roll(phi[k], -1, axis=0) +
                np.roll(phi[k], 1, axis=1) +
                np.roll(phi[k], -1, axis=1) -
                4 * phi[k]
            )
            grad_smooth = -2 * self.lambda_smooth * laplacian

            # æ•°æ®é¡¹: 2*Î»_d*Ï†_k*(f-c_k)Â²
            diff = np.sum((f - centers[k][:, None, None])**2, axis=0)
            grad_data = 2 * self.lambda_data * phi[k] * diff

            # æ¯”ä¾‹é¡¹: 2*Î¼*(Î±_k - Î±Ì‚_k)/|Î©|
            actual_alpha_k = np.sum(phi[k]) / (H * W)
            grad_proportion = 2 * self.mu_proportion * \
                             (actual_alpha_k - self.expected_proportions[k]) / (H * W)

            grad[k] = grad_smooth + grad_data + grad_proportion

        # æ¢¯åº¦ä¸‹é™
        phi -= lr * grad

        # æŠ•å½±åˆ°çº¦æŸé›†
        phi = np.maximum(phi, 0)
        phi_sum = np.sum(phi, axis=0, keepdims=True)
        phi = phi / (phi_sum + 1e-8)

        return phi

    def _compute_energy(self, phi, f, centers):
        """è®¡ç®—æ€»èƒ½é‡"""
        K, H, W = phi.shape
        C = f.shape[0]

        # å¹³æ»‘é¡¹
        smoothness = 0
        for k in range(K):
            grad_x = np.gradient(phi[k], axis=1)
            grad_y = np.gradient(phi[k], axis=0)
            smoothness += np.sum(grad_x**2 + grad_y**2)

        energy_smooth = self.lambda_smooth * smoothness

        # æ•°æ®é¡¹
        data_fidelity = 0
        for k in range(K):
            diff = np.sum((f - centers[k][:, None, None])**2, axis=0)
            data_fidelity += np.sum(phi[k]**2 * diff)

        energy_data = self.lambda_data * data_fidelity

        # æ¯”ä¾‹é¡¹
        actual_proportions = np.array([
            np.sum(phi[k]) / (H * W) for k in range(K)
        ])
        proportion_penalty = self.mu_proportion * np.sum(
            (actual_proportions - self.expected_proportions)**2
        )

        total_energy = energy_smooth + energy_data + proportion_penalty

        return total_energy

    def visualize_results(self, image, segmentation, info):
        """
        å¯è§†åŒ–åˆ†å‰²ç»“æœ
        """
        import matplotlib.pyplot as plt

        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        # åŸå›¾
        axes[0, 0].imshow(image)
        axes[0, 0].set_title('Input Image')
        axes[0, 0].axis('off')

        # åˆ†å‰²ç»“æœ
        axes[0, 1].imshow(segmentation, cmap='jet')
        axes[0, 1].set_title('Segmentation')
        axes[0, 1].axis('off')

        # éš¶å±å‡½æ•° (é€‰æ‹©å‡ ä¸ªç±»åˆ«)
        for i in range(min(4, self.n_classes)):
            row = i // 2
            col = (i % 2) + 1
            if row < 2 and col < 3:
                axes[row, col].imshow(info['phi'][i], cmap='hot')
                axes[row, col].set_title(f'Membership Class {i}')
                axes[row, col].axis('off')

        # æ¯”ä¾‹å¯¹æ¯”
        x = np.arange(self.n_classes)
        width = 0.35

        axes[1, 0].bar(x - width/2, info['expected_proportions'],
                      width, label='Expected')
        axes[1, 0].bar(x + width/2, info['actual_proportions'],
                      width, label='Actual')
        axes[1, 0].set_xlabel('Class')
        axes[1, 0].set_ylabel('Proportion')
        axes[1, 0].set_title('Proportion Comparison')
        axes[1, 0].legend()
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels([f'C{i}' for i in range(self.n_classes)])

        # èƒ½é‡æ›²çº¿
        axes[1, 1].plot(info['energies'])
        axes[1, 1].set_xlabel('Iteration')
        axes[1, 1].set_ylabel('Energy')
        axes[1, 1].set_title('Energy Convergence')
        axes[1, 1].grid(True)

        plt.tight_layout()
        plt.show()
```

### ç»„ä»¶2: è‡ªé€‚åº”æ¯”ä¾‹é€‰æ‹©

```python
class AdaptiveProportionSelection:
    """
    è‡ªé€‚åº”æ¯”ä¾‹é€‰æ‹©

    æ ¹æ®å›¾åƒå†…å®¹è‡ªåŠ¨ç¡®å®šæœŸæœ›æ¯”ä¾‹
    """

    @staticmethod
    def from_histogram(image, n_classes):
        """
        ä»ç›´æ–¹å›¾ä¼°è®¡æ¯”ä¾‹

        å‡è®¾æ¯ä¸ªç±»åˆ«å¯¹åº”ç›´æ–¹å›¾çš„ä¸€ä¸ªå³°å€¼
        """
        if image.ndim == 3:
            # è½¬æ¢ä¸ºç°åº¦
            gray = np.mean(image, axis=2)
        else:
            gray = image

        # è®¡ç®—ç›´æ–¹å›¾
        hist, bins = np.histogram(gray.flatten(), bins=256)

        # å¯»æ‰¾å³°å€¼
        from scipy.signal import find_peaks
        peaks, _ = find_peaks(hist, distance=20)

        # é€‰æ‹©æœ€å¼ºçš„Kä¸ªå³°
        if len(peaks) >= n_classes:
            top_peaks = peaks[np.argsort(hist[peaks])[-n_classes:]]
        else:
            top_peaks = peaks

        # æ ¹æ®å³°é¢ç§¯ä¼°è®¡æ¯”ä¾‹
        proportions = []
        for peak in top_peaks:
            # ä¼°è®¡è¯¥å³°çš„å®½åº¦
            left = max(0, peak - 20)
            right = min(256, peak + 20)
            area = np.sum(hist[left:right])
            proportions.append(area)

        proportions = np.array(proportions)
        proportions /= proportions.sum()

        return proportions

    @staticmethod
    def from_kmeans(image, n_classes):
        """
        ä½¿ç”¨K-meansèšç±»ä¼°è®¡æ¯”ä¾‹
        """
        from sklearn.cluster import KMeans

        if image.ndim == 3:
            pixels = image.reshape(-1, 3)
        else:
            pixels = image.reshape(-1, 1)

        # K-meansèšç±»
        kmeans = KMeans(n_clusters=n_classes, random_state=42)
        labels = kmeans.fit_predict(pixels)

        # è®¡ç®—æ¯ä¸ªèšç±»çš„æ¯”ä¾‹
        proportions = np.bincount(labels, minlength=n_classes)
        proportions = proportions / proportions.sum()

        return proportions

    @staticmethod
    def from_superpixels(image, n_classes, n_superpixels=100):
        """
        åŸºäºè¶…åƒç´ ä¼°è®¡æ¯”ä¾‹
        """
        try:
            from skimage.segmentation import slic
        except ImportError:
            # å¦‚æœæ²¡æœ‰skimageï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•
            return AdaptiveProportionSelection.from_kmeans(image, n_classes)

        # è®¡ç®—è¶…åƒç´ 
        if image.ndim == 3:
            superpixels = slic(image, n_segments=n_superpixels)
        else:
            superpixels = slic(image, n_segments=n_superpixels, channel_axis=None)

        # è®¡ç®—æ¯ä¸ªè¶…åƒç´ çš„å¹³å‡é¢œè‰²
        n_sp = superpixels.max() + 1
        sp_colors = []
        for i in range(n_sp):
            mask = superpixels == i
            if image.ndim == 3:
                sp_colors.append(image[mask].mean(axis=0))
            else:
                sp_colors.append(image[mask].mean())

        sp_colors = np.array(sp_colors)

        # å¯¹è¶…åƒç´ é¢œè‰²èšç±»
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=n_classes, random_state=42)
        sp_labels = kmeans.fit_predict(sp_colors)

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¶…åƒç´ æ•°
        sp_counts = np.bincount(sp_labels, minlength=n_classes)

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åƒç´ æ•°
        proportions = []
        for i in range(n_classes):
            pixel_count = np.sum([superpixels == j for j in np.where(sp_labels == i)[0]])
            proportions.append(pixel_count)

        proportions = np.array(proportions)
        proportions /= proportions.sum()

        return proportions
```

### ç»„ä»¶3: ä½¿ç”¨ç¤ºä¾‹

```python
# ä½¿ç”¨ç¤ºä¾‹
def example_semantic_proportions():
    """
    è¯­ä¹‰æ¯”ä¾‹åˆ†å‰²ä½¿ç”¨ç¤ºä¾‹
    """
    import cv2

    # è¯»å–å›¾åƒ
    image = cv2.imread('example.jpg')
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # æ–¹æ³•1: ä½¿ç”¨é»˜è®¤æ¯”ä¾‹
    segmenter1 = SemanticProportionsSegmentation(
        n_classes=4,
        lambda_smooth=0.1,
        mu_proportion=0.5
    )
    seg1, info1 = segmenter1.segment(image)

    # æ–¹æ³•2: æŒ‡å®šæœŸæœ›æ¯”ä¾‹
    expected_props = [0.5, 0.3, 0.15, 0.05]  # å¼ºè°ƒå°ç›®æ ‡
    segmenter2 = SemanticProportionsSegmentation(
        n_classes=4,
        expected_proportions=expected_props,
        lambda_smooth=0.1,
        mu_proportion=1.0  # å¢å¼ºæ¯”ä¾‹çº¦æŸ
    )
    seg2, info2 = segmenter2.segment(image)

    # æ–¹æ³•3: è‡ªé€‚åº”æ¯”ä¾‹
    adaptive_props = AdaptiveProportionSelection.from_kmeans(image, 4)
    segmenter3 = SemanticProportionsSegmentation(
        n_classes=4,
        expected_proportions=adaptive_props,
        lambda_smooth=0.1,
        mu_proportion=0.8
    )
    seg3, info3 = segmenter3.segment(image)

    # å¯è§†åŒ–
    segmenter1.visualize_results(image, seg1, info1)

    return seg1, info1
```

---

## ğŸ”— ä¸å…¶ä»–å·¥ä½œçš„å…³ç³»

### 6.1 Xiaohao Caiç ”ç©¶è„‰ç»œ

```
å˜åˆ†æ³•åˆ†å‰²æ–¹æ³•æ¼”è¿›:

[1-04] å˜åˆ†æ³•åŸºç¡€
    â†“ Mumford-Shahæ¨¡å‹
    â†“
[2-01] å‡¸ä¼˜åŒ–åˆ†å‰²
    â†“ å‡¸æ¾å¼›æŠ€æœ¯
    â†“
[2-03] SLaTä¸‰é˜¶æ®µ â† å§Šå¦¹ç¯‡
    â†“
[2-05] è¯­ä¹‰æ¯”ä¾‹ â† æœ¬ç¯‡
    â†“ æ¯”ä¾‹çº¦æŸ
    â†“
[2-09] æ¡†æ¶åˆ†å‰²
```

### 6.2 æ ¸å¿ƒè®ºæ–‡çš„å…³ç³»

| è®ºæ–‡ | å…³ç³» | è¯´æ˜ |
|:---|:---|:---|
| [1-04] å˜åˆ†æ³•åŸºç¡€ | **ç†è®ºåŸºçŸ³** | Mumford-Shahæ¨¡å‹ |
| [2-01] å‡¸ä¼˜åŒ–åˆ†å‰² | **æ–¹æ³•å…³è”** | å‡¸æ¾å¼›æŠ€æœ¯ |
| [2-03] SLaTä¸‰é˜¶æ®µ | **å§Šå¦¹ç¯‡** | åŒæœŸå‘è¡¨ï¼Œäº’è¡¥æ–¹æ³• |
| [2-12] Neural Varifolds | **èŒƒå¼å¯¹æ¯”** | ä¼ ç»Ÿ vs ç¥ç» |

---

## ğŸ“ ä¸ªäººæ€è€ƒä¸æ€»ç»“

### 7.1 æ ¸å¿ƒæ”¶è·

#### æ”¶è·1: æ¯”ä¾‹å…ˆéªŒçš„ä»·å€¼

```
ä¼ ç»Ÿåˆ†å‰²:
â”œâ”€â”€ åªå…³æ³¨å±€éƒ¨ç›¸ä¼¼æ€§
â”œâ”€â”€ å¿½ç•¥å…¨å±€æ¯”ä¾‹
â””â”€â”€ å°ç›®æ ‡æ˜“ä¸¢å¤±

æ¯”ä¾‹çº¦æŸ:
â”œâ”€â”€ å…¨å±€çº¦æŸ
â”œâ”€â”€ å…ˆéªŒçŸ¥è¯†èå…¥
â””â”€â”€ å°ç›®æ ‡æ•æ„Ÿ
```

#### æ”¶è·2: å‡¸æ¾å¼›çš„å¨åŠ›

```
ç¦»æ•£ä¼˜åŒ–é—®é¢˜:
â”œâ”€â”€ NPéš¾
â”œâ”€â”€ å±€éƒ¨æœ€ä¼˜
â””â”€â”€ åˆå§‹åŒ–æ•æ„Ÿ

å‡¸æ¾å¼›:
â”œâ”€â”€ å¤šé¡¹å¼å¯è§£
â”œâ”€â”€ å…¨å±€æœ€ä¼˜
â””â”€â”€ åˆå§‹åŒ–ç‹¬ç«‹
```

#### æ”¶è·3: SLaTä¸Semantic Prop.å¯¹æ¯”

```
SLaTä¼˜åŠ¿:
â”œâ”€â”€ ä¸‰é˜¶æ®µè®¾è®¡æ¸…æ™°
â”œâ”€â”€ Kå€¼çµæ´»å¯è°ƒ
â””â”€â”€ é€€åŒ–å›¾åƒé²æ£’

Semantic Prop.ä¼˜åŠ¿:
â”œâ”€â”€ æ¯”ä¾‹å…ˆéªŒæ˜ç¡®
â”œâ”€â”€ å°ç›®æ ‡æ•æ„Ÿ
â””â”€â”€ å•é˜¶æ®µä¼˜åŒ–ç®€æ´

é€‰æ‹©å»ºè®®:
â”œâ”€â”€ éœ€è¦çµæ´»Kå€¼ â†’ SLaT
â”œâ”€â”€ æœ‰æ¯”ä¾‹å…ˆéªŒ â†’ Semantic Prop.
â””â”€â”€ å°ç›®æ ‡æ£€æµ‹ â†’ Semantic Prop.
```

### 7.2 å±€é™æ€§

| å±€é™ | æ”¹è¿›æ–¹å‘ |
|:---|:---|
| **æ¯”ä¾‹éœ€é¢„è®¾** | è‡ªé€‚åº”æ¯”ä¾‹é€‰æ‹© |
| **è®¡ç®—å¤æ‚åº¦** | åŠ é€Ÿç®—æ³• |
| **ä»…ç”¨é¢œè‰²ä¿¡æ¯** | åŠ å…¥çº¹ç†/æ·±åº¦ |
| **å‚æ•°è°ƒä¼˜** | è‡ªåŠ¨å‚æ•°é€‰æ‹© |

---

## âœ… ç²¾è¯»æ£€æŸ¥æ¸…å•

- [x] **æ¡†æ¶ç†è§£**: è¯­ä¹‰æ¯”ä¾‹å»ºæ¨¡
- [x] **æ•°å­¦æ¨å¯¼**: å‡¸æ¾å¼›è¿‡ç¨‹
- [x] **ä»£ç å®ç°**: å®Œæ•´å®ç°æ¡†æ¶
- [x] **å‚æ•°ç†è§£**: Î», Î¼çš„ä½œç”¨
- [x] **åº”ç”¨è¿ç§»**: å°ç›®æ ‡æ£€æµ‹åœºæ™¯

---

**ç²¾è¯»å®Œæˆæ—¶é—´**: 2026å¹´2æœˆ9æ—¥
**è®ºæ–‡ç±»å‹**: æ–¹æ³•åˆ›æ–°
**å§Šå¦¹ç¯‡**: [2-03] SLaTä¸‰é˜¶æ®µåˆ†å‰²

---

*æœ¬ç²¾è¯»ç¬”è®°åŸºäºXiaohao Caiç­‰äººçš„Journal of Scientific Computing 2017è®ºæ–‡*
*é‡ç‚¹å…³æ³¨: è¯­ä¹‰æ¯”ä¾‹å»ºæ¨¡ã€å‡¸æ¾å¼›ã€å°ç›®æ ‡æ£€æµ‹*
