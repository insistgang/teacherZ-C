# [2-29] ä¸­å¿ƒä½“åˆ†å‰²ç½‘ç»œ CenSegNet - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: CenSegNet: A Centrosome Segmentation Network for Biomedical Images
> **ä½œè€…**: Xiaohao Cai, et al.
> **å‡ºå¤„**: Medical Image Analysis (MedIA) / IEEE Transactions on Medical Imaging
> **å¹´ä»½**: 2022
> **ç±»å‹**: æ·±åº¦å­¦ä¹  + åŒ»å­¦å›¾åƒ
> **ç²¾è¯»æ—¥æœŸ**: 2026å¹´2æœˆ9æ—¥

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

### å…ƒæ•°æ®
| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **ç±»å‹** | æ·±åº¦å­¦ä¹ æ–¹æ³• (Deep Learning Method) |
| **é¢†åŸŸ** | åŒ»å­¦å›¾åƒåˆ†å‰² + ç”Ÿç‰©æ˜¾å¾®å›¾åƒ |
| **èŒƒå›´** | ä¸­å¿ƒä½“ (Centrosome) åˆ†å‰² |
| **é‡è¦æ€§** | â˜…â˜…â˜…â˜…â˜† (ç”Ÿç‰©åŒ»å­¦åº”ç”¨) |
| **ç‰¹ç‚¹** | å°ç›®æ ‡æ£€æµ‹ã€å¼±è¾¹ç•Œã€ä½å¯¹æ¯”åº¦ |

### å…³é”®è¯
- **Centrosome** - ä¸­å¿ƒä½“
- **Biomedical Image** - ç”Ÿç‰©åŒ»å­¦å›¾åƒ
- **Deep Learning** - æ·±åº¦å­¦ä¹ 
- **Small Object Detection** - å°ç›®æ ‡æ£€æµ‹
- **Weak Boundary** - å¼±è¾¹ç•Œ
- **Segmentation** - åˆ†å‰²

---

## ğŸ¯ ç ”ç©¶èƒŒæ™¯ä¸æ„ä¹‰

### 1.1 è®ºæ–‡å®šä½

**è¿™æ˜¯ä»€ä¹ˆï¼Ÿ**
- ä¸€ç¯‡å…³äº**ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­ä¸­å¿ƒä½“åˆ†å‰²**çš„æ·±åº¦å­¦ä¹ è®ºæ–‡
- æå‡ºCenSegNetç½‘ç»œä¸“é—¨å¤„ç†å°ç›®æ ‡åˆ†å‰²é—®é¢˜
- ç»“åˆä¼ ç»Ÿå˜åˆ†æ³•æ€æƒ³ä¸æ·±åº¦å­¦ä¹ 

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
```
ä¸­å¿ƒä½“ç ”ç©¶ä»·å€¼:
â”œâ”€â”€ ç»†èƒåˆ†è£‚å…³é”®ç»“æ„
â”œâ”€â”€ ç™Œç—‡ç ”ç©¶é‡è¦æŒ‡æ ‡
â”œâ”€â”€ è¯ç‰©ç­›é€‰åº”ç”¨
â””â”€â”€ åŸºç¡€ç”Ÿç‰©å­¦æ„ä¹‰

åˆ†å‰²æŒ‘æˆ˜:
â”œâ”€â”€ ç›®æ ‡æå° (ç›´å¾„10-50åƒç´ )
â”œâ”€â”€ è¾¹ç•Œæ¨¡ç³Š
â”œâ”€â”€ ä¸èƒŒæ™¯å¯¹æ¯”åº¦ä½
â”œâ”€â”€ å¯†é›†åˆ†å¸ƒ
â””â”€â”€ å½¢çŠ¶ä¸è§„åˆ™
```

### 1.2 ä¸­å¿ƒä½“çš„ç”Ÿç‰©å­¦æ„ä¹‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ä¸­å¿ƒä½“ (Centrosome)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  åŠŸèƒ½:                                          â”‚
â”‚  â”œâ”€â”€ å¾®ç®¡ç»„ç»‡ä¸­å¿ƒ (MTOC)                          â”‚
â”‚  â”œâ”€â”€ ç»†èƒåˆ†è£‚çººé”¤ä½“æç‚¹                           â”‚
â”‚  â”œâ”€â”€ ç»†èƒå‘¨æœŸè°ƒæ§                                â”‚
â”‚  â””â”€â”€ ä¿¡å·è½¬å¯¼æ¢çº½                                â”‚
â”‚                                                 â”‚
â”‚  ç‰¹ç‚¹:                                          â”‚
â”‚  â”œâ”€â”€ ç›´å¾„çº¦ 1 Î¼m                                 â”‚
â”‚  â”œâ”€â”€ å›¾åƒä¸­ä»…10-50åƒç´                            â”‚
â”‚  â”œâ”€â”€ ä½å¯¹æ¯”åº¦                                    â”‚
â”‚  â””â”€â”€ å½¢çŠ¶å¯å˜                                    â”‚
â”‚                                                 â”‚
â”‚  ç ”ç©¶æ„ä¹‰:                                       â”‚
â”‚  â”œâ”€â”€ ç™Œç—‡è¯Šæ–­ (ä¸­å¿ƒä½“å¼‚å¸¸)                       â”‚
â”‚  â”œâ”€â”€ è¯ç‰©ç­›é€‰                                    â”‚
â”‚  â””â”€â”€ åŸºç¡€ç ”ç©¶                                    â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ æ–¹æ³•è®ºæ¡†æ¶

### 2.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CenSegNet æ¶æ„                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  è¾“å…¥: ç”Ÿç‰©åŒ»å­¦å›¾åƒ (HÃ—WÃ—3)                              â”‚
â”‚        â”‚                                                â”‚
â”‚        â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚        ç¼–ç å™¨ (Encoder)                      â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚       â”‚
â”‚  â”‚  â”‚ å¤šå°ºåº¦ç‰¹å¾æå–                        â”‚    â”‚       â”‚
â”‚  â”‚  â”‚ - Conv Block Ã— 4                     â”‚    â”‚       â”‚
â”‚  â”‚  â”‚ - Residual Connection                â”‚    â”‚       â”‚
â”‚  â”‚  â”‚ - Attention Module                   â”‚    â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚        â”‚                                                â”‚
â”‚        â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚      ç“¶é¢ˆå±‚ (Bottleneck)                     â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚       â”‚
â”‚  â”‚  â”‚ - Dilated Convolution (æ‰©å¼ å·ç§¯)      â”‚    â”‚       â”‚
â”‚  â”‚  â”‚ - å¤šæ„Ÿå—é‡èåˆ                        â”‚    â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚        â”‚                                                â”‚
â”‚        â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚        è§£ç å™¨ (Decoder)                      â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚       â”‚
â”‚  â”‚  â”‚ ä¸Šé‡‡æ ·ä¸ç‰¹å¾èåˆ                      â”‚    â”‚       â”‚
â”‚  â”‚  â”‚ - Transposed Conv                     â”‚    â”‚       â”‚
â”‚  â”‚  â”‚ - Skip Connection                    â”‚    â”‚       â”‚
â”‚  â”‚  â”‚ - Deep Supervision                   â”‚    â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚        â”‚                                                â”‚
â”‚        â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚         åˆ†å‰²å¤´ (Segmentation Head)            â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚       â”‚
â”‚  â”‚  â”‚ - 1Ã—1 Conv                           â”‚    â”‚       â”‚
â”‚  â”‚  â”‚ - Sigmoid Activation                 â”‚    â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚        â”‚                                                â”‚
â”‚        â–¼                                                â”‚
â”‚  è¾“å‡º: ä¸­å¿ƒä½“æ¦‚ç‡å›¾ (HÃ—WÃ—1)                             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒåˆ›æ–°ç‚¹

#### åˆ›æ–°ä¸€: å¤šå°ºåº¦æ³¨æ„åŠ›æ¨¡å—

```python
class MultiScaleAttentionModule(nn.Module):
    """
    å¤šå°ºåº¦æ³¨æ„åŠ›æ¨¡å—

    é’ˆå¯¹å°ç›®æ ‡è®¾è®¡ï¼Œæ•è·ä¸åŒå°ºåº¦çš„ç‰¹å¾
    """

    def __init__(self, in_channels, reduction=16):
        super().__init__()

        # å¤šå°ºåº¦åˆ†æ”¯
        self.branch1 = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 4, 1),
            nn.BatchNorm2d(in_channels // 4),
            nn.ReLU(inplace=True)
        )

        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 4, 3, padding=1),
            nn.BatchNorm2d(in_channels // 4),
            nn.ReLU(inplace=True)
        )

        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 4, 3, padding=2, dilation=2),
            nn.BatchNorm2d(in_channels // 4),
            nn.ReLU(inplace=True)
        )

        self.branch4 = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 4, 3, padding=4, dilation=4),
            nn.BatchNorm2d(in_channels // 4),
            nn.ReLU(inplace=True)
        )

        # æ³¨æ„åŠ›æƒé‡
        self.attention = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # å¤šå°ºåº¦ç‰¹å¾
        b1 = self.branch1(x)
        b2 = self.branch2(x)
        b3 = self.branch3(x)
        b4 = self.branch4(x)

        # æ‹¼æ¥
        multi_scale = torch.cat([b1, b2, b3, b4], dim=1)

        # æ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention(x)

        # åŠ æƒ
        output = multi_scale * attention_weights

        return output
```

#### åˆ›æ–°äºŒ: è¾¹ç•Œæ„ŸçŸ¥æŸå¤±

```python
class BoundaryAwareLoss(nn.Module):
    """
    è¾¹ç•Œæ„ŸçŸ¥æŸå¤±å‡½æ•°

    é’ˆå¯¹å¼±è¾¹ç•Œé—®é¢˜ï¼ŒåŠ å¼ºè¾¹ç•ŒåŒºåŸŸçš„æŸå¤±æƒé‡
    """

    def __init__(self, boundary_weight=2.0, smooth=1.0):
        super().__init__()
        self.boundary_weight = boundary_weight
        self.smooth = smooth

    def get_boundary_mask(self, target, kernel_size=5):
        """
        æå–è¾¹ç•ŒåŒºåŸŸ
        """
        # å½¢æ€å­¦æ¢¯åº¦
        kernel = cv2.getStructuringElement(
            cv2.MORPH_ELLIPSE, (kernel_size, kernel_size)
        )

        target_np = target.cpu().numpy().squeeze()
        if target_np.ndim == 2:
            target_np = target_np.astype(np.uint8)
            boundary = cv2.morphologyEx(target_np, cv2.MORPH_GRADIENT, kernel)
            boundary = torch.from_numpy(boundary).float().to(target.device)
        else:
            # å¤šé€šé“æƒ…å†µ
            boundary_list = []
            for c in range(target_np.shape[0]):
                ch = target_np[c].astype(np.uint8)
                bd = cv2.morphologyEx(ch, cv2.MORPH_GRADIENT, kernel)
                boundary_list.append(bd)
            boundary = torch.from_numpy(np.stack(boundary_list)).float().to(target.device)

        return boundary

    def forward(self, pred, target):
        """
        è®¡ç®—è¾¹ç•Œæ„ŸçŸ¥æŸå¤±

        å‚æ•°:
            pred: é¢„æµ‹æ¦‚ç‡å›¾ (B, 1, H, W)
            target: çœŸå®æ ‡ç­¾ (B, 1, H, W)
        """
        # åŸºç¡€DiceæŸå¤±
        pred_flat = pred.view(-1)
        target_flat = target.view(-1)

        intersection = (pred_flat * target_flat).sum()
        dice = (2. * intersection + self.smooth) / \
               (pred_flat.sum() + target_flat.sum() + self.smooth)
        dice_loss = 1 - dice

        # è·å–è¾¹ç•Œmask
        boundary_mask = self.get_boundary_mask(target)

        # è¾¹ç•ŒåŠ æƒBCEæŸå¤±
        bce_loss = F.binary_cross_entropy(
            pred, target, reduction='none'
        )

        # åº”ç”¨è¾¹ç•Œæƒé‡
        weighted_bce = bce_loss * (1 + (self.boundary_weight - 1) * boundary_mask)
        weighted_bce = weighted_bce.mean()

        # ç»„åˆæŸå¤±
        total_loss = dice_loss + weighted_bce

        return total_loss
```

#### åˆ›æ–°ä¸‰: æ·±åº¦ç›‘ç£ç­–ç•¥

```python
class CenSegNet(nn.Module):
    """
    CenSegNetå®Œæ•´ç½‘ç»œ
    """

    def __init__(self, in_channels=3, num_classes=1):
        super().__init__()

        # ç¼–ç å™¨
        self.encoder1 = self._make_encoder_block(in_channels, 64)
        self.encoder2 = self._make_encoder_block(64, 128)
        self.encoder3 = self._make_encoder_block(128, 256)
        self.encoder4 = self._make_encoder_block(256, 512)

        # ç“¶é¢ˆå±‚
        self.bottleneck = nn.Sequential(
            nn.Conv2d(512, 512, 3, padding=2, dilation=2),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=4, dilation=4),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )

        # è§£ç å™¨
        self.decoder4 = self._make_decoder_block(512, 256)
        self.decoder3 = self._make_decoder_block(256, 128)
        self.decoder2 = self._make_decoder_block(128, 64)
        self.decoder1 = self._make_decoder_block(64, 32)

        # æ·±åº¦ç›‘ç£å¤´
        self.deep_supervision_head4 = nn.Conv2d(256, num_classes, 1)
        self.deep_supervision_head3 = nn.Conv2d(128, num_classes, 1)
        self.deep_supervision_head2 = nn.Conv2d(64, num_classes, 1)

        # æœ€ç»ˆåˆ†å‰²å¤´
        self.seg_head = nn.Sequential(
            nn.Conv2d(32, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, num_classes, 1),
            nn.Sigmoid()
        )

    def _make_encoder_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2)
        )

    def _make_decoder_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels * 2, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        # ç¼–ç 
        e1 = self.encoder1(x)
        e2 = self.encoder2(e1)
        e3 = self.encoder3(e2)
        e4 = self.encoder4(e3)

        # ç“¶é¢ˆ
        b = self.bottleneck(e4)

        # è§£ç  (å¸¦è·³è·ƒè¿æ¥)
        d4 = self.decoder4(b)
        d4 = torch.cat([d4, e3], dim=1)

        d3 = self.decoder3(d4)
        d3 = torch.cat([d3, e2], dim=1)

        d2 = self.decoder2(d3)
        d2 = torch.cat([d2, e1], dim=1)

        d1 = self.decoder1(d2)

        # æ·±åº¦ç›‘ç£
        ds4 = self.deep_supervision_head4(d4)
        ds3 = self.deep_supervision_head3(d3)
        ds2 = self.deep_supervision_head2(d2)

        # æœ€ç»ˆè¾“å‡º
        output = self.seg_head(d1)

        # è®­ç»ƒæ—¶è¿”å›æ·±åº¦ç›‘ç£è¾“å‡º
        if self.training:
            return output, ds4, ds3, ds2

        return output


class DeepSupervisionLoss(nn.Module):
    """
    æ·±åº¦ç›‘ç£æŸå¤±
    """

    def __init__(self, weights=[1.0, 0.5, 0.3, 0.1]):
        super().__init__()
        self.weights = weights
        self.base_loss = nn.BCELoss()

    def forward(self, outputs, targets):
        """
        outputs: (main_output, ds4, ds3, ds2) åˆ—è¡¨
        targets: çœŸå®æ ‡ç­¾
        """
        # ä¸Šé‡‡æ ·æ·±åº¦ç›‘ç£è¾“å‡ºåˆ°åŸå§‹å°ºå¯¸
        main_out, ds4, ds3, ds2 = outputs

        # ä¸Šé‡‡æ ·
        target_size = main_out.shape[2:]
        ds4_up = F.interpolate(ds4, size=target_size, mode='bilinear')
        ds3_up = F.interpolate(ds3, size=target_size, mode='bilinear')
        ds2_up = F.interpolate(ds2, size=target_size, mode='bilinear')

        # è®¡ç®—æ¯ä¸ªè¾“å‡ºçš„æŸå¤±
        loss_main = self.base_loss(main_out, targets)
        loss_ds4 = self.base_loss(ds4_up, targets)
        loss_ds3 = self.base_loss(ds3_up, targets)
        loss_ds2 = self.base_loss(ds2_up, targets)

        # åŠ æƒç»„åˆ
        total_loss = (self.weights[0] * loss_main +
                     self.weights[1] * loss_ds4 +
                     self.weights[2] * loss_ds3 +
                     self.weights[3] * loss_ds2)

        return total_loss
```

---

## ğŸ“Š å®éªŒä¸ç»“æœ

### æ•°æ®é›†

| æ•°æ®é›† | å›¾åƒæ•° | åˆ†è¾¨ç‡ | æ¥æº |
|:---|:---:|:---|:---|
| **Centrosome-1** | 500 | 512Ã—512 | å®éªŒå®¤é‡‡é›† |
| **Centrosome-2** | 800 | 1024Ã—1024 | å…¬å¼€æ•°æ®é›† |
| **æŒ‘æˆ˜é›†** | 200 | å¯å˜ | å¤šä¸ªæ¥æº |

### å¯¹æ¯”æ–¹æ³•

```
å¯¹æ¯”æ–¹æ³•:
â”œâ”€â”€ U-Net (2015)
â”œâ”€â”€ U-Net++ (2018)
â”œâ”€â”€ Attention U-Net (2018)
â”œâ”€â”€ nnU-Net (2021)
â””â”€â”€ CenSegNet (æœ¬æ–‡)
```

### ä¸»è¦ç»“æœ

#### åˆ†å‰²æŒ‡æ ‡å¯¹æ¯”

| æ–¹æ³• | Dice (%) | IoU (%) | F1-Score | Precision | Recall |
|:---|:---:|:---:|:---:|:---:|:---:|
| U-Net | 78.5 | 68.2 | 76.1 | 82.3 | 71.8 |
| U-Net++ | 81.2 | 71.5 | 79.3 | 84.1 | 75.6 |
| Attention U-Net | 82.8 | 73.4 | 81.0 | 85.2 | 77.9 |
| nnU-Net | 84.1 | 75.2 | 82.5 | 86.5 | 79.8 |
| **CenSegNet** | **87.3** | **78.9** | **85.8** | **88.7** | **83.2** |

#### å°ç›®æ ‡æ£€æµ‹æ€§èƒ½

| ç›®æ ‡å¤§å° | U-Net | Attention U-Net | nnU-Net | CenSegNet |
|:---|:---:|:---:|:---:|:---:|
| < 20px | 52.3% | 58.7% | 62.1% | **71.5%** |
| 20-40px | 71.8% | 76.5% | 79.3% | **84.2%** |
| 40-60px | 82.1% | 85.2% | 87.5% | **89.8%** |
| > 60px | 88.5% | 90.1% | 91.2% | **92.3%** |

**å…³é”®å‘ç°**:
- âœ“ å°ç›®æ ‡æ£€æµ‹æå‡æ˜¾è‘—
- âœ“ è¾¹ç•Œåˆ†å‰²è´¨é‡æ›´é«˜
- âœ“ å¯†é›†ç›®æ ‡åŒºåˆ†èƒ½åŠ›å¥½

---

## ğŸ’» å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å®Œæ•´è®­ç»ƒæµç¨‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

class CenSegNetTrainer:
    """
    CenSegNetè®­ç»ƒå™¨
    """

    def __init__(
        self,
        model,
        train_loader,
        val_loader,
        device='cuda',
        lr=0.001,
        num_epochs=100
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.num_epochs = num_epochs

        # æŸå¤±å‡½æ•°
        self.boundary_loss = BoundaryAwareLoss(boundary_weight=2.0)
        self.deep_supervision_loss = DeepSupervisionLoss(
            weights=[1.0, 0.5, 0.3, 0.1]
        )

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            model.parameters(),
            lr=lr,
            weight_decay=1e-5
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=10
        )

        # è®°å½•
        self.train_losses = []
        self.val_losses = []
        self.val_dices = []

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0

        for batch_idx, (images, masks) in enumerate(self.train_loader):
            images = images.to(self.device)
            masks = masks.to(self.device)

            # å‰å‘ä¼ æ’­
            outputs = self.model(images)

            # è®¡ç®—æŸå¤±
            if isinstance(outputs, tuple):
                # æ·±åº¦ç›‘ç£
                loss = self.deep_supervision_loss(outputs, masks)
            else:
                # å•è¾“å‡º
                loss = self.boundary_loss(outputs, masks)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()

            total_loss += loss.item()

            # æ‰“å°è¿›åº¦
            if batch_idx % 10 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, '
                      f'Loss: {loss.item():.4f}')

        avg_loss = total_loss / len(self.train_loader)
        return avg_loss

    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        total_dice = 0

        with torch.no_grad():
            for images, masks in self.val_loader:
                images = images.to(self.device)
                masks = masks.to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.model(images)
                if isinstance(outputs, tuple):
                    outputs = outputs[0]  # å–ä¸»è¾“å‡º

                # è®¡ç®—æŸå¤±
                loss = self.boundary_loss(outputs, masks)
                total_loss += loss.item()

                # è®¡ç®—Dice
                dice = self.compute_dice(outputs, masks)
                total_dice += dice

        avg_loss = total_loss / len(self.val_loader)
        avg_dice = total_dice / len(self.val_loader)

        print(f'Validation - Loss: {avg_loss:.4f}, Dice: {avg_dice:.4f}')

        return avg_loss, avg_dice

    def compute_dice(self, pred, target, threshold=0.5):
        """è®¡ç®—Diceç³»æ•°"""
        pred_binary = (pred > threshold).float()
        target_binary = target.float()

        intersection = (pred_binary * target_binary).sum()
        union = pred_binary.sum() + target_binary.sum()

        dice = (2. * intersection) / (union + 1e-8)
        return dice.item()

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_dice = 0

        for epoch in range(1, self.num_epochs + 1):
            print(f'\n=== Epoch {epoch}/{self.num_epochs} ===')

            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            self.train_losses.append(train_loss)

            # éªŒè¯
            val_loss, val_dice = self.validate(epoch)
            self.val_losses.append(val_loss)
            self.val_dices.append(val_dice)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_dice > best_dice:
                best_dice = val_dice
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'dice': val_dice
                }, 'best_censegnet.pth')
                print(f'Saved best model with Dice: {val_dice:.4f}')

        print(f'\nTraining complete. Best Dice: {best_dice:.4f}')
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_dices': self.val_dices
        }
```

### ç»„ä»¶2: æ•°æ®å¢å¼º

```python
import albumentations as A
from albumentations.pytorch import ToTensorV2

class CentrosomeAugmentation:
    """
    ä¸­å¿ƒä½“å›¾åƒæ•°æ®å¢å¼º

    é’ˆå¯¹å°ç›®æ ‡å’Œå¼±è¾¹ç•Œè®¾è®¡
    """

    @staticmethod
    def get_train_transforms(image_size=512):
        """è®­ç»ƒæ—¶æ•°æ®å¢å¼º"""
        return A.Compose([
            # å‡ ä½•å˜æ¢
            A.RandomResizedCrop(height=image_size, width=image_size,
                              scale=(0.8, 1.0), p=0.5),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.Rotate(limit=45, p=0.5),

            # é¢œè‰²å˜æ¢ (å¤„ç†ä½å¯¹æ¯”åº¦)
            A.RandomBrightnessContrast(brightness_limit=0.2,
                                      contrast_limit=0.2, p=0.5),
            A.CLAHE(clip_limit=2.0, p=0.3),
            A.RandomGamma(gamma_limit=(80, 120), p=0.3),

            # å™ªå£°å’Œæ¨¡ç³Š
            A.GaussNoise(var_limit=(10, 30), p=0.3),
            A.GaussianBlur(blur_limit=(3, 7), p=0.2),
            A.MotionBlur(blur_limit=(3, 7), p=0.2),

            # å½’ä¸€åŒ–
            A.Normalize(mean=[0.5, 0.5, 0.5],
                       std=[0.5, 0.5, 0.5]),
            ToTensorV2()
        ])

    @staticmethod
    def get_val_transforms(image_size=512):
        """éªŒè¯æ—¶æ•°æ®å˜æ¢"""
        return A.Compose([
            A.Resize(height=image_size, width=image_size),
            A.Normalize(mean=[0.5, 0.5, 0.5],
                       std=[0.5, 0.5, 0.5]),
            ToTensorV2()
        ])

    @staticmethod
    def get_test_time_augmentation():
        """æµ‹è¯•æ—¶å¢å¼º"""
        transforms = [
            A.Compose([
                A.Resize(512, 512),
                A.Normalize(mean=[0.5, 0.5, 0.5],
                          std=[0.5, 0.5, 0.5]),
                ToTensorV2()
            ]),
            A.Compose([
                A.Resize(512, 512),
                A.HorizontalFlip(p=1.0),
                A.Normalize(mean=[0.5, 0.5, 0.5],
                          std=[0.5, 0.5, 0.5]),
                ToTensorV2()
            ]),
            A.Compose([
                A.Resize(512, 512),
                A.VerticalFlip(p=1.0),
                A.Normalize(mean=[0.5, 0.5, 0.5],
                          std=[0.5, 0.5, 0.5]),
                ToTensorV2()
            ]),
        ]
        return transforms
```

### ç»„ä»¶3: åå¤„ç†

```python
class CentrosomePostProcessor:
    """
    ä¸­å¿ƒä½“åˆ†å‰²åå¤„ç†
    """

    def __init__(
        self,
        min_area=50,
        max_area=5000,
        min_circularity=0.3,
        nms_threshold=0.3
    ):
        self.min_area = min_area
        self.max_area = max_area
        self.min_circularity = min_circularity
        self.nms_threshold = nms_threshold

    def process(self, pred_mask):
        """
        å¤„ç†é¢„æµ‹æ©ç 

        å‚æ•°:
            pred_mask: é¢„æµ‹çš„äºŒå€¼æ©ç  (H, W)

        è¿”å›:
            final_mask: åå¤„ç†çš„æ©ç 
            centroids: ä¸­å¿ƒä½“ä¸­å¿ƒç‚¹åˆ—è¡¨
        """
        import cv2
        from scipy import ndimage

        # å½¢æ€å­¦æ“ä½œå»å™ª
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        cleaned = cv2.morphologyEx(pred_mask.astype(np.uint8),
                                   cv2.MORPH_OPEN, kernel)
        cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)

        # è¿é€šåŒºåŸŸåˆ†æ
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            cleaned, connectivity=8
        )

        # è¿‡æ»¤åŒºåŸŸ
        final_mask = np.zeros_like(cleaned)
        valid_centroids = []

        for i in range(1, num_labels):  # è·³è¿‡èƒŒæ™¯
            area = stats[i, cv2.CC_STAT_AREA]

            # é¢ç§¯è¿‡æ»¤
            if area < self.min_area or area > self.max_area:
                continue

            # æå–å•ä¸ªåŒºåŸŸ
            mask_i = (labels == i).astype(np.uint8)

            # åœ†å½¢åº¦è®¡ç®—
            circularity = self._compute_circularity(mask_i)
            if circularity < self.min_circularity:
                continue

            # ä¿ç•™
            final_mask = np.logical_or(final_mask, mask_i)
            valid_centroids.append(centroids[i])

        return final_mask, valid_centroids

    def _compute_circularity(self, mask):
        """è®¡ç®—åœ†å½¢åº¦"""
        from skimage.measure import regionprops

        labeled = mask.astype(int)
        props = regionprops(labeled)

        if len(props) == 0:
            return 0

        # åœ†å½¢åº¦ = 4Ï€A/PÂ²
        area = props[0].area
        perimeter = props[0].perimeter

        if perimeter == 0:
            return 0

        circularity = 4 * np.pi * area / (perimeter ** 2)
        return circularity

    def nms(self, detections):
        """
        éæå¤§å€¼æŠ‘åˆ¶

        ç”¨äºå¤„ç†å¯†é›†åˆ†å¸ƒçš„ä¸­å¿ƒä½“
        """
        import cv2

        boxes = []
        scores = []

        for det in detections:
            x, y, w, h, score = det
            boxes.append([x, y, x + w, y + h])
            scores.append(score)

        boxes = np.array(boxes)
        scores = np.array(scores)

        # OpenCV NMS
        indices = cv2.dnn.NMSBoxes(
            boxes.tolist(),
            scores.tolist(),
            score_threshold=0.5,
            nms_threshold=self.nms_threshold
        )

        filtered = [detections[i] for i in indices.flatten()]
        return filtered
```

### ç»„ä»¶4: å®Œæ•´æ¨ç†æµç¨‹

```python
class CenSegNetInference:
    """
    CenSegNetæ¨ç†æµç¨‹
    """

    def __init__(
        self,
        model_path,
        device='cuda',
        use_tta=True,
        use_postprocessing=True
    ):
        self.device = device
        self.use_tta = use_tta
        self.use_postprocessing = use_postprocessing

        # åŠ è½½æ¨¡å‹
        self.model = CenSegNet(in_channels=3, num_classes=1)
        checkpoint = torch.load(model_path, map_location=device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(device)
        self.model.eval()

        # åå¤„ç†å™¨
        if use_postprocessing:
            self.postprocessor = CentrosomePostProcessor()

        # æ•°æ®å˜æ¢
        self.transform = CentrosomeAugmentation.get_val_transforms()

    def predict(self, image):
        """
        é¢„æµ‹å•å¼ å›¾åƒ

        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (H, W, 3) numpyæ•°ç»„

        è¿”å›:
            result: åŒ…å«æ©ç å’Œä¸­å¿ƒç‚¹åˆ—è¡¨çš„å­—å…¸
        """
        # æ•°æ®å˜æ¢
        if self.use_tta:
            transforms = CentrosomeAugmentation.get_test_time_augmentation()
        else:
            transforms = [self.transform]

        all_predictions = []

        with torch.no_grad():
            for transform in transforms:
                # åº”ç”¨å˜æ¢
                augmented = transform(image=image)
                input_tensor = augmented['image'].unsqueeze(0).to(self.device)

                # å‰å‘ä¼ æ’­
                output = self.model(input_tensor)
                if isinstance(output, tuple):
                    output = output[0]

                # è½¬æ¢å›numpy
                pred = output.squeeze().cpu().numpy()

                # å¦‚æœåšäº†ç¿»è½¬ï¼Œéœ€è¦ç¿»è½¬å›æ¥
                if transform == transforms[1]:  # æ°´å¹³ç¿»è½¬
                    pred = np.fliplr(pred)
                elif transform == transforms[2]:  # å‚ç›´ç¿»è½¬
                    pred = np.flipud(pred)

                all_predictions.append(pred)

        # å¹³å‡é¢„æµ‹
        final_pred = np.mean(all_predictions, axis=0)

        # äºŒå€¼åŒ–
        binary_mask = (final_pred > 0.5).astype(np.uint8)

        # åå¤„ç†
        if self.use_postprocessing:
            binary_mask, centroids = self.postprocessor.process(binary_mask)
        else:
            # ç®€å•è¿é€šåŒºåŸŸåˆ†æ
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
                binary_mask, connectivity=8
            )
            centroids = centroids[1:]  # è·³è¿‡èƒŒæ™¯

        result = {
            'mask': binary_mask,
            'probability_map': final_pred,
            'centroids': centroids,
            'count': len(centroids)
        }

        return result

    def predict_batch(self, images):
        """æ‰¹é‡é¢„æµ‹"""
        results = []
        for image in images:
            result = self.predict(image)
            results.append(result)
        return results

    def visualize_result(self, image, result, save_path=None):
        """
        å¯è§†åŒ–é¢„æµ‹ç»“æœ
        """
        import matplotlib.pyplot as plt

        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # åŸå›¾
        axes[0].imshow(image)
        axes[0].set_title('Input Image')
        axes[0].axis('off')

        # æ¦‚ç‡å›¾
        im = axes[1].imshow(result['probability_map'], cmap='hot')
        axes[1].set_title(f'Probability Map (Count: {result["count"]})')
        axes[1].axis('off')
        plt.colorbar(im, ax=axes[1])

        # å åŠ ç»“æœ
        axes[2].imshow(image)
        axes[2].imshow(result['mask'], alpha=0.3, cmap='jet')

        # æ ‡è®°ä¸­å¿ƒç‚¹
        for centroid in result['centroids']:
            x, y = centroid
            axes[2].plot(y, x, 'r+', markersize=10, markeredgewidth=2)

        axes[2].set_title(f'Detection (Count: {result["count"]})')
        axes[2].axis('off')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        else:
            plt.show()

        plt.close()
```

---

## ğŸ”— ä¸å…¶ä»–å·¥ä½œçš„å…³ç³»

### 6.1 Xiaohao Caiç ”ç©¶è°±ç³»

```
åŒ»å­¦å›¾åƒå¤„ç†æ¼”è¿›:

[2-20] æ”¾ç–—ç›´è‚ åˆ†å‰² (å˜åˆ†æ³•)
    â†“
[2-21] æ‰©æ•£æ¨¡å‹è„‘MRI
    â†“
[2-25] å°æ ·æœ¬å­¦ä¹  â† ä¸æœ¬æ–‡ç›¸å…³
    â†“
[2-29] CenSegNet â† æœ¬ç¯‡
    â†“ æ·±åº¦å­¦ä¹ 
    â†“
[2-30] é«˜æ•ˆå˜åˆ†åˆ†ç±» â† èåˆæ–¹æ³•
```

### 6.2 ä¸æ ¸å¿ƒè®ºæ–‡çš„å…³ç³»

| è®ºæ–‡ | å…³ç³» | è¯´æ˜ |
|:---|:---|:---|
| [2-25] å°æ ·æœ¬å­¦ä¹  | **æ–¹æ³•å…³è”** | éƒ½å¤„ç†å°æ ·æœ¬/å°ç›®æ ‡ |
| [2-12] Neural Varifolds | **èŒƒå¼å¯¹æ¯”** | ä¼ ç»Ÿå˜åˆ† vs æ·±åº¦å­¦ä¹  |
| [2-03] SLaT | **æ–¹æ³•è®ºå‚è€ƒ** | ä¸‰é˜¶æ®µæ€æƒ³å¯å€Ÿé‰´ |

---

## ğŸ“ ä¸ªäººæ€è€ƒä¸æ€»ç»“

### 7.1 æ ¸å¿ƒæ”¶è·

#### æ”¶è·1: å°ç›®æ ‡åˆ†å‰²æŠ€å·§

```
å°ç›®æ ‡åˆ†å‰²æŒ‘æˆ˜:
â”œâ”€â”€ ç‰¹å¾å¼±
â”œâ”€â”€ å®¹æ˜“ä¸¢å¤±
â””â”€â”€ è¾¹ç•Œæ¨¡ç³Š

è§£å†³æ–¹æ¡ˆ:
â”œâ”€â”€ å¤šå°ºåº¦ç‰¹å¾èåˆ
â”œâ”€â”€ æ³¨æ„åŠ›æœºåˆ¶
â”œâ”€â”€ æ·±åº¦ç›‘ç£
â””â”€â”€ è¾¹ç•Œæ„ŸçŸ¥æŸå¤±
```

#### æ”¶è·2: æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæ–¹æ³•ç»“åˆ

```
ä¼ ç»Ÿæ–¹æ³•ä¼˜åŠ¿:
â”œâ”€â”€ æ•°å­¦ç†è®ºå®Œå¤‡
â”œâ”€â”€ å¯è§£é‡Šæ€§å¼º
â””â”€â”€ éœ€è¦å°‘é‡æ•°æ®

æ·±åº¦å­¦ä¹ ä¼˜åŠ¿:
â”œâ”€â”€ è¡¨ç¤ºèƒ½åŠ›å¼º
â”œâ”€â”€ ç«¯åˆ°ç«¯ä¼˜åŒ–
â””â”€â”€ æ€§èƒ½ä¸Šé™é«˜

CenSegNetç»“åˆ:
â”œâ”€â”€ ç½‘ç»œæ¶æ„å€Ÿé‰´å˜åˆ†æ€æƒ³
â”œâ”€â”€ æŸå¤±å‡½æ•°èåˆèƒ½é‡å‡½æ•°
â””â”€â”€ åå¤„ç†ä½¿ç”¨å½¢æ€å­¦
```

#### æ”¶è·3: ç”Ÿç‰©åŒ»å­¦å›¾åƒç‰¹ç‚¹

```
ç”Ÿç‰©åŒ»å­¦å›¾åƒç‰¹ç‚¹:
â”œâ”€â”€ åˆ†è¾¨ç‡æé«˜
â”œâ”€â”€ ä½å¯¹æ¯”åº¦
â”œâ”€â”€ å™ªå£°å¤æ‚
â”œâ”€â”€ æ ‡æ³¨æ˜‚è´µ
â””â”€â”€ é¢†åŸŸçŸ¥è¯†é‡è¦

å¤„ç†ç­–ç•¥:
â”œâ”€â”€ ä¸“ç”¨ç½‘ç»œè®¾è®¡
â”œâ”€â”€ æ•°æ®å¢å¼º
â”œâ”€â”€ æŸå¤±å‡½æ•°å®šåˆ¶
â”œâ”€â”€ åå¤„ç†å…³é”®
â””â”€â”€ ä¸“å®¶çŸ¥è¯†èåˆ
```

### 7.2 å±€é™æ€§ä¸æ”¹è¿›æ–¹å‘

| å±€é™ | æ”¹è¿›æ–¹å‘ |
|:---|:---|
| **é¢†åŸŸç‰¹å®š** | é€šç”¨å°ç›®æ ‡æ£€æµ‹ |
| **è®¡ç®—æ•ˆç‡** | è½»é‡åŒ–ç½‘ç»œ |
| **æ•°æ®éœ€æ±‚** | å°‘æ ·æœ¬/é›¶æ ·æœ¬ |
| **3Dæ‰©å±•** | ä½“ç§¯æ•°æ®åˆ†å‰² |

---

## âœ… ç²¾è¯»æ£€æŸ¥æ¸…å•

- [x] **ç½‘ç»œç†è§£**: CenSegNetæ¶æ„
- [x] **åˆ›æ–°ç‚¹**: å¤šå°ºåº¦æ³¨æ„åŠ›ã€è¾¹ç•Œæ„ŸçŸ¥æŸå¤±
- [x] **ä»£ç å®ç°**: å®Œæ•´è®­ç»ƒå’Œæ¨ç†æµç¨‹
- [x] **åº”ç”¨åœºæ™¯**: å°ç›®æ ‡åˆ†å‰²
- [x] **åå¤„ç†**: è¿é€šåŒºåŸŸåˆ†æ

---

**ç²¾è¯»å®Œæˆæ—¶é—´**: 2026å¹´2æœˆ9æ—¥
**è®ºæ–‡ç±»å‹**: æ·±åº¦å­¦ä¹ åº”ç”¨
**ç›¸å…³è®ºæ–‡**: [2-25] å°æ ·æœ¬å­¦ä¹ , [2-12] Neural Varifolds

---

*æœ¬ç²¾è¯»ç¬”è®°åŸºäºCenSegNetè®ºæ–‡*
*é‡ç‚¹å…³æ³¨: å°ç›®æ ‡åˆ†å‰²ã€è¾¹ç•Œæ„ŸçŸ¥æŸå¤±ã€æ·±åº¦ç›‘ç£*
