# 深度学习架构手册

> **创建日期**: 2026年2月9日
> **范围**: 图像分割、点云处理、多模态学习的网络架构
> **基于论文**: [2-12] Neural Varifolds, [3-02] tCURLoRA, [3-06] Talk2Radar

---

## 目录

1. [架构设计原则](#架构设计原则)
2. [点云处理架构](#点云处理架构)
3. [分割网络架构](#分割网络架构)
4. [多模态融合架构](#多模态融合架构)
5. [参数高效微调架构](#参数高效微调架构)
6. [代码实现模板](#代码实现模板)

---

## 架构设计原则

### 设计范式

```
输入 → 编码器 → 瓶颈层 → 解码器 → 输出
         ↑                          ↓
      跳跃连接 ←←←←←←←←←←←←←←←←←
```

### 核心设计要素

| 要素 | 说明 | 典型实现 |
|------|------|----------|
| **多尺度特征** | 捕获不同粒度信息 | FPN, UNet++ |
| **注意力机制** | 关注重要区域 | Self-Attention, Cross-Attention |
| **残差连接** | 缓解梯度消失 | ResNet, DenseNet |
| **归一化** | 稳定训练 | BN, LN, IN |
| **激活函数** | 引入非线性 | ReLU, GELU, Swish |

---

## 点云处理架构

### 架构1: PointCloudEncoder (基于Neural Varifolds)

```python
import torch
import torch.nn as nn

class PointCloudEncoder(nn.Module):
    """
    点云神经编码器

    基于: [2-12] Neural Varifolds
    """
    def __init__(self, input_dim=6, embed_dim=256, num_layers=4, num_heads=8):
        super().__init__()

        # 点特征提取 (坐标 + 法线)
        self.point_mlp = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, embed_dim)
        )

        # 位置编码
        self.pos_encoding = PositionalEncoding(embed_dim)

        # Transformer层
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim * 4,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        # 全局池化
        self.global_pool = nn.AdaptiveAvgPool1d(1)

    def forward(self, points, normals=None):
        """
        Args:
            points: (B, N, 3) 点云坐标
            normals: (B, N, 3) 法线 (可选)

        Returns:
            global_feat: (B, D) 全局特征
            point_feat: (B, N, D) 点特征
        """
        # 拼接坐标和法线
        if normals is not None:
            x = torch.cat([points, normals], dim=-1)  # (B, N, 6)
        else:
            x = points  # (B, N, 3)

        # 点特征提取
        x = self.point_mlp(x)  # (B, N, D)

        # 位置编码
        x = self.pos_encoding(x)

        # Transformer处理
        point_feat = self.transformer(x)  # (B, N, D)

        # 全局特征
        global_feat = self.global_pool(
            point_feat.transpose(1, 2)
        ).squeeze(-1)  # (B, D)

        return global_feat, point_feat


class PositionalEncoding(nn.Module):
    """位置编码"""
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()

        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() *
            (-torch.log(torch.tensor(10000.0)) / d_model)
        )

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]
```

### 架构2: Varifolds度量模块

```python
class VarifoldsMetric(nn.Module):
    """
    Varifolds相似性度量

    基于: [2-12] Neural Varifolds
    """
    def __init__(self, sigma_pos=1.0, sigma_norm=0.5):
        super().__init__()
        self.sigma_pos = sigma_pos
        self.sigma_norm = sigma_norm

    def position_kernel(self, x, y):
        """位置高斯核"""
        dist = torch.sum((x - y)**2, dim=-1)
        return torch.exp(-dist / (2 * self.sigma_pos**2))

    def normal_kernel(self, n1, n2):
        """法线内积核"""
        return torch.sum(n1 * n2, dim=-1)

    def forward(self, points1, normals1, points2, normals2):
        """
        计算两组点云的Varifolds相似性

        Args:
            points1, points2: (B, N, 3)
            normals1, normals2: (B, N, 3)

        Returns:
            similarity: (B,) 相似性得分
        """
        B, N, _ = points1.shape
        device = points1.device

        # 计算核矩阵 (向量化实现)
        # 位置核矩阵
        pos_diff = points1.unsqueeze(2) - points2.unsqueeze(1)  # (B, N, N, 3)
        pos_dist = torch.sum(pos_diff**2, dim=-1)
        pos_sim = torch.exp(-pos_dist / (2 * self.sigma_pos**2))  # (B, N, N)

        # 法线核矩阵
        norm_sim = torch.sum(normals1.unsqueeze(2) * normals2.unsqueeze(1), dim=-1)  # (B, N, N)

        # 组合
        varifold_sim = pos_sim * torch.abs(norm_sim)

        # 总相似性
        similarity = torch.sum(varifold_sim, dim=[1, 2])

        return similarity


class NeuralVarifoldsModel(nn.Module):
    """
    完整的神经Varifolds模型
    """
    def __init__(self, encoder, metric):
        super().__init__()
        self.encoder = encoder
        self.metric = metric

    def forward(self, points, normals=None):
        """前向传播"""
        global_feat, point_feat = self.encoder(points, normals)
        return global_feat, point_feat

    def compute_similarity(self, points1, normals1, points2, normals2):
        """计算相似性"""
        _, feat1 = self.encoder(points1, normals1)
        _, feat2 = self.encoder(points2, normals2)

        # 使用cosine相似度作为神经表示的度量
        similarity = F.cosine_similarity(
            feat1.mean(dim=1), feat2.mean(dim=1), dim=-1
        )
        return similarity
```

### 架构3: CornerPoint3D检测头

```python
class EdgeHead(nn.Module):
    """
    边缘关注模块

    基于: [2-11] CornerPoint3D
    """
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.conv3 = nn.Conv2d(out_channels, out_channels, 3, padding=1)

        # 边缘注意力
        self.edge_attn = nn.Sequential(
            nn.Conv2d(out_channels, 1, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        """
        Args:
            x: (B, C, H, W) 特征图

        Returns:
            edge_features: (B, C, H, W) 边缘增强特征
        """
        x = self.conv1(x)
        x = self.conv2(x)

        # 边缘注意力
        edge_mask = self.edge_attn(x)
        x = x * edge_mask

        x = self.conv3(x)

        return x


class CornerDetectionHead(nn.Module):
    """
    角点检测头

    预测最近角点而非中心
    """
    def __init__(self, in_channels, num_corners=4):
        super().__init__()

        # 特征提取
        self.features = nn.Sequential(
            nn.Conv2d(in_channels, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(inplace=True)
        )

        # 边缘关注
        self.edge_head = EdgeHead(128, 128)

        # 角点预测
        self.corner_pred = nn.Conv2d(128, num_corners, 1)

        # 偏移预测
        self.offset_pred = nn.Conv2d(128, num_corners * 2, 1)

    def forward(self, x):
        """
        Args:
            x: (B, C, H, W) 输入特征

        Returns:
            corners: (B, num_corners, H, W) 角点热图
            offsets: (B, num_corners*2, H, W) 偏移预测
        """
        # 特征提取
        feat = self.features(x)

        # 边缘关注
        feat = self.edge_head(feat)

        # 预测
        corners = self.corner_pred(feat)
        offsets = self.offset_pred(feat)

        return corners, offsets
```

---

## 分割网络架构

### 架构4: 改进U-Net

```python
class ImprovedUNet(nn.Module):
    """
    改进U-Net架构

    特点:
    - 深层监督
    - 注意力跳跃连接
    - 残差块
    """
    def __init__(self, in_channels=3, num_classes=2, base_channels=64):
        super().__init__()

        # Encoder
        self.enc1 = ResBlock(in_channels, base_channels)
        self.enc2 = ResBlock(base_channels, base_channels*2)
        self.enc3 = ResBlock(base_channels*2, base_channels*4)
        self.enc4 = ResBlock(base_channels*4, base_channels*8)

        # Bottleneck
        self.bottleneck = nn.Sequential(
            ResBlock(base_channels*8, base_channels*16),
            ResBlock(base_channels*16, base_channels*16)
        )

        # Decoder with attention
        self.dec4 = AttentionUpBlock(base_channels*16, base_channels*8)
        self.dec3 = AttentionUpBlock(base_channels*8, base_channels*4)
        self.dec2 = AttentionUpBlock(base_channels*4, base_channels*2)
        self.dec1 = AttentionUpBlock(base_channels*2, base_channels)

        # Deep supervision
        self.deep_supervision = nn.ModuleList([
            nn.Conv2d(base_channels*8, num_classes, 1),
            nn.Conv2d(base_channels*4, num_classes, 1),
            nn.Conv2d(base_channels*2, num_classes, 1)
        ])

        # Final output
        self.final = nn.Conv2d(base_channels, num_classes, 1)

        self.pool = nn.MaxPool2d(2)

    def forward(self, x):
        # Encoder with skip connections
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))

        # Bottleneck
        b = self.bottleneck(self.pool(e4))

        # Decoder with attention skip connections
        d4 = self.dec4(b, e4)
        d3 = self.dec3(d4, e3)
        d2 = self.dec2(d3, e2)
        d1 = self.dec1(d2, e1)

        # Output
        out = self.final(d1)

        # Deep supervision outputs (training only)
        ds_outs = [
            self.deep_supervision[0](d4),
            self.deep_supervision[1](d3),
            self.deep_supervision[2](d2)
        ]

        if self.training:
            return out, ds_outs
        return out


class ResBlock(nn.Module):
    """残差块"""
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self shortcut = nn.Sequential()
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1),
                nn.BatchNorm2d(out_channels)
            )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = self.shortcut(x)

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out += residual
        out = self.relu(out)

        return out


class AttentionUpBlock(nn.Module):
    """注意力上采样块"""
    def __init__(self, in_channels, skip_channels):
        super().__init__()

        # 上采样
        self.up = nn.ConvTranspose2d(in_channels, skip_channels, 2, stride=2)

        # 注意力门
        self.attention = AttentionGate(skip_channels, skip_channels)

        # 融合卷积
        self.conv = ResBlock(skip_channels * 2, skip_channels)

    def forward(self, x, skip):
        # 上采样
        x = self.up(x)

        # 注意力
        skip = self.attention(skip, x)

        # 拼接
        x = torch.cat([x, skip], dim=1)

        # 卷积
        x = self.conv(x)

        return x


class AttentionGate(nn.Module):
    """注意力门控模块"""
    def __init__(self, gate_channels, signal_channels):
        super().__init__()

        self.W_g = nn.Sequential(
            nn.Conv2d(gate_channels, gate_channels, 1),
            nn.BatchNorm2d(gate_channels)
        )

        self.W_s = nn.Sequential(
            nn.Conv2d(signal_channels, gate_channels, 1),
            nn.BatchNorm2d(gate_channels)
        )

        self.relu = nn.ReLU(inplace=True)
        self.psi = nn.Sequential(
            nn.Conv2d(gate_channels, 1, 1),
            nn.Sigmoid()
        )

    def forward(self, gate, signal):
        # Gate处理
        g1 = self.W_g(gate)

        # Signal处理
        s1 = self.W_s(signal)

        # 相加 + ReLU
        psi = self.relu(g1 + s1)

        # 注意力系数
        psi = self.psi(psi)

        # 应用注意力
        return gate * psi
```

---

## 多模态融合架构

### 架构5: 跨模态注意力融合

```python
class CrossModalAttention(nn.Module):
    """
    跨模态注意力模块

    基于: [3-06] Talk2Radar, [3-07] GAMED
    """
    def __init__(self, embed_dim, num_heads=8, dropout=0.1):
        super().__init__()

        self.multihead_attn = nn.MultiheadAttention(
            embed_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.norm = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 4, embed_dim),
            nn.Dropout(dropout)
        )
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, query, key, value):
        """
        Args:
            query: (B, N_q, D) 查询模态
            key: (B, N_k, D) 键模态
            value: (B, N_k, D) 值模态

        Returns:
            out: (B, N_q, D) 融合特征
        """
        # 多头注意力
        attn_out, _ = self.multihead_attn(query, key, value)

        # 残差 + LayerNorm
        query = self.norm(query + attn_out)

        # FFN
        ffn_out = self.ffn(query)

        # 残差 + LayerNorm
        out = self.norm2(query + ffn_out)

        return out


class MultiModalFusion(nn.Module):
    """
    多模态融合模块

    支持: 图像+文本, 点云+图像, 雷达+语言
    """
    def __init__(self, modal_dims, fusion_dim, num_heads=8):
        super().__init__()

        # 投影层
        self.projections = nn.ModuleList([
            nn.Sequential(
                nn.Linear(dim, fusion_dim),
                nn.LayerNorm(fusion_dim)
            )
            for dim in modal_dims
        ])

        # 跨模态注意力层
        num_modals = len(modal_dims)
        self.cross_attn_layers = nn.ModuleList([
            CrossModalAttention(fusion_dim, num_heads)
            for _ in range(num_modals)
        ])

        # 自注意力层
        self.self_attn = nn.MultiheadAttention(
            fusion_dim, num_heads, batch_first=True
        )

        # 最终融合
        self.fusion = nn.Sequential(
            nn.Linear(fusion_dim * num_modals, fusion_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(fusion_dim, fusion_dim)
        )

    def forward(self, modals):
        """
        Args:
            modals: List of tensors, each (B, N_i, D_i)

        Returns:
            fused: (B, fusion_dim) 融合特征
        """
        # 投影到统一空间
        projected = [
            proj(m) for proj, m in zip(self.projections, modals)
        ]

        # 跨模态注意力
        fused_features = []
        for i, feat in enumerate(projected):
            # 其他模态作为key/value
            other_feats = [projected[j] for j in range(len(projected)) if j != i]
            if other_feats:
                other_feats = torch.cat(other_feats, dim=1)
                fused = self.cross_attn_layers[i](feat, other_feats, other_feats)
            else:
                fused = feat
            fused_features.append(fused)

        # 堆叠并应用自注意力
        stacked = torch.stack(fused_features, dim=1)  # (B, num_modals, N, D)
        B, M, N, D = stacked.shape
        stacked = stacked.view(B, M*N, D)

        self_attended, _ = self.self_attn(stacked, stacked, stacked)
        self_attended = self_attended.view(B, M, N, D)

        # 池化
        pooled = self_attended.mean(dim=[1, 2])  # (B, D)

        # 最终融合
        concat = torch.cat(fused_features, dim=-1)
        fused = self.fusion(concat) + pooled

        return fused
```

### 架构6: 对比学习框架

```python
class ContrastiveMultiModal(nn.Module):
    """
    跨模态对比学习框架

    基于: [3-06] Talk2Radar
    """
    def __init__(self, modal_encoders, embed_dim=256, temperature=0.07):
        super().__init__()

        # 模态编码器
        self.encoders = nn.ModuleList(modal_encoders)

        # 投影头
        self.projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(enc.out_dim, embed_dim),
                nn.ReLU(inplace=True),
                nn.Linear(embed_dim, 128)
            )
            for enc in modal_encoders
        ])

        self.temperature = temperature

    def forward(self, modals):
        """
        前向传播

        Args:
            modals: List of modal inputs

        Returns:
            embeddings: List of projected embeddings
            losses: Contrastive losses
        """
        batch_size = modals[0].size(0)

        # 编码
        embeddings = []
        for i, (encoder, projector, modal) in enumerate(
            zip(self.encoders, self.projectors, modals)
        ):
            feat = encoder(modal)
            z = projector(feat)
            z = F.normalize(z, dim=-1)
            embeddings.append(z)

        # 计算对比损失
        loss = self.contrastive_loss(embedings)

        return embeddings, loss

    def contrastive_loss(self, embeddings):
        """
        InfoNCE对比损失

        Args:
            embeddings: List of (B, N, D) normalized embeddings

        Returns:
            loss: 对比损失值
        """
        num_modals = len(embeddings)
        batch_size = embeddings[0].size(0)
        device = embeddings[0].device

        # 创建标签 (对角线为正样本)
        labels = torch.arange(batch_size, device=device)

        total_loss = 0
        count = 0

        for i in range(num_modals):
            for j in range(i+1, num_modals):
                # 模态i到模态j的损失
                z_i = embeddings[i]  # (B, N, D)
                z_j = embeddings[j]  # (B, N, D)

                # 计算相似度矩阵
                logits_ij = torch.matmul(z_i, z_j.transpose(-2, -1)) / self.temperature
                logits_ji = torch.matmul(z_j, z_i.transpose(-2, -1)) / self.temperature

                # 损失
                loss_i = F.cross_entropy(logits_ij, labels)
                loss_j = F.cross_entropy(logits_ji, labels)

                total_loss += (loss_i + loss_j) / 2
                count += 1

        return total_loss / count if count > 0 else total_loss
```

---

## 参数高效微调架构

### 架构7: tCURLoRA

```python
class TCURLoRA(nn.Module):
    """
    张量CUR分解LoRA

    基于: [3-02] tCURLoRA (ICML 2024)
    """
    def __init__(self, original_weight, rank, tensor_shape=None):
        super().__init__()

        # 冻结原始权重
        self.original_weight = nn.Parameter(
            original_weight.clone(), requires_grad=False
        )

        d, k = original_weight.shape

        # 推断张量形状
        if tensor_shape is None:
            tensor_shape = self._infer_tensor_shape(d, k)

        self.tensor_shape = tensor_shape
        self.rank = rank

        # 初始化CUR分解
        self.C, self.U, self.Rs = self._initialize_cur(
            original_weight, tensor_shape, rank
        )

    def _infer_tensor_shape(self, d, k):
        """推断合适的张量分解形状"""
        factors = []
        temp = d * k

        # 寻找合适的因子
        for _ in range(3):
            for i in range(int(temp**0.5), 0, -1):
                if temp % i == 0:
                    factors.append(i)
                    temp = temp // i
                    break

        return tuple(factors) if len(factors) == 3 else (d, k)

    def _initialize_cur(self, weight, tensor_shape, rank):
        """初始化CUR分解"""
        T = weight.reshape(tensor_shape)

        # 选择重要列和行
        C, col_idx = self._select_columns(T, rank)
        Rs, row_indices = self._select_rows(T, rank)

        # 计算核心张量
        U = torch.randn([rank] * len(tensor_shape)) * 0.01

        # 只微调U
        C = nn.Parameter(C, requires_grad=False)
        U = nn.Parameter(U, requires_grad=True)
        Rs = [nn.Parameter(R, requires_grad=False) for R in Rs]

        return C, U, Rs

    def _select_columns(self, T, rank):
        """基于重要性选择列"""
        importance = torch.sum(T**2, dim=tuple(range(T.ndim-1)))
        prob = importance / torch.sum(importance)
        indices = torch.multinomial(prob, rank)
        return torch.index_select(T, -1, indices), indices

    def _select_rows(self, T, rank):
        """选择重要行"""
        Rs = []
        for mode in range(T.ndim):
            importance = torch.sum(T**2, dim=[
                i for i in range(T.ndim) if i != mode
            ])
            prob = importance / torch.sum(importance)
            indices = torch.multinomial(prob, rank)
            Rs.append(torch.index_select(T, mode, indices))
        return Rs, None

    def forward(self, x):
        """前向传播"""
        delta = self._compute_delta()
        W_eff = self.original_weight + delta
        return F.linear(x, W_eff)

    def _compute_delta(self):
        """计算微调增量"""
        # 简化实现: 返回核心张量reshape
        delta = self.U.reshape(self.original_weight.shape)
        return delta

    def get_trainable_parameters(self):
        """获取可训练参数(只有U)"""
        return [self.U]


class TCURWrapper(nn.Module):
    """tCURLoRA层包装器"""
    def __init__(self, original_layer, rank=32):
        super().__init__()

        self.original_layer = original_layer
        weight = original_layer.weight.data

        self.tcur = TCURLoRA(weight, rank=rank)

        if original_layer.bias is not None:
            self.bias = nn.Parameter(
                original_layer.bias.data.clone(),
                requires_grad=True
            )
        else:
            self.bias = None

    def forward(self, x):
        out = self.original_layer(x)
        delta = self.tcur(x)
        out = out + delta

        if self.bias is not None:
            out = out + self.bias

        return out
```

---

## 附录

### 网络架构对比

| 架构 | 适用任务 | 优势 | 局限 |
|------|----------|------|------|
| U-Net | 医学图像分割 | 简单有效 | 边缘模糊 |
| Attention U-Net | 精细分割 | 注意力机制 | 计算量大 |
| PointNet++ | 点云分类 | 点云无序性 | 局部特征弱 |
| Neural Varifolds | 点云配准 | 几何度量 | 训练复杂 |
| tCURLoRA | 高效微调 | 参数效率 | 实现复杂 |

### PyTorch实现要点

```python
# 关键模块
import torch
import torch.nn as nn
import torch.nn.functional as F

from einops import rearrange  # 张量操作
from einops.layers.torch import Rearrange

# 推荐实践
# 1. 使用nn.Parameter定义可学习参数
# 2. 使用register_buffer保存非参数张量
# 3. 使用type_as保持设备一致性
# 4. 使用inplace操作节省显存
```

---

*文档创建时间: 2026年2月9日*
*基于: Xiaohao Cai 师门论文分析*
