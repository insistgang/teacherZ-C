# 张量分解方法手册

> **创建日期**: 2026年2月9日
> **范围**: 张量分解方法体系与PEFT应用
> **基于论文**: [3-02] tCURLoRA (ICML 2024), [3-04], [3-05]

---

## 目录

1. [张量分解基础](#张量分解基础)
2. [分解方法详解](#分解方法详解)
3. [tCURLoRA方法](#tCURLoRA方法)
4. [应用指南](#应用指南)
5. [代码实现](#代码实现)

---

## 张量分解基础

### 张量表示

```python
# 张量定义
import torch

# 标量 (0阶张量)
scalar = torch.tensor(3.14)

# 向量 (1阶张量)
vector = torch.tensor([1, 2, 3])  # shape: (3,)

# 矩阵 (2阶张量)
matrix = torch.tensor([[1, 2], [3, 4]])  # shape: (2, 2)

# 高阶张量 (3阶及以上)
tensor_3d = torch.randn(3, 4, 5)  # shape: (3, 4, 5)
tensor_4d = torch.randn(2, 3, 4, 5)  # shape: (2, 3, 4, 5)
```

### 张量运算

```python
# 张量基本运算
A = torch.randn(3, 4, 5)
B = torch.randn(3, 4, 5)

# 逐元素运算
C = A + B
D = A * B
E = torch.matmul(A, B)  # 矩阵乘法在最后两维

# 张量模数
norm_1 = torch.norm(A, p=1)  # L1范数
norm_2 = torch.norm(A, p=2)  # L2/Frobenius范数
norm_inf = torch.norm(A, p=float('inf'))  # 无穷范数

# 张量重塑
A_reshaped = A.reshape(3, 20)  # 3×4×5 → 3×20
A_transposed = A.permute(2, 0, 1)  # 维度重排: 3×4×5 → 5×3×4
```

### 张量秩

```python
def tensor_rank(A):
    """
    计算张量的秩

    Args:
        A: (..., M, N) 张量

    Returns:
        rank: 张量的秩
    """
    # 展平为二维矩阵
    A_flat = A.reshape(-1, A.shape[-1])

    # 计算矩阵秩
    rank = torch.matrix_rank(A_flat)
    return rank.item()


def tensor_rank_approx(A, target_rank):
    """
    计算张量的近似秩（基于SVD）
    """
    U, S, V = torch.svd(A.reshape(-1, A.shape[-1]))
    return (S > 1e-6).sum().item()
```

---

## 分解方法详解

### 知识链全景图

```
张量分解方法体系
│
├── 1. 矩阵分解 (2D张量)
│    ├── SVD (奇异值分解)
│    ├── QR分解
│    ├── LU分解
│    └── Cholesky分解
│
├── 2. CANDECOMP/PARAFAC (CP分解)
│    └── 分解为: T ≈ Σ λ_i a_i ⊗ b_i ⊗ c_i
│
├── 3. Tucker分解
│    └── 分解为: T ≈ G ×₁ A ×₂ B ×₃ C
│
├── 4. TT (Tensor Train) 分解
│    └── 分解为: T ≈ G₁ × G₂ × ... × G_N
│
├── 5. CUR分解
│    └── 分解为: T ≈ C × U × R
│    └── 特点: 保留实际行/列
│
└── 6. 混合分解
    ├── Tucker + CUR
    └── TT + CUR
```

---

### 1. SVD (奇异值分解)

**矩阵分解**:
```
A ∈ R^{m×n}

A = U Σ V^T

其中:
- U ∈ R^{m×m}: 左奇异向量 (正交矩阵)
- Σ ∈ R^{m×n}: 奇异值对角矩阵
- V ∈ R^{n×n}: 右奇异向量 (正交矩阵)
```

**截断SVD (低秩近似)**:
```python
def truncated_svd(A, rank):
    """
    截断SVD低秩近似

    Args:
        A: (m, n) 矩阵
        rank: 目标秩

    Returns:
        A_approx: 低秩近似矩阵
    """
    U, S, V = torch.svd(A)

    # 截断到目标秩
    U_r = U[:, :rank]
    S_r = S[:rank]
    V_r = V[:, :rank]

    # 重构
    A_approx = U_r @ torch.diag(S_r) @ V_r.T

    return A_approx
```

**应用**:
- 图像压缩
- 降噪
- 推荐系统
- 秩计算法

---

### 2. CP分解 (CANDECOMP/PARAFAC)

**分解公式**:
```
T ∈ R^{I₁×I₂×...×I_N}

T ≈ Σ_{r=1}^R λ_r ∘_{n=1}^N a_r^{(n)}

其中:
- R: CP秩
- λ_r: 权重因子
- a_r^{(n)}: 第n模的因子向量
- ∘: 向量外积
```

**实现代码**:
```python
class CPDecomposition:
    """CP分解"""

    def __init__(self, rank, max_iter=1000, tol=1e-6):
        self.rank = rank
        self.max_iter = max_iter
        self.tol = tol

    def decompose(self, T):
        """
        Args:
            T: (I1, I2, ..., IN) 张量

        Returns:
            factors: [A1, A2, ..., AN] 因子矩阵列表
            weights: (R,) 权重因子
        """
        N = len(T.shape)
        shape = T.shape

        # 初始化因子
        factors = []
        for n in range(N):
            # 随机初始化
            A = torch.randn(shape[n], self.rank)
            A = F.normalize(A, dim=0)
            factors.append(A)

        weights = torch.ones(self.rank)

        # ALS迭代
        for _ in range(self.max_iter):
            for n in range(N):
                # 更新第n个因子
                factors[n] = self._update_factor(T, factors, weights, n)

            # 更新权重
            weights = self._update_weights(T, factors)

        return factors, weights

    def _update_factor(self, T, factors, weights, n):
        """更新第n个因子"""
        # 计算Khatri-Rao积
        kr = self._khatri_rao_prod(factors, exclude=n)

        # 最小二乘求解
        # ... (实现细节略)
        return updated_factor

    def reconstruct(self, factors, weights):
        """从CP分解重构张量"""
        N = len(factors)
        rank = weights.shape[0]

        # 重构
        T_reconstructed = torch.zeros([f.shape[0] for f in factors])
        for r in range(rank):
            outer = weights[r]
            for n in range(N):
                outer = outer.unsqueeze(-1) * factors[n][:, r]
            T_reconstructed = T_reconstructed + outer

        return T_reconstructed
```

**优缺点**:
- ✓ 适合稀疏张量
- ✓ 存在唯一分解性(在特定条件下)
- ✗ 数值不稳定(病态)
- ✗ CP秩可能很大

---

### 3. Tucker分解

**分解公式**:
```
T ∈ R^{I₁×I₂×...×I_N}

T ≃ G ×₁ A₁ ×₂ A₂ ×₃ ... ×ₙ A_N

其中:
- G: 核心张量 (core tensor)
- A_n: 第n模的因子矩阵
- ×ₙ: 模-n张量积
```

**可视化**:
```
    A₁           A₂           A₃
    ↑             ↑            ↑
    ├────────────────────────┤
    │       Core Tensor G     │
    │   (小尺寸, 捕信息)    │
    ├────────────────────────┤
    ↓             ↓            ↓
  重构的原始张量 T_approx
```

**HOSVD (Higher-Order SVD)**:
```python
def hosvd(T, ranks):
    """
    高阶SVD Tucker分解

    Args:
        T: (I1, I2, ..., IN) 张量
        ranks: (r1, r2, ..., rN) 各模的秩

    Returns:
        core: 核心张量
        factors: 因子矩阵列表
    """
    N = len(T.shape)

    # 对每个模进行展开
    factors = []
    for n in range(N):
        # 展开第n模
        T_n = torch.movedim(T, n, 0)  # 将第n维移到第一维
        I_n = T.shape[0]
        new_shape = [I_n] + list(T.shape[1:n] + T.shape[n+1:])

        # 重塑为矩阵
        matrix = T_n.reshape(I_n, -1)

        # SVD分解
        U, S, V = torch.svd(matrix)

        # 截断
        r_n = ranks[n]
        factors.append(V[:, :r_n])

    # 计算核心张量
    core = _compute_core_tensor(T, factors, ranks)

    return core, factors


def tucker_to_tensor(core, factors):
    """从Tucker分解重构张量"""
    # 对每个模应用因子矩阵
    T_approx = core.clone()
    for n, factor in enumerate(factors):
        # 模-n乘法
        T_approx = torch.tensordot(T_approx, factor.T, dims=([0, n]))

    return T_approx
```

**应用**:
- 数据压缩
    - 特征提取
    - 多维数据分析
    - 张量补全

**与CP分解对比**:
| 特性 | CP | Tucker |
|------|-----|--------|
| 分解形式 | 积的和 | 核心×因子 |
| 核心 | 对角 | 密集小张量 |
| 唯一性 | 条件唯一 | 需要约束 |
| 适用场景 | 稀疏张量 | 密集张量 |

---

### 4. TT (Tensor Train) 分解

**分解公式**:
```
T ∈ R^{I₁×I₂×...×Iₙ}

T ≃ G₁ × G₂ × ... × G_N

其中:
- G_n ∈ R^{r_{n-1}×I_n×r_n}
- r_0 = r_N = 1
- r_n: TT秩 (TT-rank)
```

**可视化**:
```
原始张量: 100×200×150×300

TT分解:
  G₁: 1×100×5
    ↓
  G₂: 5×200×10
    ↓
  G₃: 10×150×8
    ↓
  G₄: 8×300×1

存储量: 大幅降低
```

**核心算法 (ALS)**:
```python
class TTDecomposition:
    """TT分解"""

    def __init__(self, tt_ranks, max_iter=100):
        self.tt_ranks = tt_ranks  # [r1, r2, ..., r_N-1]
        self.max_iter = max_iter

    def decompose(self, T):
        """
        TT分解ALS算法

        Args:
            T: (I1, I2, ..., IN) 张量

        Returns:
            cores: [G1, G2, ..., GN] TT核心列表
        """
        N = len(T.shape)
        shape = T.shape

        # 初始化核心
        cores = self._initialize_cores(shape, self.tt_ranks)

        # ALS迭代
        for _ in range(self.max_iter):
            for n in range(N):
                cores[n] = self._update_core(T, cores, n)

        return cores

    def _update_core(self, T, cores, n):
        """更新第n个核心"""
        # 左正交化
        left_cores = cores[:n]
        right_cores = cores[n+1:]

        # 计算左/右展开
        left_contracted = self._contract_left(left_cores)
        right_contracted = self._contract_right(right_cores)

        # 求解最小二乘问题
        # ... (实现细节略)

        return updated_core

    def reconstruct(self, cores):
        """从TT核心重构张量"""
        result = cores[0]
        for core in cores[1:]:
            result = torch.tensordot(result, core, dims=([0]))

        return result
```

**优缺点**:
- ✓ 参数效率高
- ✓ 数值稳定
- ✓ 支持逐元素运算
- ✗ TT秩可能较大
- ✗ 难以处理高阶张量

---

### 5. CUR分解

**分解公式**:
```
A ∈ R^{m×n}

A ≈ C × U × R

其中:
- C ∈ R^{m×r}: 列选择矩阵 (从A的列选择)
- U ∈ R^{r×r}: 核心/交汇矩阵
- R ∈ R^{r×n}: 行选择矩阵 (从A的行选择)
- r: 目标秩

特点: C和R是A的**实际**行/列，保持物理意义
```

**与SVD对比**:
| 特性 | SVD | CUR |
|------|-----|-----|
| 左/右奇异向量 | 虚构的 | **实际的** |
| 物理可解释性 | 弱 | **强** |
| 增量更新 | 困难 | **容易** |
| 适用场景 | 通用 | 大规模/增量 |

**实现代码**:
```python
def cur_decomposition(A, rank):
    """
    CUR分解

    Args:
        A: (m, n) 矩阵
        rank: 目标秩

    Returns:
        C: (m, rank) 列矩阵
        U: (rank, rank) 核心矩阵
        R: (rank, n) 行矩阵
    """
    m, n = A.shape

    # 1. 重要性采样选择列
    col_importance = torch.sum(A**2, dim=0)  # 列范数平方
    col_prob = col_importance / col_importance.sum()
    col_indices = torch.multinomial(
        torch.ones(m, dtype=torch.float64),
        num_samples=rank,
        replacement=False
    )
    C = A[:, col_indices]

    # 2. 重要性采样选择行
    row_importance = torch.sum(A**2, dim=1)  # 行范数平方
    row_prob = row_importance / row_importance.sum()
    row_indices = torch.multinomial(
        row_prob, num_samples=rank, replacement=False
    )
    R = A[row_indices, :]

    # 3. 计算核心矩阵 U
    # 使用最小二乘: U = pinv(C) × A × pinv(R)
    U = torch.pinverse(C) @ A @ torch.pinverse(R)

    return C, U, R
```

**应用**:
- 大规模矩阵分解
- 增式学习
- 推荐系统
- 图信号处理

---

### 6. 混合分解

**Tucker + CUR**:
```
T ≈ (C_Tucker × U_Tucker × R_Tucker) × (C_CUR × U_CUR × R_CUR)
```

**TT + CUR**:
```
T ≃ (G₁ × C_CUR) × (G₂ × C_CUR) × ... × (G_N × C_CUR)
```

---

## tCURLoRA方法

### 核心思想

**将张量CUR分解应用于LoRA，实现参数高效微调**

```
传统LoRA (矩阵分解):
W' = W + AB

tCURLoRA (张量分解):
W → Reshape → T → CUR分解
W' = W + reshape(C × U' × R)
只微调核心张量U
```

### 数学公式

**标准LoRA**:
```python
# LoRA
W' = W + AB
其中:
W ∈ R^{d×k} (原始权重)
A ∈ R^{d×r} (降维矩阵)
B ∈ R^{r×k} (升维矩阵)
r << min(d, k) (低秩秩数)
```

**tCURLoRA**:
```python
# tCURLoRA
# 1. Reshape权重为张量
T = reshape(W, [d₁, d₂, ..., dₙ])

# 2. CUR分解
T ≈ C × U × R

# 3. 微调
T' = T + C × (U + ΔU) × R
# 只微调 ΔU

# 4. 前向传播
W' = reshape(T', [d, k])
```

### 完整实现

```python
import torch
import torch.nn as nn

class TCURLoRA(nn.Module):
    """
    张量CUR分解LoRA (tCURLoRA)

    ICML 2024论文: Tensor CUR Decomposition for Low-Rank Adaptation
    """
    def __init__(self, original_weight, rank, tensor_shape=None):
        super().__init__()

        # 冻结原始权重
        self.original_weight = nn.Parameter(
            original_weight.clone(), requires_grad=False
        )

        d, k = original_weight.shape

        # 确定张量形状
        if tensor_shape is None:
            tensor_shape = self._infer_tensor_shape(d, k)

        self.tensor_shape = tensor_shape
        self.rank = rank

        # 初始化CUR分解
        self.C, self.U, self.Rs = self._initialize_cur(
            original_weight, tensor_shape, rank
        )

    def _infer_tensor_shape(self, d, k):
        """推断合适的张量分解形状"""
        factors = []
        temp = d * k

        # 3阶或4阶张量
        for _ in range(3):
            for i in range(int(temp**0.5), 0, -1):
                if temp % i == 0:
                    factors.append(i)
                    temp = temp // i
                    break

        return tuple(factors) if len(factors) == 3 else (d, k)

    def _initialize_cur(self, weight, tensor_shape, rank):
        """初始化CUR分解"""

        # Reshape为张量
        T = weight.reshape(tensor_shape)

        # 选择重要列和行 (基于重要性采样)
        C, col_idx = self._select_columns(T, rank)
        Rs, row_indices = self._select_rows(T, rank)

        # 计算核心张量
        U = torch.randn([rank] * len(tensor_shape)) * 0.01

        # 创建可训练参数
        C = nn.Parameter(C, requires_grad=False)
        U = nn.Parameter(U, requires_grad=True)  # 只微调U
        Rs = [nn.Parameter(R, requires_grad=False) for R in Rs]

        return C, U, Rs

    def _select_columns(self, T, rank):
        """基于重要性采样选择列"""
        # 计算列重要性
        n = T.shape[-1]
        importance = torch.sum(T**2, dim=tuple(range(T.ndim-1)))

        # 归一化为概率
        prob = importance / torch.sum(importance)

        # 采样列索引
        indices = torch.multinomial(prob, rank, replacement=False)

        # 提取列
        C = torch.index_select(T, -1, indices)

        return C, indices

    def _select_rows(self, T, rank):
        """选择重要行"""
        Rs = []
        for mode in range(T.ndim):
            # 计算该模态的行重要性
            importance = torch.sum(T**2, dim=[
                i for i in range(T.ndim) if i != mode
            ])

            # 归一化
            prob = importance / torch.sum(importance)

            # 采样
            indices = torch.multinomial(prob, rank, replacement=False)

            # 提取
            R = torch.index_select(T, mode, indices)
            Rs.append(R)

        return Rs, None

    def forward(self, x):
        """前向传播"""
        delta = self._compute_delta()
        W_eff = self.original_weight + delta
        return F.linear(x, W_eff)

    def _compute_delta(self):
        """计算微调增量"""
        # C × (U + ΔU) × Rs
        # 简化实现: 返回核心张量reshape
        delta = self.U.reshape(self.original_weight.shape)
        return delta

    def get_trainable_parameters(self):
        """获取可训练参数(只有U)"""
        return [self.U]
```

### 与其他PEFT方法对比

| 方法 | 可训练参数 | 适用场景 | 特点 |
|------|------------|----------|------|
| LoRA | A·r + r·B | 通用PEFT | 简单高效 |
| Adapter | d·h | 深度微调 | 模块独立 |
| Prefix | n·l·d | 生成任务 | 仅微调提示 |
| **tCURLoRA** | U (核心张量) | 多模态/医学 | 结构感知 |

---

## 应用指南

### 应用决策树

```
需要张量分解?

高维数据 (3D+)
    ↓
数据类型?
├─ 图像/视频 → Tucker, TT
├─ 推荐系统 → CP, CUR
├─ 信号处理 → Tucker, TT
└─ 文本/语言 → Transformer (TT适用)

大规模?
    ↓ Yes
├─ 需要增量更新? → CUR
└─ 不需要 → Tucker, TT

数据稀疏?
    ↓ Yes
├─ 极度稀疏 → CP
└─ 中度稀疏 → Tucker

需要可解释性?
    ↓ Yes
├─ 物理意义 → CUR
└─ 数学性质 → SVD, Tucker
```

### 参数选择指南

```python
def suggest_rank(T, method='tucker'):
    """
    建议合适的秩

    Args:
        T: 张量
        method: 分解方法

    Returns:
        rank: 建议的秩
    """
    if method == 'tucker':
        # Tucker秩: 各模的秩
        ranks = []
        for n in range(T.ndim):
            shape = list(T.shape)
            new_shape = [shape[n], -1]
            matrix = T.movedim(n, 0).reshape(new_shape)
            rank = torch.matrix_rank(matrix).item()
            ranks.append(min(rank, matrix.shape[1]//2))
        return tuple(ranks)

    elif method == 'cp':
        # CP秩: 基于解释方差比例
        # ... (实现细节略)
        pass

    elif method == 'tt':
        # TT秩: 基于近似精度
        # ... (实现细节略)
        pass
```

### 与tCURLoRA关联

```python
# tCURLoRA与张量分解方法的关联
张量分解体系关联 = {
    "tCURLoRA基础": "CUR分解",
    "扩展方向1": "Tucker-CUR混合分解",
    "扩展方向2": "TT-CUR混合分解",
    "应用场景": "医学图像分割PEFT",
    "优势": "结构感知 + 物理可解释性"
}
```

---

## 代码实现

### 综合对比工具

```python
class TensorDecompositionToolbox:
    """张量分解工具箱"""

    def __init__(self):
        self.methods = {
            'svd': self.svd_decomposition,
            'cp': self.cp_decomposition,
            'tucker': self.tucker_decomposition,
            'tt': self.tt_decomposition,
            'cur': self.cur_decomposition
        }

    def decompose(self, T, method='tucker', **kwargs):
        """
        统一分解接口

        Args:
            T: 输入张量
            method: 分解方法
            **kwargs: 方法特定参数

        Returns:
            result: 分解结果
        """
        if method not in self.methods:
            raise ValueError(f"Unknown method: {method}")

        return self.methods[method](T, **kwargs)

    def svd_decomposition(self, T, rank=None):
        """SVD分解"""
        U, S, V = torch.svd(T.reshape(-1, T.shape[-1]))

        if rank is not None:
            U = U[:, :rank]
            S = S[:rank]
            V = V[:, :rank]

        return {'U': U, 'S': S, 'V': V}

    def reconstruct_svd(self, decomp):
        """从SVD重构"""
        return decomp['U'] @ torch.diag(decomp['S']) @ decomp['V'].T
```

### 评估指标

```python
def evaluate_decomposition(T_original, T_approx):
    """
    评估张量分解质量

    Args:
        T_original: 原始张量
        T_approx: 重构张量

    Returns:
        metrics: 评估指标字典
    """
    # 相对误差
    rel_error = torch.norm(T_approx - T_original) / torch.norm(T_original)

    # 核心一致性
    core_consistency = torch.sum(T_approx * T_original) / (
        torch.norm(T_approx) * torch.norm(T_original)
    )

    # 负荷
    compression_ratio = T_original.numel() / T_approx.numel()

    return {
        'relative_error': rel_error.item(),
        'core_consistency': core_consistency.item(),
        'compression_ratio': compression_ratio.item()
    }
```

---

## 更新日志

### 2026年2月9日
- 创建张量分解方法手册
- 建立SVD、CP、Tucker、TT、CUR完整知识链
- 添加tCURLoRA方法详解
- 添加代码实现和评估工具

---

*文档创建时间: 2026年2月9日*
*基于: [3-02] tCURLoRA, [3-04], [3-05] 张量分解论文*
