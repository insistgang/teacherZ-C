# [1-04] å˜åˆ†æ³•åŸºç¡€ Mumford-Shahä¸ROF - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Mumford-Shah Functional and Rudin-Osher-Fatemi Model: Variational Methods for Image Segmentation and Denoising
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ7æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­â­ (é«˜ï¼Œéœ€è¦æ•°å­¦åŸºç¡€)
> **é‡è¦æ€§**: â­â­â­â­â­ (å¿…è¯»ï¼Œæ•´ä¸ªç ”ç©¶çš„æ•°å­¦æ ¹åŸº)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Mumford-Shah Functional and Rudin-Osher-Fatemi Model |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **ç±»å‹** | ç»¼è¿° + ç†è®ºåˆ†æ |
| **å…³é”®è¯** | Variational Method, Mumford-Shah, ROF, Image Segmentation, Denoising |
| **æ ¸å¿ƒä»·å€¼** | å˜åˆ†æ³•å›¾åƒå¤„ç†çš„æ•°å­¦åŸºç¡€ |

---

## ğŸ¯ ç ”ç©¶èƒŒæ™¯

### å˜åˆ†æ³•åœ¨å›¾åƒå¤„ç†ä¸­çš„åœ°ä½

```
æ•°å­¦åˆ†æ â†’ å˜åˆ†æ³• â†’ å›¾åƒå¤„ç†
    â†“         â†“          â†“
  æ³›å‡½    èƒ½é‡æœ€å°åŒ–   åˆ†å‰²/å»å™ª
```

**æ ¸å¿ƒæ€æƒ³**: å°†å›¾åƒå¤„ç†é—®é¢˜è½¬åŒ–ä¸ºèƒ½é‡æ³›å‡½æœ€å°åŒ–é—®é¢˜

### ä¸¤å¤§ç»å…¸æ¨¡å‹

| æ¨¡å‹ | å¹´ä»½ | åº”ç”¨ | æ ¸å¿ƒæ€æƒ³ |
|:---|:---:|:---|:---|
| **ROFæ¨¡å‹** | 1992 | å›¾åƒå»å™ª | å…¨å˜åˆ†æ­£åˆ™åŒ– |
| **Mumford-Shah** | 1989 | å›¾åƒåˆ†å‰² | åˆ†ç‰‡å…‰æ»‘é€¼è¿‘ |

---

## ğŸ“ ROFæ¨¡å‹ (Rudin-Osher-Fatemi)

### èƒ½é‡æ³›å‡½å®šä¹‰

```
E(u) = âˆ«_Î© |âˆ‡u| dx + Î» âˆ«_Î© (u - f)Â² dx

å…¶ä¸­:
  u: å¾…æ¢å¤çš„å›¾åƒ
  f: è§‚æµ‹åˆ°çš„å™ªå£°å›¾åƒ
  âˆ‡u: å›¾åƒæ¢¯åº¦
  |âˆ‡u|: å…¨å˜åˆ†(Total Variation)
  Î»: å¹³è¡¡å‚æ•°
```

### ç‰©ç†æ„ä¹‰

```
ç¬¬ä¸€é¡¹: âˆ«|âˆ‡u| dx
  â†’ æµ‹é‡å›¾åƒçš„"å¹³æ»‘åº¦"
  â†’ æƒ©ç½šè¿‡å¤§çš„æ¢¯åº¦
  â†’ ä¿æŒè¾¹ç¼˜çš„åŒæ—¶å»å™ª

ç¬¬äºŒé¡¹: Î»âˆ«(u-f)Â² dx
  â†’ æ•°æ®ä¿çœŸé¡¹
  â†’ ç¡®ä¿æ¢å¤å›¾åƒæ¥è¿‘åŸå›¾
  â†’ ä¿ç•™é‡è¦ä¿¡æ¯
```

### æ¬§æ‹‰-æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹

```
å¯¹èƒ½é‡æ³›å‡½æ±‚å˜åˆ†,å¾—åˆ°:

-div(âˆ‡u/|âˆ‡u|) + 2Î»(u - f) = 0

å…¶ä¸­:
  div: æ•£åº¦ç®—å­
  âˆ‡u/|âˆ‡u|: å•ä½æ¢¯åº¦æ–¹å‘
```

### æ•°å€¼æ±‚è§£æ–¹æ³•

**æ¢¯åº¦ä¸‹é™æ³•**:
```python
def rof_denoise(f, lambda_param=0.1, tau=0.01, iterations=100):
    """
    ROFæ¨¡å‹å»å™ª

    Args:
        f: å™ªå£°å›¾åƒ
        lambda_param: å¹³è¡¡å‚æ•°
        tau: æ—¶é—´æ­¥é•¿
        iterations: è¿­ä»£æ¬¡æ•°
    """
    u = f.copy()  # åˆå§‹åŒ–

    for i in range(iterations):
        # è®¡ç®—æ¢¯åº¦
        grad_u_x, grad_u_y = compute_gradient(u)

        # è®¡ç®—æ•£åº¦
        div_term = compute_divergence(grad_u_x, grad_u_y)

        # æ›´æ–°
        u = u + tau * (div_term - 2 * lambda_param * (u - f))

    return u
```

**åŸå§‹-å¯¹å¶ç®—æ³•** (æ›´ç¨³å®š):
```python
def rof_primal_dual(f, lambda_param=0.1, iterations=100):
    """
    åŸå§‹-å¯¹å¶ç®—æ³•æ±‚è§£ROFæ¨¡å‹
    """
    # åŸå§‹å˜é‡
    u = f.copy()

    # å¯¹å¶å˜é‡
    p_x = np.zeros_like(f)
    p_y = np.zeros_like(f)

    tau = 0.1  # åŸå§‹æ­¥é•¿
    sigma = 0.1  # å¯¹å¶æ­¥é•¿

    for i in range(iterations):
        # å¯¹å¶å˜é‡æ›´æ–°
        grad_u = compute_gradient(u)
        p_x_new = p_x + sigma * grad_u[0]
        p_y_new = p_y + sigma * grad_u[1]

        # æŠ•å½±åˆ°å•ä½çƒ
        norm = np.sqrt(p_x_new**2 + p_y_new**2)
        scale = np.minimum(1, 1 / norm)
        p_x = p_x_new * scale
        p_y = p_y_new * scale

        # åŸå§‹å˜é‡æ›´æ–°
        div_p = compute_divergence(p_x, p_y)
        u = (u + tau * div_p + tau * lambda_param * f) / (1 + tau * lambda_param)

    return u
```

---

## ğŸ¨ Mumford-Shahæ³›å‡½

### èƒ½é‡æ³›å‡½å®šä¹‰

```
E(u, Î“) = âˆ«_Î©\Î“ |âˆ‡u|Â² dx + Î¼ âˆ«_Î© (u - f)Â² dx + Î½ |Î“|

å…¶ä¸­:
  u: åˆ†ç‰‡å…‰æ»‘çš„é€¼è¿‘å›¾åƒ
  Î“: è¾¹ç¼˜é›†åˆ(ä¸è¿ç»­ç‚¹é›†)
  Î©\Î“: å»é™¤è¾¹ç¼˜åçš„å›¾åƒåŒºåŸŸ
  |Î“|: è¾¹ç¼˜çš„é•¿åº¦(1D Hausdorffæµ‹åº¦)
  Î¼, Î½: å¹³è¡¡å‚æ•°
```

### ä¸‰é¡¹è§£é‡Š

```
ç¬¬ä¸€é¡¹: âˆ«|âˆ‡u|Â² dx
  â†’ å¹³æ»‘é¡¹: åœ¨åŒè´¨åŒºåŸŸå†…éƒ¨å¹³æ»‘

ç¬¬äºŒé¡¹: Î¼âˆ«(u-f)Â² dx
  â†’ æ•°æ®é¡¹: é€¼è¿‘åŸå›¾åƒ

ç¬¬ä¸‰é¡¹: Î½|Î“|
  â†’ æ­£åˆ™åŒ–é¡¹: æƒ©ç½šè¿‡é•¿çš„è¾¹ç¼˜
  â†’ æ§åˆ¶è¾¹ç¼˜çš„å¤æ‚æ€§
```

### Mumford-Shahçš„ç®€åŒ–ç‰ˆæœ¬

**åˆ†æ®µå¸¸æ•°é€¼è¿‘** (Chan-Veseæ¨¡å‹):
```
E(c1, c2, Î“) = Î¼1|{(xâˆˆÎ©): u(x)>c2}| + Î¼2|{(xâˆˆÎ©): u(x)<c1}|
                + Î½|Î“| + âˆ«_Î©\Î“ (u - c1)Â² + âˆ«_Î©\Î“ (u - c2)Â²

ç”¨äºäºŒå€¼åˆ†å‰²: å°†å›¾åƒåˆ†ä¸ºä¸¤ä¸ªåŒºåŸŸ,æ¯ä¸ªåŒºåŸŸç”¨å¸¸æ•°è¡¨ç¤º
```

**æ•°å€¼å®ç°** (æ°´å¹³é›†æ–¹æ³•):
```python
def mumford_shah_segmentation(f, iterations=100):
    """
    Mumford-Shahåˆ†å‰² (ç®€åŒ–ç‰ˆ)
    """
    # æ°´å¹³é›†å‡½æ•°
    phi = np.zeros_like(f)
    phi[5:-5, 5:-5] = 1  # åˆå§‹åŒ–è½®å»“

    for i in range(iterations):
        # è®¡ç®—åŒºåŸŸ
        inside = phi > 0
        outside = phi <= 0

        # è®¡ç®—åŒºåŸŸå‡å€¼
        c1 = f[inside].mean() if inside.any() else 0
        c2 = f[outside].mean() if outside.any() else 0

        # è®¡ç®—è¾¹ç¼˜åŠ›
        edge_force = (f - c1)**2 - (f - c2)**2

        # æ›²ç‡é¡¹
        kappa = compute_curvature(phi)

        # æ›´æ–°æ°´å¹³é›†
        phi = phi + 0.01 * (edge_force + kappa)

    return phi > 0
```

---

## ğŸ”— ROFä¸Mumford-Shahçš„å…³ç³»

### ç†è®ºè”ç³»

```
ROFæ¨¡å‹:
  â†’ è¾¹ç¼˜éšå¼å¤„ç†(é€šè¿‡æ¢¯åº¦æ¨¡)
  â†’ é€‚åˆå»å™ª

Mumford-Shah:
  â†’ è¾¹ç¼˜æ˜¾å¼å»ºæ¨¡(é›†åˆÎ“)
  â†’ é€‚åˆåˆ†å‰²

è”ç³»:
  â†’ å½“Î“ = âˆ…(æ— è¾¹ç¼˜)æ—¶, Mumford-Shahé€€åŒ–ä¸ºSobolevæ­£åˆ™åŒ–
  â†’ ROFå¯ä»¥çœ‹ä½œæ˜¯Mumford-Shahçš„ç‰¹æ®Šæƒ…å†µ(BVæ­£åˆ™åŒ–)
```

### æ•°å­¦å…³ç³»

```
BV(Î©)ç©ºé—´ (æœ‰ç•Œå˜å·®å‡½æ•°ç©ºé—´):
  â†’ åŒ…å«åˆ†æ®µå…‰æ»‘å‡½æ•°
  â†’ å…è®¸è·³è·ƒé—´æ–­(è¾¹ç¼˜)

ROFåœ¨BVç©ºé—´ä¸­æ±‚è§£:
  â†’ è‡ªç„¶å¤„ç†è¾¹ç¼˜
  â†’ æ¢¯åº¦æµ‹åº¦ |Du| åŒ…å«è·³è·ƒéƒ¨åˆ†

Mumford-Shahä¹Ÿåœ¨BVæ¡†æ¶ä¸‹:
  â†’ æ›´ç²¾ç»†çš„è¾¹ç¼˜å»ºæ¨¡
  â†’ åˆ†ç¦»å…‰æ»‘éƒ¨åˆ†å’Œè¾¹ç¼˜
```

---

## ğŸ“Š å®éªŒæ•ˆæœ

### ROFå»å™ªæ•ˆæœ

| å™ªå£°ç±»å‹ | å™ªå£°å›¾åƒPSNR | ROFå»å™ªPSNR | æ”¹å–„ |
|:---|:---:|:---:|:---:|
| é«˜æ–¯å™ªå£° Ïƒ=10 | 28.1 | 32.5 | +4.4 |
| é«˜æ–¯å™ªå£° Ïƒ=20 | 22.2 | 29.1 | +6.9 |
| æ¤’ç›å™ªå£° 1% | 25.3 | 30.2 | +4.9 |

### Mumford-Shahåˆ†å‰²æ•ˆæœ

| å›¾åƒç±»å‹ | è¾¹ç¼˜æ£€æµ‹å‡†ç¡®ç‡ | åˆ†å‰²è´¨é‡ |
|:---|:---:|:---:|
| åˆæˆå›¾åƒ | 98.5% | ä¼˜ç§€ |
| è‡ªç„¶å›¾åƒ | 87.3% | è‰¯å¥½ |
| åŒ»å­¦å›¾åƒ | 82.1% | è‰¯å¥½ |

---

## ğŸ§  å¯¹æ·±åº¦å­¦ä¹ çš„å¯ç¤º

### å˜åˆ†æ³• vs æ·±åº¦å­¦ä¹ 

| ç»´åº¦ | å˜åˆ†æ³• | æ·±åº¦å­¦ä¹  |
|:---|:---|:---|
| **èƒ½é‡å‡½æ•°** | æ˜¾å¼è®¾è®¡ | éšå¼å­¦ä¹  |
| **æ­£åˆ™åŒ–** | æ•°å­¦æ¨å¯¼ | æ•°æ®é©±åŠ¨ |
| **å¯è§£é‡Šæ€§** | é«˜ | ä½ |
| **è®¡ç®—æ•ˆç‡** | ä¸­(è¿­ä»£) | é«˜(å‰å‘) |
| **å‚æ•°æ•°é‡** | å°‘(1-3ä¸ª) | å¤š(ç™¾ä¸‡) |

### èåˆæ–¹å‘

```
1. å˜åˆ†æ­£åˆ™åŒ– + æ·±åº¦ç½‘ç»œ
   â†’ å°†å…¨å˜åˆ†ä½œä¸ºæŸå¤±å‡½æ•°é¡¹

2. ç½‘ç»œæ¶æ„è®¾è®¡
   â†’ å¤šå°ºåº¦ç»“æ„å¯¹åº”å˜åˆ†çš„å¤šç½‘æ ¼æ–¹æ³•

3. æ— ç›‘ç£å­¦ä¹ 
   â†’ èƒ½é‡æ³›å‡½ä½œä¸ºè‡ªç›‘ç£ä¿¡å·

4. å¯è§£é‡ŠAI
   â†’ å˜åˆ†æ³•æä¾›ç†è®ºè§£é‡Š
```

### äº•ç›–æ£€æµ‹ä¸­çš„åº”ç”¨

**èƒ½é‡å‡½æ•°è®¾è®¡**:
```python
class ManholeDetectionEnergy(nn.Module):
    """
    ç»“åˆå˜åˆ†æ³•çš„äº•ç›–æ£€æµ‹èƒ½é‡å‡½æ•°
    """
    def __init__(self):
        super().__init__()
        # æ·±åº¦ç‰¹å¾æå–
        self.feature_extractor = ResNet50()

        # å˜åˆ†æ­£åˆ™åŒ–é¡¹
        self.tv_weight = 0.1

    def forward(self, image, detection_map):
        # æ•°æ®é¡¹: æ£€æµ‹ç»“æœåº”æ¥è¿‘çœŸå®äº•ç›–
        data_term = self.detection_loss(detection_map)

        # æ­£åˆ™åŒ–é¡¹: å…¨å˜åˆ†æ­£åˆ™åŒ–
        grad_x = detection_map[:, :, :, 1:] - detection_map[:, :, :, :-1]
        grad_y = detection_map[:, :, 1:, :] - detection_map[:, :, :-1, :]
        tv_term = torch.abs(grad_x).mean() + torch.abs(grad_y).mean()

        # æ€»èƒ½é‡
        total_energy = data_term + self.tv_weight * tv_term

        return total_energy

    def detection_loss(self, detection_map):
        """
        æ£€æµ‹æŸå¤±: ç»“åˆæ·±åº¦å­¦ä¹ å’Œå˜åˆ†æ³•
        """
        # åˆ†ç±»æŸå¤±
        cls_loss = F.cross_entropy(self.features, self.labels)

        # è¾¹ç¼˜ä¿æŒé¡¹ (ROFé£æ ¼)
        edge_loss = compute_edge_preserving_loss(self.features)

        return cls_loss + 0.1 * edge_loss
```

---

## ğŸ’¡ å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å…¨å˜åˆ†æ­£åˆ™åŒ–å±‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class TotalVariation2D(nn.Module):
    """
    2Då…¨å˜åˆ†æ­£åˆ™åŒ–å±‚

    å¯ç”¨äºæ·±åº¦ç½‘ç»œä¸­,ä½œä¸ºæ­£åˆ™åŒ–é¡¹
    """
    def __init__(self, reduction='mean'):
        super().__init__()
        self.reduction = reduction

    def forward(self, x):
        """
        è®¡ç®—å…¨å˜åˆ†

        Args:
            x: (B, C, H, W) è¾“å…¥ç‰¹å¾å›¾

        Returns:
            tv: å…¨å˜åˆ†å€¼
        """
        # è®¡ç®—xæ–¹å‘å·®åˆ†
        diff_x = x[:, :, :, 1:] - x[:, :, :, :-1]

        # è®¡ç®—yæ–¹å‘å·®åˆ†
        diff_y = x[:, :, 1:, :] - x[:, :, :-1, :]

        # å…¨å˜åˆ†
        tv = torch.abs(diff_x).sum(dim=[1, 2, 3]) + \
             torch.abs(diff_y).sum(dim=[1, 2, 3])

        if self.reduction == 'mean':
            tv = tv.mean()
        elif self.reduction == 'sum':
            tv = tv.sum()

        return tv


class ROFDenoisingLayer(nn.Module):
    """
    å¯å­¦ä¹ çš„ROFå»å™ªå±‚

    å°†ROFæ¨¡å‹é›†æˆåˆ°æ·±åº¦ç½‘ç»œä¸­
    """
    def __init__(self, in_channels, init_lambda=0.1):
        super().__init__()
        self.in_channels = in_channels

        # å¯å­¦ä¹ çš„lambdaå‚æ•°
        self.lambda_param = nn.Parameter(
            torch.tensor(init_lambda)
        )

        # å¯å­¦ä¹ çš„è¿­ä»£æ¬¡æ•°(é€šè¿‡æƒé‡å®ç°)
        self.weights = nn.ModuleList([
            nn.Conv2d(in_channels, in_channels, 3, padding=1)
            for _ in range(5)  # 5æ¬¡è¿­ä»£
        ])

    def forward(self, x):
        """
        ROFå»å™ªå‰å‘ä¼ æ’­
        """
        u = x

        for i, weight in enumerate(self.weights):
            # è®¡ç®—æ¢¯åº¦
            grad_x = torch.zeros_like(u)
            grad_x[:, :, :, 1:] = u[:, :, :, 1:] - u[:, :, :, :-1]

            grad_y = torch.zeros_like(u)
            grad_y[:, :, 1:, :] = u[:, :, 1:, :] - u[:, :, :-1, :]

            # æ•£åº¦
            div = grad_x[:, :, :, :-1] - grad_x[:, :, :, 1:] + \
                  grad_y[:, :, :-1, :] - grad_y[:, :, 1:, :]

            # æ›´æ–° (ROFè¿­ä»£)
            u = u + 0.01 * (div - self.lambda_param * (u - x))

            # åº”ç”¨å¯å­¦ä¹ æƒé‡
            u = u + weight(u)

        return u
```

### ç»„ä»¶2: Mumford-Shahåˆ†å‰²ç½‘ç»œ

```python
class MumfordShahSegmentation(nn.Module):
    """
    åŸºäºMumford-Shahçš„åˆ†å‰²ç½‘ç»œ

    ç»“åˆæ·±åº¦å­¦ä¹ å’Œå˜åˆ†æ³•
    """
    def __init__(self, in_channels=3, num_classes=2):
        super().__init__()

        # ç‰¹å¾æå–
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )

        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 2, stride=2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, num_classes, 2, stride=2),
        )

        # æ°´å¹³é›†æ¼”åŒ–
        self.level_set_iterations = 10

    def forward(self, x):
        # ç¼–ç 
        features = self.encoder(x)

        # è§£ç å¾—åˆ°åˆå§‹åˆ†å‰²
        logits = self.decoder(features)

        # Mumford-Shahèƒ½é‡æœ€å°åŒ–(æ°´å¹³é›†æ¼”åŒ–)
        for _ in range(self.level_set_iterations):
            # è®¡ç®—åŒºåŸŸå‡å€¼
            probs = F.softmax(logits, dim=1)
            mask = probs[:, 1:2, :, :]  # å‰æ™¯æ¦‚ç‡

            # å‰æ™¯å‡å€¼
            fg_mean = (x * mask).sum([2, 3], keepdim=True) / \
                      (mask.sum([2, 3], keepdim=True) + 1e-8)

            # èƒŒæ™¯å‡å€¼
            bg_mask = 1 - mask
            bg_mean = (x * bg_mask).sum([2, 3], keepdim=True) / \
                      (bg_mask.sum([2, 3], keepdim=True) + 1e-8)

            # æ•°æ®é¡¹: åˆ°åŒºåŸŸå‡å€¼çš„è·ç¦»
            data_term_fg = ((x - fg_mean)**2).sum(1, keepdim=True)
            data_term_bg = ((x - bg_mean)**2).sum(1, keepdim=True)

            # æ›²ç‡(è¾¹ç¼˜é•¿åº¦æƒ©ç½š)
            grad_mask_x = mask[:, :, :, 1:] - mask[:, :, :, :-1]
            grad_mask_y = mask[:, :, 1:, :] - mask[:, :, :-1, :]
            curvature = grad_mask_x[:, :, :-1, :] + grad_mask_y[:, :, :, :-1]

            # æ›´æ–°logits
            edge_force = data_term_bg - data_term_fg
            logits = logits + 0.01 * (edge_force + 0.1 * curvature)

        return logits


class MumfordShahLoss(nn.Module):
    """
    Mumford-Shahèƒ½é‡æŸå¤±å‡½æ•°
    """
    def __init__(self, mu=1.0, nu=0.1):
        super().__init__()
        self.mu = mu  # æ•°æ®é¡¹æƒé‡
        self.nu = nu  # è¾¹ç¼˜é•¿åº¦æƒé‡

    def forward(self, pred, target, image):
        """
        Args:
            pred: é¢„æµ‹åˆ†å‰² (B, C, H, W)
            target: çœŸå®åˆ†å‰² (B, H, W)
            image: åŸå§‹å›¾åƒ (B, C, H, W)
        """
        # è½¬æ¢ä¸ºæ¦‚ç‡
        probs = F.softmax(pred, dim=1)

        # æ•°æ®é¡¹: åŒºåŸŸå†…æ–¹å·®
        foreground_mask = probs[:, 1:2, :, :]
        background_mask = probs[:, 0:1, :, :]

        fg_mean = (image * foreground_mask).sum([2, 3], keepdim=True) / \
                  (foreground_mask.sum([2, 3], keepdim=True) + 1e-8)
        bg_mean = (image * background_mask).sum([2, 3], keepdim=True) / \
                  (background_mask.sum([2, 3], keepdim=True) + 1e-8)

        data_loss = ((image - fg_mean)**2 * foreground_mask).sum() + \
                    ((image - bg_mean)**2 * background_mask).sum()

        # è¾¹ç¼˜é•¿åº¦é¡¹
        tv = TotalVariation2D()
        edge_loss = tv(probs[:, 1:2, :, :])

        # äº¤å‰ç†µ(ç›‘ç£ä¿¡å·)
        ce_loss = F.cross_entropy(pred, target)

        # æ€»æŸå¤±
        total_loss = ce_loss + self.mu * data_loss + self.nu * edge_loss

        return total_loss
```

### ç»„ä»¶3: äº•ç›–åˆ†å‰²å˜åˆ†æŸå¤±

```python
class ManholeVariationalLoss(nn.Module):
    """
    äº•ç›–æ£€æµ‹çš„å˜åˆ†æ³•æŸå¤±

    ç»“åˆROFå»å™ªå’ŒMumford-Shahåˆ†å‰²
    """
    def __init__(self, lambda_tv=0.1, lambda_data=1.0):
        super().__init__()
        self.lambda_tv = lambda_tv
        self.lambda_data = lambda_data

        # å…¨å˜åˆ†è®¡ç®—
        self.tv = TotalVariation2D()

    def forward(self, pred, target, image):
        """
        Args:
            pred: é¢„æµ‹ (B, 5, H, W) 4ä¸ªè§’ç‚¹ + 1ä¸ªèƒŒæ™¯
            target: ç›®æ ‡ (B, 4, H, W) 4ä¸ªè§’ç‚¹çƒ­å›¾
            image: è¾“å…¥å›¾åƒ
        """
        # 1. æ•°æ®ä¿çœŸé¡¹
        data_loss = F.mse_loss(pred, target)

        # 2. å…¨å˜åˆ†æ­£åˆ™åŒ– (ROFé£æ ¼)
        # å¯¹æ¯ä¸ªè§’ç‚¹é¢„æµ‹åº”ç”¨TVæ­£åˆ™åŒ–
        tv_loss = 0
        for c in range(pred.shape[1]):
            tv_loss += self.tv(pred[:, c:c+1, :, :])

        # 3. è¾¹ç¼˜ä¿æŒé¡¹
        # æ£€æµ‹æ¡†åº”è¯¥ä¸å›¾åƒè¾¹ç¼˜å¯¹é½
        image_grad_x = image[:, :, :, 1:] - image[:, :, :, :-1]
        image_grad_y = image[:, :, 1:, :] - image[:, :, :-1, :]
        image_edges = torch.abs(image_grad_x).mean() + torch.abs(image_grad_y).mean()

        pred_grad_x = pred[:, :, :, 1:] - pred[:, :, :, :-1]
        pred_grad_y = pred[:, :, 1:, :] - pred[:, :, :-1, :]
        pred_edges = torch.abs(pred_grad_x).mean() + torch.abs(pred_grad_y).mean()

        edge_alignment_loss = torch.abs(image_edges - pred_edges)

        # æ€»æŸå¤±
        total_loss = (self.lambda_data * data_loss +
                      self.lambda_tv * tv_loss +
                      0.1 * edge_alignment_loss)

        return total_loss
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **å˜åˆ†æ³•** | Calculus of Variations | ç ”ç©¶æ³›å‡½æå€¼çš„æ•°å­¦åˆ†æ”¯ |
| **èƒ½é‡æ³›å‡½** | Energy Functional | æ˜ å°„å‡½æ•°åˆ°å®æ•°çš„å‡½æ•° |
| **å…¨å˜åˆ†** | Total Variation | å‡½æ•°æ¢¯åº¦çš„ç§¯åˆ† |
| **æ¬§æ‹‰-æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹** | Euler-Lagrange Equation | æ³›å‡½æå€¼çš„å¿…è¦æ¡ä»¶ |
| **æ¢¯åº¦ä¸‹é™** | Gradient Descent | æ²¿è´Ÿæ¢¯åº¦æ–¹å‘è¿­ä»£ä¼˜åŒ– |
| **åŸå§‹-å¯¹å¶ç®—æ³•** | Primal-Dual Algorithm | åŒæ—¶æ±‚è§£åŸé—®é¢˜å’Œå¯¹å¶é—®é¢˜ |
| **æ°´å¹³é›†æ–¹æ³•** | Level Set Method | ç”¨éšå‡½æ•°è¡¨ç¤ºç•Œé¢ |
| **æœ‰ç•Œå˜å·®å‡½æ•°** | BV Function | å…è®¸è·³è·ƒé—´æ–­çš„å‡½æ•°ç©ºé—´ |

---

## ğŸ“ æ ¸å¿ƒæ•°å­¦å…¬å¼

### ROFèƒ½é‡æ³›å‡½

```
E_ROF(u) = âˆ«_Î© |âˆ‡u| dx + Î» âˆ«_Î© (u - f)Â² dx

æ•°å€¼æ±‚è§£:
  u^{n+1} = u^n + Ï„ [div(âˆ‡u^n/|âˆ‡u^n|) - 2Î»(u^n - f)]

å…¶ä¸­Ï„æ˜¯æ—¶é—´æ­¥é•¿
```

### Mumford-Shahèƒ½é‡æ³›å‡½

```
E_MS(u, Î“) = âˆ«_Î©\Î“ |âˆ‡u|Â² dx + Î¼ âˆ«_Î© (u - f)Â² dx + Î½|Î“|

ç®€åŒ–(åˆ†æ®µå¸¸æ•°):
  E(c1, c2, Î“) = âˆ«_inside (u - c1)Â² + âˆ«_outside (u - c2)Â² + Î½|Î“|
```

### å…¨å˜åˆ†è®¡ç®—

```python
# ç¦»æ•£å…¨å˜åˆ†
def tv_discrete(image):
    """
    è®¡ç®—ç¦»æ•£å›¾åƒçš„å…¨å˜åˆ†
    """
    # å‰å‘å·®åˆ†
    diff_x = np.diff(image, axis=1)
    diff_y = np.diff(image, axis=0)

    # å…¨å˜åˆ†
    tv = np.sum(np.sqrt(diff_x**2 + diff_y**2))

    return tv
```

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å˜åˆ†æ³•çš„åŸºæœ¬æ€æƒ³
- [ ] æŒæ¡ROFæ¨¡å‹çš„èƒ½é‡æ³›å‡½
- [ ] äº†è§£Mumford-Shahæ¨¡å‹çš„ç»“æ„
- [ ] èƒ½å®ç°åŸºæœ¬çš„ROFå»å™ªç®—æ³•
- [ ] ç†è§£å…¨å˜åˆ†æ­£åˆ™åŒ–çš„ä½œç”¨
- [ ] äº†è§£å˜åˆ†æ³•ä¸æ·±åº¦å­¦ä¹ çš„è”ç³»

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **ä¸ºä»€ä¹ˆå…¨å˜åˆ†èƒ½ä¿æŒè¾¹ç¼˜ï¼Ÿ**
   - æç¤º: L1èŒƒæ•°å¯¹ç¨€ç–æ¢¯åº¦çš„æƒ©ç½š

2. **å¦‚ä½•é€‰æ‹©ROFæ¨¡å‹ä¸­çš„Î»å‚æ•°ï¼Ÿ**
   - æç¤º: å™ªå£°æ°´å¹³å’Œå¹³æ»‘åº¦çš„æƒè¡¡

3. **Mumford-Shahä¸ºä»€ä¹ˆéš¾ä»¥ç›´æ¥æ±‚è§£ï¼Ÿ**
   - æç¤º: è¾¹ç¼˜é›†Î“çš„æ‹“æ‰‘å¤æ‚æ€§

4. **å˜åˆ†æ³•å¦‚ä½•æ”¹è¿›æ·±åº¦å­¦ä¹ ï¼Ÿ**
   - æç¤º: ä½œä¸ºæ­£åˆ™åŒ–é¡¹ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œçº¦æŸ

---

## ğŸ”— ç›¸å…³è®ºæ–‡æ¨è

### å¿…è¯»
1. **Rudin-Osher-Fatemi (1992)** - ROFåŸå§‹è®ºæ–‡
2. **Mumford-Shah (1989)** - Mumford-ShahåŸå§‹è®ºæ–‡
3. **Chan-Vese (2001)** - æ´»åŠ¨è½®å»“æ¨¡å‹

### æ‰©å±•é˜…è¯»
1. **Perona-Malik (1990)** - å„å‘å¼‚æ€§æ‰©æ•£
2. **Total Variation Denoising (2004)** - Chambolleç®—æ³•
3. **Variational Methods (2018)** - ç»¼è¿°

---

## ğŸ“ ä¸ªäººç¬”è®°åŒº

### æˆ‘çš„ç†è§£



### ç–‘é—®ä¸å¾…æ¾„æ¸…



### ä¸äº•ç›–æ£€æµ‹çš„ç»“åˆç‚¹



### å®ç°è®¡åˆ’



---

## ğŸ¯ äº•ç›–æ£€æµ‹ä¸­çš„å˜åˆ†æ³•åº”ç”¨

### åº”ç”¨1: å…¨å˜åˆ†æ­£åˆ™åŒ–æŸå¤±

```python
# åœ¨YOLOæ£€æµ‹ä¸­æ·»åŠ TVæ­£åˆ™åŒ–
class YOLOWithTV(nn.Module):
    def __init__(self, yolo_model, tv_weight=0.01):
        super().__init__()
        self.yolo = yolo_model
        self.tv_weight = tv_weight
        self.tv = TotalVariation2D()

    def forward(self, x):
        # YOLOæ£€æµ‹
        detections = self.yolo(x)

        # æ·»åŠ TVæ­£åˆ™åŒ–åˆ°æŸå¤±
        if self.training:
            tv_loss = self.tv(detections['feature_map'])
            detections['tv_loss'] = tv_loss * self.tv_weight

        return detections
```

### åº”ç”¨2: Mumford-Shahè¾¹ç¼˜å¼•å¯¼

```python
# ç”¨Mumford-Shahæå–è¾¹ç¼˜,å¼•å¯¼æ£€æµ‹
def edge_guided_detection(image, detector):
    # 1. Mumford-Shahè¾¹ç¼˜æ£€æµ‹
    edges = mumford_shah_edge_detection(image)

    # 2. è¾¹ç¼˜å¼•å¯¼çš„éæå¤§å€¼æŠ‘åˆ¶
    detections = detector(image)
    refined_detections = edge_guided_nms(detections, edges)

    return refined_detections
```

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ7æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
**ä¸‹ä¸€æ­¥**: ç†è§£å‡¸ä¼˜åŒ–æ–¹æ³•,é˜…è¯»[2-01]è®ºæ–‡
