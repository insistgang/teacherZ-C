# [4-23] åŒå±‚ä¼˜åŒ–å½¢å¼åŒ– Bilevel Formalism - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Bilevel Optimization: Theory and Applications
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (é«˜)
> **é‡è¦æ€§**: â­â­â­ (ä¼˜åŒ–ç†è®ºåŸºç¡€)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Bilevel Optimization: Theory and Applications |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **æ ¸å¿ƒé¢†åŸŸ** | åŒå±‚ä¼˜åŒ–ã€æ•°å­¦è§„åˆ’ |
| **å…³é”®è¯** | Bilevel Optimization, Stackelberg Game, Nested Optimization |
| **æ ¸å¿ƒä»·å€¼** | åŒå±‚ä¼˜åŒ–çš„å½¢å¼åŒ–ç†è®ºä¸åº”ç”¨æ¡†æ¶ |

---

## ğŸ¯ åŒå±‚ä¼˜åŒ–æ ¸å¿ƒé—®é¢˜

### é—®é¢˜å®šä¹‰

```
åŒå±‚ä¼˜åŒ–é—®é¢˜å®šä¹‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä¸Šå±‚é—®é¢˜ (Leader):
  min_{x} F(x, y*(x))
  s.t. G(x, y*(x)) â‰¤ 0

å…¶ä¸­ y*(x) æ˜¯ä¸‹å±‚é—®é¢˜çš„è§£:

ä¸‹å±‚é—®é¢˜ (Follower):
  y*(x) = argmin_{y} f(x, y)
          s.t. g(x, y) â‰¤ 0

ç‰¹ç‚¹:
  - åµŒå¥—ç»“æ„: ä¸Šå±‚å†³ç­–å½±å“ä¸‹å±‚,ä¸‹å±‚åé¦ˆå½±å“ä¸Šå±‚
  - å±‚æ¬¡ä¾èµ–: y*(x) æ˜¯xçš„éšå‡½æ•°
  - éå‡¸æ€§: å³ä½¿ä¸Šä¸‹å±‚éƒ½å‡¸,æ•´ä½“ä¹Ÿå¯èƒ½éå‡¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### ä¸å•å±‚ä¼˜åŒ–çš„å¯¹æ¯”

| ç‰¹æ€§ | å•å±‚ä¼˜åŒ– | åŒå±‚ä¼˜åŒ– |
|:---|:---|:---|
| **ç»“æ„** | min f(x) | min F(x, y*(x)) |
| **å˜é‡** | å•å±‚å˜é‡ x | ä¸Šå±‚x + ä¸‹å±‚y |
| **çº¦æŸ** | æ˜¾å¼çº¦æŸ | éšå¼çº¦æŸ (ä¸‹å±‚æœ€ä¼˜æ€§) |
| **æ±‚è§£** | æ¢¯åº¦ä¸‹é™ç­‰ | éœ€è¦ç‰¹æ®Šå¤„ç†åµŒå¥—ç»“æ„ |
| **åº”ç”¨** | æ ‡å‡†MLé—®é¢˜ | NASã€å…ƒå­¦ä¹ ã€åšå¼ˆè®º |

---

## ğŸ”¬ åŒå±‚ä¼˜åŒ–æ–¹æ³•è®º

### æ•°å­¦å½¢å¼åŒ–

```
æ ‡å‡†å½¢å¼:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

(P)  min_{xâˆˆX} F(x, y)
     s.t. y âˆˆ S(x) = argmin_{yâˆˆY} {f(x, y) : g(x,y) â‰¤ 0}
          G(x, y) â‰¤ 0

å…¶ä¸­:
  - x âˆˆ R^n: ä¸Šå±‚å†³ç­–å˜é‡
  - y âˆˆ R^m: ä¸‹å±‚å†³ç­–å˜é‡
  - F: R^n Ã— R^m â†’ R: ä¸Šå±‚ç›®æ ‡å‡½æ•°
  - f: R^n Ã— R^m â†’ R: ä¸‹å±‚ç›®æ ‡å‡½æ•°
  - G, g: çº¦æŸå‡½æ•°

è§£é›†æ˜ å°„ S(x):
  å¯¹æ¯ä¸ªå›ºå®šçš„x, S(x)ç»™å‡ºä¸‹å±‚çš„æœ€ä¼˜è§£é›†
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### æ±‚è§£æ–¹æ³•åˆ†ç±»

```
åŒå±‚ä¼˜åŒ–æ±‚è§£æ–¹æ³•:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. åŸºäºKKTæ¡ä»¶çš„è½¬åŒ–æ³•
   - å°†ä¸‹å±‚KKTæ¡ä»¶ä½œä¸ºä¸Šå±‚çº¦æŸ
   - è½¬åŒ–ä¸ºå•å±‚çº¦æŸä¼˜åŒ–
   - é€‚ç”¨: ä¸‹å±‚å‡¸ä¸”æ»¡è¶³çº¦æŸè§„èŒƒ

2. éšå‡½æ•°æ¢¯åº¦æ³•
   - åˆ©ç”¨éšå‡½æ•°å®šç†è®¡ç®— dy*/dx
   - ä¸Šå±‚æ¢¯åº¦: dF/dx = âˆ‚F/âˆ‚x + âˆ‚F/âˆ‚y Â· dy*/dx
   - é€‚ç”¨: ä¸‹å±‚æœ‰å”¯ä¸€è§£ä¸”å…‰æ»‘

3. è¿­ä»£ä¼˜åŒ–æ³•
   - äº¤æ›¿æ›´æ–°ä¸Šå±‚å’Œä¸‹å±‚
   - æ¢¯åº¦ä¸‹é™-ä¸Šå‡æˆ–å›ºå®šç‚¹è¿­ä»£
   - é€‚ç”¨: å¤§è§„æ¨¡é—®é¢˜

4. å¯å‘å¼æ–¹æ³•
   - è¿›åŒ–ç®—æ³•
   - ä»£ç†æ¨¡å‹ä¼˜åŒ–
   - é€‚ç”¨: éå‡¸ã€ä¸å¯å¾®é—®é¢˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### æ ¸å¿ƒç»„ä»¶1: KKTæ¡ä»¶è½¬åŒ–

```python
"""
åŒå±‚ä¼˜åŒ– â†’ å•å±‚çº¦æŸä¼˜åŒ– (MPEC)

ä¸‹å±‚é—®é¢˜:
  min_y f(x, y)
  s.t. g(x, y) â‰¤ 0

KKTæ¡ä»¶:
  âˆ‡_y f(x, y) + Î»^T âˆ‡_y g(x, y) = 0  (å¹³ç¨³æ€§)
  g(x, y) â‰¤ 0                         (åŸå§‹å¯è¡Œæ€§)
  Î» â‰¥ 0                               (å¯¹å¶å¯è¡Œæ€§)
  Î»_i Â· g_i(x, y) = 0                 (äº’è¡¥æ¾å¼›)

è½¬åŒ–ä¸ºå•å±‚:
  min_{x,y,Î»} F(x, y)
  s.t. KKTæ¡ä»¶
       G(x, y) â‰¤ 0
"""

import numpy as np
from scipy.optimize import minimize

class KKTTransformation:
    """
    KKTæ¡ä»¶è½¬åŒ–æ±‚è§£å™¨
    """

    def __init__(self, upper_obj, lower_obj, upper_constr, lower_constr):
        """
        Args:
            upper_obj: ä¸Šå±‚ç›®æ ‡å‡½æ•° F(x, y)
            lower_obj: ä¸‹å±‚ç›®æ ‡å‡½æ•° f(x, y)
            upper_constr: ä¸Šå±‚çº¦æŸ G(x, y)
            lower_constr: ä¸‹å±‚çº¦æŸ g(x, y)
        """
        self.F = upper_obj
        self.f = lower_obj
        self.G = upper_constr
        self.g = lower_constr

    def kkt_constraints(self, vars):
        """
        æ„å»ºKKTçº¦æŸ

        Args:
            vars = [x, y, lambda]

        Returns:
            constraints: ç­‰å¼å’Œä¸ç­‰å¼çº¦æŸåˆ—è¡¨
        """
        n_x = self.n_x
        n_y = self.n_y

        x = vars[:n_x]
        y = vars[n_x:n_x+n_y]
        lam = vars[n_x+n_y:]

        constraints = []

        # 1. å¹³ç¨³æ€§: âˆ‡_y f + Î»^T âˆ‡_y g = 0
        grad_f_y = self.grad_lower_y(x, y)
        grad_g_y = self.grad_constr_y(x, y)
        stationarity = grad_f_y + lam @ grad_g_y

        for i, s in enumerate(stationarity):
            constraints.append({'type': 'eq', 'fun': lambda v, i=i: stationarity[i]})

        # 2. åŸå§‹å¯è¡Œæ€§: g(x, y) â‰¤ 0
        g_val = self.g(x, y)
        for i, g_i in enumerate(g_val):
            constraints.append({'type': 'ineq', 'fun': lambda v, i=i: -g_val[i]})

        # 3. å¯¹å¶å¯è¡Œæ€§: Î» â‰¥ 0
        for i, l in enumerate(lam):
            constraints.append({'type': 'ineq', 'fun': lambda v, i=i: lam[i]})

        # 4. äº’è¡¥æ¾å¼›: Î»_i Â· g_i = 0
        for i in range(len(lam)):
            compl = lam[i] * g_val[i]
            constraints.append({'type': 'eq', 'fun': lambda v, i=i: compl})

        return constraints

    def solve(self, x0, y0, lam0):
        """
        æ±‚è§£è½¬åŒ–åçš„å•å±‚é—®é¢˜
        """
        z0 = np.concatenate([x0, y0, lam0])

        def objective(z):
            n_x = self.n_x
            n_y = self.n_y
            x = z[:n_x]
            y = z[n_x:n_x+n_y]
            return self.F(x, y)

        constraints = self.kkt_constraints(z0)

        result = minimize(objective, z0, method='SLSQP',
                         constraints=constraints)

        n_x = self.n_x
        n_y = self.n_y
        x_opt = result.x[:n_x]
        y_opt = result.x[n_x:n_x+n_y]

        return x_opt, y_opt, result.fun
```

---

### æ ¸å¿ƒç»„ä»¶2: éšå‡½æ•°æ¢¯åº¦æ³•

```python
import torch
import torch.nn as nn

class ImplicitGradientBilevel:
    """
    éšå‡½æ•°æ¢¯åº¦æ³•æ±‚è§£åŒå±‚ä¼˜åŒ–

    é€‚ç”¨äºæ·±åº¦å­¦ä¹ åœºæ™¯ (å¦‚å…ƒå­¦ä¹ ã€NAS)
    """

    def __init__(self, upper_loss, lower_loss, lower_optimizer):
        """
        Args:
            upper_loss: ä¸Šå±‚æŸå¤±å‡½æ•° L_val(Î¸, Ï†)
            lower_loss: ä¸‹å±‚æŸå¤±å‡½æ•° L_train(Î¸, Ï†)
            lower_optimizer: ä¸‹å±‚ä¼˜åŒ–å™¨
        """
        self.upper_loss = upper_loss
        self.lower_loss = lower_loss
        self.lower_opt = lower_optimizer

    def compute_hypergradient(self, theta, phi, train_data, val_data):
        """
        è®¡ç®—è¶…æ¢¯åº¦ dL_val/dtheta

        ä½¿ç”¨éšå‡½æ•°å®šç†:
        dÏ†*/dÎ¸ = -(âˆ‡Â²_{Ï†Ï†} L_train)^{-1} Â· âˆ‡Â²_{Î¸Ï†} L_train

        dL_val/dÎ¸ = âˆ‡_Î¸ L_val + âˆ‡_Ï† L_val Â· dÏ†*/dÎ¸
        """
        # 1. æ±‚è§£ä¸‹å±‚é—®é¢˜ (å¾—åˆ°æœ€ä¼˜Ï†*)
        phi_star = self.solve_lower(theta, phi, train_data)

        # 2. è®¡ç®—ä¸Šå±‚æ¢¯åº¦
        val_loss = self.upper_loss(theta, phi_star, val_data)
        grad_theta_val = torch.autograd.grad(val_loss, theta,
                                            create_graph=True)[0]
        grad_phi_val = torch.autograd.grad(val_loss, phi_star,
                                          create_graph=True)[0]

        # 3. è®¡ç®—éšå‡½æ•°æ¢¯åº¦ (ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•é¿å…æ±‚é€†)
        implicit_grad = self.implicit_gradient(theta, phi_star,
                                              train_data, grad_phi_val)

        # 4. æ€»æ¢¯åº¦
        hypergradient = grad_theta_val + implicit_grad

        return hypergradient

    def solve_lower(self, theta, phi, train_data, num_steps=100):
        """æ±‚è§£ä¸‹å±‚ä¼˜åŒ–é—®é¢˜"""
        phi_current = phi.clone().requires_grad_(True)

        for _ in range(num_steps):
            loss = self.lower_loss(theta, phi_current, train_data)
            grad = torch.autograd.grad(loss, phi_current)[0]

            with torch.no_grad():
                phi_current = phi_current - 0.01 * grad

        return phi_current

    def implicit_gradient(self, theta, phi, train_data, grad_phi_val):
        """
        è®¡ç®—éšå‡½æ•°æ¢¯åº¦é¡¹

        æ±‚è§£: (âˆ‡Â²_{Ï†Ï†} L_train) Â· v = grad_phi_val
        ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•
        """
        def hessian_vector_product(v):
            """è®¡ç®—Hessian-å‘é‡ç§¯"""
            loss = self.lower_loss(theta, phi, train_data)
            grad_phi = torch.autograd.grad(loss, phi,
                                          create_graph=True)[0]
            hvp = torch.autograd.grad(grad_phi, phi, v,
                                     retain_graph=True)[0]
            return hvp + 0.01 * v  # æ·»åŠ æ­£åˆ™åŒ–

        # å…±è½­æ¢¯åº¦æ³•æ±‚è§£çº¿æ€§ç³»ç»Ÿ
        v = self.conjugate_gradient(hessian_vector_product, grad_phi_val)

        # è®¡ç®— âˆ‡Â²_{Î¸Ï†} L_train Â· v
        loss = self.lower_loss(theta, phi, train_data)
        grad_theta = torch.autograd.grad(loss, theta,
                                        create_graph=True,
                                        allow_unused=True)[0]

        if grad_theta is None:
            return torch.zeros_like(theta)

        grad_grad = torch.autograd.grad(grad_theta, phi, v,
                                       retain_graph=True)[0]

        return -grad_grad

    def conjugate_gradient(self, A_func, b, max_iter=10, tol=1e-6):
        """
        å…±è½­æ¢¯åº¦æ³•æ±‚è§£ Ax = b
        """
        x = torch.zeros_like(b)
        r = b - A_func(x)
        p = r.clone()
        rs_old = torch.sum(r * r)

        for _ in range(max_iter):
            Ap = A_func(p)
            alpha = rs_old / (torch.sum(p * Ap) + 1e-10)

            x = x + alpha * p
            r = r - alpha * Ap

            rs_new = torch.sum(r * r)
            if torch.sqrt(rs_new) < tol:
                break

            p = r + (rs_new / rs_old) * p
            rs_old = rs_new

        return x
```

---

### æ ¸å¿ƒç»„ä»¶3: è¿­ä»£ä¼˜åŒ–æ³•

```python
class IterativeBilevelOptimization:
    """
    è¿­ä»£å¼åŒå±‚ä¼˜åŒ–

    äº¤æ›¿æ›´æ–°ä¸Šå±‚å’Œä¸‹å±‚å˜é‡
    """

    def __init__(self, upper_obj, lower_obj, upper_lr=0.01, lower_lr=0.01):
        self.upper_obj = upper_obj
        self.lower_obj = lower_obj
        self.upper_lr = upper_lr
        self.lower_lr = lower_lr

    def solve(self, x_init, y_init, num_iterations=1000):
        """
        è¿­ä»£æ±‚è§£

        ç®—æ³•:
        for t = 1, 2, ...:
          # ä¸‹å±‚æ›´æ–° (å¤šæ­¥)
          for k = 1, ..., K:
            y_{k} = y_{k-1} - Î±_l Â· âˆ‡_y f(x_t, y_{k-1})

          # ä¸Šå±‚æ›´æ–°
          x_{t+1} = x_t - Î±_u Â· âˆ‡_x F(x_t, y_K)
        """
        x = x_init.clone()
        y = y_init.clone()

        history = {'x': [], 'y': [], 'F': [], 'f': []}

        for t in range(num_iterations):
            # ä¸‹å±‚ä¼˜åŒ– (å†…å¾ªç¯)
            y_current = y.clone()
            for k in range(10):  # Kæ­¥ä¸‹å±‚æ›´æ–°
                grad_y = self.grad_lower_y(x, y_current)
                y_current = y_current - self.lower_lr * grad_y

            y = y_current

            # ä¸Šå±‚ä¼˜åŒ–
            grad_x = self.grad_upper_x(x, y)
            x = x - self.upper_lr * grad_x

            # è®°å½•
            history['x'].append(x.clone())
            history['y'].append(y.clone())
            history['F'].append(self.upper_obj(x, y).item())
            history['f'].append(self.lower_obj(x, y).item())

            if t % 100 == 0:
                print(f"Iter {t}: F={history['F'][-1]:.4f}, f={history['f'][-1]:.4f}")

        return x, y, history

    def grad_upper_x(self, x, y):
        """ä¸Šå±‚å…³äºxçš„æ¢¯åº¦"""
        x_var = x.clone().requires_grad_(True)
        F_val = self.upper_obj(x_var, y)
        return torch.autograd.grad(F_val, x_var)[0]

    def grad_lower_y(self, x, y):
        """ä¸‹å±‚å…³äºyçš„æ¢¯åº¦"""
        y_var = y.clone().requires_grad_(True)
        f_val = self.lower_obj(x, y_var)
        return torch.autograd.grad(f_val, y_var)[0]
```

---

## ğŸ“Š åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1: è¶…å‚æ•°ä¼˜åŒ–

```python
class HyperparameterOptimization:
    """
    åŒå±‚ä¼˜åŒ–ç”¨äºè¶…å‚æ•°ä¼˜åŒ–

    ä¸Šå±‚: é€‰æ‹©è¶…å‚æ•° Î»
    ä¸‹å±‚: è®­ç»ƒæ¨¡å‹æƒé‡ w
    """

    def __init__(self, model, train_loader, val_loader):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader

    def upper_objective(self, lambda_reg, w_star):
        """
        ä¸Šå±‚ç›®æ ‡: éªŒè¯é›†æ€§èƒ½

        Args:
            lambda_reg: æ­£åˆ™åŒ–è¶…å‚æ•°
            w_star: ä¸‹å±‚ä¼˜åŒ–å¾—åˆ°çš„æœ€ä¼˜æƒé‡
        """
        val_loss = 0
        for x, y in self.val_loader:
            pred = self.model(x, w_star)
            val_loss += F.cross_entropy(pred, y)

        return val_loss / len(self.val_loader)

    def lower_objective(self, lambda_reg, w):
        """
        ä¸‹å±‚ç›®æ ‡: è®­ç»ƒé›†æŸå¤± + æ­£åˆ™åŒ–
        """
        train_loss = 0
        reg_loss = 0

        for x, y in self.train_loader:
            pred = self.model(x, w)
            train_loss += F.cross_entropy(pred, y)
            reg_loss += lambda_reg * torch.sum(w ** 2)

        return train_loss / len(self.train_loader) + reg_loss

    def optimize(self, lambda_init, w_init, num_outer=100):
        """åŒå±‚ä¼˜åŒ–"""
        lambda_reg = lambda_init
        w = w_init

        bilevel_opt = IterativeBilevelOptimization(
            upper_obj=lambda l, w: self.upper_objective(l, w),
            lower_obj=lambda l, w: self.lower_objective(l, w),
            upper_lr=0.001,
            lower_lr=0.01
        )

        lambda_opt, w_opt, _ = bilevel_opt.solve(lambda_reg, w, num_outer)

        return lambda_opt, w_opt
```

### æ¡ˆä¾‹2: NASä¸­çš„åŒå±‚ä¼˜åŒ–

```python
class NASBilevelOptimization:
    """
    DARTSä¸­çš„åŒå±‚ä¼˜åŒ–

    ä¸Šå±‚: æ¶æ„å‚æ•° Î±
    ä¸‹å±‚: ç½‘ç»œæƒé‡ w
    """

    def __init__(self, model):
        self.model = model

    def train_step(self, train_data, val_data, alpha, w,
                   alpha_lr=0.001, w_lr=0.01):
        """
        ä¸€æ­¥åŒå±‚ä¼˜åŒ–

        1. ä¸‹å±‚: åœ¨è®­ç»ƒé›†ä¸Šæ›´æ–°w
        2. ä¸Šå±‚: åœ¨éªŒè¯é›†ä¸Šæ›´æ–°Î±
        """
        # ä¸‹å±‚æ›´æ–° (è¿‘ä¼¼)
        train_loss = self.model.loss(train_data, alpha, w)
        grad_w = torch.autograd.grad(train_loss, w)[0]
        w_prime = w - w_lr * grad_w

        # ä¸Šå±‚æ›´æ–° (ä½¿ç”¨w'è¿‘ä¼¼w*)
        val_loss = self.model.loss(val_data, alpha, w_prime)
        grad_alpha = torch.autograd.grad(val_loss, alpha)[0]
        alpha = alpha - alpha_lr * grad_alpha

        # å®é™…æ›´æ–°w
        w = w - w_lr * torch.autograd.grad(train_loss, w)[0]

        return alpha, w
```

---

## ğŸ’¡ å¯å¤ç”¨ä»£ç ç»„ä»¶

### é€šç”¨åŒå±‚ä¼˜åŒ–æ±‚è§£å™¨

```python
class BilevelOptimizer:
    """
    é€šç”¨åŒå±‚ä¼˜åŒ–æ±‚è§£å™¨

    æ”¯æŒå¤šç§æ±‚è§£ç­–ç•¥
    """

    def __init__(self, method='implicit', **kwargs):
        """
        Args:
            method: 'kkt', 'implicit', 'iterative'
        """
        self.method = method
        self.kwargs = kwargs

    def solve(self, upper_obj, lower_obj, x0, y0):
        """
        æ±‚è§£åŒå±‚ä¼˜åŒ–é—®é¢˜

        Args:
            upper_obj: ä¸Šå±‚ç›®æ ‡å‡½æ•°
            lower_obj: ä¸‹å±‚ç›®æ ‡å‡½æ•°
            x0, y0: åˆå§‹å€¼

        Returns:
            x_opt, y_opt: æœ€ä¼˜è§£
        """
        if self.method == 'kkt':
            solver = KKTTransformation(upper_obj, lower_obj, None, None)
            return solver.solve(x0, y0, np.zeros(len(y0)))

        elif self.method == 'implicit':
            solver = ImplicitGradientBilevel(upper_obj, lower_obj, None)
            # ...

        elif self.method == 'iterative':
            solver = IterativeBilevelOptimization(
                upper_obj, lower_obj,
                self.kwargs.get('upper_lr', 0.01),
                self.kwargs.get('lower_lr', 0.01)
            )
            return solver.solve(x0, y0, self.kwargs.get('max_iter', 1000))

        else:
            raise ValueError(f"Unknown method: {self.method}")
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **Stackelbergåšå¼ˆ** | Stackelberg Game | é¢†å¯¼è€…-è·Ÿéšè€…åšå¼ˆæ¨¡å‹ |
| **KKTæ¡ä»¶** | Karush-Kuhn-Tucker | çº¦æŸä¼˜åŒ–çš„æœ€ä¼˜æ€§æ¡ä»¶ |
| **éšå‡½æ•°å®šç†** | Implicit Function Theorem | éšå‡½æ•°å¾®åˆ†ç†è®º |
| **è¶…æ¢¯åº¦** | Hypergradient | ä¸Šå±‚å…³äºä¸Šå±‚å˜é‡çš„æ¢¯åº¦ |
| **MPEC** | Mathematical Program with Equilibrium Constraints | å¸¦å‡è¡¡çº¦æŸçš„æ•°å­¦è§„åˆ’ |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£åŒå±‚ä¼˜åŒ–çš„é—®é¢˜ç»“æ„
- [ ] æŒæ¡KKTæ¡ä»¶è½¬åŒ–æ–¹æ³•
- [ ] äº†è§£éšå‡½æ•°æ¢¯åº¦è®¡ç®—
- [ ] ç†è§£è¿­ä»£ä¼˜åŒ–æ³•çš„åŸç†
- [ ] èƒ½å¤Ÿåº”ç”¨åˆ°NASå’Œå…ƒå­¦ä¹ 

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **åŒå±‚ä¼˜åŒ–ä¸ºä»€ä¹ˆæ¯”å•å±‚æ›´éš¾æ±‚è§£ï¼Ÿ**
   - æç¤º: åµŒå¥—ç»“æ„ã€éå‡¸æ€§

2. **KKTè½¬åŒ–çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ**
   - æç¤º: äº’è¡¥çº¦æŸçš„éå…‰æ»‘æ€§

3. **éšå‡½æ•°æ¢¯åº¦æ³•çš„è®¡ç®—ç“¶é¢ˆåœ¨å“ªé‡Œï¼Ÿ**
   - æç¤º: Hessianæ±‚é€†

4. **å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ±‚è§£æ–¹æ³•ï¼Ÿ**
   - æç¤º: é—®é¢˜è§„æ¨¡ã€å…‰æ»‘æ€§ã€ç²¾åº¦è¦æ±‚

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
