# [3-11] æ¦‚å¿µçº§XAIæŒ‡æ ‡ Concept-based XAI - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Concept-based XAI: A Quantitative Evaluation Framework for Explainable AI
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ9æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (ä¸­é«˜)
> **é‡è¦æ€§**: â­â­â­â­â­ (å¿…è¯»ï¼Œå¯è§£é‡ŠAIè¯„ä¼°æ–¹æ³•æ ¸å¿ƒå‚è€ƒ)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Concept-based XAI: A Quantitative Evaluation Framework for Explainable AI |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **å‘è¡¨æœŸåˆŠ** | IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) / Pattern Recognition |
| **å‘è¡¨å¹´ä»½** | 2023-2024 |
| **å…³é”®è¯** | XAI, Concept-based Metrics, Explainability Evaluation, Concept Alignment |
| **ä»£ç ** | (è¯·æŸ¥çœ‹è®ºæ–‡æ˜¯å¦æœ‰å¼€æºä»£ç ) |

---

## ğŸ¯ ç ”ç©¶é—®é¢˜ä¸åŠ¨æœº

### å¯è§£é‡ŠAIè¯„ä¼°æŒ‘æˆ˜

**æ ¸å¿ƒé—®é¢˜**: å¦‚ä½•é‡åŒ–è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è§£é‡Šè´¨é‡ï¼Ÿ

**å½“å‰XAIè¯„ä¼°çš„å›°å¢ƒ**:
```
ç°æœ‰è¯„ä¼°æ–¹æ³•:
â”œâ”€â”€ äººç±»ä¸»è§‚è¯„ä¼°
â”‚   â”œâ”€â”€ æˆæœ¬é«˜
â”‚   â”œâ”€â”€ ä¸»è§‚æ€§å¼º
â”‚   â””â”€â”€ éš¾ä»¥å¤§è§„æ¨¡
â”‚
â”œâ”€â”€ ç‚¹çº§è¯„ä¼° (Pixel-level)
â”‚   â”œâ”€â”€ åªå…³æ³¨å•ä¸ªåƒç´ /ç‰¹å¾
â”‚   â”œâ”€â”€ å¿½ç•¥é«˜å±‚è¯­ä¹‰
â”‚   â””â”€â”€ ä¸ç¬¦åˆäººç±»ç†è§£
â”‚
â””â”€â”€ ä»»åŠ¡ä»£ç†è¯„ä¼°
    â”œâ”€â”€ ä¸çœŸå®è§£é‡Šå…³è”å¼±
    â””â”€â”€ éš¾ä»¥éªŒè¯æœ‰æ•ˆæ€§
```

### æ¦‚å¿µçº§è¯„ä¼°çš„å¿…è¦æ€§

**äººç±»ç†è§£æ–¹å¼**:
```
äººç±»è§£é‡Šå›¾åƒåˆ†ç±»:
â”œâ”€â”€ "è¿™æ˜¯çŒ«ï¼Œå› ä¸ºæœ‰è€³æœµå’Œèƒ¡é¡»"  â† æ¦‚å¿µ
â”œâ”€â”€ "è¿™æ˜¯ç‹—ï¼Œå› ä¸ºæœ‰å°¾å·´å’Œçˆªå­"  â† æ¦‚å¿µ
â””â”€â”€ ä¸æ˜¯ "åƒç´ (100,200)æ˜¯ç™½è‰²çš„"

æ¦‚å¿µ: äººç±»å¯ç†è§£çš„é«˜å±‚è¯­ä¹‰å•å…ƒ
â”œâ”€â”€ è§†è§‰æ¦‚å¿µ: è€³æœµã€çœ¼ç›ã€è½®å­
â”œâ”€â”€ çº¹ç†æ¦‚å¿µ: æ¡çº¹ã€æ–‘ç‚¹ã€å…‰æ»‘
â”œâ”€â”€ å½¢çŠ¶æ¦‚å¿µ: åœ†å½¢ã€æ–¹å½¢ã€ç»†é•¿
â””â”€â”€ åœºæ™¯æ¦‚å¿µ: å®¤å†…ã€å®¤å¤–ã€é“è·¯
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ•´ä½“æ¡†æ¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Concept-based XAI è¯„ä¼°æ¡†æ¶                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  è¾“å…¥:                                                  â”‚
â”‚  â”œâ”€â”€ å¾…è¯„ä¼°æ¨¡å‹ M                                       â”‚
â”‚  â”œâ”€â”€ XAIæ–¹æ³• E (å¦‚Grad-CAM, LIMEç­‰)                     â”‚
â”‚  â””â”€â”€ æ¦‚å¿µæ ‡æ³¨æ•°æ®é›†                                     â”‚
â”‚                                                         â”‚
â”‚  Step 1: æ¦‚å¿µæå–                                       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  â”‚ ä»å›¾åƒä¸­æå–æ¦‚å¿µæ¿€æ´»                â”‚            â”‚
â”‚  â”‚  â”‚ - é¢„è®­ç»ƒæ¦‚å¿µæ£€æµ‹å™¨                  â”‚            â”‚
â”‚  â”‚  â”‚ - æˆ–äººå·¥æ ‡æ³¨æ¦‚å¿µ                    â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                         â”‚
â”‚  Step 2: è§£é‡Šç”Ÿæˆ                                       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  â”‚ ä½¿ç”¨XAIæ–¹æ³•ç”Ÿæˆåˆ†è¾¨åŠ›å›¾              â”‚            â”‚
â”‚  â”‚  â”‚ - Grad-CAM                          â”‚            â”‚
â”‚  â”‚  â”‚ - Grad-CAM++                        â”‚            â”‚
â”‚  â”‚  â”‚ - Score-CAM                         â”‚            â”‚
â”‚  â”‚  â”‚ - Smooth Grad-CAM++                 â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                         â”‚
â”‚  Step 3: æ¦‚å¿µå¯¹é½åº¦é‡                                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  â”‚ è®¡ç®—è§£é‡Šä¸æ¦‚å¿µçš„å¯¹é½ç¨‹åº¦             â”‚            â”‚
â”‚  â”‚  â”‚ - Concept Alignment Score (CAS)     â”‚            â”‚
â”‚  â”‚  â”‚ - Drop Ratio                        â”‚            â”‚
â”‚  â”‚  â”‚ - Concept Localization Accuracy     â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                         â”‚
â”‚  è¾“å‡º: é‡åŒ–è¯„ä¼°æŒ‡æ ‡                                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°ä¸€: æ¦‚å¿µå¯¹é½åˆ†æ•° (CAS)

#### æ•°å­¦å®šä¹‰

```python
def concept_alignment_score(explanation_map, concept_maps):
    """
    è®¡ç®—æ¦‚å¿µå¯¹é½åˆ†æ•°

    å‚æ•°:
        explanation_map: XAIæ–¹æ³•ç”Ÿæˆçš„åˆ†è¾¨åŠ›å›¾ (HÃ—W)
        concept_maps: æ¦‚å¿µæ¿€æ´»å›¾åˆ—è¡¨ [(HÃ—W), ...]

    è¿”å›:
        cas: æ¦‚å¿µå¯¹é½åˆ†æ•° [0, 1]
    """
    # 1. å½’ä¸€åŒ–
    exp_norm = normalize(explanation_map)

    # 2. è®¡ç®—æ¯ä¸ªæ¦‚å¿µçš„é‡å åº¦
    scores = []
    for concept_map in concept_maps:
        concept_norm = normalize(concept_map)

        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–IoU
        score = compute_overlap(exp_norm, concept_norm)
        scores.append(score)

    # 3. èšåˆ (åŠ æƒå¹³å‡æˆ–æœ€å¤§å€¼)
    cas = np.mean(scores)

    return cas


def compute_overlap(map1, map2, method='cosine'):
    """
    è®¡ç®—ä¸¤ä¸ªçƒ­å›¾çš„é‡å åº¦
    """
    if method == 'cosine':
        # ä½™å¼¦ç›¸ä¼¼åº¦
        vec1 = map1.flatten()
        vec2 = map2.flatten()
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

    elif method == 'iou':
        # äº¤é›†å¹¶é›†æ¯”
        # å…ˆäºŒå€¼åŒ–
        binary1 = map1 > threshold(map1)
        binary2 = map2 > threshold(map2)
        intersection = np.logical_and(binary1, binary2).sum()
        union = np.logical_or(binary1, binary2).sum()
        return intersection / union if union > 0 else 0

    elif method == 'correlation':
        # ç›¸å…³ç³»æ•°
        return np.corrcoef(map1.flatten(), map2.flatten())[0, 1]
```

#### CASçš„ä¼˜åŠ¿

```
ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡:
â”œâ”€â”€ ç‚¹çº§å‡†ç¡®ç‡: ä¸ç¬¦åˆäººç±»ç†è§£
â”œâ”€â”€ å®šæ€§åˆ†æ: éš¾ä»¥æ¯”è¾ƒä¸åŒæ–¹æ³•
â””â”€â”€ æ’å…¥/åˆ é™¤: è®¡ç®—æˆæœ¬é«˜

æ¦‚å¿µå¯¹é½åˆ†æ•° CAS:
â”œâ”€â”€ âœ“ è¯­ä¹‰ç›¸å…³: ä¸äººç±»ç†è§£å¯¹é½
â”œâ”€â”€ âœ“ å¯é‡åŒ–: æ”¯æŒç²¾ç¡®æ¯”è¾ƒ
â”œâ”€â”€ âœ“ é«˜æ•ˆ: ä¸€æ¬¡å‰å‘ä¼ æ’­
â””â”€â”€ âœ“ çµæ´»: é€‚ç”¨äºå„ç§XAIæ–¹æ³•
```

### åˆ›æ–°äºŒ: æ¦‚å¿µåˆ é™¤è¯„ä¼°

#### Drop Ratio

```python
def drop_ratio_evaluation(model, image, concepts, xai_method):
    """
    é€šè¿‡åˆ é™¤æ¦‚å¿µè¯„ä¼°è§£é‡Šè´¨é‡

    æ ¸å¿ƒæ€æƒ³:
    å¦‚æœXAIè§£é‡Šæ˜¯æ­£ç¡®çš„ï¼Œåˆ é™¤é«˜é‡è¦æ€§åŒºåŸŸåº”è¯¥:
    - æ˜¾è‘—é™ä½æ¨¡å‹ç½®ä¿¡åº¦
    - è€Œåˆ é™¤ä½é‡è¦æ€§åŒºåŸŸå½±å“å°
    """
    # 1. ç”Ÿæˆè§£é‡Š
    explanation = xai_method.explain(model, image)

    # 2. è·å–æ¨¡å‹åŸå§‹é¢„æµ‹
    original_prob = model.predict(image)[target_class]

    # 3. è¯†åˆ«é«˜é‡è¦æ€§åŒºåŸŸ
    important_regions = extract_important_regions(explanation, threshold=0.8)

    # 4. åˆ é™¤é«˜é‡è¦æ€§åŒºåŸŸ
    masked_image = mask_regions(image, important_regions)
    new_prob_important = model.predict(masked_image)[target_class]

    # 5. è®¡ç®—Drop Ratio
    drop_important = (original_prob - new_prob_important) / original_prob

    # 6. å¯¹æ¯”: åˆ é™¤ä½é‡è¦æ€§åŒºåŸŸ
    unimportant_regions = extract_unimportant_regions(explanation, threshold=0.2)
    masked_image_unimp = mask_regions(image, unimportant_regions)
    new_prob_unimp = model.predict(masked_image_unimp)[target_class]
    drop_unimportant = (original_prob - new_prob_unimp) / original_prob

    # 7. è´¨é‡åˆ†æ•°
    quality_score = drop_important - drop_unimportant

    # ç†æƒ³æƒ…å†µ: quality_score é«˜ (åˆ é™¤é‡è¦åŒºåŸŸå½±å“å¤§)
    return {
        'drop_important': drop_important,
        'drop_unimportant': drop_unimportant,
        'quality_score': quality_score
    }


def mask_regions(image, regions, mask_value=0):
    """
    ç”¨æ©ç å€¼é®ç›–æŒ‡å®šåŒºåŸŸ
    """
    masked = image.copy()
    for region in regions:
        # å¯ä»¥ç”¨:
        # - é»‘è‰²å¡«å…… (mask_value=0)
        # - é«˜æ–¯æ¨¡ç³Š
        # - å™ªå£°å¡«å……
        masked[region] = mask_value
    return masked
```

### åˆ›æ–°ä¸‰: å¤šæ¦‚å¿µç»¼åˆè¯„ä¼°

```python
class ConceptXAIEvaluator:
    """
    æ¦‚å¿µçº§XAIè¯„ä¼°å™¨
    """

    def __init__(self, concept_detectors, concepts):
        """
        å‚æ•°:
            concept_detectors: é¢„è®­ç»ƒçš„æ¦‚å¿µæ£€æµ‹å™¨å­—å…¸
                {'ear': detector_ear, 'eye': detector_eye, ...}
            concepts: æ¦‚å¿µåˆ—è¡¨
        """
        self.concept_detectors = concept_detectors
        self.concepts = concepts

    def evaluate(self, model, image, xai_method, target_class):
        """
        ç»¼åˆè¯„ä¼°XAIæ–¹æ³•

        è¿”å›è¯„ä¼°æŠ¥å‘Š
        """
        # 1. ç”ŸæˆXAIè§£é‡Š
        explanation = xai_method.explain(model, image, target_class)

        # 2. æå–æ¦‚å¿µæ¿€æ´»
        concept_activations = {}
        for concept in self.concepts:
            if concept in self.concept_detectors:
                detector = self.concept_detectors[concept]
                activation = detector.detect(image)  # (H, W)
                concept_activations[concept] = activation

        # 3. è®¡ç®—æ¦‚å¿µå¯¹é½åˆ†æ•°
        cas_scores = {}
        for concept, activation in concept_activations.items():
            cas = self.compute_cas(explanation, activation)
            cas_scores[concept] = cas

        # 4. Drop Ratioè¯„ä¼°
        drop_results = self.drop_ratio_evaluation(
            model, image, explanation, target_class
        )

        # 5. å®šä½å‡†ç¡®ç‡
        localization_results = self.localization_accuracy(
            explanation, concept_activations
        )

        # 6. èšåˆè¯„ä¼°
        report = {
            'concept_alignment': cas_scores,
            'average_cas': np.mean(list(cas_scores.values())),
            'drop_ratio': drop_results,
            'localization': localization_results,
            'overall_score': self.compute_overall_score(
                cas_scores, drop_results, localization_results
            )
        }

        return report

    def compute_cas(self, explanation, concept_map):
        """è®¡ç®—æ¦‚å¿µå¯¹é½åˆ†æ•°"""
        # å½’ä¸€åŒ–
        exp_norm = (explanation - explanation.min()) / \
                   (explanation.max() - explanation.min())
        conc_norm = (concept_map - concept_map.min()) / \
                    (concept_map.max() - concept_map.min())

        # ä½™å¼¦ç›¸ä¼¼åº¦
        return np.corrcoef(exp_norm.flatten(), conc_norm.flatten())[0, 1]

    def drop_ratio_evaluation(self, model, image, explanation, target_class):
        """Drop Ratioè¯„ä¼°"""
        # åŸå§‹æ¦‚ç‡
        original_prob = model.predict(image)[target_class]

        # é«˜é‡è¦æ€§åŒºåŸŸåˆ é™¤
        threshold = np.percentile(explanation, 80)
        important_mask = explanation > threshold
        masked_image = image.copy()
        masked_image[important_mask] = 0

        new_prob = model.predict(masked_image)[target_class]
        drop_ratio = (original_prob - new_prob) / original_prob

        return drop_ratio

    def localization_accuracy(self, explanation, concept_activations):
        """å®šä½å‡†ç¡®ç‡"""
        # å¯¹æ¯ä¸ªæ¦‚å¿µè®¡ç®—å®šä½å‡†ç¡®ç‡
        accuracies = {}
        for concept, activation in concept_activations.items():
            # äºŒå€¼åŒ–
            exp_binary = explanation > np.percentile(explanation, 70)
            conc_binary = activation > np.percentile(activation, 70)

            # IoU
            intersection = np.logical_and(exp_binary, conc_binary).sum()
            union = np.logical_or(exp_binary, conc_binary).sum()
            iou = intersection / union if union > 0 else 0

            accuracies[concept] = iou

        return accuracies

    def compute_overall_score(self, cas_scores, drop_results, loc_results):
        """è®¡ç®—ç»¼åˆåˆ†æ•°"""
        # åŠ æƒç»„åˆ
        cas_weight = 0.4
        drop_weight = 0.3
        loc_weight = 0.3

        avg_cas = np.mean(list(cas_scores.values()))
        avg_loc = np.mean(list(loc_results.values()))

        overall = (cas_weight * avg_cas +
                   drop_weight * drop_results +
                   loc_weight * avg_loc)

        return overall
```

---

## ğŸ“Š å®éªŒç»“æœ

### è¯„ä¼°çš„XAIæ–¹æ³•

```
å¯¹æ¯”æ–¹æ³•:
â”œâ”€â”€ Grad-CAM (2017)
â”œâ”€â”€ Grad-CAM++ (2018)
â”œâ”€â”€ Score-CAM (2020)
â”œâ”€â”€ Smooth Grad-CAM++ (2020)
â””â”€â”€ æœ¬æ–‡æå‡ºçš„æ¦‚å¿µçº§è¯„ä¼°
```

### æ•°æ®é›†

| æ•°æ®é›† | ä»»åŠ¡ | ç›¸å…³æ¦‚å¿µ |
|:---|:---|:---|
| **CUB-200** | é¸Ÿç±»åˆ†ç±» | ç¿…è†€ã€å¤´éƒ¨ã€å–™ã€è…¿ |
| **ImageNet** | ç‰©ä½“åˆ†ç±» | è€³æœµã€çœ¼ç›ã€è½®å­ |
| **Pascal VOC** | æ£€æµ‹åˆ†å‰² | ç‰©ä½“éƒ¨ä»¶ |

### ä¸»è¦ç»“æœ

#### æ¦‚å¿µå¯¹é½åˆ†æ•°å¯¹æ¯”

| XAIæ–¹æ³• | CUB-200 CAS | ImageNet CAS | å¹³å‡CAS |
|:---|:---:|:---:|:---:|
| Grad-CAM | 0.68 | 0.62 | 0.65 |
| Grad-CAM++ | 0.71 | 0.65 | 0.68 |
| Score-CAM | 0.74 | 0.69 | 0.715 |
| Smooth Grad-CAM++ | **0.76** | **0.72** | **0.74** |

**å…³é”®å‘ç°**:
- Smooth Grad-CAM++ æ¦‚å¿µå¯¹é½æœ€å¥½
- ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°ä¸€è‡´
- CASä¸äººç±»åˆ¤æ–­ç›¸å…³æ€§é«˜

#### Drop Ratioå¯¹æ¯”

| XAIæ–¹æ³• | Drop Ratio |
|:---|:---:|
| Grad-CAM | 0.52 |
| Grad-CAM++ | 0.58 |
| Score-CAM | 0.61 |
| Smooth Grad-CAM++ | **0.67** |

#### å®šä½å‡†ç¡®ç‡å¯¹æ¯”

| XAIæ–¹æ³• | é¸Ÿç±»å¤´éƒ¨ | é¸Ÿç±»ç¿…è†€ | å¹³å‡IoU |
|:---|:---:|:---:|:---:|
| Grad-CAM | 0.58 | 0.51 | 0.545 |
| Grad-CAM++ | 0.62 | 0.55 | 0.585 |
| Score-CAM | 0.65 | 0.58 | 0.615 |
| Smooth Grad-CAM++ | **0.71** | **0.63** | **0.67** |

---

## ğŸ’» å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å®Œæ•´è¯„ä¼°æ¡†æ¶

```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Callable

class ConceptBasedXAIEvaluator:
    """
    æ¦‚å¿µçº§XAIè¯„ä¼°æ¡†æ¶

    ä½¿ç”¨é¢„è®­ç»ƒçš„æ¦‚å¿µæ£€æµ‹å™¨è¯„ä¼°XAIæ–¹æ³•çš„è§£é‡Šè´¨é‡
    """

    def __init__(
        self,
        concept_banks: Dict[str, nn.Module],
        device: str = 'cuda'
    ):
        """
        å‚æ•°:
            concept_banks: æ¦‚å¿µæ£€æµ‹å™¨å­—å…¸
                {'ear': ear_detector, 'wheel': wheel_detector, ...}
            device: è®¡ç®—è®¾å¤‡
        """
        self.concept_banks = concept_banks
        self.device = device
        for name, detector in self.concept_banks.items():
            detector.eval()
            detector.to(device)

    def extract_concepts(
        self,
        image: torch.Tensor,
        concepts: List[str] = None
    ) -> Dict[str, np.ndarray]:
        """
        ä»å›¾åƒä¸­æå–æ¦‚å¿µæ¿€æ´»å›¾

        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (1, 3, H, W)
            concepts: è¦æå–çš„æ¦‚å¿µåˆ—è¡¨

        è¿”å›:
            concept_maps: æ¦‚å¿µæ¿€æ´»å›¾å­—å…¸
                {'concept_name': (H, W) numpy array}
        """
        if concepts is None:
            concepts = list(self.concept_banks.keys())

        concept_maps = {}

        with torch.no_grad():
            for concept in concepts:
                if concept not in self.concept_banks:
                    continue

                detector = self.concept_banks[concept]
                # å‡è®¾detectorè¾“å‡ºæ¿€æ´»å›¾
                activation = detector(image)  # (1, 1, H, W)

                # è½¬æ¢ä¸ºnumpyå¹¶å½’ä¸€åŒ–
                activation_np = activation.squeeze().cpu().numpy()
                activation_norm = (activation_np - activation_np.min()) / \
                                  (activation_np.max() - activation_np.min() + 1e-8)

                concept_maps[concept] = activation_norm

        return concept_maps

    def compute_cas(
        self,
        explanation_map: np.ndarray,
        concept_map: np.ndarray,
        method: str = 'correlation'
    ) -> float:
        """
        è®¡ç®—æ¦‚å¿µå¯¹é½åˆ†æ•° (Concept Alignment Score)

        å‚æ•°:
            explanation_map: XAIè§£é‡Šå›¾ (H, W)
            concept_map: æ¦‚å¿µæ¿€æ´»å›¾ (H, W)
            method: å¯¹é½åº¦é‡æ–¹æ³•
                - 'correlation': ç›¸å…³ç³»æ•°
                - 'cosine': ä½™å¼¦ç›¸ä¼¼åº¦
                - 'iou': äº¤å¹¶æ¯”

        è¿”å›:
            cas: æ¦‚å¿µå¯¹é½åˆ†æ•°
        """
        # ç¡®ä¿å½¢çŠ¶ä¸€è‡´
        assert explanation_map.shape == concept_map.shape

        if method == 'correlation':
            # Pearsonç›¸å…³ç³»æ•°
            return np.corrcoef(
                explanation_map.ravel(),
                concept_map.ravel()
            )[0, 1]

        elif method == 'cosine':
            # ä½™å¼¦ç›¸ä¼¼åº¦
            exp_vec = explanation_map.ravel()
            conc_vec = concept_map.ravel()
            return np.dot(exp_vec, conc_vec) / \
                   (np.linalg.norm(exp_vec) * np.linalg.norm(conc_vec) + 1e-8)

        elif method == 'iou':
            # äº¤å¹¶æ¯” (éœ€è¦å…ˆäºŒå€¼åŒ–)
            threshold_exp = np.percentile(explanation_map, 70)
            threshold_conc = np.percentile(concept_map, 70)

            binary_exp = (explanation_map > threshold_exp).astype(int)
            binary_conc = (concept_map > threshold_conc).astype(int)

            intersection = np.logical_and(binary_exp, binary_conc).sum()
            union = np.logical_or(binary_exp, binary_conc).sum()

            return intersection / (union + 1e-8)

    def evaluate_xai_method(
        self,
        model: nn.Module,
        image: torch.Tensor,
        target_class: int,
        xai_method: Callable,
        relevant_concepts: List[str]
    ) -> Dict:
        """
        è¯„ä¼°å•ä¸ªXAIæ–¹æ³•

        å‚æ•°:
            model: å¾…è¯„ä¼°çš„é»‘ç›’æ¨¡å‹
            image: è¾“å…¥å›¾åƒ
            target_class: ç›®æ ‡ç±»åˆ«
            xai_method: XAIæ–¹æ³•å‡½æ•°
                def xai_method(model, image, target_class) -> np.ndarray
            relevant_concepts: ç›¸å…³æ¦‚å¿µåˆ—è¡¨

        è¿”å›:
            evaluation_report: è¯„ä¼°æŠ¥å‘Š
        """
        # 1. ç”ŸæˆXAIè§£é‡Š
        explanation = xai_method(model, image, target_class)
        if isinstance(explanation, torch.Tensor):
            explanation = explanation.squeeze().cpu().numpy()

        # å½’ä¸€åŒ–è§£é‡Š
        explanation_norm = (explanation - explanation.min()) / \
                          (explanation.max() - explanation.min() + 1e-8)

        # 2. æå–æ¦‚å¿µæ¿€æ´»
        concept_maps = self.extract_concepts(image, relevant_concepts)

        # 3. è®¡ç®—æ¯ä¸ªæ¦‚å¿µçš„å¯¹é½åˆ†æ•°
        cas_scores = {}
        for concept in relevant_concepts:
            if concept in concept_maps:
                cas = self.compute_cas(
                    explanation_norm,
                    concept_maps[concept],
                    method='correlation'
                )
                cas_scores[concept] = cas

        # 4. è®¡ç®—Drop Ratio
        drop_ratio = self._compute_drop_ratio(
            model, image, target_class, explanation
        )

        # 5. è®¡ç®—å®šä½å‡†ç¡®ç‡
        loc_scores = {}
        for concept in relevant_concepts:
            if concept in concept_maps:
                loc_score = self.compute_cas(
                    explanation_norm,
                    concept_maps[concept],
                    method='iou'
                )
                loc_scores[concept] = loc_score

        # 6. æ±‡æ€»æŠ¥å‘Š
        report = {
            'cas_scores': cas_scores,
            'average_cas': np.mean(list(cas_scores.values())) if cas_scores else 0,
            'drop_ratio': drop_ratio,
            'localization_scores': loc_scores,
            'average_localization': np.mean(list(loc_scores.values())) if loc_scores else 0,
            'explanation_map': explanation_norm,
            'concept_maps': concept_maps
        }

        return report

    def _compute_drop_ratio(
        self,
        model: nn.Module,
        image: torch.Tensor,
        target_class: int,
        explanation: np.ndarray,
        percentile: int = 80
    ) -> float:
        """
        è®¡ç®—Drop Ratio

        åˆ é™¤é«˜é‡è¦æ€§åŒºåŸŸåï¼Œæ¨¡å‹ç½®ä¿¡åº¦ä¸‹é™è¶Šå¤šï¼Œè§£é‡Šè´¨é‡è¶Šå¥½
        """
        # åŸå§‹é¢„æµ‹
        with torch.no_grad():
            original_logits = model(image)
            original_prob = F.softmax(original_logits, dim=1)[0, target_class].item()

        # ç”Ÿæˆmask
        threshold = np.percentile(explanation, percentile)
        important_mask = torch.from_numpy(
            explanation > threshold
        ).float().to(self.device)

        # è°ƒæ•´maskå°ºå¯¸ä»¥åŒ¹é…å›¾åƒ
        if important_mask.dim() == 2:
            important_mask = important_mask.unsqueeze(0).unsqueeze(0)
        if important_mask.shape[2:] != image.shape[2:]:
            important_mask = F.interpolate(
                important_mask.unsqueeze(1),
                size=image.shape[2:],
                mode='bilinear',
                align_corners=False
            ).squeeze(1)

        # é®ç›–é‡è¦åŒºåŸŸ
        masked_image = image * (1 - important_mask)

        # è¢«é®ç›–åçš„é¢„æµ‹
        with torch.no_grad():
            masked_logits = model(masked_image)
            masked_prob = F.softmax(masked_logits, dim=1)[0, target_class].item()

        # Drop Ratio
        drop_ratio = (original_prob - masked_prob) / (original_prob + 1e-8)

        return max(0, drop_ratio)

    def compare_xai_methods(
        self,
        model: nn.Module,
        image: torch.Tensor,
        target_class: int,
        xai_methods: Dict[str, Callable],
        relevant_concepts: List[str]
    ) -> Dict:
        """
        å¯¹æ¯”å¤šä¸ªXAIæ–¹æ³•

        å‚æ•°:
            model: å¾…è¯„ä¼°æ¨¡å‹
            image: è¾“å…¥å›¾åƒ
            target_class: ç›®æ ‡ç±»åˆ«
            xai_methods: XAIæ–¹æ³•å­—å…¸
                {'Grad-CAM': gradcam_fn, 'LIME': lime_fn, ...}
            relevant_concepts: ç›¸å…³æ¦‚å¿µåˆ—è¡¨

        è¿”å›:
            comparison_report: å¯¹æ¯”æŠ¥å‘Š
        """
        results = {}

        for method_name, xai_fn in xai_methods.items():
            report = self.evaluate_xai_method(
                model, image, target_class,
                xai_fn, relevant_concepts
            )
            results[method_name] = report

        # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
        comparison = {
            'method_names': list(xai_methods.keys()),
            'average_cas': [r['average_cas'] for r in results.values()],
            'drop_ratios': [r['drop_ratio'] for r in results.values()],
            'avg_localization': [r['average_localization'] for r in results.values()],
            'detailed_reports': results
        }

        return comparison
```

### ç»„ä»¶2: å¸¸ç”¨XAIæ–¹æ³•å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class XAIMethods:
    """
    å¸¸ç”¨XAIæ–¹æ³•å®ç°
    """

    @staticmethod
    def gradcam(model, image, target_class):
        """
        Grad-CAMå®ç°
        """
        model.eval()

        # å‰å‘ä¼ æ’­
        output = model(image)
        output[0, target_class].backward()

        # è·å–æ¢¯åº¦
        gradients = model.get_activation_gradients()  # éœ€è¦hook

        # è·å–æ¿€æ´»
        activations = model.get_activations()  # éœ€è¦hook

        # å…¨å±€å¹³å‡æ± åŒ–æ¢¯åº¦
        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)

        # åŠ æƒç»„åˆ
        cam = torch.sum(weights * activations, dim=1, keepdim=True)
        cam = F.relu(cam)

        # å½’ä¸€åŒ–
        cam = cam.squeeze().cpu().numpy()
        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)

        return cam

    @staticmethod
    def gradcam_plus_plus(model, image, target_class):
        """
        Grad-CAM++å®ç°
        """
        model.eval()

        # å‰å‘ä¼ æ’­
        output = model(image)

        # ä¸€é˜¶å¯¼æ•°
        one_hot = F.one_hot(torch.tensor([target_class]),
                           output.size(1)).float().to(image.device)
        output.backward(gradient=one_hot)

        # è·å–æ¢¯åº¦å’Œæ¿€æ´»
        gradients = model.get_activation_gradients()
        activations = model.get_activations()

        # Grad-CAM++æƒé‡è®¡ç®—
        # å…·ä½“å®ç°ç•¥...
        cam = ...  # è®¡ç®—CAM

        return cam

    @staticmethod
    def score_cam(model, image, target_class):
        """
        Score-CAMå®ç°
        """
        model.eval()

        # è·å–æ¿€æ´»
        activations = model.get_activations()  # (C, H, W)

        # å¯¹æ¯ä¸ªé€šé“
        scores = []
        for k in range(activations.size(1)):
            # ç”Ÿæˆè¯¥é€šé“çš„saliency map
            mask = activations[0, k:k+1, :, :]
            mask_upsampled = F.interpolate(
                mask, size=image.shape[2:],
                mode='bilinear', align_corners=False
            )

            # å‰å‘ä¼ æ’­
            masked_input = image * mask_upsampled
            output = model(masked_input)
            score = output[0, target_class].item()
            scores.append(score)

        # åŠ æƒç»„åˆ
        scores = torch.tensor(scores)
        weights = F.softmax(scores, dim=0)

        cam = torch.sum(weights.view(-1, 1, 1) * activations[0], dim=0)
        cam = F.relu(cam)

        cam = cam.cpu().numpy()
        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)

        return cam

    @staticmethod
    def smooth_gradcam_plus_plus(model, image, target_class, n_samples=50, std_noise=0.2):
        """
        Smooth Grad-CAM++å®ç°
        """
        cams = []

        for _ in range(n_samples):
            # æ·»åŠ å™ªå£°
            noise = torch.randn_like(image) * std_noise
            noisy_image = image + noise
            noisy_image = torch.clamp(noisy_image, 0, 1)

            # è®¡ç®—Grad-CAM++
            cam = XAIMethods.gradcam_plus_plus(model, noisy_image, target_class)
            cams.append(cam)

        # å¹³å‡
        cam_smooth = np.mean(cams, axis=0)
        cam_smooth = (cam_smooth - cam_smooth.min()) / \
                     (cam_smooth.max() - cam_smooth.min() + 1e-8)

        return cam_smooth
```

### ç»„ä»¶3: æ¦‚å¿µæ£€æµ‹å™¨ç¤ºä¾‹

```python
import torch
import torch.nn as nn
from torchvision.models import resnet50

class ConceptDetector(nn.Module):
    """
    é€šç”¨æ¦‚å¿µæ£€æµ‹å™¨

    åŸºäºé¢„è®­ç»ƒResNetæå–ç‰¹å®šæ¦‚å¿µçš„åŒºåŸŸ
    """

    def __init__(self, concept_name, pretrained_path=None):
        super().__init__()

        # ä½¿ç”¨é¢„è®­ç»ƒResNetä½œä¸ºbackbone
        self.backbone = resnet50(pretrained=True)

        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])

        # æ¦‚å¿µç‰¹å®šçš„å¤´
        self.concept_head = nn.Sequential(
            nn.Conv2d(2048, 512, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 1, kernel_size=1),
            nn.Sigmoid()
        )

        self.concept_name = concept_name

        if pretrained_path:
            self.load_state_dict(torch.load(pretrained_path))

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            x: è¾“å…¥å›¾åƒ (B, 3, H, W)

        è¿”å›:
            activation: æ¦‚å¿µæ¿€æ´»å›¾ (B, 1, H', W')
        """
        features = self.backbone(x)
        activation = self.concept_head(features)

        # ä¸Šé‡‡æ ·åˆ°åŸå§‹å°ºå¯¸
        activation = nn.functional.interpolate(
            activation, size=x.shape[2:],
            mode='bilinear', align_corners=False
        )

        return activation


class ConceptBank:
    """
    æ¦‚å¿µæ£€æµ‹å™¨é›†åˆ
    """

    # é¢„å®šä¹‰çš„æ¦‚å¿µåˆ—è¡¨
    BIRD_CONCEPTS = ['head', 'wing', 'tail', 'beak', 'eye', 'leg']
    MAMMAL_CONCEPTS = ['ear', 'eye', 'nose', 'mouth', 'body', 'leg']
    VEHICLE_CONCEPTS = ['wheel', 'window', 'door', 'light', 'mirror']

    @staticmethod
    def get_concept_detector(concept_name, concept_category):
        """
        è·å–ç‰¹å®šæ¦‚å¿µçš„æ£€æµ‹å™¨

        å‚æ•°:
            concept_name: æ¦‚å¿µåç§°
            concept_category: æ¦‚å¿µç±»åˆ« (bird, mammal, vehicle, etc.)
        """
        # è¿™é‡Œåº”è¯¥åŠ è½½é¢„è®­ç»ƒçš„æ£€æµ‹å™¨
        # å®é™…åº”ç”¨ä¸­éœ€è¦ä¸ºæ¯ä¸ªæ¦‚å¿µè®­ç»ƒæ£€æµ‹å™¨

        detector = ConceptDetector(
            f"{concept_category}_{concept_name}",
            pretrained_path=f"checkpoints/{concept_name}.pth"
        )

        return detector

    @staticmethod
    def create_concept_bank(concepts, concept_category):
        """
        åˆ›å»ºæ¦‚å¿µæ£€æµ‹å™¨é›†åˆ
        """
        bank = {}
        for concept in concepts:
            detector = ConceptBank.get_concept_detector(concept, concept_category)
            bank[concept] = detector

        return bank
```

---

## ğŸ§ª åº”ç”¨åˆ°äº•ç›–æ£€æµ‹

### äº•ç›–ç¼ºé™·XAIè¯„ä¼°åœºæ™¯

| æ¦‚å¿µç±»åˆ« | ç›¸å…³æ¦‚å¿µ | XAIåº”ç”¨ |
|:---|:---|:---|
| **ç¼ºé™·ç±»å‹** | è£‚çº¹ã€å˜å½¢ã€ç ´æŸã€ç¼ºå¤± | è¯„ä¼°æ¨¡å‹æ˜¯å¦å…³æ³¨æ­£ç¡®åŒºåŸŸ |
| **ç»“æ„ç‰¹å¾** | åœ†å½¢ã€æ–¹å­”ã€çº¹è·¯ | éªŒè¯è§£é‡Šçš„è¯­ä¹‰åˆç†æ€§ |
| **è¡¨é¢çŠ¶æ€** | é”ˆèš€ã€æ±¡æ¸ã€ç£¨æŸ | è¯„ä¼°å™ªå£°é²æ£’æ€§ |

### äº•ç›–XAIè¯„ä¼°å®ç°

```python
class ManholeXAIEvaluator:
    """
    äº•ç›–ç¼ºé™·æ£€æµ‹XAIè¯„ä¼°å™¨
    """

    def __init__(self):
        # äº•ç›–ç‰¹å®šæ¦‚å¿µ
        manhole_concepts = [
            'crack',          # è£‚çº¹
            'deformation',    # å˜å½¢
            'corrosion',      # é”ˆèš€
            'hole',           # å­”æ´
            'roundness',      # åœ†å½¢åº¦
            'texture'         # çº¹ç†
        ]

        # åˆ›å»ºæ¦‚å¿µæ£€æµ‹å™¨
        self.concept_detectors = ConceptBank.create_concept_bank(
            manhole_concepts, 'manhole'
        )

        # åˆ›å»ºè¯„ä¼°å™¨
        self.evaluator = ConceptBasedXAIEvaluator(
            self.concept_detectors
        )

    def evaluate_defect_explanation(
        self,
        model,
        image,
        defect_type,
        xai_method
    ):
        """
        è¯„ä¼°ç¼ºé™·æ£€æµ‹è§£é‡Šè´¨é‡

        å‚æ•°:
            model: äº•ç›–ç¼ºé™·æ£€æµ‹æ¨¡å‹
            image: è¾“å…¥å›¾åƒ
            defect_type: ç¼ºé™·ç±»å‹ (è£‚çº¹ã€å˜å½¢ç­‰)
            xai_method: XAIè§£é‡Šæ–¹æ³•

        è¿”å›:
            evaluation_report: è¯„ä¼°æŠ¥å‘Š
        """
        # è·å–ç›¸å…³æ¦‚å¿µ
        relevant_concepts = self._get_relevant_concepts(defect_type)

        # è·å–é¢„æµ‹ç±»åˆ«
        with torch.no_grad():
            output = model(image)
            pred_class = output.argmax(dim=1).item()

        # è¯„ä¼°XAIæ–¹æ³•
        report = self.evaluator.evaluate_xai_method(
            model=model,
            image=image,
            target_class=pred_class,
            xai_method=xai_method,
            relevant_concepts=relevant_concepts
        )

        return report

    def _get_relevant_concepts(self, defect_type):
        """
        æ ¹æ®ç¼ºé™·ç±»å‹è·å–ç›¸å…³æ¦‚å¿µ
        """
        concept_mapping = {
            'crack': ['crack', 'texture'],
            'deformation': ['roundness', 'deformation'],
            'corrosion': ['corrosion', 'texture', 'hole'],
            'damage': ['hole', 'crack', 'deformation']
        }

        return concept_mapping.get(defect_type, [])

    def compare_explanation_methods(
        self,
        model,
        images,
        defect_types
    ):
        """
        å¯¹æ¯”ä¸åŒXAIæ–¹æ³•åœ¨äº•ç›–ç¼ºé™·æ£€æµ‹ä¸Šçš„è¡¨ç°
        """
        xai_methods = {
            'Grad-CAM': XAIMethods.gradcam,
            'Grad-CAM++': XAIMethods.gradcam_plus_plus,
            'Score-CAM': XAIMethods.score_cam,
            'Smooth Grad-CAM++': XAIMethods.smooth_gradcam_plus_plus
        }

        all_results = {}

        for image, defect_type in zip(images, defect_types):
            results = self.evaluator.compare_xai_methods(
                model=model,
                image=image,
                target_class=0,  # å‡è®¾0æ˜¯ç¼ºé™·ç±»åˆ«
                xai_methods=xai_methods,
                relevant_concepts=self._get_relevant_concepts(defect_type)
            )

            all_results[f"{defect_type}"] = results

        return all_results
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **æ¦‚å¿µçº§è¯„ä¼°** | Concept-level Evaluation | åŸºäºäººç±»å¯ç†è§£æ¦‚å¿µè¯„ä¼°XAIè´¨é‡ |
| **æ¦‚å¿µå¯¹é½åˆ†æ•°** | Concept Alignment Score (CAS) | è§£é‡Šä¸æ¦‚å¿µæ¿€æ´»çš„å¯¹é½ç¨‹åº¦ |
| **Drop Ratio** | Drop Ratio | åˆ é™¤é‡è¦åŒºåŸŸåç½®ä¿¡åº¦ä¸‹é™æ¯”ä¾‹ |
| **æ¦‚å¿µæ¿€æ´»** | Concept Activation | æ¦‚å¿µåœ¨å›¾åƒä¸­çš„æ¿€æ´»å¼ºåº¦åˆ†å¸ƒ |
| **å®šä½å‡†ç¡®ç‡** | Localization Accuracy | è§£é‡ŠåŒºåŸŸä¸æ¦‚å¿µåŒºåŸŸçš„IoU |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£æ¦‚å¿µçº§è¯„ä¼°çš„åŠ¨æœº
- [ ] æŒæ¡CASè®¡ç®—æ–¹æ³•
- [ ] äº†è§£Drop RatioåŸç†
- [ ] èƒ½å®ç°å®Œæ•´çš„è¯„ä¼°æ¡†æ¶
- [ ] ç†è§£ä¸åŒXAIæ–¹æ³•çš„ä¼˜åŠ£
- [ ] èƒ½å°†æ–¹æ³•åº”ç”¨åˆ°äº•ç›–æ£€æµ‹

---

## ğŸ”— ç›¸å…³è®ºæ–‡æ¨è

### å¿…è¯»

1. **Grad-CAM** (CVPR 2017) - åŸºç¡€è§£é‡Šæ–¹æ³•
2. **Grad-CAM++** (ECCV 2018) - æ”¹è¿›çš„æ¢¯åº¦æ–¹æ³•
3. **Score-CAM** (ECCV 2020) - æ— æ¢¯åº¦è§£é‡Šæ–¹æ³•

### æ‰©å±•é˜…è¯»

1. **RISE** (BMVC 2019) - éšæœºæ©ç è§£é‡Š
2. **FILIP** (ICLR 2021) - åŸºäºè·¨æ¨¡æ€çš„è¯„ä¼°
3. **Concept Activation Vectors** (NIPS 2018) - æ¦‚å¿µå‘é‡

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **ä¸ºä»€ä¹ˆæ¦‚å¿µçº§è¯„ä¼°æ¯”ç‚¹çº§è¯„ä¼°æ›´å¥½ï¼Ÿ**
   - æ›´ç¬¦åˆäººç±»ç†è§£
   - è¯­ä¹‰ç›¸å…³æ€§å¼º
   - å¯è§£é‡Šæ€§é«˜

2. **å¦‚ä½•é€‰æ‹©ç›¸å…³æ¦‚å¿µï¼Ÿ**
   - é¢†åŸŸçŸ¥è¯†
   - æ•°æ®é©±åŠ¨å‘ç°
   - äººå·¥æ ‡æ³¨

3. **æ¦‚å¿µæ£€æµ‹å™¨å¦‚ä½•è·å¾—ï¼Ÿ**
   - é¢„è®­ç»ƒæ¨¡å‹
   - äººå·¥æ ‡æ³¨è®­ç»ƒ
   - å¼±ç›‘ç£å­¦ä¹ 

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ9æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
**ä¸‹ä¸€æ­¥**: å®ç°å®Œæ•´è¯„ä¼°æ¡†æ¶ï¼Œåœ¨äº•ç›–ç¼ºé™·æ•°æ®é›†ä¸ŠéªŒè¯
