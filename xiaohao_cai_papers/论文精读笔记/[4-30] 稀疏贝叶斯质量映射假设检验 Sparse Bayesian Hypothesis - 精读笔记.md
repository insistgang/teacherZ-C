# [4-30] ç¨€ç–è´å¶æ–¯è´¨é‡æ˜ å°„å‡è®¾æ£€éªŒ - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Sparse Bayesian Mass Mapping: Hypothesis Testing
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (é«˜)
> **é‡è¦æ€§**: â­â­â­â­ (å¤©ä½“ç»Ÿè®¡æ–¹æ³•)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Sparse Bayesian Mass Mapping: Hypothesis Testing |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **åº”ç”¨é¢†åŸŸ** | å¤©ä½“ç‰©ç†å­¦ã€å¼•åŠ›é€é•œã€ç»Ÿè®¡æ¨æ–­ |
| **å…³é”®è¯** | Sparse Bayesian, Mass Mapping, Hypothesis Testing, Weak Lensing |
| **æ ¸å¿ƒä»·å€¼** | ç¨€ç–è´å¶æ–¯æ–¹æ³•åœ¨å¤©ä½“è´¨é‡åˆ†å¸ƒé‡å»ºä¸­çš„åº”ç”¨ |

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

### å¼±å¼•åŠ›é€é•œè´¨é‡æ˜ å°„

```
å¼±å¼•åŠ›é€é•œè´¨é‡æ˜ å°„é—®é¢˜:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

èƒŒæ™¯:
  - å¤§è´¨é‡å¤©ä½“(æ˜Ÿç³»å›¢)ä¼šå¼¯æ›²å‘¨å›´æ—¶ç©º
  - èƒŒæ™¯æ˜Ÿç³»çš„å…‰çº¿è¢«åæŠ˜
  - è§‚æµ‹åˆ°èƒŒæ™¯æ˜Ÿç³»çš„å½¢çŠ¶ç•¸å˜(å‰ªåˆ‡)

é—®é¢˜:
  ç»™å®šè§‚æµ‹çš„å‰ªåˆ‡åœº Î³,é‡å»ºè´¨é‡åˆ†å¸ƒ Îº

æ•°å­¦æ¨¡å‹:
  Î³ = P * Îº + n

  å…¶ä¸­:
  - Î³: è§‚æµ‹å‰ªåˆ‡ (å¯è§‚æµ‹)
  - Îº: æ”¶æ•›åœº (å¾…é‡å»ºçš„è´¨é‡åˆ†å¸ƒ)
  - P: æŠ•å½±ç®—å­
  - n: å™ªå£°

æŒ‘æˆ˜:
  1. é—®é¢˜ç—…æ€ (ill-posed)
  2. å™ªå£°æ˜¾è‘—
  3. éœ€è¦ç¨€ç–å…ˆéªŒ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### å‡è®¾æ£€éªŒåœ¨è´¨é‡æ˜ å°„ä¸­çš„ä½œç”¨

| å‡è®¾ | é—®é¢˜ | æ–¹æ³• |
|:---|:---|:---|
| **H0: Îº=0** | æŸåŒºåŸŸæ˜¯å¦æœ‰è´¨é‡èšé›†? | æ˜¾è‘—æ€§æ£€éªŒ |
| **å­ç»“æ„æ£€æµ‹** | æ˜¯å¦å­˜åœ¨æš—ç‰©è´¨å­ç»“æ„? | å³°å€¼æ£€æµ‹ |
| **å³°å€¼æ˜¾è‘—æ€§** | æ£€æµ‹åˆ°çš„å³°å€¼æ˜¯å¦çœŸå®? | på€¼è®¡ç®— |

---

## ğŸ”¬ æ–¹æ³•è®º

### ç¨€ç–è´å¶æ–¯æ¡†æ¶

```
ç¨€ç–è´å¶æ–¯è´¨é‡æ˜ å°„:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è´å¶æ–¯æ¨æ–­:
  P(Îº|Î³) âˆ P(Î³|Îº) Â· P(Îº)

ä¼¼ç„¶å‡½æ•°:
  P(Î³|Îº) = N(Î³ | PÎº, Î£_n)
  å‡è®¾å™ªå£°æœä»é«˜æ–¯åˆ†å¸ƒ

å…ˆéªŒåˆ†å¸ƒ (ç¨€ç–å…ˆéªŒ):
  P(Îº) âˆ exp(-Î»||Îº||_1)  (L1ç¨€ç–å…ˆéªŒ)
  æˆ–
  P(Îº) = âˆ_i p(Îº_i)  (ç¨€ç–è´å¶æ–¯å­¦ä¹ )

åéªŒæ¨æ–­:
  é€šè¿‡å˜åˆ†æ¨æ–­æˆ–MCMCé‡‡æ ·è·å¾—åéªŒåˆ†å¸ƒ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### æ ¸å¿ƒç»„ä»¶1: ç¨€ç–å…ˆéªŒå»ºæ¨¡

```python
import numpy as np
from scipy import stats
import torch
import torch.nn as nn

class SparseBayesianPrior:
    """
    ç¨€ç–è´å¶æ–¯å…ˆéªŒ

    ä½¿ç”¨è‡ªåŠ¨ç›¸å…³æ€§ç¡®å®š(ARD)å®ç°ç¨€ç–æ€§
    """

    def __init__(self, dim, alpha=1e-6, beta=1e-6):
        """
        Args:
            dim: å‚æ•°ç»´åº¦
            alpha, beta: Gammaåˆ†å¸ƒè¶…å‚æ•°
        """
        self.dim = dim

        # ARDç²¾åº¦å‚æ•°
        self.alpha = alpha
        self.beta = beta

        # æ¯ä¸ªç»´åº¦çš„ç²¾åº¦ (é€†æ–¹å·®)
        self.eta = np.ones(dim)  # åˆå§‹åŒ–

    def log_prior(self, kappa):
        """
        è®¡ç®—å¯¹æ•°å…ˆéªŒ

        P(Îº|Î·) = âˆ_i N(Îº_i | 0, Î·_i^(-1))
        """
        log_p = 0
        for i in range(self.dim):
            log_p += -0.5 * self.eta[i] * kappa[i]**2
            log_p += 0.5 * np.log(self.eta[i])

        return log_p

    def update_eta(self, kappa_mean, kappa_var):
        """
        æ›´æ–°ç²¾åº¦å‚æ•° (EMç®—æ³•)

        Î·_i = (1 + 2Î±) / (Îº_i^2 + 2Î² + v_i)
        """
        self.eta = (1 + 2 * self.alpha) / (
            kappa_mean**2 + kappa_var + 2 * self.beta
        )

        return self.eta

    def get_sparsity_pattern(self, threshold=1e3):
        """
        è·å–ç¨€ç–æ¨¡å¼

        å¤§Î·å€¼å¯¹åº”å°æ–¹å·®,å³å¼ºçº¦æŸå‘0
        """
        return self.eta > threshold


class StudentTPrior:
    """
    Student-tç¨€ç–å…ˆéªŒ

    æ¯”L1/L2æ›´é²æ£’çš„ç¨€ç–å…ˆéªŒ
    """

    def __init__(self, nu=1.0, scale=1.0):
        """
        Args:
            nu: è‡ªç”±åº¦ (nu=1ä¸ºCauchyåˆ†å¸ƒ)
            scale: å°ºåº¦å‚æ•°
        """
        self.nu = nu
        self.scale = scale

    def log_prior(self, kappa):
        """Student-tå¯¹æ•°å¯†åº¦"""
        return stats.t.logpdf(kappa, self.nu, scale=self.scale).sum()

    def gradient(self, kappa):
        """æ¢¯åº¦ (ç”¨äºä¼˜åŒ–)"""
        return -(self.nu + 1) * kappa / (self.nu * self.scale**2 + kappa**2)
```

---

### æ ¸å¿ƒç»„ä»¶2: å˜åˆ†æ¨æ–­

```python
class VariationalBayesMassMapping:
    """
    å˜åˆ†è´å¶æ–¯è´¨é‡æ˜ å°„

    è¿‘ä¼¼åéªŒåˆ†å¸ƒ
    """

    def __init__(self, grid_size, sigma_noise=0.1):
        self.grid_size = grid_size
        self.sigma_noise = sigma_noise

        # å˜åˆ†å‚æ•°
        self.kappa_mean = np.zeros(grid_size)
        self.kappa_var = np.ones(grid_size)

        # å…ˆéªŒ
        self.prior = SparseBayesianPrior(grid_size)

    def forward_model(self, kappa):
        """
        å‰å‘æ¨¡å‹: Îº â†’ Î³

         Kaiser-Squires ç®—å­
        """
        # å‚…é‡Œå¶ç©ºé—´æ“ä½œ
        kappa_fft = np.fft.fft2(kappa)

        # æ„é€ æ»¤æ³¢å™¨
        k1, k2 = np.meshgrid(
            np.fft.fftfreq(self.grid_size[0]),
            np.fft.fftfreq(self.grid_size[1])
        )
        k_squared = k1**2 + k2**2
        k_squared[0, 0] = 1  # é¿å…é™¤é›¶

        # æ”¶æ•›åˆ°å‰ªåˆ‡
        D1 = (k1**2 - k2**2) / k_squared
        D2 = (2 * k1 * k2) / k_squared

        gamma1_fft = D1 * kappa_fft
        gamma2_fft = D2 * kappa_fft

        gamma1 = np.fft.ifft2(gamma1_fft).real
        gamma2 = np.fft.ifft2(gamma2_fft).real

        return gamma1, gamma2

    def elbo(self, gamma_obs):
        """
        è¯æ®ä¸‹ç•Œ (ELBO)

        ELBO = E_q[log P(Î³|Îº)] + E_q[log P(Îº)] - E_q[log q(Îº)]
        """
        # ä¼¼ç„¶é¡¹
        gamma_pred1, gamma_pred2 = self.forward_model(self.kappa_mean)

        likelihood = -0.5 * np.sum(
            (gamma_obs[0] - gamma_pred1)**2 +
            (gamma_obs[1] - gamma_pred2)**2
        ) / self.sigma_noise**2

        # å…ˆéªŒé¡¹
        prior = self.prior.log_prior(self.kappa_mean)

        # ç†µé¡¹ (é«˜æ–¯å˜åˆ†åˆ†å¸ƒ)
        entropy = 0.5 * np.sum(np.log(2 * np.pi * np.e * self.kappa_var))

        elbo = likelihood + prior + entropy

        return elbo

    def update(self, gamma_obs, max_iter=100):
        """
        å˜åˆ†æ¨æ–­è¿­ä»£æ›´æ–°
        """
        for i in range(max_iter):
            elbo_old = self.elbo(gamma_obs)

            # æ›´æ–°Îºçš„å‡å€¼ (æ¢¯åº¦ä¸Šå‡)
            grad = self.compute_gradient(gamma_obs)
            self.kappa_mean += 0.01 * grad

            # æ›´æ–°Îºçš„æ–¹å·®
            hessian_diag = self.compute_hessian_diag()
            self.kappa_var = 1.0 / (hessian_diag + self.prior.eta)

            # æ›´æ–°å…ˆéªŒå‚æ•°
            self.prior.update_eta(self.kappa_mean, self.kappa_var)

            elbo_new = self.elbo(gamma_obs)

            if abs(elbo_new - elbo_old) < 1e-6:
                print(f"Converged at iteration {i}")
                break

        return self.kappa_mean, self.kappa_var

    def compute_gradient(self, gamma_obs):
        """è®¡ç®—ELBOå…³äºÎºå‡å€¼çš„æ¢¯åº¦"""
        gamma_pred1, gamma_pred2 = self.forward_model(self.kappa_mean)

        # ä¼¼ç„¶æ¢¯åº¦
        residual1 = gamma_obs[0] - gamma_pred1
        residual2 = gamma_obs[1] - gamma_pred2

        # ä¼´éšæ–¹æ³•è®¡ç®—æ¢¯åº¦
        grad_likelihood = self.adjoint_operator(residual1, residual2)
        grad_likelihood /= self.sigma_noise**2

        # å…ˆéªŒæ¢¯åº¦
        grad_prior = -self.prior.eta * self.kappa_mean

        return grad_likelihood + grad_prior

    def adjoint_operator(self, gamma1, gamma2):
        """ä¼´éšç®—å­ (Kaiser-Squiresé€†)"""
        gamma1_fft = np.fft.fft2(gamma1)
        gamma2_fft = np.fft.fft2(gamma2)

        k1, k2 = np.meshgrid(
            np.fft.fftfreq(self.grid_size[0]),
            np.fft.fftfreq(self.grid_size[1])
        )
        k_squared = k1**2 + k2**2
        k_squared[0, 0] = 1

        D1 = (k1**2 - k2**2) / k_squared
        D2 = (2 * k1 * k2) / k_squared

        kappa_fft = D1 * gamma1_fft + D2 * gamma2_fft

        return np.fft.ifft2(kappa_fft).real

    def compute_hessian_diag(self):
        """è®¡ç®—Hessianå¯¹è§’çº¿è¿‘ä¼¼"""
        # ç®€åŒ–: ä½¿ç”¨å…ˆéªŒç²¾åº¦
        return self.prior.eta + 1.0 / self.sigma_noise**2
```

---

### æ ¸å¿ƒç»„ä»¶3: å‡è®¾æ£€éªŒ

```python
class MassMappingHypothesisTest:
    """
    è´¨é‡æ˜ å°„å‡è®¾æ£€éªŒ

    æ£€æµ‹è´¨é‡èšé›†çš„æ˜¾è‘—æ€§
    """

    def __init__(self, vb_map):
        """
        Args:
            vb_map: å˜åˆ†æ¨æ–­ç»“æœ (å‡å€¼å’Œæ–¹å·®)
        """
        self.kappa_mean = vb_map['mean']
        self.kappa_std = np.sqrt(vb_map['var'])

    def test_peak_significance(self, peak_position):
        """
        æ£€éªŒå³°å€¼çš„æ˜¾è‘—æ€§

        H0: Îº_peak = 0 (æ— è´¨é‡èšé›†)
        H1: Îº_peak > 0 (å­˜åœ¨è´¨é‡èšé›†)
        """
        kappa_val = self.kappa_mean[peak_position]
        kappa_err = self.kappa_std[peak_position]

        # Zåˆ†æ•°
        z_score = kappa_val / (kappa_err + 1e-10)

        # å•ä¾§på€¼
        p_value = 1 - stats.norm.cdf(z_score)

        return {
            'z_score': z_score,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'kappa': kappa_val,
            'kappa_err': kappa_err
        }

    def test_substructure(self, region_mask):
        """
        æ£€éªŒåŒºåŸŸå†…æ˜¯å¦å­˜åœ¨å­ç»“æ„

        H0: åŒºåŸŸå†…Îº=0
        H1: åŒºåŸŸå†…Îºâ‰ 0
        """
        # åŒºåŸŸå†…å¹³å‡Îº
        kappa_region = self.kappa_mean[region_mask]
        var_region = self.kappa_std[region_mask]**2

        # åŠ æƒå¹³å‡
        weights = 1.0 / (var_region + 1e-10)
        weighted_mean = np.sum(weights * kappa_region) / np.sum(weights)
        weighted_var = 1.0 / np.sum(weights)

        # Zæ£€éªŒ
        z_score = weighted_mean / np.sqrt(weighted_var)
        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

        return {
            'z_score': z_score,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'kappa_mean': weighted_mean,
            'kappa_std': np.sqrt(weighted_var)
        }

    def multiple_testing_correction(self, tests, method='fdr'):
        """
        å¤šé‡æ£€éªŒæ ¡æ­£

        Args:
            tests: å¤šä¸ªæ£€éªŒçš„på€¼åˆ—è¡¨
            method: 'bonferroni' æˆ– 'fdr'
        """
        p_values = [t['p_value'] for t in tests]

        if method == 'bonferroni':
            # Bonferroniæ ¡æ­£
            corrected = np.minimum(np.array(p_values) * len(p_values), 1.0)

        elif method == 'fdr':
            # Benjamini-Hochberg FDRæ ¡æ­£
            from statsmodels.stats.multitest import multipletests
            _, corrected, _, _ = multipletests(p_values, method='fdr_bh')

        # æ›´æ–°æ£€éªŒç»“æœ
        for i, test in enumerate(tests):
            test['p_value_corrected'] = corrected[i]
            test['significant_corrected'] = corrected[i] < 0.05

        return tests

    def compute_detection_threshold(self, n_sigma=3):
        """
        è®¡ç®—æ£€æµ‹é˜ˆå€¼

        åŸºäºå™ªå£°æ°´å¹³çš„n-sigmaé˜ˆå€¼
        """
        return n_sigma * self.kappa_std

    def find_peaks(self, threshold_sigma=3):
        """
        å¯»æ‰¾æ˜¾è‘—å³°å€¼
        """
        from scipy.ndimage import maximum_filter

        threshold = self.compute_detection_threshold(threshold_sigma)

        # å±€éƒ¨æå¤§å€¼
        local_max = maximum_filter(self.kappa_mean, size=3) == self.kappa_mean

        # æ˜¾è‘—æ€§é˜ˆå€¼
        significant = self.kappa_mean > threshold

        # å³°å€¼ä½ç½®
        peaks = local_max & significant
        peak_positions = np.argwhere(peaks)

        # æ£€éªŒæ¯ä¸ªå³°å€¼
        peak_results = []
        for pos in peak_positions:
            result = self.test_peak_significance(tuple(pos))
            peak_results.append({
                'position': pos,
                **result
            })

        return peak_results
```

---

## ğŸ“Š å®éªŒç»“æœ

### æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•

| æ–¹æ³• | é‡å»ºè¯¯å·® | å³°å€¼æ£€æµ‹ç‡ | å‡é˜³æ€§ç‡ |
|:---|:---:|:---:|:---:|
| Kaiser-Squires | é«˜ | 85% | 15% |
| Wieneræ»¤æ³¢ | ä¸­ | 88% | 12% |
| L1æ­£åˆ™åŒ– | ä½ | 90% | 8% |
| **ç¨€ç–è´å¶æ–¯** | **æœ€ä½** | **95%** | **3%** |

### å‡è®¾æ£€éªŒæ€§èƒ½

| æ£€éªŒç±»å‹ | ç»Ÿè®¡åŠŸæ•ˆ | å‡é˜³æ€§æ§åˆ¶ |
|:---|:---:|:---:|
| å•å³°å€¼æ£€éªŒ | 0.92 | è‰¯å¥½ |
| å­ç»“æ„æ£€éªŒ | 0.88 | è‰¯å¥½ |
| å¤šé‡æ£€éªŒ(FDR) | 0.85 | ä¼˜ç§€ |

---

## ğŸ’¡ å¯¹äº•ç›–æ£€æµ‹çš„å¯ç¤º

### å¼‚å¸¸æ£€æµ‹æ¡†æ¶

```python
class BayesianAnomalyDetector:
    """
    å€Ÿé‰´ç¨€ç–è´å¶æ–¯æ–¹æ³•çš„å¼‚å¸¸æ£€æµ‹

    ç”¨äºæ£€æµ‹äº•ç›–å¼‚å¸¸çŠ¶æ€
    """

    def __init__(self):
        self.prior = SparseBayesianPrior(dim=feature_dim)

    def detect_anomaly(self, features):
        """
        è´å¶æ–¯å¼‚å¸¸æ£€æµ‹

        å‡è®¾æ­£å¸¸çŠ¶æ€æ˜¯ç¨€ç–çš„åŸºçº¿
        å¼‚å¸¸è¡¨ç°ä¸ºåç¦»åŸºçº¿
        """
        # æ¨æ–­åéªŒ
        mean, var = self.variational_inference(features)

        # å¼‚å¸¸åˆ†æ•° (åç¦»0çš„ç¨‹åº¦)
        anomaly_score = np.abs(mean) / np.sqrt(var + 1e-10)

        # å‡è®¾æ£€éªŒ
        p_value = 2 * (1 - stats.norm.cdf(anomaly_score))

        return {
            'anomaly_score': anomaly_score,
            'p_value': p_value,
            'is_anomaly': p_value < 0.01
        }
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **å¼±å¼•åŠ›é€é•œ** | Weak Gravitational Lensing | å…‰çº¿åœ¨å¤§è´¨é‡å¤©ä½“é™„è¿‘çš„å¾®å¼±åæŠ˜ |
| **æ”¶æ•›åœº** | Convergence Field | è´¨é‡åˆ†å¸ƒçš„æŠ•å½± |
| **ARD** | Automatic Relevance Determination | è‡ªåŠ¨ç›¸å…³æ€§ç¡®å®š |
| **å˜åˆ†æ¨æ–­** | Variational Inference | è¿‘ä¼¼åéªŒåˆ†å¸ƒçš„æ–¹æ³• |
| **ELBO** | Evidence Lower Bound | è¯æ®ä¸‹ç•Œ |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å¼±å¼•åŠ›é€é•œè´¨é‡æ˜ å°„é—®é¢˜
- [ ] æŒæ¡ç¨€ç–è´å¶æ–¯å…ˆéªŒ
- [ ] äº†è§£å˜åˆ†æ¨æ–­æ–¹æ³•
- [ ] ç†è§£å‡è®¾æ£€éªŒåœ¨è´¨é‡æ˜ å°„ä¸­çš„åº”ç”¨

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
