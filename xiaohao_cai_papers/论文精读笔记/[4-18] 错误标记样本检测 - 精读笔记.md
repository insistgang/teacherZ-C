# [4-18] é”™è¯¯æ ‡è®°æ ·æœ¬æ£€æµ‹ - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Detecting Mislabelled Specimens in Training Data
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­ (ä¸­ç­‰)
> **é‡è¦æ€§**: â­â­â­ (æ•°æ®æ¸…æ´—)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Detecting Mislabelled Specimens in Training Data |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **å‘è¡¨æœŸåˆŠ** | Pattern Recognition Letters |
| **å‘è¡¨å¹´ä»½** | 2019 |
| **æ–‡ç« ç±»å‹** | å…¨æ–‡è®ºæ–‡ |
| **å…³é”®è¯** | Mislabel Detection, Data Cleaning, Training Data Quality |
| **å½±å“å› å­** | Pattern Recognition Letters (2019) ~3.5 |

---

## ğŸ¯ ç ”ç©¶é—®é¢˜

### é”™è¯¯æ ‡è®°æ£€æµ‹æŒ‘æˆ˜

**æ ¸å¿ƒé—®é¢˜**: å¦‚ä½•è‡ªåŠ¨æ£€æµ‹è®­ç»ƒæ•°æ®ä¸­çš„é”™è¯¯æ ‡è®°æ ·æœ¬

**é”™è¯¯æ ‡è®°æ¥æº**:
```
â”œâ”€â”€ äººå·¥æ ‡æ³¨é”™è¯¯
â”œâ”€â”€ è‡ªåŠ¨æ ‡æ³¨é”™è¯¯
â”œâ”€â”€ æ•°æ®æ±¡æŸ“
â””â”€â”€ ç±»åˆ«æ­§ä¹‰
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ ¸å¿ƒç»„ä»¶: é”™è¯¯æ ‡è®°æ£€æµ‹ç®—æ³•

```python
import numpy as np
from sklearn.model_selection import cross_val_predict
from sklearn.ensemble import RandomForestClassifier

class MislabelDetector:
    """
    é”™è¯¯æ ‡è®°æ ·æœ¬æ£€æµ‹å™¨
    """
    def __init__(self, confidence_threshold=0.5):
        self.confidence_threshold = confidence_threshold
        self.model = None

    def detect(self, X, y, method='confidence'):
        """
        æ£€æµ‹å¯èƒ½çš„é”™è¯¯æ ‡è®°

        Args:
            X: ç‰¹å¾
            y: æ ‡ç­¾
            method: æ£€æµ‹æ–¹æ³•

        Returns:
            suspicious_indices: å¯ç–‘æ ·æœ¬ç´¢å¼•
            scores: å¯ç–‘åº¦åˆ†æ•°
        """
        if method == 'confidence':
            return self._confidence_based_detection(X, y)
        elif method == 'consensus':
            return self._consensus_based_detection(X, y)
        elif method == 'loss':
            return self._loss_based_detection(X, y)

    def _confidence_based_detection(self, X, y):
        """åŸºäºç½®ä¿¡åº¦çš„æ£€æµ‹"""
        from sklearn.linear_model import LogisticRegression

        model = LogisticRegression(max_iter=1000)
        model.fit(X, y)

        # é¢„æµ‹æ¦‚ç‡
        probas = model.predict_proba(X)

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬å¯¹å…¶æ ‡ç­¾çš„ç½®ä¿¡åº¦
        label_probas = probas[np.arange(len(y)), y]

        # ä½ç½®ä¿¡åº¦æ ·æœ¬å¯ç–‘
        suspicious = label_probas < self.confidence_threshold
        scores = 1 - label_probas

        return np.where(suspicious)[0], scores

    def _consensus_based_detection(self, X, y, n_folds=5):
        """åŸºäºäº¤å‰éªŒè¯ä¸€è‡´æ€§çš„æ£€æµ‹"""
        model = RandomForestClassifier(n_estimators=100)

        # äº¤å‰éªŒè¯é¢„æµ‹
        predictions = cross_val_predict(model, X, y, cv=n_folds)

        # é¢„æµ‹ä¸æ ‡ç­¾ä¸ä¸€è‡´çš„æ ·æœ¬
        disagreements = predictions != y

        # è®¡ç®—å¯ç–‘åº¦åˆ†æ•°
        scores = np.zeros(len(y))
        scores[disagreements] = 1.0

        return np.where(disagreements)[0], scores

    def _loss_based_detection(self, X, y):
        """åŸºäºæŸå¤±å€¼çš„æ£€æµ‹"""
        from sklearn.neural_network import MLPClassifier

        model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)
        model.fit(X, y)

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
        probas = model.predict_proba(X)
        losses = -np.log(probas[np.arange(len(y)), y] + 1e-10)

        # é«˜æŸå¤±æ ·æœ¬å¯ç–‘
        threshold = np.percentile(losses, 90)
        suspicious = losses > threshold

        return np.where(suspicious)[0], losses

    def iterative_cleaning(self, X, y, max_iterations=5):
        """
        è¿­ä»£æ¸…æ´—

        é€æ­¥ç§»é™¤å¯ç–‘æ ·æœ¬å¹¶é‡æ–°è®­ç»ƒ
        """
        clean_indices = np.arange(len(y))
        removed_indices = []

        for iteration in range(max_iterations):
            X_clean = X[clean_indices]
            y_clean = y[clean_indices]

            # æ£€æµ‹
            suspicious, scores = self.detect(X_clean, y_clean)

            if len(suspicious) == 0:
                break

            # ç§»é™¤æœ€å¯ç–‘çš„æ ·æœ¬
            to_remove = suspicious[scores[suspicious] > 0.8]
            removed_indices.extend(clean_indices[to_remove])
            clean_indices = np.delete(clean_indices, to_remove)

            print(f"Iteration {iteration + 1}: Removed {len(to_remove)} samples")

        return clean_indices, removed_indices
```

---

## ğŸ’¡ å¯¹è¿å»ºæ£€æµ‹çš„è¿ç§»

```python
class ChangeDetectionDataCleaner:
    """
    å˜åŒ–æ£€æµ‹æ•°æ®æ¸…æ´— - åŸºäº[4-18]æ€æƒ³

    æ£€æµ‹è®­ç»ƒæ•°æ®ä¸­çš„é”™è¯¯æ ‡è®°
    """
    def __init__(self):
        self.detector = MislabelDetector()

    def clean_training_data(self, image_pairs, change_labels):
        """
        æ¸…æ´—å˜åŒ–æ£€æµ‹è®­ç»ƒæ•°æ®

        Args:
            image_pairs: [(img_t1, img_t2), ...]
            change_labels: [change_mask, ...]
        """
        # æå–ç‰¹å¾
        features = []
        for img1, img2 in image_pairs:
            feat = self._extract_pair_features(img1, img2)
            features.append(feat)

        X = np.array(features)

        # ç®€åŒ–: å°†å˜åŒ–æ ‡ç­¾è½¬ä¸ºåˆ†ç±»
        y = np.array([np.any(label) for label in change_labels]).astype(int)

        # æ£€æµ‹é”™è¯¯æ ‡è®°
        clean_indices, removed = self.detector.iterative_cleaning(X, y)

        print(f"Original: {len(y)}, Clean: {len(clean_indices)}, Removed: {len(removed)}")

        return clean_indices, removed

    def _extract_pair_features(self, img1, img2):
        """æå–å›¾åƒå¯¹ç‰¹å¾"""
        diff = np.abs(img1.astype(float) - img2.astype(float))
        return [
            np.mean(diff),
            np.std(diff),
            np.max(diff),
            np.percentile(diff, 95)
        ]
```

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
