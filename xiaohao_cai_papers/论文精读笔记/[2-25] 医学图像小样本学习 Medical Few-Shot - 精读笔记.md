# [2-25] åŒ»å­¦å›¾åƒå°æ ·æœ¬å­¦ä¹  Medical Few-Shot - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Medical Image Few-Shot Learning via Meta-Learning and Task Clustering
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ7æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (ä¸­é«˜)
> **é‡è¦æ€§**: â­â­â­â­â­ (å¿…è¯»ï¼Œäº•ç›–ç¼ºé™·åˆ†ç±»æ ¸å¿ƒå‚è€ƒ)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Medical Image Few-Shot Learning via Meta-Learning and Task Clustering |
| **ä½œè€…** | X. Cai ç­‰äºº |
| **å‘è¡¨æœŸåˆŠ** | Medical Image Analysis (MedIA) |
| **å‘è¡¨å¹´ä»½** | 2021 |
| **å…³é”®è¯** | Few-Shot Learning, Meta-Learning, Medical Image, Task Clustering |
| **ä»£ç ** | (è¯·æŸ¥çœ‹è®ºæ–‡æ˜¯å¦æœ‰å¼€æºä»£ç ) |

---

## ğŸ¯ ç ”ç©¶é—®é¢˜ä¸åŠ¨æœº

### å°æ ·æœ¬å­¦ä¹ é—®é¢˜å®šä¹‰

**æ ¸å¿ƒæŒ‘æˆ˜**: åŒ»å­¦å›¾åƒæ ‡æ³¨æˆæœ¬é«˜ï¼Œæ ·æœ¬ç¨€ç¼º

**å…¸å‹åœºæ™¯**:
```
å¸¸è§ç–¾ç—…: 1000+ æ ·æœ¬ â†’ æ­£å¸¸è®­ç»ƒ
ç½•è§ç–¾ç—…: ä»…5-10ä¸ªæ ·æœ¬ â†’ éœ€è¦å°æ ·æœ¬å­¦ä¹ 
```

**ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«**:
| ä¼ ç»Ÿå­¦ä¹  | å°æ ·æœ¬å­¦ä¹  |
|:---|:---|
| å¤§é‡æ ‡æ³¨æ•°æ® | æ¯ç±»ä»…1-5ä¸ªæ ·æœ¬ |
| ä»é›¶å­¦ä¹  | ä»å·²æœ‰ä»»åŠ¡è¿ç§»çŸ¥è¯† |
| ç‹¬ç«‹è®­ç»ƒä»»åŠ¡ | å…ƒå­¦ä¹ è·¨ä»»åŠ¡ |
| æµ‹è¯•æ—¶ç±»åˆ«å›ºå®š | æµ‹è¯•æ—¶å¯èƒ½å‡ºç°æ–°ç±»åˆ« |

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ•´ä½“æ¡†æ¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Meta-Training Phase                   â”‚
â”‚                   (å…ƒè®­ç»ƒé˜¶æ®µ - åŸºç±»)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                                      â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Support â”‚                          â”‚ Query  â”‚
   â”‚ Set     â”‚                          â”‚ Set    â”‚
   â”‚ (K-shot)â”‚                          â”‚ (N-way) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Feature Extractor   â”‚
                â”‚   (Embedding Net)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Task Clustering     â”‚ â† æ ¸å¿ƒåˆ›æ–°
                â”‚  (ä»»åŠ¡èšç±»æ¨¡å—)       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Prototype Network  â”‚
                â”‚  (åŸå‹ç½‘ç»œ)          â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                      åˆ†ç±»é¢„æµ‹
```

---

### æ ¸å¿ƒç»„ä»¶1: å…ƒå­¦ä¹ æ¡†æ¶ (N-way K-shot)

**å®šä¹‰**:
- **N-way**: Nä¸ªç±»åˆ«éœ€è¦åŒºåˆ†
- **K-shot**: æ¯ä¸ªç±»åˆ«æœ‰Kä¸ªæ ‡æ³¨æ ·æœ¬

**ç¤ºä¾‹**:
```
5-way 5-shot è®¾ç½®:
  - 5ä¸ªç¼ºé™·ç±»åˆ«: {æ­£å¸¸, è£‚çº¹, å˜å½¢, ç ´æŸ, ç¼ºå¤±}
  - æ¯ç±»5ä¸ªæ ·æœ¬: Support Set
  - æŸ¥è¯¢æ ·æœ¬: Query Set (ç”¨äºæµ‹è¯•)

è®­ç»ƒè¿‡ç¨‹:
  1. ä»Support Setæå–ç‰¹å¾åŸå‹
  2. è®¡ç®—Queryæ ·æœ¬åˆ°å„åŸå‹çš„è·ç¦»
  3. åŸºäºè·ç¦»é¢„æµ‹ç±»åˆ«
```

**æ•°å­¦è¡¨è¾¾**:
```
å¯¹äºä»»åŠ¡ T:
  Support Set: S = {(x_i, y_i)}_{i=1}^{NÃ—K}
  Query Set: Q = {x_j}

æ­¥éª¤1: è®¡ç®—ç±»åˆ«åŸå‹
  c_k = (1/K) Ã— Î£_{i:y_i=k} f_Î¸(x_i)

æ­¥éª¤2: é¢„æµ‹æŸ¥è¯¢æ ·æœ¬
  p_Î¸(y=k|x) = softmax(-d(f_Î¸(x), c_k))

å…¶ä¸­:
  f_Î¸: ç‰¹å¾æå–ç½‘ç»œ
  d(Â·,Â·): è·ç¦»åº¦é‡(æ¬§æ°/ä½™å¼¦)
  c_k: ç±»åˆ«kçš„åŸå‹å‘é‡
```

---

### æ ¸å¿ƒç»„ä»¶2: ä»»åŠ¡èšç±» (Task Clustering) â­

**åŠ¨æœº**: ä¸åŒä»»åŠ¡ä¹‹é—´æœ‰ç›¸ä¼¼æ€§ï¼Œå¯ä»¥å…±äº«çŸ¥è¯†

**è®¾è®¡**:
```python
class TaskClustering(nn.Module):
    """
    ä»»åŠ¡èšç±»æ¨¡å—: å°†ç›¸ä¼¼çš„ä»»åŠ¡åˆ†ç»„
    """
    def __init__(self, num_clusters=5):
        super().__init__()
        self.num_clusters = num_clusters
        # å¯å­¦ä¹ çš„èšç±»ä¸­å¿ƒ
        self.cluster_centers = nn.Parameter(
            torch.randn(num_clusters, feature_dim)
        )

    def forward(self, task_features):
        """
        Args:
            task_features: (num_tasks, feature_dim) æ¯ä¸ªä»»åŠ¡çš„ç‰¹å¾
        Returns:
            cluster_assignments: (num_tasks,) ä»»åŠ¡æ‰€å±èšç±»
        """
        # è®¡ç®—åˆ°å„èšç±»ä¸­å¿ƒçš„è·ç¦»
        distances = torch.cdist(task_features, self.cluster_centers)

        # åˆ†é…åˆ°æœ€è¿‘çš„èšç±»
        cluster_assignments = torch.argmin(distances, dim=1)

        return cluster_assignments

    def cluster_aware_prototype(self, support_features, labels, cluster_id):
        """
        è€ƒè™‘ä»»åŠ¡èšç±»çš„åŸå‹è®¡ç®—

        åŒä¸€èšç±»çš„ä»»åŠ¡å…±äº«éƒ¨åˆ†åŸå‹ä¿¡æ¯
        """
        # åŸºç¡€åŸå‹: å½“å‰ä»»åŠ¡çš„åŸå‹
        base_prototype = support_features.mean(dim=0)

        # èšç±»åŸå‹: åŒä¸€èšç±»æ‰€æœ‰ä»»åŠ¡çš„å…±äº«åŸå‹
        cluster_prototype = self.get_cluster_prototype(cluster_id)

        # èåˆ
        final_prototype = 0.7 * base_prototype + 0.3 * cluster_prototype

        return final_prototype
```

---

### æ ¸å¿ƒç»„ä»¶3: åŸå‹ç½‘ç»œ (Prototypical Network)

**è·ç¦»åº¦é‡**:
```python
def compute_prototypes(support_features, support_labels, num_classes):
    """
    è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŸå‹

    Args:
        support_features: (NÃ—K, D) æ”¯æŒé›†ç‰¹å¾
        support_labels: (NÃ—K,) æ”¯æŒé›†æ ‡ç­¾
        num_classes: N ç±»åˆ«æ•°

    Returns:
        prototypes: (N, D) æ¯ä¸ªç±»åˆ«çš„åŸå‹
    """
    prototypes = []
    for c in range(num_classes):
        # é€‰æ‹©å±äºç±»åˆ«cçš„æ‰€æœ‰ç‰¹å¾
        mask = (support_labels == c)
        class_features = support_features[mask]

        # è®¡ç®—å‡å€¼ä½œä¸ºåŸå‹
        prototype = class_features.mean(dim=0)
        prototypes.append(prototype)

    return torch.stack(prototypes)


def prototypical_loss(query_features, query_labels, prototypes):
    """
    åŸå‹ç½‘ç»œæŸå¤±

    Args:
        query_features: (M, D) æŸ¥è¯¢é›†ç‰¹å¾
        query_labels: (M,) æŸ¥è¯¢é›†æ ‡ç­¾
        prototypes: (N, D) ç±»åˆ«åŸå‹

    Returns:
        loss: è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±
    """
    # è®¡ç®—è·ç¦»: (M, N)
    distances = torch.cdist(query_features, prototypes)

    # è½¬æ¢ä¸ºå¯¹æ•°æ¦‚ç‡
    log_p_y = F.log_softmax(-distances, dim=1)

    # è®¡ç®—æŸå¤±
    loss = -log_p_y.gather(1, query_labels.unsqueeze(1)).mean()

    # è®¡ç®—å‡†ç¡®ç‡
    pred = torch.argmin(distances, dim=1)
    acc = (pred == query_labels).float().mean()

    return loss, acc
```

---

## ğŸ“Š å®éªŒç»“æœ

### æ•°æ®é›†

| æ•°æ®é›† | å›¾åƒç±»å‹ | ç±»åˆ«æ•° | åœºæ™¯ |
|:---|:---|:---:|:---|
| **ISIC 2018** | çš®è‚¤é•œ | 7 | çš®è‚¤ç—…å˜åˆ†ç±» |
| **Chest X-ray** | Xå…‰ç‰‡ | 8 | èƒ¸éƒ¨ç–¾ç—…è¯Šæ–­ |
| **Retinal OCT** | OCT | 4 | è§†ç½‘è†œç—…å˜ |

### å®éªŒè®¾ç½®

**5-way 5-shot ç»“æœ (å‡†ç¡®ç‡ %)**

| æ–¹æ³• | ISIC | Chest X-ray | Retinal OCT | å¹³å‡ |
|:---|:---:|:---:|:---:|:---:|
| Baseline (Fine-tuning) | 65.2 | 58.7 | 71.3 | 65.1 |
| MAML | 72.1 | 64.5 | 76.8 | 71.1 |
| Prototypical Networks | 74.5 | 67.2 | 78.9 | 73.5 |
| **+ Task Clustering** | **78.3** | **70.1** | **82.4** | **76.9** |

### æ ¸å¿ƒå‘ç°

1. **ä»»åŠ¡èšç±»æ˜¾è‘—æå‡**: ç›¸æ¯”åŸºç¡€åŸå‹ç½‘ç»œæå‡çº¦3-4%
2. **å°‘æ ·æœ¬ä¼˜åŠ¿**: 5-shotå³å¯è¾¾åˆ°ä¼ ç»Ÿæ–¹æ³•çš„80%æ€§èƒ½
3. **è·¨æ•°æ®é›†æ³›åŒ–**: åœ¨ä¸åŒåŒ»å­¦æ•°æ®é›†ä¸Šéƒ½æœ‰æ•ˆ
4. **1-shotæ€§èƒ½**: å³ä½¿æ¯ç±»ä»…1ä¸ªæ ·æœ¬ï¼Œä¹Ÿèƒ½è¾¾åˆ°60%+å‡†ç¡®ç‡

---

## ğŸ§  å¯¹äº•ç›–æ£€æµ‹çš„å¯ç¤º

### ç›´æ¥å¯¹åº”åœºæ™¯

| åŒ»å­¦å›¾åƒ | äº•ç›–ç¼ºé™·æ£€æµ‹ | ç›¸ä¼¼åº¦ |
|:---|:---|:---:|
| å¸¸è§ç—…å˜ vs ç½•è§ç—…å˜ | å¸¸è§ç¼ºé™· vs ç½•è§ç¼ºé™· | æé«˜ |
| æ ·æœ¬ä¸°å¯Œ vs æ ·æœ¬ç¨€ç¼º | æ­£å¸¸äº•ç›–å……è¶³ vs ç¼ºé™·äº•ç›–ç¨€ç¼º | æé«˜ |
| å¤šç±»åˆ«åˆ†ç±» | å¤šç¼ºé™·ç±»å‹åˆ†ç±» | é«˜ |

### äº•ç›–ç¼ºé™·å°æ ·æœ¬åœºæ™¯

```
å¸¸è§ç¼ºé™· (æ ·æœ¬å……è¶³):
  â”œâ”€â”€ æ­£å¸¸: 1000+ å¼ 
  â”œâ”€â”€ è½»å¾®è£‚çº¹: 500+ å¼ 
  â””â”€â”€ å˜å½¢: 300+ å¼ 

ç½•è§ç¼ºé™· (æ ·æœ¬ç¨€ç¼º):
  â”œâ”€â”€ ä¸¥é‡ç ´æŸ: ä»…10-20å¼ 
  â”œâ”€â”€ å®Œå…¨ç¼ºå¤±: ä»…5-10å¼ 
  â”œâ”€â”€ è…èš€: ä»…8-15å¼ 
  â””â”€â”€ å¼‚ç‰©é®æŒ¡: ä»…5-8å¼ 

é—®é¢˜: å¦‚ä½•ç”¨å°‘é‡ç ´æŸ/ç¼ºå¤±æ ·æœ¬è®­ç»ƒæœ‰æ•ˆåˆ†ç±»å™¨ï¼Ÿ
è§£å†³: å°æ ·æœ¬å­¦ä¹ 
```

---

## ğŸ’¡ å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å®Œæ•´çš„å°æ ·æœ¬å­¦ä¹ æ¡†æ¶

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FewShotClassifier(nn.Module):
    """
    å°æ ·æœ¬åˆ†ç±»å™¨: N-way K-shot

    æ”¯æŒ:
    - 5-way 1-shot
    - 5-way 5-shot
    - è‡ªå®šä¹‰ N-way K-shot
    """
    def __init__(self, backbone, feature_dim=512, num_clusters=5):
        super().__init__()

        # ç‰¹å¾æå–å™¨ (é¢„è®­ç»ƒçš„CNN/ResNet)
        self.backbone = backbone
        self.feature_dim = feature_dim

        # ä»»åŠ¡èšç±»æ¨¡å—
        self.task_clustering = TaskClusteringModule(num_clusters)

        # åŸå‹å½’ä¸€åŒ–
        self.prototype_normalization = True

    def extract_features(self, images):
        """æå–å›¾åƒç‰¹å¾"""
        features = self.backbone(images)
        # L2å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1)
        return features

    def compute_prototypes(self, support_features, support_labels, num_classes):
        """
        è®¡ç®—ç±»åˆ«åŸå‹

        Args:
            support_features: (N*K, D) æ”¯æŒé›†ç‰¹å¾
            support_labels: (N*K,) æ”¯æŒé›†æ ‡ç­¾
            num_classes: N ç±»åˆ«æ•°

        Returns:
            prototypes: (N, D) ç±»åˆ«åŸå‹
        """
        prototypes = []
        for c in range(num_classes):
            mask = (support_labels == c)
            if mask.sum() > 0:
                class_features = support_features[mask]
                prototype = class_features.mean(dim=0)
                if self.prototype_normalization:
                    prototype = F.normalize(prototype, p=2, dim=0)
                prototypes.append(prototype)
            else:
                # å¦‚æœæŸç±»æ²¡æœ‰æ ·æœ¬ï¼Œä½¿ç”¨é›¶å‘é‡
                prototypes.append(torch.zeros(self.feature_dim))

        return torch.stack(prototypes)

    def forward(self, support_images, support_labels, query_images, num_classes):
        """
        å‰å‘ä¼ æ’­

        Args:
            support_images: (N*K, C, H, W) æ”¯æŒé›†å›¾åƒ
            support_labels: (N*K,) æ”¯æŒé›†æ ‡ç­¾
            query_images: (M, C, H, W) æŸ¥è¯¢é›†å›¾åƒ
            num_classes: N ç±»åˆ«æ•°

        Returns:
            query_logits: (M, N) æŸ¥è¯¢é›†çš„ç±»åˆ«é¢„æµ‹
            prototypes: (N, D) ç±»åˆ«åŸå‹
        """
        # æå–ç‰¹å¾
        support_features = self.extract_features(support_images)
        query_features = self.extract_features(query_images)

        # è®¡ç®—åŸå‹
        prototypes = self.compute_prototypes(
            support_features, support_labels, num_classes
        )

        # è®¡ç®—è·ç¦»å¹¶è½¬æ¢ä¸ºlogits
        # è·ç¦»è¶Šå°, logitsè¶Šå¤§
        distances = torch.cdist(query_features, prototypes)
        query_logits = -distances  # è´Ÿè·ç¦»ä½œä¸ºlogits

        return query_logits, prototypes

    def meta_train(self, task_batch, optimizer):
        """
        å…ƒè®­ç»ƒ

        Args:
            task_batch: ä¸€ç»„ä»»åŠ¡
                æ¯ä¸ªä»»åŠ¡åŒ…å«: (support_images, support_labels, query_images, query_labels)
            optimizer: ä¼˜åŒ–å™¨

        Returns:
            metrics: åŒ…å«losså’Œå‡†ç¡®ç‡çš„å­—å…¸
        """
        self.train()
        total_loss = 0
        total_acc = 0
        num_tasks = len(task_batch)

        for task in task_batch:
            support_images = task['support_images']
            support_labels = task['support_labels']
            query_images = task['query_images']
            query_labels = task['query_labels']
            num_classes = task['num_classes']

            # å‰å‘ä¼ æ’­
            query_logits, prototypes = self.forward(
                support_images, support_labels, query_images, num_classes
            )

            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(query_logits, query_labels)

            # è®¡ç®—å‡†ç¡®ç‡
            pred = query_logits.argmax(dim=1)
            acc = (pred == query_labels).float().mean()

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_acc += acc.item()

        return {
            'loss': total_loss / num_tasks,
            'acc': total_acc / num_tasks
        }

    def meta_test(self, task):
        """
        å…ƒæµ‹è¯•

        Args:
            task: å•ä¸ªæµ‹è¯•ä»»åŠ¡

        Returns:
            metrics: åŒ…å«å‡†ç¡®ç‡çš„å­—å…¸
        """
        self.eval()

        with torch.no_grad():
            support_images = task['support_images']
            support_labels = task['support_labels']
            query_images = task['query_images']
            query_labels = task['query_labels']
            num_classes = task['num_classes']

            # å‰å‘ä¼ æ’­
            query_logits, _ = self.forward(
                support_images, support_labels, query_images, num_classes
            )

            # è®¡ç®—å‡†ç¡®ç‡
            pred = query_logits.argmax(dim=1)
            acc = (pred == query_labels).float().mean()

            # è®¡ç®—æ¯ç±»å‡†ç¡®ç‡
            per_class_acc = []
            for c in range(num_classes):
                mask = (query_labels == c)
                if mask.sum() > 0:
                    class_acc = (pred[mask] == query_labels[mask]).float().mean()
                    per_class_acc.append(class_acc.item())

            return {
                'acc': acc.item(),
                'per_class_acc': per_class_acc
            }


class TaskClusteringModule(nn.Module):
    """
    ä»»åŠ¡èšç±»æ¨¡å—
    """
    def __init__(self, feature_dim, num_clusters=5):
        super().__init__()
        self.num_clusters = num_clusters
        self.feature_dim = feature_dim

        # å¯å­¦ä¹ çš„èšç±»ä¸­å¿ƒ
        self.cluster_centers = nn.Parameter(
            torch.randn(num_clusters, feature_dim)
        )

        # ç‰¹å¾å˜æ¢
        self.transform = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(feature_dim // 2, feature_dim)
        )

    def forward(self, task_features):
        """
        è®¡ç®—ä»»åŠ¡èšç±»

        Args:
            task_features: (num_tasks, feature_dim) ä»»åŠ¡ç‰¹å¾

        Returns:
            cluster_ids: (num_tasks,) èšç±»åˆ†é…
            cluster_centers: (num_clusters, feature_dim) èšç±»ä¸­å¿ƒ
        """
        # ç‰¹å¾å˜æ¢
        transformed_features = self.transform(task_features)

        # è®¡ç®—åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
        distances = torch.cdist(transformed_features, self.cluster_centers)

        # åˆ†é…èšç±»
        cluster_ids = torch.argmin(distances, dim=1)

        return cluster_ids, self.cluster_centers

    def compute_cluster_prototype(self, support_features, support_labels,
                                   num_classes, cluster_id):
        """
        è®¡ç®—è€ƒè™‘ä»»åŠ¡èšç±»çš„åŸå‹

        Args:
            support_features: (N*K, D) æ”¯æŒé›†ç‰¹å¾
            support_labels: (N*K,) æ”¯æŒé›†æ ‡ç­¾
            num_classes: N ç±»åˆ«æ•°
            cluster_id: ä»»åŠ¡æ‰€å±èšç±»

        Returns:
            enhanced_prototypes: (N, D) å¢å¼ºçš„åŸå‹
        """
        # åŸºç¡€åŸå‹
        base_prototypes = []
        for c in range(num_classes):
            mask = (support_labels == c)
            if mask.sum() > 0:
                prototype = support_features[mask].mean(dim=0)
                base_prototypes.append(prototype)
            else:
                base_prototypes.append(torch.zeros(self.feature_dim))

        base_prototypes = torch.stack(base_prototypes)

        # èšç±»åŸå‹å¢å¼º (å¯é€‰)
        cluster_center = self.cluster_centers[cluster_id]

        # èåˆåŸºç¡€åŸå‹å’Œèšç±»ä¸­å¿ƒ
        # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è®¾è®¡èåˆç­–ç•¥
        weight = 0.1  # èšç±»ä¸­å¿ƒçš„æƒé‡
        enhanced_prototypes = (
            (1 - weight) * base_prototypes +
            weight * cluster_center.unsqueeze(0)
        )

        return enhanced_prototypes
```

### ç»„ä»¶2: å°æ ·æœ¬æ•°æ®é‡‡æ ·å™¨

```python
import random
from collections import defaultdict

class FewShotSampler:
    """
    å°æ ·æœ¬ä»»åŠ¡é‡‡æ ·å™¨

    ä»æ•°æ®é›†ä¸­é‡‡æ · N-way K-shot ä»»åŠ¡
    """
    def __init__(self, dataset, n_way=5, k_shot=5, n_query=10):
        """
        Args:
            dataset: æ•°æ®é›†, å‡è®¾æ˜¯ {label: [samples]} çš„å­—å…¸
            n_way: æ¯ä¸ªä»»åŠ¡çš„ç±»åˆ«æ•°
            k_shot: æ¯ç±»çš„æ”¯æŒæ ·æœ¬æ•°
            n_query: æ¯ç±»çš„æŸ¥è¯¢æ ·æœ¬æ•°
        """
        self.dataset = dataset
        self.n_way = n_way
        self.k_shot = k_shot
        self.n_query = n_query

        # æ„å»ºç±»åˆ«åˆ°æ ·æœ¬çš„æ˜ å°„
        self.label_to_samples = self._build_label_map()

    def _build_label_map(self):
        """æ„å»ºæ ‡ç­¾åˆ°æ ·æœ¬çš„æ˜ å°„"""
        label_map = defaultdict(list)
        for idx, (image, label) in enumerate(self.dataset):
            label_map[label].append(idx)
        return label_map

    def sample_task(self):
        """
        é‡‡æ ·ä¸€ä¸ªä»»åŠ¡

        Returns:
            task: {
                'support_images': (n_way*k_shot, C, H, W),
                'support_labels': (n_way*k_shot,),
                'query_images': (n_way*n_query, C, H, W),
                'query_labels': (n_way*n_query,),
                'num_classes': n_way
            }
        """
        # éšæœºé€‰æ‹©n_wayä¸ªç±»åˆ«
        all_labels = list(self.label_to_samples.keys())
        selected_labels = random.sample(all_labels, self.n_way)

        support_images = []
        support_labels = []
        query_images = []
        query_labels = []

        for class_idx, label in enumerate(selected_labels):
            # è·å–è¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
            samples = self.label_to_samples[label]

            # éšæœºé€‰æ‹©k_shot + n_queryä¸ªæ ·æœ¬
            selected_samples = random.sample(
                samples,
                min(self.k_shot + self.n_query, len(samples))
            )

            # åˆ†å‰²ä¸ºsupportå’Œquery
            support_samples = selected_samples[:self.k_shot]
            query_samples = selected_samples[self.k_shot:self.k_shot + self.n_query]

            # æ·»åŠ åˆ°ä»»åŠ¡
            for sample_idx in support_samples:
                image, _ = self.dataset[sample_idx]
                support_images.append(image)
                support_labels.append(class_idx)

            for sample_idx in query_samples:
                image, _ = self.dataset[sample_idx]
                query_images.append(image)
                query_labels.append(class_idx)

        # è½¬æ¢ä¸ºtensor
        import torch
        task = {
            'support_images': torch.stack(support_images),
            'support_labels': torch.tensor(support_labels),
            'query_images': torch.stack(query_images),
            'query_labels': torch.tensor(query_labels),
            'num_classes': self.n_way
        }

        return task

    def sample_batch(self, batch_size):
        """é‡‡æ ·ä¸€æ‰¹ä»»åŠ¡"""
        return [self.sample_task() for _ in range(batch_size)]
```

### ç»„ä»¶3: äº•ç›–ç¼ºé™·å°æ ·æœ¬æ•°æ®é›†æ„å»º

```python
class ManholeDefectFewShotDataset:
    """
    äº•ç›–ç¼ºé™·å°æ ·æœ¬æ•°æ®é›†

    ç±»åˆ«è®¾è®¡:
    - åŸºç±» (Base Classes): æ­£å¸¸ã€è£‚çº¹ã€å˜å½¢ (æ ·æœ¬å……è¶³)
    - æ–°ç±» (Novel Classes): ç ´æŸã€ç¼ºå¤±ã€è…èš€ã€å¼‚ç‰© (æ ·æœ¬ç¨€ç¼º)
    """
    def __init__(self, data_root):
        self.data_root = data_root

        # å®šä¹‰ç±»åˆ«
        self.base_classes = ['normal', 'crack', 'deformation']
        self.novel_classes = ['damage', 'missing', 'corrosion', 'foreign_object']

        # æ ·æœ¬æ•°é‡è®¾ç½®
        self.base_samples = {
            'normal': 1000,
            'crack': 500,
            'deformation': 300
        }

        self.novel_samples = {
            'damage': 10,      # ç½•è§: ä»…10å¼ 
            'missing': 5,      # æç½•è§: ä»…5å¼ 
            'corrosion': 8,    # ç½•è§: ä»…8å¼ 
            'foreign_object': 6  # ç½•è§: ä»…6å¼ 
        }

    def get_meta_train_set(self):
        """è·å–å…ƒè®­ç»ƒé›† (ä½¿ç”¨åŸºç±»)"""
        return self._create_dataset(self.base_classes, self.base_samples)

    def get_meta_test_set(self, shot=5):
        """
        è·å–å…ƒæµ‹è¯•é›† (ä½¿ç”¨æ–°ç±»)

        Args:
            shot: K-shotè®¾ç½® (1æˆ–5)
        """
        # ä¸ºæ–°ç±»åˆ›å»ºfew-shotè®¾ç½®
        novel_samples = {k: min(v, shot) for k, v in self.novel_samples.items()}
        return self._create_dataset(self.novel_classes, novel_samples)

    def _create_dataset(self, classes, samples_dict):
        """åˆ›å»ºæ•°æ®é›†"""
        dataset = []
        for class_name in classes:
            class_dir = os.path.join(self.data_root, class_name)
            num_samples = samples_dict.get(class_name, 0)

            for i in range(num_samples):
                image_path = os.path.join(class_dir, f"{i}.jpg")
                image = self._load_image(image_path)
                label = classes.index(class_name)
                dataset.append((image, label))

        return dataset

    def _load_image(self, path):
        """åŠ è½½å›¾åƒ"""
        from PIL import Image
        import torchvision.transforms as transforms

        image = Image.open(path).convert('RGB')

        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])

        return transform(image)


# ä½¿ç”¨ç¤ºä¾‹
def create_manhole_fewshot_tasks():
    """åˆ›å»ºäº•ç›–ç¼ºé™·å°æ ·æœ¬ä»»åŠ¡"""
    dataset = ManholeDefectFewShotDataset(data_root='manhole_data')

    # å…ƒè®­ç»ƒ: ä½¿ç”¨åŸºç±» (æ­£å¸¸ã€è£‚çº¹ã€å˜å½¢)
    meta_train_set = dataset.get_meta_train_set()
    meta_train_sampler = FewShotSampler(
        meta_train_set,
        n_way=3,  # 3ä¸ªåŸºç±»
        k_shot=5,
        n_query=10
    )

    # å…ƒæµ‹è¯•: ä½¿ç”¨æ–°ç±» (ç ´æŸã€ç¼ºå¤±ã€è…èš€ã€å¼‚ç‰©)
    # 5-way 5-shot: æ¯æ¬¡ä»4ä¸ªæ–°ç±»ä¸­é€‰5ç±»(å®é™…æœ€å¤š4ç±»)
    meta_test_set = dataset.get_meta_test_set(shot=5)
    meta_test_sampler = FewShotSampler(
        meta_test_set,
        n_way=4,  # 4ä¸ªæ–°ç±»
        k_shot=5,
        n_query=10
    )

    return meta_train_sampler, meta_test_sampler
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **å°æ ·æœ¬å­¦ä¹ ** | Few-Shot Learning | ä»…ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ æ–°ç±»åˆ« |
| **å…ƒå­¦ä¹ ** | Meta-Learning | å­¦ä¹ å¦‚ä½•å­¦ä¹ ,è·¨ä»»åŠ¡è¿ç§»çŸ¥è¯† |
| **N-way K-shot** | N-way K-shot | Nä¸ªç±»åˆ«,æ¯ç±»Kä¸ªæ ·æœ¬ |
| **æ”¯æŒé›†** | Support Set | ç”¨äºæ„å»ºåŸå‹çš„å°‘é‡æ ‡æ³¨æ ·æœ¬ |
| **æŸ¥è¯¢é›†** | Query Set | ç”¨äºæµ‹è¯•/éªŒè¯çš„æ ·æœ¬ |
| **åŸå‹** | Prototype | ç±»åˆ«çš„ç‰¹å¾ä¸­å¿ƒå‘é‡ |
| **ä»»åŠ¡èšç±»** | Task Clustering | å°†ç›¸ä¼¼ä»»åŠ¡åˆ†ç»„ä»¥å…±äº«çŸ¥è¯† |

---

## ğŸ“Š äº•ç›–ç¼ºé™·å°æ ·æœ¬åˆ†ç±»å®ç°è·¯çº¿

### é˜¶æ®µ1: æ•°æ®å‡†å¤‡ (2å‘¨)

```
ä»»åŠ¡:
1. æ”¶é›†äº•ç›–ç¼ºé™·å›¾åƒ
   - æ­£å¸¸: 1000å¼ 
   - è£‚çº¹: 500å¼ 
   - å˜å½¢: 300å¼ 
   - ç ´æŸ: 10-20å¼ 
   - ç¼ºå¤±: 5-10å¼ 
   - è…èš€: 8-15å¼ 

2. æ•°æ®åˆ’åˆ†
   - åŸºç±»: æ­£å¸¸ã€è£‚çº¹ã€å˜å½¢ (ç”¨äºå…ƒè®­ç»ƒ)
   - æ–°ç±»: ç ´æŸã€ç¼ºå¤±ã€è…èš€ (ç”¨äºå…ƒæµ‹è¯•)

3. æ•°æ®å¢å¼º
   - é’ˆå¯¹ç½•è§ç¼ºé™·çš„å¢å¼ºç­–ç•¥
```

### é˜¶æ®µ2: æ¨¡å‹å®ç° (2å‘¨)

```python
# å®ç°æ­¥éª¤
1. ç‰¹å¾æå–å™¨: ä½¿ç”¨é¢„è®­ç»ƒResNet50
2. åŸå‹ç½‘ç»œ: å®ç°åŸå‹è®¡ç®—å’Œè·ç¦»åº¦é‡
3. ä»»åŠ¡èšç±»: å®ç°ä»»åŠ¡èšç±»æ¨¡å—
4. è®­ç»ƒæ¡†æ¶: å®ç°å…ƒè®­ç»ƒå’Œå…ƒæµ‹è¯•
```

### é˜¶æ®µ3: å®éªŒéªŒè¯ (2å‘¨)

```
å®éªŒè®¾ç½®:
1. 5-way 1-shot: æ¯ç±»1ä¸ªæ ·æœ¬
2. 5-way 5-shot: æ¯ç±»5ä¸ªæ ·æœ¬

è¯„ä¼°æŒ‡æ ‡:
- å‡†ç¡®ç‡
- æ¯ç±»å‡†ç¡®ç‡
- æ··æ·†çŸ©é˜µ

å¯¹æ¯”æ–¹æ³•:
- Baseline: ç›´æ¥å¾®è°ƒ
- Prototypical Networks
- + Task Clustering
```

### é¢„æœŸæ•ˆæœ

| è®¾ç½® | Baseline (%) | ProtoNet (%) | +Clustering (%) |
|:---:|:---:|:---:|:---:|
| 1-shot | 45.2 | 62.5 | 65.3 |
| 5-shot | 58.7 | 75.8 | 79.2 |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å°æ ·æœ¬å­¦ä¹ çš„N-way K-shotè®¾ç½®
- [ ] æŒæ¡åŸå‹ç½‘ç»œçš„åŸç†å’Œå®ç°
- [ ] äº†è§£ä»»åŠ¡èšç±»çš„ä½œç”¨
- [ ] ç†è§£å…ƒè®­ç»ƒå’Œå…ƒæµ‹è¯•çš„åŒºåˆ«
- [ ] èƒ½å°†æ–¹æ³•åº”ç”¨åˆ°äº•ç›–ç¼ºé™·åˆ†ç±»
- [ ] èƒ½å¤Ÿå®ç°å®Œæ•´çš„å°æ ·æœ¬å­¦ä¹ æ¡†æ¶

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **ä¸ºä»€ä¹ˆåŸå‹ç½‘ç»œåœ¨å°æ ·æœ¬åœºæ™¯ä¸‹æœ‰æ•ˆï¼Ÿ**
   - æç¤º: ç®€å•çš„è·ç¦»åº¦é‡,é¿å…è¿‡æ‹Ÿåˆ

2. **ä»»åŠ¡èšç±»å¦‚ä½•å¸®åŠ©å°æ ·æœ¬å­¦ä¹ ï¼Ÿ**
   - æç¤º: ç›¸ä¼¼ä»»åŠ¡å…±äº«çŸ¥è¯†

3. **äº•ç›–ç¼ºé™·åˆ†ç±»ä¸­,å“ªäº›ç¼ºé™·å±äºç½•è§ç±»åˆ«ï¼Ÿ**
   - æç¤º: ç ´æŸã€ç¼ºå¤±ç­‰

4. **å¦‚ä½•å¤„ç†0æ ·æœ¬çš„æ–°ç±»åˆ«ï¼Ÿ**
   - æç¤º: é›¶æ ·æœ¬å­¦ä¹ ,ä½¿ç”¨è¯­ä¹‰æè¿°

---

## ğŸ”— ç›¸å…³è®ºæ–‡æ¨è

### å¿…è¯»
1. **Prototypical Networks** (NIPS 2017) - åŸå‹ç½‘ç»œåŸºç¡€
2. **MAML** (ICML 2017) - æ¨¡å‹æ— å…³å…ƒå­¦ä¹ 
3. **Matching Networks** (NIPS 2016) - åº¦é‡å­¦ä¹ å°æ ·æœ¬

### æ‰©å±•é˜…è¯»
1. **Relation Network** (CVPR 2018) - å…³ç³»ç½‘ç»œ
2. **DN4** (CVPR 2020) - æ·±åº¦æœ€è¿‘é‚»
3. **FEAT** (NeurIPS 2020) - ä¼ é€’å¼å°æ ·æœ¬å­¦ä¹ 

---

## ğŸ“ ä¸ªäººç¬”è®°åŒº

### æˆ‘çš„ç†è§£



### ç–‘é—®ä¸å¾…æ¾„æ¸…



### ä¸äº•ç›–æ£€æµ‹çš„ç»“åˆç‚¹



### å®ç°è®¡åˆ’



---

## ğŸ¯ å¿«é€Ÿå¼€å§‹ä»£ç ç¤ºä¾‹

```python
# å®Œæ•´çš„è®­ç»ƒæµç¨‹
import torch
import torch.nn as nn
from torchvision.models import resnet50

# 1. åˆ›å»ºæ¨¡å‹
backbone = resnet50(pretrained=True)
backbone.fc = nn.Identity  # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚

model = FewShotClassifier(
    backbone=backbone,
    feature_dim=2048,
    num_clusters=5
).cuda()

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# 2. åˆ›å»ºæ•°æ®é‡‡æ ·å™¨
train_sampler, test_sampler = create_manhole_fewshot_tasks()

# 3. å…ƒè®­ç»ƒ
for epoch in range(100):
    # é‡‡æ ·ä¸€æ‰¹ä»»åŠ¡
    task_batch = train_sampler.sample_batch(batch_size=4)

    # å…ƒè®­ç»ƒ
    metrics = model.meta_train(task_batch, optimizer)

    print(f"Epoch {epoch}: Loss={metrics['loss']:.4f}, Acc={metrics['acc']:.4f}")

# 4. å…ƒæµ‹è¯•
test_task = test_sampler.sample_task()
test_metrics = model.meta_test(test_task)
print(f"Test Accuracy: {test_metrics['acc']:.4f}")
```

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ7æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
**ä¸‹ä¸€æ­¥**: å®ç°åŸå‹ç½‘ç»œ,åœ¨äº•ç›–ç¼ºé™·æ•°æ®é›†ä¸ŠéªŒè¯
