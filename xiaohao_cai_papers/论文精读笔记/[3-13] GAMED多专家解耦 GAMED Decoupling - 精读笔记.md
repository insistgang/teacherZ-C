# [3-13] GAMEDå¤šä¸“å®¶è§£è€¦ GAMED Decoupling - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: GAMED: Knowledge-Adaptive Multi-Expert Decoupling for Multimodal Learning
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (ä¸­é«˜)
> **é‡è¦æ€§**: â­â­â­ (è¡¥å……è®ºæ–‡ï¼Œå¤šä¸“å®¶æ¶æ„)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | GAMED: Knowledge-Adaptive Multi-Expert Decoupling for Multimodal Learning |
| **ä½œè€…** | X. Cai ç­‰äºº |
| **å‘è¡¨æœŸåˆŠ** | ACM International Conference on Multimedia (ACM MM) 2022 |
| **å‘è¡¨å¹´ä»½** | 2022 |
| **å…³é”®è¯** | Multi-Expert, Decoupling, Multimodal, Knowledge-Adaptive, Gating |
| **ä»£ç ** | (è¯·æŸ¥çœ‹è®ºæ–‡æ˜¯å¦æœ‰å¼€æºä»£ç ) |

---

## ğŸ¯ ç ”ç©¶é—®é¢˜ä¸åŠ¨æœº

### å¤šæ¨¡æ€å­¦ä¹ æŒ‘æˆ˜

**æ¨¡æ€é—´çš„å¼‚è´¨æ€§**:
```
ä¸åŒæ¨¡æ€çš„å†²çª:
- å›¾åƒ: ç©ºé—´ä¿¡æ¯ï¼Œè¿ç»­
- æ–‡æœ¬: åºåˆ—ä¿¡æ¯ï¼Œç¦»æ•£
- éŸ³é¢‘: æ—¶é¢‘ä¿¡æ¯ï¼Œè¿ç»­

ç®€å•èåˆçš„é—®é¢˜:
- æ—©æœŸèåˆ: ç‰¹å¾ä¸å¯¹é½
- æ™šæœŸèåˆ: äº¤äº’ä¸å……åˆ†
- æ³¨æ„åŠ›èåˆ: å¯èƒ½å¿½ç•¥é‡è¦æ¨¡æ€
```

**ä¸“å®¶æ··åˆ(MoE)çš„å±€é™**:
```
ä¼ ç»ŸMoE:
- æ‰€æœ‰ä¸“å®¶å¤„ç†æ‰€æœ‰è¾“å…¥
- ç¼ºä¹æ¨¡æ€ç‰¹åŒ–
- çŸ¥è¯†è€¦åˆä¸¥é‡

éœ€è¦çš„æ”¹è¿›:
- æ¨¡æ€ç‰¹åŒ–ä¸“å®¶
- è‡ªé€‚åº”é—¨æ§
- çŸ¥è¯†è§£è€¦
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ•´ä½“æ¡†æ¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GAMED å¤šä¸“å®¶è§£è€¦æ¡†æ¶                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  å¤šæ¨¡æ€è¾“å…¥                                              â”‚
â”‚  â”œâ”€ å›¾åƒç‰¹å¾: f_i âˆˆ R^d                                 â”‚
â”‚  â”œâ”€ æ–‡æœ¬ç‰¹å¾: f_t âˆˆ R^d                                 â”‚
â”‚  â””â”€ ç¤¾äº¤ç‰¹å¾: f_s âˆˆ R^d                                 â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           æ¨¡æ€ç‰¹åŒ–ä¸“å®¶ç½‘ç»œ                        â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   è§†è§‰ä¸“å®¶ E_v: å¤„ç†å›¾åƒç‰¹å¾ â†’ h_v               â”‚   â”‚
â”‚  â”‚   æ–‡æœ¬ä¸“å®¶ E_t: å¤„ç†æ–‡æœ¬ç‰¹å¾ â†’ h_t               â”‚   â”‚
â”‚  â”‚   ç¤¾äº¤ä¸“å®¶ E_s: å¤„ç†ç¤¾äº¤ç‰¹å¾ â†’ h_s               â”‚   â”‚
â”‚  â”‚   è·¨æ¨¡æ€ä¸“å®¶ E_c: å¤„ç†èåˆç‰¹å¾ â†’ h_c             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           çŸ¥è¯†é€‚åº”é—¨æ§ç½‘ç»œ                        â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   G(f_i, f_t, f_s) â†’ [w_v, w_t, w_s, w_c]      â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   æ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€é€‰æ‹©ä¸“å®¶                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           ä¸“å®¶è¾“å‡ºèšåˆ                            â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   h_final = w_vÂ·h_v + w_tÂ·h_t + w_sÂ·h_s + w_cÂ·h_câ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           è§£è€¦å­¦ä¹ çº¦æŸ                            â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   - ä¸“å®¶é—´æ­£äº¤æ€§çº¦æŸ                             â”‚   â”‚
â”‚  â”‚   - çŸ¥è¯†è’¸é¦é˜²æ­¢å´©æºƒ                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                                     â”‚
â”‚                 åˆ†ç±»è¾“å‡º                                 â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ ¸å¿ƒæ–¹æ³•1: æ¨¡æ€ç‰¹åŒ–ä¸“å®¶

```python
class ModalitySpecificExpert(nn.Module):
    """
    æ¨¡æ€ç‰¹åŒ–ä¸“å®¶ç½‘ç»œ

    æ¯ä¸ªä¸“å®¶ä¸“é—¨å¤„ç†ç‰¹å®šæ¨¡æ€çš„ç‰¹å¾
    """
    def __init__(self, input_dim: int, hidden_dim: int = 256, num_layers: int = 3):
        super().__init__()

        layers = []
        for i in range(num_layers):
            in_dim = input_dim if i == 0 else hidden_dim
            out_dim = hidden_dim

            layers.extend([
                nn.Linear(in_dim, out_dim),
                nn.LayerNorm(out_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])

        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, input_dim) æ¨¡æ€ç‰¹å¾

        Returns:
            h: (B, hidden_dim) ä¸“å®¶è¾“å‡º
        """
        return self.network(x)


class MultiExpertNetwork(nn.Module):
    """
    å¤šä¸“å®¶ç½‘ç»œ

    åŒ…å«å¤šä¸ªæ¨¡æ€ç‰¹åŒ–ä¸“å®¶
    """
    def __init__(
        self,
        feature_dim: int,
        hidden_dim: int = 256,
        num_experts: int = 4
    ):
        super().__init__()

        # åˆ›å»ºä¸“å®¶
        self.experts = nn.ModuleList([
            ModalitySpecificExpert(feature_dim, hidden_dim)
            for _ in range(num_experts)
        ])

        # ä¸“å®¶ç±»å‹æ ‡è¯†
        self.expert_types = ['visual', 'text', 'social', 'cross_modal']

    def forward(self, features: dict) -> dict:
        """
        Args:
            features: {modality: feature_tensor}

        Returns:
            expert_outputs: {expert_id: output_tensor}
        """
        outputs = {}

        # è§†è§‰ä¸“å®¶å¤„ç†å›¾åƒ
        if 'image' in features:
            outputs['visual'] = self.experts[0](features['image'])

        # æ–‡æœ¬ä¸“å®¶å¤„ç†æ–‡æœ¬
        if 'text' in features:
            outputs['text'] = self.experts[1](features['text'])

        # ç¤¾äº¤ä¸“å®¶å¤„ç†ç¤¾äº¤ç‰¹å¾
        if 'social' in features:
            outputs['social'] = self.experts[2](features['social'])

        # è·¨æ¨¡æ€ä¸“å®¶å¤„ç†èåˆç‰¹å¾
        if len(features) > 1:
            fused = torch.cat(list(features.values()), dim=-1)
            # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
            fused = F.linear(fused, torch.eye(fused.size(-1))[:, :256])
            outputs['cross_modal'] = self.experts[3](fused)

        return outputs
```

---

### æ ¸å¿ƒæ–¹æ³•2: çŸ¥è¯†é€‚åº”é—¨æ§

```python
class KnowledgeAdaptiveGating(nn.Module):
    """
    çŸ¥è¯†é€‚åº”é—¨æ§ç½‘ç»œ

    æ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€é€‰æ‹©ä¸“å®¶
    """
    def __init__(
        self,
        feature_dim: int,
        num_experts: int = 4,
        temperature: float = 1.0
    ):
        super().__init__()
        self.num_experts = num_experts
        self.temperature = temperature

        # é—¨æ§ç½‘ç»œ
        self.gate = nn.Sequential(
            nn.Linear(feature_dim * 3, 512),  # å‡è®¾æœ€å¤š3ä¸ªæ¨¡æ€
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, num_experts)
        )

        # çŸ¥è¯†åµŒå…¥ (å¯å­¦ä¹ )
        self.knowledge_embeddings = nn.Parameter(
            torch.randn(num_experts, 128)
        )

        # ä¸Šä¸‹æ–‡ç¼–ç 
        self.context_encoder = nn.GRU(
            input_size=feature_dim,
            hidden_size=128,
            batch_first=True
        )

    def forward(self, features: dict) -> torch.Tensor:
        """
        Args:
            features: {modality: feature_tensor}

        Returns:
            weights: (B, num_experts) ä¸“å®¶æƒé‡
        """
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        feature_list = list(features.values())
        combined = torch.cat(feature_list, dim=-1)

        # å¡«å……åˆ°å›ºå®šç»´åº¦
        if combined.size(-1) < 3 * 256:
            padding = torch.zeros(combined.size(0), 3 * 256 - combined.size(-1))
            combined = torch.cat([combined, padding], dim=-1)

        # åŸºç¡€é—¨æ§åˆ†æ•°
        base_logits = self.gate(combined)  # (B, num_experts)

        # çŸ¥è¯†é€‚åº”
        # è®¡ç®—è¾“å…¥ä¸çŸ¥è¯†åµŒå…¥çš„åŒ¹é…åº¦
        context = self._encode_context(features)  # (B, 128)
        knowledge_match = torch.matmul(
            context,
            self.knowledge_embeddings.T
        )  # (B, num_experts)

        # èåˆåŸºç¡€åˆ†æ•°å’ŒçŸ¥è¯†åŒ¹é…
        final_logits = base_logits + 0.5 * knowledge_match

        # Softmaxå½’ä¸€åŒ–
        weights = F.softmax(final_logits / self.temperature, dim=-1)

        return weights

    def _encode_context(self, features: dict) -> torch.Tensor:
        """ç¼–ç ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        # ç®€å•å®ç°: å¹³å‡æ‰€æœ‰ç‰¹å¾
        stacked = torch.stack(list(features.values()), dim=1)  # (B, num_mod, D)

        # ä½¿ç”¨GRUç¼–ç 
        output, hidden = self.context_encoder(stacked)
        context = hidden.squeeze(0)  # (B, 128)

        return context
```

---

### æ ¸å¿ƒæ–¹æ³•3: è§£è€¦å­¦ä¹ çº¦æŸ

```python
class DecouplingConstraints:
    """
    è§£è€¦å­¦ä¹ çº¦æŸ

    ç¡®ä¿ä¸“å®¶å­¦ä¹ ä¸åŒçš„çŸ¥è¯†
    """
    def __init__(self, num_experts: int = 4):
        self.num_experts = num_experts

    def orthogonal_constraint(self, expert_outputs: dict) -> torch.Tensor:
        """
        ä¸“å®¶è¾“å‡ºæ­£äº¤æ€§çº¦æŸ

        é¼“åŠ±ä¸“å®¶å­¦ä¹ ä¸åŒçš„ç‰¹å¾è¡¨ç¤º
        """
        outputs = list(expert_outputs.values())
        num_experts = len(outputs)

        # è®¡ç®—ä¸“å®¶è¾“å‡ºä¹‹é—´çš„ç›¸å…³æ€§
        correlation_loss = 0
        for i in range(num_experts):
            for j in range(i + 1, num_experts):
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = F.cosine_similarity(
                    outputs[i],
                    outputs[j],
                    dim=-1
                ).mean()

                # é¼“åŠ±ç›¸ä¼¼åº¦æ¥è¿‘0 (æ­£äº¤)
                correlation_loss += similarity ** 2

        return correlation_loss / (num_experts * (num_experts - 1) / 2)

    def diversity_constraint(self, expert_weights: torch.Tensor) -> torch.Tensor:
        """
        ä¸“å®¶ä½¿ç”¨å¤šæ ·æ€§çº¦æŸ

        é¼“åŠ±ä½¿ç”¨æ‰€æœ‰ä¸“å®¶ï¼Œé¿å…æŸäº›ä¸“å®¶è¢«å¿½ç•¥
        """
        # è®¡ç®—æƒé‡çš„ç†µ
        entropy = -torch.sum(
            expert_weights * torch.log(expert_weights + 1e-8),
            dim=-1
        ).mean()

        # æœ€å¤§åŒ–ç†µ (é¼“åŠ±å¤šæ ·æ€§)
        return -entropy  # ä½œä¸ºæŸå¤±ï¼Œéœ€è¦æœ€å°åŒ–

    def knowledge_distillation_loss(
        self,
        student_outputs: dict,
        teacher_output: torch.Tensor,
        temperature: float = 4.0
    ) -> torch.Tensor:
        """
        çŸ¥è¯†è’¸é¦æŸå¤±

        é˜²æ­¢ä¸“å®¶å´©æºƒï¼Œä¿æŒæ•´ä½“æ€§èƒ½
        """
        # æ•™å¸ˆæ¨¡å‹è¾“å‡º (æ‰€æœ‰ä¸“å®¶çš„åŠ æƒå¹³å‡)
        teacher_probs = F.softmax(teacher_output / temperature, dim=-1)

        # æ¯ä¸ªä¸“å®¶çš„è’¸é¦æŸå¤±
        kd_loss = 0
        for expert_name, expert_out in student_outputs.items():
            student_probs = F.log_softmax(expert_out / temperature, dim=-1)
            kd_loss += F.kl_div(
                student_probs,
                teacher_probs,
                reduction='batchmean'
            )

        return kd_loss / len(student_outputs)

    def compute_total_loss(
        self,
        expert_outputs: dict,
        expert_weights: torch.Tensor,
        final_output: torch.Tensor,
        labels: torch.Tensor
    ) -> dict:
        """
        è®¡ç®—æ€»æŸå¤±

        Returns:
            losses: åŒ…å«å„æŸå¤±åˆ†é‡çš„å­—å…¸
        """
        losses = {}

        # åˆ†ç±»æŸå¤±
        losses['classification'] = F.cross_entropy(final_output, labels)

        # æ­£äº¤æ€§çº¦æŸ
        losses['orthogonal'] = self.orthogonal_constraint(expert_outputs)

        # å¤šæ ·æ€§çº¦æŸ
        losses['diversity'] = self.diversity_constraint(expert_weights)

        # çŸ¥è¯†è’¸é¦
        losses['distillation'] = self.knowledge_distillation_loss(
            expert_outputs,
            final_output
        )

        # æ€»æŸå¤±
        losses['total'] = (
            losses['classification'] +
            0.1 * losses['orthogonal'] +
            0.1 * losses['diversity'] +
            0.05 * losses['distillation']
        )

        return losses
```

---

## ğŸ“Š å®éªŒç»“æœ

### è™šå‡æ–°é—»æ£€æµ‹æ€§èƒ½

| æ–¹æ³• | å‡†ç¡®ç‡ | F1åˆ†æ•° | AUC |
|:---|:---:|:---:|:---:|
| å•æ¨¡æ€ (å›¾åƒ) | 68.5% | 0.672 | 0.741 |
| å•æ¨¡æ€ (æ–‡æœ¬) | 74.2% | 0.738 | 0.805 |
| æ—©æœŸèåˆ | 76.8% | 0.762 | 0.832 |
| æ™šæœŸèåˆ | 78.1% | 0.775 | 0.845 |
| æ³¨æ„åŠ›èåˆ | 80.3% | 0.798 | 0.867 |
| **GAMED** | **83.5%** | **0.831** | **0.891** |

### æ¶ˆèå®éªŒ

| ç»„ä»¶ | å‡†ç¡®ç‡ | æå‡ |
|:---|:---:|:---:|
| åŸºçº¿ (æ— ä¸“å®¶) | 78.1% | - |
| + æ¨¡æ€ç‰¹åŒ–ä¸“å®¶ | 80.5% | +2.4% |
| + çŸ¥è¯†é€‚åº”é—¨æ§ | 82.1% | +1.6% |
| + è§£è€¦çº¦æŸ | 83.5% | +1.4% |

---

## ğŸ’¡ å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å®Œæ•´GAMEDæ¨¡å‹

```python
class GAMED(nn.Module):
    """
    å®Œæ•´çš„GAMEDæ¨¡å‹

    çŸ¥è¯†é€‚åº”å¤šä¸“å®¶è§£è€¦ç½‘ç»œ
    """
    def __init__(
        self,
        feature_dims: dict,  # {modality: dim}
        hidden_dim: int = 256,
        num_classes: int = 2,
        num_experts: int = 4
    ):
        super().__init__()

        # ç‰¹å¾æŠ•å½±
        self.feature_projectors = nn.ModuleDict({
            modality: nn.Linear(dim, hidden_dim)
            for modality, dim in feature_dims.items()
        })

        # å¤šä¸“å®¶ç½‘ç»œ
        self.experts = MultiExpertNetwork(
            hidden_dim,
            hidden_dim,
            num_experts
        )

        # é—¨æ§ç½‘ç»œ
        self.gating = KnowledgeAdaptiveGating(
            hidden_dim,
            num_experts
        )

        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(hidden_dim, num_classes)

        # è§£è€¦çº¦æŸ
        self.decoupling = DecouplingConstraints(num_experts)

    def forward(self, inputs: dict, labels=None) -> dict:
        """
        Args:
            inputs: {modality: raw_features}
            labels: æ ‡ç­¾ (ç”¨äºè®­ç»ƒ)

        Returns:
            outputs: åŒ…å«é¢„æµ‹å’ŒæŸå¤±çš„å­—å…¸
        """
        # ç‰¹å¾æŠ•å½±
        projected = {
            modality: proj(inputs[modality])
            for modality, proj in self.feature_projectors.items()
        }

        # ä¸“å®¶è¾“å‡º
        expert_outputs = self.experts(projected)

        # é—¨æ§æƒé‡
        expert_weights = self.gating(projected)

        # åŠ æƒèšåˆ
        expert_tensors = torch.stack(list(expert_outputs.values()), dim=1)  # (B, K, D)
        aggregated = torch.einsum('bkd,bk->bd', expert_tensors, expert_weights)

        # åˆ†ç±»
        logits = self.classifier(aggregated)

        outputs = {
            'logits': logits,
            'predictions': logits.argmax(dim=-1),
            'expert_weights': expert_weights,
            'expert_outputs': expert_outputs
        }

        # è®¡ç®—æŸå¤±
        if labels is not None:
            losses = self.decoupling.compute_total_loss(
                expert_outputs,
                expert_weights,
                logits,
                labels
            )
            outputs['losses'] = losses

        return outputs
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **MoE** | Mixture of Experts | ä¸“å®¶æ··åˆ |
| **é—¨æ§ç½‘ç»œ** | Gating Network | é€‰æ‹©ä¸“å®¶çš„æœºåˆ¶ |
| **çŸ¥è¯†é€‚åº”** | Knowledge-Adaptive | æ ¹æ®çŸ¥è¯†åŠ¨æ€è°ƒæ•´ |
| **è§£è€¦** | Decoupling | åˆ†ç¦»ä¸åŒçŸ¥è¯† |
| **æ­£äº¤çº¦æŸ** | Orthogonal Constraint | é¼“åŠ±ä¸“å®¶å¤šæ ·æ€§ |
| **æ¨¡æ€ç‰¹åŒ–** | Modality-Specific | é’ˆå¯¹ç‰¹å®šæ¨¡æ€ä¼˜åŒ– |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å¤šä¸“å®¶æ¶æ„çš„ä¼˜åŠ¿
- [ ] æŒæ¡æ¨¡æ€ç‰¹åŒ–ä¸“å®¶è®¾è®¡
- [ ] ç†è§£çŸ¥è¯†é€‚åº”é—¨æ§æœºåˆ¶
- [ ] äº†è§£è§£è€¦å­¦ä¹ çº¦æŸ
- [ ] èƒ½å¤Ÿå®ç°åŸºæœ¬çš„å¤šä¸“å®¶ç½‘ç»œ

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **ä¸ºä»€ä¹ˆéœ€è¦æ¨¡æ€ç‰¹åŒ–ä¸“å®¶ï¼Ÿ**
   - æç¤º: æ¨¡æ€å¼‚è´¨æ€§

2. **é—¨æ§ç½‘ç»œå¦‚ä½•é¿å…æ€»æ˜¯é€‰æ‹©åŒä¸€ä¸“å®¶ï¼Ÿ**
   - æç¤º: å¤šæ ·æ€§çº¦æŸ

3. **è§£è€¦çº¦æŸå¦‚ä½•å¸®åŠ©æ¨¡å‹æ€§èƒ½ï¼Ÿ**
   - æç¤º: ä¸“å®¶äº’è¡¥æ€§

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
