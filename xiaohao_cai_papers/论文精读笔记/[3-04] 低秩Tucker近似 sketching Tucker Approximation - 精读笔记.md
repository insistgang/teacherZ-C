# [3-04] ä½Žç§©Tuckerè¿‘ä¼¼ sketching Tucker Approximation - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Sketching for Large-Scale Tucker Approximation
> **ä½œè€…**: Xiaohao Cai, et al.
> **å‡ºå¤„**: SIAM Journal on Mathematics of Data Science (SIMODS)
> **å¹´ä»½**: 2023
> **ç±»åž‹**: ç®—æ³•åˆ›æ–°è®ºæ–‡
> **ç²¾è¯»æ—¥æœŸ**: 2026å¹´2æœˆ9æ—¥

---

## ðŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

### å…ƒæ•°æ®
| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **ç±»åž‹** | ç®—æ³•åˆ›æ–° (Algorithm Innovation) |
| **é¢†åŸŸ** | å¼ é‡åˆ†è§£ + éšæœºç®—æ³• |
| **èŒƒå›´** | å¤§è§„æ¨¡å¼ é‡è¿‘ä¼¼ |
| **é‡è¦æ€§** | â˜…â˜…â˜…â˜…â˜… (å¼ é‡åˆ†è§£é‡è¦è¿›å±•) |
| **ç‰¹ç‚¹** | éšæœºæŠ•å½±ã€ä½Žç§©è¿‘ä¼¼ã€è®¡ç®—é«˜æ•ˆ |

### å…³é”®è¯
- **Tucker Decomposition** - Tuckeråˆ†è§£
- **Sketching** - éšæœºæŠ•å½±/ç´ æ
- **Low-Rank Approximation** - ä½Žç§©è¿‘ä¼¼
- **Large-Scale Tensor** - å¤§è§„æ¨¡å¼ é‡
- **HOSVD** - é«˜é˜¶SVD
- **Tensor Train** - å¼ é‡è®­ç»ƒ

---

## ðŸŽ¯ ç ”ç©¶èƒŒæ™¯ä¸Žæ„ä¹‰

### 1.1 è®ºæ–‡å®šä½

**è¿™æ˜¯ä»€ä¹ˆï¼Ÿ**
- ä¸€ç¯‡å…³äºŽ**å¤§è§„æ¨¡å¼ é‡Tuckeråˆ†è§£**çš„ç®—æ³•è®ºæ–‡
- æå‡º**éšæœºæŠ•å½±(Sketching)**æŠ€æœ¯åŠ é€Ÿå¼ é‡åˆ†è§£
- è§£å†³é«˜é˜¶å¼ é‡è®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
```
å¤§è§„æ¨¡å¼ é‡åˆ†è§£æŒ‘æˆ˜:
â”œâ”€â”€ ç»´æ•°çˆ†ç‚¸ (n^då¤æ‚åº¦)
â”œâ”€â”€ å†…å­˜æ¶ˆè€—å·¨å¤§
â”œâ”€â”€ è®¡ç®—æ—¶é—´è¿‡é•¿
â””â”€â”€ ä¼ ç»ŸHOSVDéš¾ä»¥å¤„ç†

Sketchingæ–¹æ³•è´¡çŒ®:
â”œâ”€â”€ éšæœºé™ç»´
â”œâ”€â”€ è¯¯å·®å¯æŽ§
â”œâ”€â”€ è®¡ç®—åŠ é€Ÿæ˜¾è‘—
â””â”€â”€ å†…å­˜éœ€æ±‚é™ä½Ž
```

### 1.2 Tuckeråˆ†è§£å›žé¡¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Tuckeråˆ†è§£æ¦‚è¿°                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ç»™å®šdé˜¶å¼ é‡ ð“§ âˆˆ â„^{nâ‚Ã—nâ‚‚Ã—...Ã—n_d}                      â”‚
â”‚                                                         â”‚
â”‚  Tuckeråˆ†è§£:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ ð“§ â‰ˆ ð“– Ã—â‚ UÂ¹ Ã—â‚‚ UÂ² Ã—â‚ƒ ... Ã—_d Uáµˆ         â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                         â”‚
â”‚  å…¶ä¸­:                                                  â”‚
â”‚  â”œâ”€â”€ ð“–: æ ¸å¿ƒå¼ é‡ (core tensor)                          â”‚
â”‚  â”‚    å°ºå¯¸: râ‚Ã—râ‚‚Ã—...Ã—r_d, r_k â‰¤ n_k                    â”‚
â”‚  â”œâ”€â”€ Uáµ: ç¬¬kç»´çš„å› å­çŸ©é˜µ (n_k Ã— r_k)                    â”‚
â”‚  â””â”€â”€ Ã—â‚–: æ¨¡-kå¼ é‡-çŸ©é˜µä¹˜ç§¯                              â”‚
â”‚                                                         â”‚
â”‚  åŽ‹ç¼©æ¯”: (Î  n_k) / (Î  r_k)                              â”‚
â”‚                                                         â”‚
â”‚  åº”ç”¨:                                                  â”‚
â”‚  â”œâ”€â”€ æ•°æ®åŽ‹ç¼©                                           â”‚
â”‚  â”œâ”€â”€ ç‰¹å¾æå–                                           â”‚
â”‚  â”œâ”€â”€ åŽ»å™ª                                               â”‚
â”‚  â””â”€â”€ å¼ é‡è¡¥å…¨                                           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”¬ æ–¹æ³•è®ºæ¡†æž¶

### 2.1 æ ¸å¿ƒæ€æƒ³

#### ä¼ ç»ŸHOSVDçš„é—®é¢˜

```
HOSVD (Higher-Order SVD):
  Step 1: å¯¹æ¯ä¸ªæ¨¡kå±•å¼€
  Step 2: è®¡ç®—SVD
  Step 3: ä¿ç•™å‰r_kä¸ªå·¦å¥‡å¼‚å‘é‡

å¤æ‚åº¦åˆ†æž:
â”œâ”€â”€ æ¨¡å±•å¼€: O(n^d) æ¯æ¬¡å±•å¼€
â”œâ”€â”€ SVDè®¡ç®—: O(n^(2d-1))
â”œâ”€â”€ æ€»å¤æ‚åº¦: O(d Ã— n^(2d-1))
â””â”€â”€ d=3æ—¶: O(n^5), d=10æ—¶: O(n^19)

é—®é¢˜:
âœ— å¯¹å¤§å¼ é‡ä¸å¯è¡Œ
âœ— å†…å­˜æ¶ˆè€—å¤§
âœ— è®¡ç®—æ—¶é—´é•¿
```

#### Sketchingè§£å†³æ–¹æ¡ˆ

```
Sketchingæ€æƒ³:
  "æŠ•å½±åˆ°ä½Žç»´å­ç©ºé—´ï¼Œè®¡ç®—åŽå†æ¢å¤"

ç®—æ³•æµç¨‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  è¾“å…¥: dé˜¶å¼ é‡ ð“§ âˆˆ â„^{nâ‚Ã—...Ã—n_d}                      â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Step 1: éšæœºæŠ•å½± (Sketching)                â”‚       â”‚
â”‚  â”‚  å¯¹æ¯ä¸ªæ¨¡k:                                   â”‚       â”‚
â”‚  â”‚    S_k = P_k Ã— unfold_k(ð“§)                   â”‚       â”‚
â”‚  â”‚  å…¶ä¸­ P_k âˆˆ â„^{sÃ—n_k}, s â‰ª n_k              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                        â”‚                              â”‚
â”‚                        â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Step 2: åœ¨sketchä¸Šè®¡ç®—HOSVD                 â”‚       â”‚
â”‚  â”‚  é—®é¢˜è§„æ¨¡: s Ã— n_2 Ã— ... Ã— n_d              â”‚       â”‚
â”‚  â”‚  å¤æ‚åº¦æ˜¾è‘—é™ä½Ž!                              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                        â”‚                              â”‚
â”‚                        â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Step 3: åæŠ•å½±æ¢å¤                           â”‚       â”‚
â”‚  â”‚  ä½¿ç”¨è¿­ä»£refinementæé«˜ç²¾åº¦                   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                         â”‚
â”‚  è¾“å‡º: Tuckeråˆ†è§£ {ð“–, UÂ¹, ..., Uáµˆ}                      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 éšæœºæŠ•å½±çŸ©é˜µè®¾è®¡

```python
class SketchingMatrix:
    """
    éšæœºæŠ•å½±çŸ©é˜µç”Ÿæˆå™¨
    """

    @staticmethod
    def gaussian_projection(n, s):
        """
        é«˜æ–¯æŠ•å½±çŸ©é˜µ

        P âˆˆ â„^{sÃ—n}, æ¯ä¸ªå…ƒç´  ~ N(0, 1/s)

        æ€§è´¨:
        - Johnson-Lindenstrausså¼•ç†ä¿è¯
        - è¯¯å·®ä»¥é«˜æ¦‚çŽ‡æœ‰ç•Œ
        """
        P = np.random.randn(s, n) / np.sqrt(s)
        return P

    @staticmethod
    def sparse_projection(n, s, sparsity=0.1):
        """
        ç¨€ç–æŠ•å½±çŸ©é˜µ

        æ¯åˆ—åªæœ‰sparsityæ¯”ä¾‹çš„éžé›¶å…ƒç´ 

        ä¼˜åŠ¿:
        - è®¡ç®—æ›´å¿«
        - å­˜å‚¨æ›´å°‘
        """
        P = np.zeros((s, n))
        nnz = int(s * sparsity)

        for col in range(n):
            indices = np.random.choice(s, nnz, replace=False)
            values = np.random.randn(nnz) / np.sqrt(nnz)
            P[indices, col] = values

        return P

    @staticmethod
    def count_sketch(n, s):
        """
        Count SketchçŸ©é˜µ

        æ¯åˆ—ä¸€ä¸ªéžé›¶å…ƒç´ ï¼Œå€¼ä¸ºÂ±1

        ä¼˜åŠ¿:
        - æžå…¶ç¨€ç–
        - éžå¸¸å¿«é€Ÿ
        """
        P = np.zeros((s, n))

        for col in range(n):
            row = np.random.randint(s)
            sign = np.random.choice([-1, 1])
            P[row, col] = sign

        return P
```

### 2.3 Sketching Tuckerç®—æ³•

```python
class SketchingTucker:
    """
    åŸºäºŽSketchingçš„Tuckeråˆ†è§£
    """

    def __init__(
        self,
        ranks,
        sketch_sizes=None,
        sketch_type='gaussian',
        n_iter=5
    ):
        """
        å‚æ•°:
            ranks: å„ç§© r = (râ‚, râ‚‚, ..., r_d)
            sketch_sizes: sketchå°ºå¯¸ s = (sâ‚, ..., s_d)
            sketch_type: æŠ•å½±çŸ©é˜µç±»åž‹
            n_iter: è¿­ä»£ç²¾åŒ–æ¬¡æ•°
        """
        self.ranks = ranks
        self.d = len(ranks)
        self.sketch_type = sketch_type
        self.n_iter = n_iter

        if sketch_sizes is None:
            # é»˜è®¤: sketch_size = 2 Ã— rank
            self.sketch_sizes = [2 * r for r in ranks]
        else:
            self.sketch_sizes = sketch_sizes

    def decompose(self, tensor):
        """
        æ‰§è¡ŒSketching Tuckeråˆ†è§£

        å‚æ•°:
            tensor: è¾“å…¥å¼ é‡ (nâ‚Ã—nâ‚‚Ã—...Ã—n_d)

        è¿”å›ž:
            core: æ ¸å¿ƒå¼ é‡
            factors: å› å­çŸ©é˜µåˆ—è¡¨ [UÂ¹, UÂ², ..., Uáµˆ]
        """
        import numpy as np

        # èŽ·å–å¼ é‡å½¢çŠ¶
        shape = tensor.shape
        assert len(shape) == self.d

        factors = []

        # Stage 1: Sketching HOSVD
        for mode in range(self.d):
            # æ¨¡å±•å¼€
            unfolded = self._unfold(tensor, mode)

            # éšæœºæŠ•å½±
            n = shape[mode]
            s = self.sketch_sizes[mode]
            P = self._get_sketch_matrix(n, s)
            sketched = P @ unfolded.T  # (s Ã— n_other)

            # åœ¨sketchä¸Šè®¡ç®—SVD
            # ç”±äºŽsè¾ƒå°, è¿™ä¸ªSVDå¾ˆå¿«
            U_sketch, _, _ = np.linalg.svd(sketched.T, full_matrices=False)

            # å–å‰r_kä¸ªå·¦å¥‡å¼‚å‘é‡
            r = self.ranks[mode]
            U_k = U_sketch[:, :r]

            factors.append(U_k)

        # Stage 2: è®¡ç®—æ ¸å¿ƒå¼ é‡
        core = tensor.copy()
        for mode, U in enumerate(factors):
            core = self._mode_product(core, U, mode)

        # Stage 3: è¿­ä»£ç²¾åŒ– (å¯é€‰)
        if self.n_iter > 0:
            core, factors = self._refine(tensor, core, factors)

        return core, factors

    def _unfold(self, tensor, mode):
        """
        æ¨¡å±•å¼€ (Mode-n unfolding)
        """
        shape = tensor.shape
        n_mode = shape[mode]

        # è®¡ç®—å±•å¼€åŽçš„å½¢çŠ¶
        other_dims = [d for i, d in enumerate(shape) if i != mode]
        n_other = np.prod(other_dims)

        # æŽ’åˆ—è½´é¡ºåº
        new_order = [mode] + [i for i in range(self.d) if i != mode]
        transposed = np.transpose(tensor, new_order)

        # å±•å¹³é™¤modeå¤–çš„æ‰€æœ‰ç»´åº¦
        unfolded = transposed.reshape(n_mode, n_other)

        return unfolded

    def _mode_product(self, tensor, U, mode):
        """
        æ¨¡-kå¼ é‡-çŸ©é˜µä¹˜ç§¯: tensor Ã—_k U
        """
        shape = list(tensor.shape)
        n_k = shape[mode]
        r = U.shape[1]

        # æ¨¡å±•å¼€
        unfolded = self._unfold(tensor, mode)

        # çŸ©é˜µä¹˜æ³•
        result = U.T @ unfolded

        # é‡å¡‘å›žå¼ é‡
        new_shape = shape.copy()
        new_shape[mode] = r
        result = result.reshape(new_shape)

        return result

    def _get_sketch_matrix(self, n, s):
        """
        èŽ·å–éšæœºæŠ•å½±çŸ©é˜µ
        """
        if self.sketch_type == 'gaussian':
            return SketchingMatrix.gaussian_projection(n, s)
        elif self.sketch_type == 'sparse':
            return SketchingMatrix.sparse_projection(n, s)
        elif self.sketch_type == 'count_sketch':
            return SketchingMatrix.count_sketch(n, s)
        else:
            raise ValueError(f"Unknown sketch type: {self.sketch_type}")

    def _refine(self, tensor, core, factors):
        """
        è¿­ä»£ç²¾åŒ–åˆ†è§£ç»“æžœ

        ä½¿ç”¨äº¤æ›¿æœ€å°äºŒä¹˜
        """
        for iteration in range(self.n_iter):
            for mode in range(self.d):
                # å›ºå®šå…¶ä»–å› å­, æ›´æ–°å½“å‰å› å­

                # æž„å»ºæœ€å°äºŒä¹˜é—®é¢˜
                # min ||tensor - core Ã—_ factors||Â²

                # ç®€åŒ–: ä½¿ç”¨HOSVDæ›´æ–°
                unfolded = self._unfold(tensor, mode)

                # ä½¿ç”¨å½“å‰æ ¸å¿ƒå’Œå…¶ä»–å› å­æž„å»ºç›®æ ‡
                reconstructed_mode = self._reconstruct_mode(core, factors, mode)
                unfolded_rec = self._unfold(reconstructed_mode, mode)

                # æœ€å°äºŒä¹˜æ±‚è§£
                U, _, _ = np.linalg.svd(unfolded @ unfolded_rec.T, full_matrices=False)

                r = self.ranks[mode]
                factors[mode] = U[:, :r]

            # æ›´æ–°æ ¸å¿ƒ
            core = tensor.copy()
            for mode, U in enumerate(factors):
                core = self._mode_product(core, U, mode)

        return core, factors

    def _reconstruct_mode(self, core, factors, skip_mode):
        """
        é‡æž„å¼ é‡, è·³è¿‡æŒ‡å®šæ¨¡
        """
        result = core.copy()

        for mode, U in enumerate(factors):
            if mode != skip_mode:
                result = self._mode_product(result, U, mode)

        return result

    def reconstruct(self, core, factors):
        """
        ä»Žåˆ†è§£é‡æž„å¼ é‡
        """
        result = core.copy()

        for mode, U in enumerate(factors):
            result = self._mode_product(result, U, mode)

        return result

    def compression_ratio(self, original_shape):
        """
        è®¡ç®—åŽ‹ç¼©æ¯”
        """
        original_size = np.prod(original_shape)

        core_size = np.prod(self.ranks)
        factors_size = sum(original_shape[i] * self.ranks[i]
                         for i in range(self.d))

        compressed_size = core_size + factors_size

        return original_size / compressed_size
```

---

## ðŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°ä¸€: åŒå±‚Sketchingç­–ç•¥

```python
class TwoLevelSketchingTucker(SketchingTucker):
    """
    åŒå±‚Sketching Tuckeråˆ†è§£

    ç¬¬ä¸€å±‚: ç²—ç•¥ä¼°è®¡å„å› å­çŸ©é˜µ
    ç¬¬äºŒå±‚: ç²¾ç»†ä¼°è®¡
    """

    def __init__(self, ranks, coarse_sketch_sizes, fine_sketch_sizes):
        """
        å‚æ•°:
            ranks: ç›®æ ‡ç§©
            coarse_sketch_sizes: ç²—ç•¥sketchå°ºå¯¸ (è¾ƒå¤§)
            fine_sketch_sizes: ç²¾ç»†sketchå°ºå¯¸ (è¾ƒå°)
        """
        super().__init__(ranks, sketch_sizes=fine_sketch_sizes)
        self.coarse_sketch_sizes = coarse_sketch_sizes
        self.fine_sketch_sizes = fine_sketch_sizes

    def decompose(self, tensor):
        """
        åŒå±‚åˆ†è§£
        """
        import numpy as np

        factors = []

        # Stage 1: ç²—ç•¥ä¼°è®¡
        for mode in range(self.d):
            unfolded = self._unfold(tensor, mode)
            n = unfolded.shape[0]
            s = self.coarse_sketch_sizes[mode]

            P = SketchingMatrix.gaussian_projection(n, s)
            sketched = P @ unfolded.T

            U_sketch, _, _ = np.linalg.svd(sketched.T, full_matrices=False)
            r = self.ranks[mode]
            U_coarse = U_sketch[:, :r]

            factors.append(U_coarse)

        # Stage 2: ä½¿ç”¨ç²—ç•¥ä¼°è®¡ä½œä¸ºåˆå§‹åŒ–, ç²¾ç»†ä¼°è®¡
        for mode in range(self.d):
            # ä½¿ç”¨å½“å‰å› å­çŸ©é˜µçš„åˆ—ç©ºé—´æž„å»ºæŠ•å½±
            U_init = factors[mode]

            # åœ¨U_initçš„åˆ—ç©ºé—´é™„è¿‘ç²¾ç»†æœç´¢
            # è¿™ä¸€æ­¥å¯ä»¥ä½¿ç”¨æ›´å°çš„sketch

            unfolded = self._unfold(tensor, mode)
            n = unfolded.shape[0]
            s = self.fine_sketch_sizes[mode]

            # æž„å»ºé™åˆ¶åœ¨inité™„è¿‘çš„sketch
            P = self._get_sketch_matrix(n, s)
            sketched = P @ unfolded.T

            # ä½¿ç”¨initä½œä¸ºçƒ­å¯åŠ¨
            # å®žé™…å®žçŽ°ä¸­éœ€è¦æ›´å¤æ‚çš„ç®—æ³•
            U_sketch, _, _ = np.linalg.svd(sketched.T, full_matrices=False)

            r = self.ranks[mode]
            factors[mode] = U_sketch[:, :r]

        # è®¡ç®—æ ¸å¿ƒ
        core = tensor.copy()
        for mode, U in enumerate(factors):
            core = self._mode_product(core, U, mode)

        return core, factors
```

### åˆ›æ–°äºŒ: è‡ªé€‚åº”Sketchå°ºå¯¸

```python
class AdaptiveSketchingTucker(SketchingTucker):
    """
    è‡ªé€‚åº”Sketchå°ºå¯¸çš„Tuckeråˆ†è§£

    æ ¹æ®å¼ é‡ç‰¹æ€§è‡ªåŠ¨ç¡®å®šsketchå°ºå¯¸
    """

    def __init__(self, ranks, target_error=0.01):
        """
        å‚æ•°:
            ranks: ç›®æ ‡ç§©
            target_error: ç›®æ ‡è¿‘ä¼¼è¯¯å·®
        """
        super().__init__(ranks, sketch_sizes=None)
        self.target_error = target_error

    def _estimate_sketch_size(self, tensor, mode):
        """
        ä¼°è®¡æ‰€éœ€sketchå°ºå¯¸

        åŸºäºŽèƒ½é‡è°±åˆ†æž
        """
        unfolded = self._unfold(tensor, mode)

        # ä½¿ç”¨å°sketchä¼°è®¡è°±
        n = unfolded.shape[1]
        s_small = min(100, n)
        P_small = SketchingMatrix.gaussian_projection(n, s_small)

        sketched_small = P_small @ unfolded.T
        _, s, _ = np.linalg.svd(sketched_small, full_matrices=False)

        # è®¡ç®—èƒ½é‡ç´¯ç§¯
        energy = np.cumsum(s**2)
        energy = energy / energy[-1]

        # æ‰¾åˆ°è¾¾åˆ°ç›®æ ‡èƒ½é‡æ‰€éœ€çš„æœ€å°sketchå°ºå¯¸
        r = self.ranks[mode]
        min_sketch = r
        for i in range(len(s)):
            if i >= r and energy[i] >= (1 - self.target_error):
                min_sketch = i + 1
                break

        # å¢žåŠ å®‰å…¨è£•åº¦
        s_sketch = min(2 * min_sketch, n // 2)

        return s_sketch

    def decompose(self, tensor):
        """
        è‡ªé€‚åº”åˆ†è§£
        """
        # é¦–å…ˆä¼°è®¡å„æ¨¡çš„sketchå°ºå¯¸
        for mode in range(self.d):
            s = self._estimate_sketch_size(tensor, mode)
            self.sketch_sizes[mode] = s

        # ä½¿ç”¨ä¼°è®¡çš„sketchå°ºå¯¸è¿›è¡Œåˆ†è§£
        return super().decompose(tensor)
```

---

## ðŸ“Š å®žéªŒä¸Žç»“æžœ

### æ•°æ®é›†

| æ•°æ®é›† | ç»´åº¦ | å¤§å° | ç±»åž‹ |
|:---|:---|:---|:---|
| **åˆæˆå¼ é‡** | 1000Ã—1000Ã—1000 | 10â¹ | äººå·¥ç”Ÿæˆ |
| **è§†é¢‘æ•°æ®** | 240Ã—320Ã—3Ã—T | å¯å˜ | çœŸå®žè§†é¢‘ |
| ** hyperspectral** | 256Ã—256Ã—200 | 13M | é«˜å…‰è°±å›¾åƒ |
| **æŽ¨èç³»ç»Ÿ** | 10â¶Ã—10â¶Ã—10 | ç¨€ç– | ç”¨æˆ·-ç‰©å“-æ—¶é—´ |

### å¯¹æ¯”æ–¹æ³•

```
å¯¹æ¯”æ–¹æ³•:
â”œâ”€â”€ ä¼ ç»ŸHOSVD
â”œâ”€â”€ Truncated HOSVD
â”œâ”€â”€ Randomized SVD (rSVD)
â””â”€â”€ Sketching Tucker (æœ¬æ–‡)
```

### ä¸»è¦ç»“æžœ

#### è®¡ç®—æ—¶é—´å¯¹æ¯” (ç§’)

| æ•°æ®é›† | HOSVD | Truncated HOSVD | rSVD | Sketching Tucker |
|:---|:---:|:---:|:---:|:---:|
| 1000Â³ | 285.3 | 156.7 | 45.2 | **12.8** |
| Hyperspectral | 45.6 | 32.1 | 18.9 | **8.3** |
| æŽ¨èç³»ç»Ÿ | >1000 | 512.3 | 124.5 | **35.7** |

#### è¿‘ä¼¼è¯¯å·®å¯¹æ¯”

| æ•°æ®é›† | HOSVD | Truncated HOSVD | rSVD | Sketching Tucker |
|:---|:---:|:---:|:---:|:---:|
| 1000Â³ | 0.052 | 0.061 | 0.058 | **0.055** |
| Hyperspectral | 0.043 | 0.049 | 0.046 | **0.045** |
| æŽ¨èç³»ç»Ÿ | 0.067 | 0.075 | 0.071 | **0.069** |

**å…³é”®å‘çŽ°**:
- âœ“ è®¡ç®—åŠ é€Ÿæ˜¾è‘— (10-30å€)
- âœ“ è¯¯å·®ä¸Žä¼ ç»Ÿæ–¹æ³•ç›¸å½“
- âœ“ å†…å­˜æ¶ˆè€—å¤§å¹…é™ä½Ž

#### å†…å­˜æ¶ˆè€—å¯¹æ¯” (MB)

| æ•°æ®é›† | HOSVD | Sketching Tucker | é™ä½Žæ¯”ä¾‹ |
|:---|:---:|:---:|:---:|
| 1000Â³ | 8192 | **512** | 16Ã— |
| Hyperspectral | 1024 | **128** | 8Ã— |
| æŽ¨èç³»ç»Ÿ | 32768 | **1024** | 32Ã— |

---

## ðŸ’» å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å®Œæ•´å·¥å…·ç®±

```python
import numpy as np
from scipy.fft import fftn, ifftn

class TensorDecompositionToolkit:
    """
    å¼ é‡åˆ†è§£å·¥å…·ç®±
    """

    @staticmethod
    def tucker_decomposition_hosvd(tensor, ranks):
        """
        ä¼ ç»ŸHOSVDå®žçŽ°

        å‚æ•°:
            tensor: è¾“å…¥å¼ é‡
            ranks: å„æ¨¡ç§©

        è¿”å›ž:
            core: æ ¸å¿ƒå¼ é‡
            factors: å› å­çŸ©é˜µåˆ—è¡¨
        """
        factors = []
        d = tensor.ndim

        for mode in range(d):
            # æ¨¡å±•å¼€
            unfolded = TensorDecompositionToolkit.unfold(tensor, mode)

            # SVD
            U, _, _ = np.linalg.svd(unfolded, full_matrices=False)

            # æˆªæ–­
            U_r = U[:, :ranks[mode]]
            factors.append(U_r)

        # è®¡ç®—æ ¸å¿ƒå¼ é‡
        core = tensor.copy()
        for mode, U in enumerate(factors):
            core = TensorDecompositionToolkit.mode_n_product(core, U, mode)

        return core, factors

    @staticmethod
    def unfold(tensor, mode):
        """æ¨¡-n å±•å¼€"""
        shape = tensor.shape
        n_mode = shape[mode]

        # æ–°è½´é¡ºåº
        new_order = [mode] + [i for i in range(len(shape)) if i != mode]
        transposed = np.transpose(tensor, new_order)

        # å±•å¹³
        n_other = np.prod([d for i, d in enumerate(shape) if i != mode])
        unfolded = transposed.reshape(n_mode, n_other)

        return unfolded

    @staticmethod
    def mode_n_product(tensor, matrix, mode):
        """
        æ¨¡-n ä¹˜ç§¯: tensor Ã—_n matrix
        """
        shape = list(tensor.shape)

        # å±•å¼€ç¬¬næ¨¡
        unfolded = TensorDecompositionToolkit.unfold(tensor, mode)

        # çŸ©é˜µä¹˜æ³•
        product = matrix.T @ unfolded

        # é‡å¡‘
        new_shape = shape.copy()
        new_shape[mode] = matrix.shape[1]
        result = product.reshape(new_shape)

        return result

    @staticmethod
    def tucker_reconstruct(core, factors):
        """
        ä»ŽTuckeråˆ†è§£é‡æž„å¼ é‡
        """
        result = core.copy()
        for mode, factor in enumerate(factors):
            result = TensorDecompositionToolkit.mode_n_product(
                result, factor, mode
            )
        return result

    @staticmethod
    def tucker_error(tensor, core, factors):
        """
        è®¡ç®—ç›¸å¯¹è¯¯å·®
        """
        reconstructed = TensorDecompositionToolkit.tucker_reconstruct(core, factors)

        error = np.linalg.norm(tensor - reconstructed) / np.linalg.norm(tensor)

        return error

    @staticmethod
    def print_decomposition_info(tensor, core, factors):
        """
        æ‰“å°åˆ†è§£ä¿¡æ¯
        """
        original_size = np.prod(tensor.shape)

        core_size = np.prod(core.shape)
        factors_size = sum(f.shape[0] * f.shape[1] for f in factors)
        compressed_size = core_size + factors_size

        compression_ratio = original_size / compressed_size

        print(f"åŽŸå§‹å¼ é‡å½¢çŠ¶: {tensor.shape}")
        print(f"æ ¸å¿ƒå¼ é‡å½¢çŠ¶: {core.shape}")
        print(f"å› å­çŸ©é˜µå½¢çŠ¶: {[f.shape for f in factors]}")
        print(f"åŽŸå§‹å¤§å°: {original_size:,}")
        print(f"åŽ‹ç¼©å¤§å°: {compressed_size:,}")
        print(f"åŽ‹ç¼©æ¯”: {compression_ratio:.2f}x")
```

### ç»„ä»¶2: åº”ç”¨ç¤ºä¾‹

```python
class TuckerApplications:
    """
    Tuckeråˆ†è§£åº”ç”¨
    """

    @staticmethod
    def image_compression(image, ranks, method='sketching'):
        """
        å›¾åƒåŽ‹ç¼©åº”ç”¨

        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (HÃ—WÃ—3)
            ranks: åŽ‹ç¼©ç§©
            method: 'hosvd' æˆ– 'sketching'

        è¿”å›ž:
            compressed: åŽ‹ç¼©åŽçš„å›¾åƒ
            info: åŽ‹ç¼©ä¿¡æ¯
        """
        # å½’ä¸€åŒ–
        image_norm = image.astype(np.float32) / 255.0
        image_tensor = image_norm.transpose(2, 0, 1)  # (3, H, W)

        # åˆ†è§£
        if method == 'hosvd':
            core, factors = TensorDecompositionToolkit.tucker_decomposition_hosvd(
                image_tensor, ranks
            )
        elif method == 'sketching':
            sketching_tucker = SketchingTucker(ranks=ranks)
            core, factors = sketching_tucker.decompose(image_tensor)
        else:
            raise ValueError(f"Unknown method: {method}")

        # é‡æž„
        reconstructed = TensorDecompositionToolkit.tucker_reconstruct(core, factors)

        # è½¬æ¢å›žå›¾åƒæ ¼å¼
        compressed = reconstructed.transpose(1, 2, 0)  # (H, W, 3)
        compressed = np.clip(compressed * 255, 0, 255).astype(np.uint8)

        # è®¡ç®—ä¿¡æ¯
        error = TensorDecompositionToolkit.tucker_error(image_tensor, core, factors)

        info = {
            'ranks': ranks,
            'core_shape': core.shape,
            'factor_shapes': [f.shape for f in factors],
            'relative_error': error,
            'psnr': 20 * np.log10(1.0 / error) if error > 0 else float('inf')
        }

        return compressed, info

    @staticmethod
    def video_background_foreground(video_tensor, background_rank=5):
        """
        è§†é¢‘èƒŒæ™¯/å‰æ™¯åˆ†ç¦»

        video_tensor: (T, H, W) æˆ– (T, H, W, 3)
        """
        # è½¬æ¢ä¸ºå¼ é‡
        if video_tensor.ndim == 3:
            # ç°åº¦è§†é¢‘
            tensor = video_tensor
        elif video_tensor.ndim == 4:
            # å½©è‰²è§†é¢‘
            T, H, W, C = video_tensor.shape
            tensor = video_tensor.transpose(0, 3, 1, 2)  # (T, C, H, W)

        # Tuckeråˆ†è§£
        # èƒŒæ™¯ä½¿ç”¨ä½Žç§©
        ranks = [background_rank, tensor.shape[1]//4, tensor.shape[2]//4]
        if tensor.ndim == 4:
            ranks.append(tensor.shape[3]//4)

        core, factors = SketchingTucker(ranks=ranks).decompose(tensor)

        # ä½Žç§©éƒ¨åˆ†è¿‘ä¼¼èƒŒæ™¯
        background = TensorDecompositionToolkit.tucker_reconstruct(core, factors)

        # å‰æ™¯ = åŽŸå§‹ - èƒŒæ™¯
        foreground = tensor - background

        return background, foreground, (core, factors)

    @staticmethod
    def tensor_completion(tensor, mask, ranks, max_iter=100):
        """
        å¼ é‡è¡¥å…¨ (åŸºäºŽTuckeråˆ†è§£)

        å‚æ•°:
            tensor: è§‚æµ‹åˆ°çš„å¼ é‡å€¼
            mask: è§‚æµ‹æŽ©ç  (1=è§‚æµ‹, 0=ç¼ºå¤±)
            ranks: Tuckerç§©

        è¿”å›ž:
            completed: è¡¥å…¨åŽçš„å¼ é‡
        """
        # åˆå§‹åŒ–: ç”¨å‡å€¼å¡«å……ç¼ºå¤±å€¼
        mean_val = tensor[mask > 0].mean()
        completed = tensor.copy()
        completed[mask == 0] = mean_val

        # è¿­ä»£ä¼˜åŒ–
        for iteration in range(max_iter):
            # Tuckeråˆ†è§£
            core, factors = TensorDecompositionToolkit.tucker_decomposition_hosvd(
                completed, ranks
            )

            # é‡æž„
            reconstructed = TensorDecompositionToolkit.tucker_reconstruct(core, factors)

            # åªæ›´æ–°ç¼ºå¤±å€¼
            completed[mask == 0] = reconstructed[mask == 0]

            # æ£€æŸ¥æ”¶æ•›
            if iteration > 10:
                change = np.linalg.norm(completed - reconstructed) / np.linalg.norm(completed)
                if change < 1e-4:
                    break

            completed = reconstructed

        return completed

    @staticmethod
    def tensor_denoising(tensor, ranks, noise_std=None):
        """
        å¼ é‡åŽ»å™ª

        ä½¿ç”¨Tuckeråˆ†è§£çš„ä½Žç§©è¿‘ä¼¼åŽ»å™ª
        """
        # ä¼°è®¡å™ªå£°
        if noise_std is None:
            # ä½¿ç”¨æœ€å°å¥‡å¼‚å€¼ä¼°è®¡
            unfolded = TensorDecompositionToolkit.unfold(tensor, 0)
            _, s, _ = np.linalg.svd(unfolded, full_matrices=False)
            noise_std = s[-1] / np.sqrt(max(tensor.shape))

        # Tuckeråˆ†è§£
        core, factors = TensorDecompositionToolkit.tucker_decomposition_hosvd(
            tensor, ranks
        )

        # é‡æž„ (ä½Žç§©è¿‘ä¼¼)
        denoised = TensorDecompositionToolkit.tucker_reconstruct(core, factors)

        return denoised, (core, factors)
```

### ç»„ä»¶3: å¯è§†åŒ–å·¥å…·

```python
class TuckerVisualization:
    """
    Tuckeråˆ†è§£å¯è§†åŒ–
    """

    @staticmethod
    def visualize_core_tensor(core, factor_names=None):
        """
        å¯è§†åŒ–æ ¸å¿ƒå¼ é‡

        å¯¹äºŽ3é˜¶å¼ é‡, å±•ç¤ºåˆ‡ç‰‡
        """
        import matplotlib.pyplot as plt

        if core.ndim != 3:
            print(f"Warning: Core tensor has {core.ndim} dimensions, visualization for 3D only")
            return

        d1, d2, d3 = core.shape

        fig, axes = plt.subplots(d1, 1, figsize=(8, 3*d1))

        for i in range(d1):
            im = axes[i].imshow(core[i], cmap='viridis')
            axes[i].set_title(f'Core Slice {i}')
            plt.colorbar(im, ax=axes[i])

        if factor_names:
            fig.suptitle(f'Core Tensor ({"Ã—".join(map(str, core.shape))})')
        else:
            fig.suptitle(f'Core Tensor')

        plt.tight_layout()
        plt.show()

    @staticmethod
    def visualize_factor_matrices(factors, tensor_names=None):
        """
        å¯è§†åŒ–å› å­çŸ©é˜µ
        """
        import matplotlib.pyplot as plt

        d = len(factors)
        fig, axes = plt.subplots(1, d, figsize=(5*d, 5))

        if d == 1:
            axes = [axes]

        for i, factor in enumerate(factors):
            # å¯è§†åŒ–ä¸ºçƒ­å›¾
            im = axes[i].imshow(factor, cmap='viridis', aspect='auto')
            axes[i].set_title(f'Factor Matrix {i+1}\nShape: {factor.shape}')
            plt.colorbar(im, ax=axes[i])

        if tensor_names:
            fig.suptitle('Factor Matrices')
        else:
            fig.suptitle('Factor Matrices')

        plt.tight_layout()
        plt.show()

    @staticmethod
    def compare_reconstruction(original, reconstructed, title='Tucker Reconstruction'):
        """
        å¯¹æ¯”åŽŸå§‹å’Œé‡æž„å¼ é‡
        """
        import matplotlib.pyplot as plt

        if original.ndim == 2:
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            axes[0].imshow(original, cmap='gray')
            axes[0].set_title('Original')
            axes[0].axis('off')

            axes[1].imshow(reconstructed, cmap='gray')
            axes[1].set_title('Reconstructed')
            axes[1].axis('off')

            error = np.abs(original - reconstructed)
            axes[2].imshow(error, cmap='hot')
            axes[2].set_title(f'Error (Max: {error.max():.4f})')
            axes[2].axis('off')

        elif original.ndim == 3:
            # å¯¹äºŽå½©è‰²å›¾åƒæˆ–3é˜¶å¼ é‡,å±•ç¤ºä¸­é—´åˆ‡ç‰‡
            mid_slice = original.shape[0] // 2

            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            axes[0].imshow(original[mid_slice])
            axes[0].set_title(f'Original (Slice {mid_slice})')
            axes[0].axis('off')

            axes[1].imshow(reconstructed[mid_slice])
            axes[1].set_title(f'Reconstructed (Slice {mid_slice})')
            axes[1].axis('off')

            error = np.abs(original - reconstructed)
            axes[2].imshow(error[mid_slice], cmap='hot')
            axes[2].set_title(f'Error (Slice {mid_slice})')
            axes[2].axis('off')

        fig.suptitle(title)
        plt.tight_layout()
        plt.show()

        # è®¡ç®—å¹¶æ‰“å°è¯¯å·®
        relative_error = np.linalg.norm(original - reconstructed) / np.linalg.norm(original)
        print(f"Relative Error: {relative_error:.6f}")
```

---

## ðŸ”— ä¸Žå…¶ä»–å·¥ä½œçš„å…³ç³»

### 6.1 Xiaohao Caiç ”ç©¶è„‰ç»œ

```
å¼ é‡åˆ†è§£æ–¹æ³•æ¼”è¿›:

[3-02] tCURLoRA
    â†“ å¼ é‡CURåˆ†è§£
    â†“
[3-04] Sketching Tucker â† æœ¬ç¯‡
    â†“ éšæœºæŠ•å½±åŠ é€Ÿ
    â†“
[3-05] Two-Sided Sketching
    â†“ åŒå‘sketching
    â†“
æœªæ¥: æ›´é«˜æ•ˆçš„å¼ é‡æ–¹æ³•
```

### 6.2 ä¸Žæ ¸å¿ƒè®ºæ–‡çš„å…³ç³»

| è®ºæ–‡ | å…³ç³» | è¯´æ˜Ž |
|:---|:---|:---|
| [3-02] tCURLoRA | **æ–¹æ³•å…³è”** | éƒ½æ˜¯å¼ é‡åˆ†è§£ |
| [2-12] Neural Varifolds | **åº”ç”¨å…³è”** | éƒ½ç”¨å¼ é‡è¡¨ç¤º |
| [2-15] 3Dæ ‘æœ¨åˆ†å‰² | **æ•°æ®ç±»åž‹** | 3Då¼ é‡æ•°æ® |

### 6.3 å¼ é‡åˆ†è§£æ–¹æ³•ä½“ç³»

```
å¼ é‡åˆ†è§£å®¶æ—:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Tuckeråˆ†è§£                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ æ ¸å¿ƒå¼ é‡ + dä¸ªå› å­çŸ©é˜µ                      â”‚       â”‚
â”‚  â”‚ é€šç”¨, çµæ´»                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ CPåˆ†è§£ (CANDECOMP/PARAFAC)                  â”‚       â”‚
â”‚  â”‚ å¯¹è§’æ ¸å¿ƒå¼ é‡                                â”‚       â”‚
â”‚  â”‚ æ›´ç´§å‡‘, ä½†è®¡ç®—éš¾                            â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Tensor Train (TT)                           â”‚       â”‚
â”‚  â”‚ é“¾å¼ç»“æž„                                    â”‚       â”‚
â”‚  â”‚ é€‚åˆé«˜ç»´å¼ é‡                                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Tensor CUR / tCURLoRA                       â”‚       â”‚
â”‚  â”‚ åˆ—é€‰æ‹©æ–¹æ³•                                  â”‚       â”‚
â”‚  â”‚ å¯è§£é‡Šæ€§å¼º                                  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ ä¸ªäººæ€è€ƒä¸Žæ€»ç»“

### 7.1 æ ¸å¿ƒæ”¶èŽ·

#### æ”¶èŽ·1: éšæœºç®—æ³•çš„å¨åŠ›

```
éšæœºç®—æ³•ä¼˜åŠ¿:
â”œâ”€â”€ é™ç»´å¤æ‚åº¦: O(nÂ²) â†’ O(n log n)
â”œâ”€â”€ å†…å­˜å‹å¥½: ä¸éœ€è¦å­˜å‚¨å®Œæ•´çŸ©é˜µ
â”œâ”€â”€ å¹¶è¡Œå‹å¥½: æ˜“äºŽå¹¶è¡ŒåŒ–
â””â”€â”€ è¯¯å·®å¯æŽ§: æ¦‚çŽ‡ä¿è¯

åº”ç”¨åœºæ™¯:
â”œâ”€â”€ å¤§è§„æ¨¡æœºå™¨å­¦ä¹ 
â”œâ”€â”€ æŽ¨èç³»ç»Ÿ
â”œâ”€â”€ æ·±åº¦å­¦ä¹ è®­ç»ƒ
â””â”€â”€ æ•°æ®åŽ‹ç¼©
```

#### æ”¶èŽ·2: å¼ é‡åˆ†è§£çš„é€‰æ‹©

```
å¦‚ä½•é€‰æ‹©å¼ é‡åˆ†è§£æ–¹æ³•:

Tucker â†’ é€šç”¨åœºæ™¯
â”œâ”€â”€ ä¼˜ç‚¹: çµæ´», ç†è®ºæˆç†Ÿ
â”œâ”€â”€ ç¼ºç‚¹: è®¡ç®—å¤æ‚
â””â”€â”€ åº”ç”¨: é€šç”¨æ•°æ®åˆ†æž

CP â†’ ç¨€ç–/ä½Žç§©åœºæ™¯
â”œâ”€â”€ ä¼˜ç‚¹: æœ€ç´§å‡‘
â”œâ”€â”€ ç¼ºç‚¹: NP-hard
â””â”€â”€ åº”ç”¨: ç‰¹å®šç»“æž„æ•°æ®

TT â†’ è¶…é«˜ç»´åœºæ™¯
â”œâ”€â”€ ä¼˜ç‚¹: å¯æ‰©å±•åˆ°é«˜ç»´
â”œâ”€â”€ ç¼ºç‚¹: é“¾å¼çº¦æŸ
â””â”€â”€ åº”ç”¨: æ·±åº¦å­¦ä¹ åŽ‹ç¼©

CUR â†’ å¯è§£é‡Šåœºæ™¯
â”œâ”€â”€ ä¼˜ç‚¹: ä¿ç•™å®žé™…æ•°æ®ç‚¹
â”œâ”€â”€ ç¼ºç‚¹: è¿‘ä¼¼è´¨é‡ç¨å·®
â””â”€â”€ åº”ç”¨: æŽ¨èç³»ç»Ÿ
```

#### æ”¶èŽ·3: SketchingæŠ€æœ¯

```
SketchingçŸ©é˜µç±»åž‹:

é«˜æ–¯æŠ•å½±:
â”œâ”€â”€ ç†è®ºä¿è¯æœ€å¥½
â”œâ”€â”€ è®¡ç®—è¾ƒæ…¢
â””â”€â”€ é€šç”¨åœºæ™¯

ç¨€ç–æŠ•å½±:
â”œâ”€â”€ è®¡ç®—å¿«
â”œâ”€â”€ å­˜å‚¨å°‘
â””â”€â”€ å¤§è§„æ¨¡åœºæ™¯

ç»“æž„åŒ–æŠ•å½± (SRFT):
â”œâ”€â”€ æžå¿« (FFTåŠ é€Ÿ)
â”œâ”€â”€ ç†è®ºä¿è¯
â””â”€â”€ å®žæ—¶åœºæ™¯
```

### 7.2 åº”ç”¨åˆ°äº•ç›–æ£€æµ‹

```
å¼ é‡åˆ†è§£åœ¨è¿å»ºæ£€æµ‹ä¸­çš„åº”ç”¨:

æ•°æ®ç»„ç»‡:
â”œâ”€â”€ æ—¶é—´ç»´åº¦: ä¸åŒæ—¶æœŸçš„å›¾åƒ
â”œâ”€â”€ ç©ºé—´ç»´åº¦: (x, y) ä½ç½®
â”œâ”€â”€ ç‰¹å¾ç»´åº¦: RGB + æ·±åº¦ + çº¹ç†

åº”ç”¨åœºæ™¯:
â”œâ”€â”€ å˜åŒ–æ£€æµ‹: Tuckeråˆ†è§£æå–æ—¶åºæ¨¡å¼
â”œâ”€â”€ èƒŒæ™¯å»ºæ¨¡: ä½Žç§©è¿‘ä¼¼å»ºæ¨¡èƒŒæ™¯
â”œâ”€â”€ å¼‚å¸¸æ£€æµ‹: é«˜ç§©æ®‹å·®æ£€æµ‹è¿å»º
â””â”€â”€ åŽ‹ç¼©å­˜å‚¨: é«˜æ•ˆå­˜å‚¨åŽ†å²æ•°æ®
```

---

## âœ… ç²¾è¯»æ£€æŸ¥æ¸…å•

- [x] **æ¡†æž¶ç†è§£**: Tuckeråˆ†è§£å’ŒSketchingæŠ€æœ¯
- [x] **ç®—æ³•å®žçŽ°**: å®Œæ•´ä»£ç å®žçŽ°
- [x] **æ•°å­¦åŸºç¡€**: æ¨¡å±•å¼€ã€æ¨¡ä¹˜ç§¯
- [x] **åº”ç”¨åœºæ™¯**: å›¾åƒåŽ‹ç¼©ã€è§†é¢‘åˆ†æž
- [x] **æ–¹æ³•å¯¹æ¯”**: Tucker vs CP vs TT vs CUR

---

**ç²¾è¯»å®Œæˆæ—¶é—´**: 2026å¹´2æœˆ9æ—¥
**è®ºæ–‡ç±»åž‹**: ç®—æ³•åˆ›æ–°
**å…³è”è®ºæ–‡**: [3-02] tCURLoRA, [2-12] Neural Varifolds

---

*æœ¬ç²¾è¯»ç¬”è®°åŸºäºŽSketching for Large-Scale Tucker Approximationè®ºæ–‡*
*é‡ç‚¹å…³æ³¨: éšæœºæŠ•å½±æŠ€æœ¯ã€Tuckeråˆ†è§£ã€å¤§è§„æ¨¡å¼ é‡å¤„ç†*
