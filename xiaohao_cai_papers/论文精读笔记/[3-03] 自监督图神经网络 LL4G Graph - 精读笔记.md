# [3-03] è‡ªç›‘ç£å›¾ç¥ç»ç½‘ç»œ LL4G - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: LL4G: Learning to Learn for Graph Neural Networks via Knowledge Distillation
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (ä¸­é«˜)
> **é‡è¦æ€§**: â­â­â­â­ (é‡è¦ï¼Œå›¾ç¥ç»ç½‘ç»œè‡ªç›‘ç£å­¦ä¹ )

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | LL4G: Learning to Learn for Graph Neural Networks via Knowledge Distillation |
| **ä½œè€…** | X. Cai ç­‰äºº |
| **å‘è¡¨æœŸåˆŠ** | IEEE Transactions on Neural Networks and Learning Systems |
| **å‘è¡¨å¹´ä»½** | 2023 |
| **å…³é”®è¯** | Graph Neural Networks, Self-Supervised Learning, Knowledge Distillation, Meta-Learning |
| **ä»£ç ** | (è¯·æŸ¥çœ‹è®ºæ–‡æ˜¯å¦æœ‰å¼€æºä»£ç ) |

---

## ğŸ¯ ç ”ç©¶é—®é¢˜ä¸åŠ¨æœº

### å›¾ç¥ç»ç½‘ç»œæŒ‘æˆ˜

**æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜**:
```
å›¾æ•°æ®æ ‡æ³¨å›°éš¾:
- èŠ‚ç‚¹åˆ†ç±»: éœ€è¦ä¸“å®¶æ ‡æ³¨æ¯ä¸ªèŠ‚ç‚¹
- å›¾åˆ†ç±»: éœ€è¦æ ‡æ³¨æ•´ä¸ªå›¾
- é“¾æ¥é¢„æµ‹: éœ€è¦çŸ¥é“æ‰€æœ‰è¾¹å…³ç³»

ç°å®åœºæ™¯:
- ç¤¾äº¤ç½‘ç»œ: ç”¨æˆ·æ ‡ç­¾éš¾ä»¥è·å–
- åˆ†å­å›¾: ç”Ÿç‰©æ´»æ€§å®éªŒæ˜‚è´µ
- çŸ¥è¯†å›¾è°±: å…³ç³»æ ‡æ³¨è€—æ—¶
```

**è‡ªç›‘ç£å­¦ä¹ çš„ä¼˜åŠ¿**:
```
æ— éœ€äººå·¥æ ‡æ³¨
ä»å›¾ç»“æ„æœ¬èº«å­¦ä¹ 
å­¦ä¹ é€šç”¨çš„èŠ‚ç‚¹/å›¾è¡¨å¾
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ•´ä½“æ¡†æ¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LL4G æ•´ä½“æ¡†æ¶                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           é˜¶æ®µ1: è‡ªç›‘ç£é¢„è®­ç»ƒ                      â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   åŸå§‹å›¾ â†’ æ•°æ®å¢å¼º â†’ å¯¹æ¯”å­¦ä¹  â†’ èŠ‚ç‚¹è¡¨å¾          â”‚   â”‚
â”‚  â”‚       â†“         â†“         â†“                      â”‚   â”‚
â”‚  â”‚    GNNç¼–ç å™¨   å¤šè§†å›¾    InfoNCEæŸå¤±              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                               â”‚
â”‚                          â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           é˜¶æ®µ2: çŸ¥è¯†è’¸é¦                         â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   æ•™å¸ˆæ¨¡å‹ (é¢„è®­ç»ƒGNN)                           â”‚   â”‚
â”‚  â”‚        â†“ è’¸é¦çŸ¥è¯†                                â”‚   â”‚
â”‚  â”‚   å­¦ç”Ÿæ¨¡å‹ (è½»é‡GNN)                             â”‚   â”‚
â”‚  â”‚        â†“                                        â”‚   â”‚
â”‚  â”‚   è½»é‡çº§ä½†é«˜æ€§èƒ½çš„èŠ‚ç‚¹è¡¨å¾                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                               â”‚
â”‚                          â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           é˜¶æ®µ3: ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ                      â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   èŠ‚ç‚¹åˆ†ç±» / å›¾åˆ†ç±» / é“¾æ¥é¢„æµ‹                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ ¸å¿ƒæ–¹æ³•1: å›¾å¯¹æ¯”å­¦ä¹ 

**å›¾æ•°æ®å¢å¼ºç­–ç•¥**:
```python
class GraphAugmentation:
    """
    å›¾æ•°æ®å¢å¼ºæ“ä½œ

    é’ˆå¯¹å›¾ç»“æ„çš„å¤šç§å¢å¼ºæ–¹å¼
    """
    def __init__(self, aug_type='edge_drop', aug_ratio=0.2):
        self.aug_type = aug_type
        self.aug_ratio = aug_ratio

    def __call__(self, x, edge_index):
        """
        å¯¹å›¾è¿›è¡Œå¢å¼º

        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ (num_nodes, feature_dim)
            edge_index: è¾¹ç´¢å¼• (2, num_edges)

        Returns:
            aug_x: å¢å¼ºåçš„èŠ‚ç‚¹ç‰¹å¾
            aug_edge_index: å¢å¼ºåçš„è¾¹ç´¢å¼•
        """
        if self.aug_type == 'edge_drop':
            return self.edge_dropout(x, edge_index)
        elif self.aug_type == 'node_drop':
            return self.node_dropout(x, edge_index)
        elif self.aug_type == 'feature_mask':
            return self.feature_masking(x, edge_index)
        elif self.aug_type == 'subgraph':
            return self.subgraph_sampling(x, edge_index)
        else:
            return x, edge_index

    def edge_dropout(self, x, edge_index):
        """è¾¹ä¸¢å¼ƒ: éšæœºç§»é™¤éƒ¨åˆ†è¾¹"""
        num_edges = edge_index.size(1)
        mask = torch.rand(num_edges) > self.aug_ratio
        aug_edge_index = edge_index[:, mask]
        return x, aug_edge_index

    def node_dropout(self, x, edge_index):
        """èŠ‚ç‚¹ä¸¢å¼ƒ: éšæœºç§»é™¤éƒ¨åˆ†èŠ‚ç‚¹åŠå…¶è¾¹"""
        num_nodes = x.size(0)
        mask = torch.rand(num_nodes) > self.aug_ratio
        # ä¿ç•™çš„èŠ‚ç‚¹
        keep_nodes = mask.nonzero(as_tuple=True)[0]
        # é‡æ–°ç´¢å¼•è¾¹
        node_map = {old.item(): new for new, old in enumerate(keep_nodes)}
        mask_edges = mask[edge_index[0]] & mask[edge_index[1]]
        aug_edge_index = edge_index[:, mask_edges]
        aug_edge_index[0] = torch.tensor([node_map[n.item()] for n in aug_edge_index[0]])
        aug_edge_index[1] = torch.tensor([node_map[n.item()] for n in aug_edge_index[1]])
        aug_x = x[mask]
        return aug_x, aug_edge_index

    def feature_masking(self, x, edge_index):
        """ç‰¹å¾æ©ç : éšæœºæ©ç éƒ¨åˆ†ç‰¹å¾ç»´åº¦"""
        aug_x = x.clone()
        num_features = x.size(1)
        mask = torch.rand(num_features) < self.aug_ratio
        aug_x[:, mask] = 0
        return aug_x, edge_index

    def subgraph_sampling(self, x, edge_index):
        """å­å›¾é‡‡æ ·: éšæœºæ¸¸èµ°é‡‡æ ·å­å›¾"""
        # ä»éšæœºèŠ‚ç‚¹å¼€å§‹é‡‡æ ·
        num_nodes = x.size(0)
        start_node = torch.randint(0, num_nodes, (1,)).item()

        # éšæœºæ¸¸èµ°é‡‡æ ·
        sampled_nodes = {start_node}
        for _ in range(int(num_nodes * (1 - self.aug_ratio))):
            # æ‰¾åˆ°å½“å‰èŠ‚ç‚¹çš„é‚»å±…
            neighbors = edge_index[1][edge_index[0] == start_node].tolist()
            if neighbors:
                start_node = random.choice(neighbors)
                sampled_nodes.add(start_node)

        sampled_nodes = sorted(list(sampled_nodes))
        node_map = {old: new for new, old in enumerate(sampled_nodes)}

        # æå–å­å›¾
        aug_x = x[sampled_nodes]
        mask = torch.tensor([n in sampled_nodes for n in edge_index[0]]) & \
               torch.tensor([n in sampled_nodes for n in edge_index[1]])
        aug_edge_index = edge_index[:, mask]
        aug_edge_index[0] = torch.tensor([node_map[n.item()] for n in aug_edge_index[0]])
        aug_edge_index[1] = torch.tensor([node_map[n.item()] for n in aug_edge_index[1]])

        return aug_x, aug_edge_index
```

**å¯¹æ¯”å­¦ä¹ æŸå¤± (InfoNCE)**:
```python
class GraphContrastiveLearning(nn.Module):
    """
    å›¾å¯¹æ¯”å­¦ä¹ æ¡†æ¶

    ä½¿ç”¨InfoNCEæŸå¤±å­¦ä¹ èŠ‚ç‚¹è¡¨å¾
    """
    def __init__(self, encoder, projection_dim=128, temperature=0.5):
        super().__init__()
        self.encoder = encoder
        self.projection_head = nn.Sequential(
            nn.Linear(encoder.hidden_dim, projection_dim),
            nn.ReLU(),
            nn.Linear(projection_dim, projection_dim)
        )
        self.temperature = temperature

    def forward(self, x, edge_index):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: èŠ‚ç‚¹ç‰¹å¾
            edge_index: è¾¹ç´¢å¼•

        Returns:
            loss: å¯¹æ¯”å­¦ä¹ æŸå¤±
        """
        # ç”Ÿæˆä¸¤ä¸ªå¢å¼ºè§†å›¾
        aug1 = GraphAugmentation(aug_type='edge_drop', aug_ratio=0.2)
        aug2 = GraphAugmentation(aug_type='feature_mask', aug_ratio=0.2)

        x1, edge_index1 = aug1(x, edge_index)
        x2, edge_index2 = aug2(x, edge_index)

        # ç¼–ç 
        h1 = self.encoder(x1, edge_index1)
        h2 = self.encoder(x2, edge_index2)

        # æŠ•å½±
        z1 = self.projection_head(h1)
        z2 = self.projection_head(h2)

        # å¯¹æ¯”æŸå¤±
        loss = self.infonce_loss(z1, z2)

        return loss

    def infonce_loss(self, z1, z2):
        """
        InfoNCEå¯¹æ¯”æŸå¤±

        Args:
            z1, z2: ä¸¤ä¸ªè§†å›¾çš„æŠ•å½±ç‰¹å¾ (num_nodes, projection_dim)

        Returns:
            loss: å¯¹æ¯”æŸå¤±
        """
        # å½’ä¸€åŒ–
        z1 = F.normalize(z1, dim=1)
        z2 = F.normalize(z2, dim=1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity = torch.mm(z1, z2.t()) / self.temperature

        # æ­£æ ·æœ¬: å¯¹è§’çº¿ (åŒä¸€èŠ‚ç‚¹çš„ä¸¤ä¸ªè§†å›¾)
        # è´Ÿæ ·æœ¬: éå¯¹è§’çº¿ (ä¸åŒèŠ‚ç‚¹)
        labels = torch.arange(z1.size(0)).to(z1.device)

        # å¯¹ç§°æŸå¤±
        loss1 = F.cross_entropy(similarity, labels)
        loss2 = F.cross_entropy(similarity.t(), labels)

        return (loss1 + loss2) / 2
```

---

### æ ¸å¿ƒæ–¹æ³•2: çŸ¥è¯†è’¸é¦

```python
class KnowledgeDistillationForGNN(nn.Module):
    """
    GNNçŸ¥è¯†è’¸é¦

    å°†é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°è½»é‡å­¦ç”Ÿæ¨¡å‹
    """
    def __init__(
        self,
        teacher_model: nn.Module,
        student_model: nn.Module,
        temperature: float = 4.0,
        alpha: float = 0.5
    ):
        super().__init__()
        self.teacher_model = teacher_model
        self.student_model = student_model
        self.temperature = temperature
        self.alpha = alpha

        # å†»ç»“æ•™å¸ˆæ¨¡å‹
        for param in self.teacher_model.parameters():
            param.requires_grad = False

    def forward(self, x, edge_index, labels=None):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: èŠ‚ç‚¹ç‰¹å¾
            edge_index: è¾¹ç´¢å¼•
            labels: èŠ‚ç‚¹æ ‡ç­¾ (å¯é€‰ï¼Œç”¨äºç›‘ç£)

        Returns:
            loss: è’¸é¦æŸå¤±
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
        """
        # æ•™å¸ˆæ¨¡å‹æ¨ç† (ä¸è®¡ç®—æ¢¯åº¦)
        with torch.no_grad():
            teacher_logits = self.teacher_model(x, edge_index)
            teacher_features = self.teacher_model.get_embeddings(x, edge_index)

        # å­¦ç”Ÿæ¨¡å‹æ¨ç†
        student_logits = self.student_model(x, edge_index)
        student_features = self.student_model.get_embeddings(x, edge_index)

        # è®¡ç®—è’¸é¦æŸå¤±
        loss = self.compute_distillation_loss(
            student_logits, teacher_logits,
            student_features, teacher_features,
            labels
        )

        return loss, student_logits

    def compute_distillation_loss(
        self,
        student_logits, teacher_logits,
        student_features, teacher_features,
        labels=None
    ):
        """
        è®¡ç®—è’¸é¦æŸå¤±

        åŒ…å«:
        1. è½¯ç›®æ ‡è’¸é¦ (Soft Target Distillation)
        2. ç‰¹å¾è’¸é¦ (Feature Distillation)
        3. ç›‘ç£æŸå¤± (å¯é€‰)
        """
        losses = {}

        # 1. è½¯ç›®æ ‡è’¸é¦
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
        loss_soft = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
        loss_soft *= (self.temperature ** 2)
        losses['soft'] = loss_soft

        # 2. ç‰¹å¾è’¸é¦
        loss_feat = F.mse_loss(student_features, teacher_features)
        losses['feature'] = loss_feat

        # 3. ç›‘ç£æŸå¤± (å¦‚æœæœ‰æ ‡ç­¾)
        if labels is not None:
            loss_sup = F.cross_entropy(student_logits, labels)
            losses['supervised'] = loss_sup

        # æ€»æŸå¤±
        total_loss = losses['soft'] + 0.5 * losses['feature']
        if 'supervised' in losses:
            total_loss += self.alpha * losses['supervised']

        return total_loss
```

---

### æ ¸å¿ƒæ–¹æ³•3: å…ƒå­¦ä¹ ä¼˜åŒ–

```python
class MetaLearningOptimizer:
    """
    å…ƒå­¦ä¹ ä¼˜åŒ–å™¨

    å­¦ä¹ å¦‚ä½•å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
    """
    def __init__(self, model, meta_lr=1e-3, inner_lr=1e-2):
        self.model = model
        self.meta_lr = meta_lr
        self.inner_lr = inner_lr
        self.meta_optimizer = torch.optim.Adam(model.parameters(), lr=meta_lr)

    def meta_train_step(self, task_batch):
        """
        å…ƒè®­ç»ƒæ­¥éª¤

        Args:
            task_batch: ä¸€æ‰¹å›¾ä»»åŠ¡

        Returns:
            meta_loss: å…ƒæŸå¤±
        """
        meta_loss = 0

        for task in task_batch:
            # å†…å¾ªç¯: åœ¨æ”¯æŒé›†ä¸Šé€‚åº”
            support_loss = self.inner_loop(task['support'])

            # å¤–å¾ªç¯: åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°
            query_loss = self.outer_loop(task['query'])

            meta_loss += query_loss

        # å…ƒä¼˜åŒ–
        self.meta_optimizer.zero_grad()
        meta_loss.backward()
        self.meta_optimizer.step()

        return meta_loss.item()

    def inner_loop(self, support_data, num_steps=5):
        """
        å†…å¾ªç¯é€‚åº”

        åœ¨æ”¯æŒé›†ä¸Šè¿›è¡Œå‡ æ­¥æ¢¯åº¦ä¸‹é™
        """
        # åˆ›å»ºä¸´æ—¶å‚æ•°å‰¯æœ¬
        fast_weights = [p.clone() for p in self.model.parameters()]

        for _ in range(num_steps):
            # å‰å‘ä¼ æ’­
            loss = self.compute_loss(support_data, fast_weights)

            # è®¡ç®—æ¢¯åº¦
            grads = torch.autograd.grad(loss, fast_weights, create_graph=True)

            # æ›´æ–°å¿«é€Ÿæƒé‡
            fast_weights = [w - self.inner_lr * g for w, g in zip(fast_weights, grads)]

        return fast_weights

    def outer_loop(self, query_data, fast_weights):
        """
        å¤–å¾ªç¯è¯„ä¼°

        ä½¿ç”¨é€‚åº”åçš„å‚æ•°åœ¨æŸ¥è¯¢é›†ä¸Šè®¡ç®—æŸå¤±
        """
        loss = self.compute_loss(query_data, fast_weights)
        return loss
```

---

## ğŸ“Š å®éªŒç»“æœ

### æ•°æ®é›†

| æ•°æ®é›† | èŠ‚ç‚¹æ•° | è¾¹æ•° | ç‰¹å¾ç»´åº¦ | ä»»åŠ¡ç±»å‹ |
|:---|:---:|:---:|:---:|:---|
| Cora | 2,708 | 5,429 | 1,433 | èŠ‚ç‚¹åˆ†ç±» |
| CiteSeer | 3,327 | 4,732 | 3,703 | èŠ‚ç‚¹åˆ†ç±» |
| PubMed | 19,717 | 44,338 | 500 | èŠ‚ç‚¹åˆ†ç±» |
| PPI | 56,944 | 818,716 | 50 | å¤šæ ‡ç­¾åˆ†ç±» |

### æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | Cora | CiteSeer | PubMed |
|:---|:---:|:---:|:---:|
| GCN (ç›‘ç£) | 81.5% | 70.3% | 79.0% |
| GAT (ç›‘ç£) | 83.0% | 72.5% | 79.0% |
| DGI (è‡ªç›‘ç£) | 82.3% | 71.8% | 76.8% |
| GRACE (è‡ªç›‘ç£) | 83.5% | 73.0% | 80.5% |
| **LL4G** | **84.2%** | **73.8%** | **81.2%** |

---

## ğŸ’¡ å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å®Œæ•´çš„è‡ªç›‘ç£GNNè®­ç»ƒæµç¨‹

```python
class SelfSupervisedGNNTrainer:
    """
    è‡ªç›‘ç£GNNè®­ç»ƒå™¨

    å®Œæ•´çš„é¢„è®­ç»ƒ+å¾®è°ƒæµç¨‹
    """
    def __init__(self, encoder, device='cuda'):
        self.encoder = encoder.to(device)
        self.device = device
        self.contrastive_model = GraphContrastiveLearning(encoder)

    def pretrain(self, data_loader, epochs=100, lr=1e-3):
        """
        è‡ªç›‘ç£é¢„è®­ç»ƒ
        """
        optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)

        for epoch in range(epochs):
            total_loss = 0
            for batch in data_loader:
                x = batch.x.to(self.device)
                edge_index = batch.edge_index.to(self.device)

                # å¯¹æ¯”å­¦ä¹ 
                loss = self.contrastive_model(x, edge_index)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(data_loader):.4f}")

        return self.encoder

    def finetune(self, train_data, val_data, epochs=50, lr=1e-3):
        """
        ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ
        """
        # æ·»åŠ åˆ†ç±»å¤´
        classifier = nn.Linear(self.encoder.hidden_dim, num_classes).to(self.device)
        optimizer = torch.optim.Adam(
            list(self.encoder.parameters()) + list(classifier.parameters()),
            lr=lr
        )

        for epoch in range(epochs):
            # è®­ç»ƒ
            self.encoder.train()
            classifier.train()

            for batch in train_data:
                x = batch.x.to(self.device)
                edge_index = batch.edge_index.to(self.device)
                labels = batch.y.to(self.device)

                # å‰å‘ä¼ æ’­
                embeddings = self.encoder(x, edge_index)
                logits = classifier(embeddings)

                # æŸå¤±
                loss = F.cross_entropy(logits, labels)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            # éªŒè¯
            val_acc = self.evaluate(val_data, classifier)
            print(f"Epoch {epoch+1}, Val Acc: {val_acc:.4f}")

        return self.encoder, classifier

    def evaluate(self, data, classifier):
        """è¯„ä¼°"""
        self.encoder.eval()
        classifier.eval()

        correct = 0
        total = 0

        with torch.no_grad():
            for batch in data:
                x = batch.x.to(self.device)
                edge_index = batch.edge_index.to(self.device)
                labels = batch.y.to(self.device)

                embeddings = self.encoder(x, edge_index)
                logits = classifier(embeddings)
                pred = logits.argmax(dim=1)

                correct += (pred == labels).sum().item()
                total += labels.size(0)

        return correct / total
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **GNN** | Graph Neural Network | å›¾ç¥ç»ç½‘ç»œ |
| **è‡ªç›‘ç£å­¦ä¹ ** | Self-Supervised Learning | æ— éœ€æ ‡æ³¨çš„å­¦ä¹ æ–¹å¼ |
| **å¯¹æ¯”å­¦ä¹ ** | Contrastive Learning | é€šè¿‡æ­£è´Ÿæ ·æœ¬å¯¹æ¯”å­¦ä¹  |
| **çŸ¥è¯†è’¸é¦** | Knowledge Distillation | å¤§æ¨¡å‹çŸ¥è¯†è¿ç§»åˆ°å°æ¨¡å‹ |
| **InfoNCE** | InfoNCE Loss | å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•° |
| **æ•°æ®å¢å¼º** | Data Augmentation | å¯¹å›¾è¿›è¡Œå˜æ¢ç”Ÿæˆæ–°æ ·æœ¬ |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å›¾è‡ªç›‘ç£å­¦ä¹ çš„åŠ¨æœº
- [ ] æŒæ¡å›¾æ•°æ®å¢å¼ºæ–¹æ³•
- [ ] ç†è§£å¯¹æ¯”å­¦ä¹ åœ¨å›¾ä¸Šçš„åº”ç”¨
- [ ] äº†è§£çŸ¥è¯†è’¸é¦åœ¨GNNä¸­çš„ä½œç”¨
- [ ] èƒ½å¤Ÿå®ç°åŸºæœ¬çš„å›¾å¯¹æ¯”å­¦ä¹ 

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **å›¾æ•°æ®å¢å¼ºä¸å›¾åƒå¢å¼ºæœ‰ä½•ä¸åŒï¼Ÿ**
   - æç¤º: ç»“æ„ vs åƒç´ 

2. **ä¸ºä»€ä¹ˆå¯¹æ¯”å­¦ä¹ é€‚ç”¨äºå›¾æ•°æ®ï¼Ÿ**
   - æç¤º: ç»“æ„ç›¸ä¼¼æ€§

3. **çŸ¥è¯†è’¸é¦å¦‚ä½•å¸®åŠ©GNNè½»é‡åŒ–ï¼Ÿ**
   - æç¤º: æ¨¡å‹å‹ç¼©

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
