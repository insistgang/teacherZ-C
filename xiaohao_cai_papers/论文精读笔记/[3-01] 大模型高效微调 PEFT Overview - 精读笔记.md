# [3-01] å¤§æ¨¡å‹é«˜æ•ˆå¾®è°ƒ PEFT Overview - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Parameter-Efficient Fine-Tuning (PEFT) for Large Language Models: A Survey
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (ä¸­é«˜ï¼Œæ¶‰åŠå¤šç§æŠ€æœ¯)
> **é‡è¦æ€§**: â­â­â­â­ (LLMå¾®è°ƒæ ¸å¿ƒæŠ€æœ¯)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Parameter-Efficient Fine-Tuning (PEFT) for Large Language Models: A Survey / Methods |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **å‘è¡¨ä¼šè®®/æœŸåˆŠ** | ç»¼è¿°/æ–¹æ³•è®ºæ–‡ (2023-2024) |
| **å…³é”®è¯** | PEFT, LoRA, Adapter, Prompt Tuning, LLM Fine-tuning |
| **æ ¸å¿ƒä»·å€¼** | ç³»ç»Ÿæ¢³ç†å¤§æ¨¡å‹é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä½“ç³» |

---

## ğŸ¯ ç ”ç©¶èƒŒæ™¯ä¸é—®é¢˜

### å¤§æ¨¡å‹å¾®è°ƒæŒ‘æˆ˜

```
ä¼ ç»Ÿå…¨å‚æ•°å¾®è°ƒçš„é—®é¢˜:
â”œâ”€â”€ å‚æ•°é‡å·¨å¤§ (GPT-3: 175B, LLaMA: 65B)
â”œâ”€â”€ æ˜¾å­˜éœ€æ±‚é«˜ (éœ€è¦å¤šå¡A100)
â”œâ”€â”€ è®­ç»ƒæ—¶é—´é•¿ (æ•°å¤©åˆ°æ•°å‘¨)
â”œâ”€â”€ å­˜å‚¨æˆæœ¬é«˜ (æ¯ä¸ªä»»åŠ¡éœ€ä¿å­˜å®Œæ•´æ¨¡å‹)
â””â”€â”€ ç¾éš¾æ€§é—å¿˜ (è¦†ç›–é¢„è®­ç»ƒçŸ¥è¯†)

PEFTè§£å†³æ–¹æ¡ˆ:
â”œâ”€â”€ åªå¾®è°ƒå°‘é‡å‚æ•° (0.1% - 1%)
â”œâ”€â”€ æ˜¾å­˜éœ€æ±‚é™ä½ (å•å¡å¯è®­ç»ƒ)
â”œâ”€â”€ è®­ç»ƒé€Ÿåº¦æå‡ (æ•°å°æ—¶å®Œæˆ)
â”œâ”€â”€ å¤šä»»åŠ¡å…±äº«åº•åº§ (åªå­˜é€‚é…å™¨)
â””â”€â”€ ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†
```

### PEFTæ–¹æ³•åˆ†ç±»

```
PEFTæ–¹æ³•ä½“ç³»:
â”‚
â”œâ”€â”€ æ·»åŠ å‚æ•°ç±» (Additive Methods)
â”‚   â”œâ”€â”€ Adapter: æ’å…¥å°å‹é€‚é…å±‚
â”‚   â”œâ”€â”€ LoRA: ä½ç§©é€‚é…
â”‚   â”œâ”€â”€ (IA)Â³: å­¦ä¹ ç¼©æ”¾å‘é‡
â”‚   â””â”€â”€ Prefix Tuning: æ·»åŠ å‰ç¼€åµŒå…¥
â”‚
â”œâ”€â”€ é€‰æ‹©å‚æ•°ç±» (Selective Methods)
â”‚   â”œâ”€â”€ BitFit: åªå¾®è°ƒåç½®é¡¹
â”‚   â”œâ”€â”€ Diff Pruning: ç¨€ç–å·®åˆ†æ›´æ–°
â”‚   â””â”€â”€ å±‚é€‰æ‹©: åªå¾®è°ƒç‰¹å®šå±‚
â”‚
â””â”€â”€ é‡å‚æ•°åŒ–ç±» (Reparameterized Methods)
    â”œâ”€â”€ LoRAç³»åˆ—: ä½ç§©åˆ†è§£
    â”œâ”€â”€ Tensor-based: å¼ é‡åˆ†è§£ (tCURLoRA)
    â””â”€â”€ Kronecker Product: å…‹ç½—å†…å…‹ç§¯åˆ†è§£
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ–¹æ³•ä¸€: LoRA (Low-Rank Adaptation)

#### æ ¸å¿ƒæ€æƒ³

```
é¢„è®­ç»ƒæƒé‡: Wâ‚€ âˆˆ â„^{dÃ—k}
ä¼ ç»Ÿå¾®è°ƒ: W = Wâ‚€ + Î”W (æ›´æ–°å…¨éƒ¨å‚æ•°)

LoRA:
W = Wâ‚€ + Î”W = Wâ‚€ + BA
å…¶ä¸­:
  - B âˆˆ â„^{dÃ—r}
  - A âˆˆ â„^{rÃ—k}
  - r â‰ª min(d, k) (é€šå¸¸ r=4,8,16)

å‚æ•°é‡: dÃ—k â†’ rÃ—(d+k)
å‹ç¼©æ¯”: ~dÃ—k / (rÃ—(d+k))
```

#### æ•°å­¦å…¬å¼

```python
# LoRAå‰å‘ä¼ æ’­
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class LoRALayer(nn.Module):
    """
    LoRA (Low-Rank Adaptation) å®ç°
    """
    def __init__(
        self,
        in_features: int,
        out_features: int,
        rank: int = 8,
        lora_alpha: float = 16,
        lora_dropout: float = 0.0
    ):
        super().__init__()

        self.rank = rank
        self.lora_alpha = lora_alpha
        self.scaling = lora_alpha / rank

        # å†»ç»“çš„é¢„è®­ç»ƒæƒé‡
        self.weight = nn.Parameter(torch.zeros(out_features, in_features))
        self.weight.requires_grad = False

        # LoRAå¯è®­ç»ƒå‚æ•°
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) / math.sqrt(in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))

        self.lora_dropout = nn.Dropout(lora_dropout) if lora_dropout > 0 else nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­: y = x @ Wâ‚€áµ€ + x @ (B @ A)áµ€ * scaling
        """
        # åŸå§‹è¾“å‡º (å†»ç»“)
        original_out = F.linear(x, self.weight)

        # LoRAåˆ†æ”¯ (å¯è®­ç»ƒ)
        lora_out = F.linear(F.linear(self.lora_dropout(x), self.lora_A.t()), self.lora_B.t())

        return original_out + lora_out * self.scaling


class LinearWithLoRA(nn.Module):
    """
    å°†æ™®é€šLinearå±‚è½¬æ¢ä¸ºLoRAå±‚
    """
    def __init__(self, linear_layer: nn.Linear, rank: int = 8, lora_alpha: float = 16):
        super().__init__()

        self.in_features = linear_layer.in_features
        self.out_features = linear_layer.out_features

        # å†»ç»“åŸå§‹æƒé‡
        self.weight = nn.Parameter(linear_layer.weight.data.clone())
        self.weight.requires_grad = False

        if linear_layer.bias is not None:
            self.bias = nn.Parameter(linear_layer.bias.data.clone())
            self.bias.requires_grad = False
        else:
            self.register_parameter('bias', None)

        # æ·»åŠ LoRAå‚æ•°
        self.lora_A = nn.Parameter(torch.randn(rank, self.in_features))
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))

        self.lora_B = nn.Parameter(torch.zeros(self.out_features, rank))

        self.scaling = lora_alpha / rank

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # åŸå§‹è¾“å‡º
        out = F.linear(x, self.weight, self.bias)

        # LoRAé€‚é…
        out += F.linear(F.linear(x, self.lora_A.t()), self.lora_B.t()) * self.scaling

        return out
```

#### LoRAä¼˜åŠ¿

```
ä¼˜åŠ¿:
â”œâ”€â”€ å‚æ•°é«˜æ•ˆ: åªè®­ç»ƒ0.1%-1%å‚æ•°
â”œâ”€â”€ æ— æ¨ç†å¼€é”€: å¯åˆå¹¶å›åŸå§‹æƒé‡
â”œâ”€â”€ æ¨¡å—åŒ–: ä¸åŒä»»åŠ¡åˆ‡æ¢ä¸åŒLoRA
â”œâ”€â”€ å­˜å‚¨å‹å¥½: æ¯ä¸ªä»»åŠ¡åªå­˜å°çŸ©é˜µ
â””â”€â”€ æ•ˆæœæ¥è¿‘: æ€§èƒ½ä¸å…¨å¾®è°ƒç›¸å½“

åº”ç”¨ä½ç½®:
â”œâ”€â”€ Transformer: åªåº”ç”¨äºQ, VæŠ•å½±
â”œâ”€â”€ æ¨è: W_q, W_v (æ³¨æ„åŠ›æŸ¥è¯¢å’Œå€¼)
â”œâ”€â”€ å¯é€‰: W_k, W_o, FFNå±‚
â””â”€â”€ é¿å…: Embeddingå’Œè¾“å‡ºå±‚
```

---

### æ–¹æ³•äºŒ: Adapter

#### æ ¸å¿ƒæ€æƒ³

```
åœ¨Transformerå±‚ä¹‹é—´æ’å…¥å°å‹é€‚é…æ¨¡å—:

åŸå§‹: x â†’ Attention â†’ FFN â†’ Output
æ·»åŠ Adapter: x â†’ Attention â†’ Adapter â†’ FFN â†’ Adapter â†’ Output

Adapterç»“æ„:
  è¾“å…¥ â†’ Down-project (dâ†’r) â†’ ReLU â†’ Up-project (râ†’d) â†’ Output
       + Skip Connection

å‚æ•°é‡: 2 Ã— d Ã— r (é€šå¸¸ r=64)
```

#### å®ç°ä»£ç 

```python
class Adapter(nn.Module):
    """
    Adapteræ¨¡å—: ç“¶é¢ˆç»“æ„é€‚é…å™¨
    """
    def __init__(
        self,
        hidden_size: int,
        adapter_dim: int = 64,
        dropout: float = 0.1
    ):
        super().__init__()

        self.down_project = nn.Linear(hidden_size, adapter_dim)
        self.activation = nn.GELU()
        self.up_project = nn.Linear(adapter_dim, hidden_size)
        self.dropout = nn.Dropout(dropout)

        # åˆå§‹åŒ–: æ¥è¿‘æ’ç­‰æ˜ å°„
        nn.init.xavier_uniform_(self.down_project.weight)
        nn.init.zeros_(self.up_project.weight)
        nn.init.zeros_(self.up_project.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        """
        residual = x

        x = self.down_project(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.up_project(x)

        return x + residual  # æ®‹å·®è¿æ¥


class TransformerLayerWithAdapter(nn.Module):
    """
    å¸¦Adapterçš„Transformerå±‚
    """
    def __init__(self, d_model: int, nhead: int, adapter_dim: int = 64):
        super().__init__()

        # åŸå§‹Transformerç»„ä»¶
        self.self_attn = nn.MultiheadAttention(d_model, nhead)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )

        # LayerNorm
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Adapteræ¨¡å—
        self.adapter_after_attn = Adapter(d_model, adapter_dim)
        self.adapter_after_ffn = Adapter(d_model, adapter_dim)

    def forward(self, x: torch.Tensor, mask=None):
        # Attentionå­å±‚
        attn_out, _ = self.self_attn(x, x, x, attn_mask=mask)
        x = self.norm1(x + attn_out)
        x = self.adapter_after_attn(x)  # æ·»åŠ Adapter

        # FFNå­å±‚
        ff_out = self.feed_forward(x)
        x = self.norm2(x + ff_out)
        x = self.adapter_after_ffn(x)  # æ·»åŠ Adapter

        return x
```

---

### æ–¹æ³•ä¸‰: Prompt Tuning / Prefix Tuning

#### æ ¸å¿ƒæ€æƒ³

```
ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œè€Œæ˜¯ä¿®æ”¹è¾“å…¥:

Prompt Tuning:
  è¾“å…¥: [å¯è®­ç»ƒè½¯æç¤º] + [çœŸå®è¾“å…¥]
  ä¾‹å¦‚: [P1][P2]...[Pk] + "ç¿»è¯‘è¿™å¥è¯"

Prefix Tuning:
  åœ¨æ¯å±‚æ³¨æ„åŠ›å‰æ·»åŠ å¯è®­ç»ƒå‰ç¼€:
  [Prefix_K] â†’ Key
  [Prefix_V] â†’ Value

ä¼˜åŠ¿:
  - å®Œå…¨ä¸ä¿®æ”¹æ¨¡å‹
  - å¯è®­ç»ƒå‚æ•°é‡æå°‘
  - ä»»åŠ¡åˆ‡æ¢åªéœ€æ¢prompt
```

#### å®ç°ä»£ç 

```python
class PrefixTuning(nn.Module):
    """
    Prefix Tuning: åœ¨æ³¨æ„åŠ›å±‚æ·»åŠ å¯è®­ç»ƒå‰ç¼€
    """
    def __init__(
        self,
        num_layers: int,
        num_heads: int,
        embed_dim: int,
        prefix_length: int = 20
    ):
        super().__init__()

        self.num_layers = num_layers
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.prefix_length = prefix_length

        # æ¯å±‚çš„prefixå‚æ•°
        # shape: (num_layers, 2, num_heads, prefix_length, head_dim)
        # 2 for key and value
        self.prefix_tokens = nn.Parameter(
            torch.randn(num_layers, 2, num_heads, prefix_length, self.head_dim)
        )

        # MLPé‡å‚æ•°åŒ– (å¯é€‰ï¼Œæé«˜ç¨³å®šæ€§)
        self.prefix_mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.Tanh(),
            nn.Linear(embed_dim // 2, num_layers * 2 * embed_dim)
        )

    def forward(self, batch_size: int, device: torch.device):
        """
        ç”Ÿæˆprefix embedding

        Returns:
            past_key_values: ç”¨äºTransformerçš„past_key_values
        """
        # ç›´æ¥ä½¿ç”¨prefix tokens
        prefix = self.prefix_tokens.unsqueeze(0)  # (1, num_layers, 2, num_heads, prefix_len, head_dim)
        prefix = prefix.expand(batch_size, -1, -1, -1, -1, -1)

        # è½¬æ¢ä¸ºpast_key_valuesæ ¼å¼
        past_key_values = []
        for layer_idx in range(self.num_layers):
            key = prefix[:, layer_idx, 0]  # (batch, num_heads, prefix_len, head_dim)
            value = prefix[:, layer_idx, 1]
            past_key_values.append((key, value))

        return past_key_values


class PromptTuning(nn.Module):
    """
    Prompt Tuning: åœ¨è¾“å…¥å‰æ·»åŠ å¯è®­ç»ƒè½¯æç¤º
    """
    def __init__(
        self,
        num_tokens: int,
        token_dim: int,
        num_prompts: int = 100
    ):
        super().__init__()

        self.num_prompts = num_prompts

        # å¯è®­ç»ƒçš„è½¯æç¤ºåµŒå…¥
        self.prompt_embeddings = nn.Parameter(
            torch.randn(num_prompts, token_dim)
        )

        # æç¤ºåˆ°tokençš„æ˜ å°„
        self.prompt_projection = nn.Linear(token_dim, num_tokens * token_dim)

    def forward(self, input_embeds: torch.Tensor, prompt_id: int = 0):
        """
        å°†è½¯æç¤ºä¸è¾“å…¥åµŒå…¥æ‹¼æ¥

        Args:
            input_embeds: (batch, seq_len, dim)
            prompt_id: ä½¿ç”¨çš„æç¤ºID

        Returns:
            combined_embeds: (batch, num_tokens + seq_len, dim)
        """
        batch_size = input_embeds.size(0)

        # è·å–æç¤ºåµŒå…¥
        prompt_embed = self.prompt_embeddings[prompt_id]

        # æŠ•å½±åˆ°å¤šä¸ªtoken
        prompt_tokens = self.prompt_projection(prompt_embed)
        prompt_tokens = prompt_tokens.view(self.num_prompts, -1, input_embeds.size(-1))

        # æ‰©å±•åˆ°batch
        prompt_embeds = prompt_tokens.unsqueeze(0).expand(batch_size, -1, -1)

        # æ‹¼æ¥
        combined = torch.cat([prompt_embeds, input_embeds], dim=1)

        return combined
```

---

### æ–¹æ³•å››: (IA)Â³ (Infused Adapter by Inhibiting and Amplifying Inner Activations)

#### æ ¸å¿ƒæ€æƒ³

```
å­¦ä¹ ç¼©æ”¾å‘é‡è€Œéæ·»åŠ æ–°å‚æ•°:

å¯¹Transformerä¸­çš„Key, Value, FFNè¾“å‡ºè¿›è¡Œç¼©æ”¾:
  Key = Key âŠ™ l_k
  Value = Value âŠ™ l_v
  FFN_out = FFN_out âŠ™ l_ff

å…¶ä¸­ l_k, l_v, l_ff æ˜¯å¯å­¦ä¹ çš„ç¼©æ”¾å‘é‡ (é€å…ƒç´ )

å‚æ•°é‡: 3 Ã— d (æ¯”Adapterå’ŒLoRAæ›´å°‘)
```

#### å®ç°ä»£ç 

```python
class IA3Layer(nn.Module):
    """
    (IA)Â³: å­¦ä¹ ç¼©æ”¾å‘é‡
    """
    def __init__(self, hidden_size: int):
        super().__init__()

        # å¯å­¦ä¹ çš„ç¼©æ”¾å‘é‡
        self.scale_k = nn.Parameter(torch.ones(hidden_size))
        self.scale_v = nn.Parameter(torch.ones(hidden_size))
        self.scale_ff = nn.Parameter(torch.ones(hidden_size))

    def forward(self, key, value, ff_output):
        """
        åº”ç”¨å­¦ä¹ åˆ°ç¼©æ”¾
        """
        key_scaled = key * self.scale_k
        value_scaled = value * self.scale_v
        ff_scaled = ff_output * self.scale_ff

        return key_scaled, value_scaled, ff_scaled


class IA3Transformer(nn.Module):
    """
    é›†æˆ(IA)Â³çš„Transformer
    """
    def __init__(self, d_model: int, nhead: int):
        super().__init__()

        self.attention = nn.MultiheadAttention(d_model, nhead)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )

        # (IA)Â³ç¼©æ”¾
        self.ia3 = IA3Layer(d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        # Attention
        attn_out, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(x + attn_out)

        # FFN
        ff_out = self.ffn(x)

        # åº”ç”¨(IA)Â³ç¼©æ”¾
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”åœ¨attentionå†…éƒ¨åº”ç”¨
        _, _, ff_scaled = self.ia3(x, x, ff_out)

        x = self.norm2(x + ff_scaled)

        return x
```

---

## ğŸ“Š æ–¹æ³•å¯¹æ¯”

### å‚æ•°é‡å¯¹æ¯”

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•°é‡ | ç›¸å¯¹å…¨å¾®è°ƒ | é€‚ç”¨åœºæ™¯ |
|:---|:---|:---:|:---|
| **Full Fine-tuning** | 100% | 100% | æ•°æ®å……è¶³ï¼Œè®¡ç®—èµ„æºå……è¶³ |
| **BitFit** | ~0.1% | åç½®é¡¹ | æå°‘é‡å‚æ•°åœºæ™¯ |
| **Prompt Tuning** | ~0.01% | è½¯æç¤º | åˆ†ç±»ä»»åŠ¡ï¼Œå¤šä»»åŠ¡ |
| **Prefix Tuning** | ~0.1% | å‰ç¼€åµŒå…¥ | ç”Ÿæˆä»»åŠ¡ |
| **Adapter** | ~0.5-2% | é€‚é…å±‚ | å¤šä»»åŠ¡ï¼Œæ¨¡å—åŒ– |
| **LoRA** | ~0.1-1% | ä½ç§©çŸ©é˜µ | é€šç”¨ï¼Œæ¨èé¦–é€‰ |
| **(IA)Â³** | ~0.01% | ç¼©æ”¾å‘é‡ | è¶…è½»é‡çº§é€‚é… |
| **tCURLoRA** | ~0.1-1% | å¼ é‡åˆ†è§£ | é«˜ç»´å‚æ•°ç»“æ„ |

### æ€§èƒ½å¯¹æ¯”

```
å…¸å‹ä»»åŠ¡æ€§èƒ½ (ç›¸å¯¹äºå…¨å¾®è°ƒ):

GLUE Benchmark:
â”œâ”€â”€ Full Fine-tuning: 100% (baseline)
â”œâ”€â”€ LoRA (r=8): 98-99%
â”œâ”€â”€ Adapter: 97-98%
â”œâ”€â”€ Prefix Tuning: 95-97%
â””â”€â”€ Prompt Tuning: 92-95%

æ³¨æ„:
- ä»»åŠ¡è¶Šå¤æ‚ï¼Œå·®è·å¯èƒ½è¶Šå¤§
- é€‚å½“å¢å¤§rankå¯ç¼©å°å·®è·
- ç»„åˆæ–¹æ³•é€šå¸¸æ•ˆæœæ›´å¥½
```

---

## ğŸ’» å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: é€šç”¨PEFTåŒ…è£…å™¨

```python
import torch
import torch.nn as nn
from typing import Optional, List, Dict
from enum import Enum

class PEFTMethod(Enum):
    LORA = "lora"
    ADAPTER = "adapter"
    PREFIX_TUNING = "prefix_tuning"
    IA3 = "ia3"


class PEFTWrapper:
    """
    é€šç”¨PEFTæ–¹æ³•åŒ…è£…å™¨

    è‡ªåŠ¨ä¸ºæ¨¡å‹æ·»åŠ PEFTé€‚é…
    """

    def __init__(
        self,
        model: nn.Module,
        method: PEFTMethod,
        config: Dict
    ):
        self.model = model
        self.method = method
        self.config = config

        # å†»ç»“åŸå§‹å‚æ•°
        self._freeze_base_model()

        # æ·»åŠ PEFTæ¨¡å—
        self._add_peft_modules()

    def _freeze_base_model(self):
        """å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°"""
        for param in self.model.parameters():
            param.requires_grad = False

    def _add_peft_modules(self):
        """æ ¹æ®æ–¹æ³•æ·»åŠ é€‚é…æ¨¡å—"""
        if self.method == PEFTMethod.LORA:
            self._apply_lora()
        elif self.method == PEFTMethod.ADAPTER:
            self._apply_adapter()
        elif self.method == PEFTMethod.PREFIX_TUNING:
            self._apply_prefix_tuning()
        elif self.method == PEFTMethod.IA3:
            self._apply_ia3()

    def _apply_lora(self):
        """åº”ç”¨LoRA"""
        target_modules = self.config.get('target_modules', ['q', 'v'])
        rank = self.config.get('rank', 8)
        alpha = self.config.get('alpha', 16)

        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¨¡å—
                if any(target in name for target in target_modules):
                    # æ›¿æ¢ä¸ºLoRAå±‚
                    lora_layer = LinearWithLoRA(module, rank, alpha)
                    parent_name = '.'.join(name.split('.')[:-1])
                    child_name = name.split('.')[-1]
                    parent = self.model.get_submodule(parent_name)
                    setattr(parent, child_name, lora_layer)

    def _apply_adapter(self):
        """åº”ç”¨Adapter"""
        adapter_dim = self.config.get('adapter_dim', 64)

        # åœ¨Transformerå±‚åæ·»åŠ Adapter
        for name, module in self.model.named_modules():
            if 'Transformer' in type(module).__name__:
                # æ·»åŠ Adapter
                pass  # å…·ä½“å®ç°å–å†³äºæ¨¡å‹ç»“æ„

    def _apply_prefix_tuning(self):
        """åº”ç”¨Prefix Tuning"""
        prefix_length = self.config.get('prefix_length', 20)
        # å®ç°ç•¥
        pass

    def _apply_ia3(self):
        """åº”ç”¨(IA)Â³"""
        # å®ç°ç•¥
        pass

    def get_trainable_parameters(self):
        """è·å–å¯è®­ç»ƒå‚æ•°"""
        return [p for p in self.model.parameters() if p.requires_grad]

    def print_trainable_parameters(self):
        """æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯"""
        trainable_params = 0
        all_params = 0

        for _, param in self.model.named_parameters():
            all_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()

        print(f"Trainable params: {trainable_params:,} || "
              f"All params: {all_params:,} || "
              f"Trainable %: {100 * trainable_params / all_params:.2f}%")

    def save_adapter(self, path: str):
        """åªä¿å­˜é€‚é…å™¨å‚æ•°"""
        adapter_state = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                adapter_state[name] = param.data.cpu()

        torch.save(adapter_state, path)
        print(f"Adapter saved to {path}")

    def load_adapter(self, path: str):
        """åŠ è½½é€‚é…å™¨å‚æ•°"""
        adapter_state = torch.load(path)
        self.model.load_state_dict(adapter_state, strict=False)
        print(f"Adapter loaded from {path}")


def apply_peft_to_model(
    model: nn.Module,
    method: str = "lora",
    **kwargs
) -> nn.Module:
    """
    ä¾¿æ·å‡½æ•°: ä¸ºæ¨¡å‹æ·»åŠ PEFT

    Args:
        model: åŸå§‹æ¨¡å‹
        method: PEFTæ–¹æ³• (lora, adapter, prefix, ia3)
        **kwargs: æ–¹æ³•ç‰¹å®šå‚æ•°

    Returns:
        å¸¦æœ‰PEFTçš„æ¨¡å‹
    """
    method_enum = PEFTMethod(method)
    wrapper = PEFTWrapper(model, method_enum, kwargs)
    wrapper.print_trainable_parameters()

    return wrapper.model
```

### ç»„ä»¶2: LoRAé…ç½®å·¥å…·

```python
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class LoRAConfig:
    """
    LoRAé…ç½®
    """
    r: int = 8  # ç§©
    lora_alpha: int = 16  # ç¼©æ”¾å‚æ•°
    lora_dropout: float = 0.0
    target_modules: List[str] = None
    bias: str = "none"  # none, all, lora_only
    modules_to_save: List[str] = None  # é¢å¤–è®­ç»ƒçš„æ¨¡å—

    def __post_init__(self):
        if self.target_modules is None:
            self.target_modules = ["q_proj", "v_proj"]


# æ¨èé…ç½®
LORA_CONFIGS = {
    "light": LoRAConfig(r=4, lora_alpha=8),
    "default": LoRAConfig(r=8, lora_alpha=16),
    "heavy": LoRAConfig(r=16, lora_alpha=32),
    "ultra": LoRAConfig(r=32, lora_alpha=64),
}
```

---

## ğŸ§ª åº”ç”¨åˆ°äº•ç›–æ£€æµ‹

### åœºæ™¯: ä½¿ç”¨é¢„è®­ç»ƒè§†è§‰æ¨¡å‹è¿›è¡Œäº•ç›–æ£€æµ‹å¾®è°ƒ

```python
class ManholePEFTDetector:
    """
    ä½¿ç”¨PEFTè¿›è¡Œäº•ç›–æ£€æµ‹
    """

    def __init__(self, pretrained_backbone: nn.Module):
        # åŠ è½½é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ (å¦‚ResNet, ViT)
        self.backbone = pretrained_backbone

        # å†»ç»“éª¨å¹²ç½‘ç»œ
        for param in self.backbone.parameters():
            param.requires_grad = False

        # æ·»åŠ LoRAé€‚é…å™¨
        self._apply_lora_to_backbone()

        # æ£€æµ‹å¤´ (å§‹ç»ˆå¯è®­ç»ƒ)
        self.detection_head = nn.Sequential(
            nn.Linear(2048, 512),
            nn.ReLU(inplace=True),
            nn.Linear(512, 5)  # x, y, w, h, confidence
        )

    def _apply_lora_to_backbone(self):
        """åœ¨éª¨å¹²ç½‘ç»œä¸Šåº”ç”¨LoRA"""
        # æ‰¾åˆ°æ‰€æœ‰Linearå±‚å¹¶æ·»åŠ LoRA
        for name, module in self.backbone.named_modules():
            if isinstance(module, nn.Linear):
                # æ›¿æ¢ä¸ºLoRAå±‚
                parent = self._get_parent_module(name)
                child_name = name.split('.')[-1]
                lora_layer = LinearWithLoRA(module, rank=8, lora_alpha=16)
                setattr(parent, child_name, lora_layer)

    def _get_parent_module(self, name: str):
        """è·å–çˆ¶æ¨¡å—"""
        parts = name.split('.')[:-1]
        module = self.backbone
        for part in parts:
            module = getattr(module, part)
        return module

    def forward(self, x):
        features = self.backbone(x)
        detections = self.detection_head(features)
        return detections

    def save_checkpoint(self, path: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹ (åªåŒ…å«LoRAå’Œæ£€æµ‹å¤´)"""
        checkpoint = {
            'lora_params': {
                k: v for k, v in self.backbone.named_parameters()
                if v.requires_grad
            },
            'head_params': self.detection_head.state_dict()
        }
        torch.save(checkpoint, path)

    def load_checkpoint(self, path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(path)
        self.backbone.load_state_dict(checkpoint['lora_params'], strict=False)
        self.detection_head.load_state_dict(checkpoint['head_params'])
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **PEFT** | Parameter-Efficient Fine-Tuning | å‚æ•°é«˜æ•ˆå¾®è°ƒ |
| **LoRA** | Low-Rank Adaptation | ä½ç§©é€‚é… |
| **Adapter** | Adapter | é€‚é…å™¨ï¼Œç“¶é¢ˆç»“æ„ |
| **Prompt Tuning** | Prompt Tuning | è½¯æç¤ºå¾®è°ƒ |
| **Prefix Tuning** | Prefix Tuning | å‰ç¼€å¾®è°ƒ |
| **(IA)Â³** | Infused Adapter | ç¼©æ”¾å‘é‡é€‚é… |
| **Rank** | Rank | ä½ç§©åˆ†è§£çš„ç§© |
| **Alpha** | Alpha | LoRAç¼©æ”¾ç³»æ•° |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£PEFTçš„æ ¸å¿ƒåŠ¨æœº
- [ ] æŒæ¡LoRAçš„åŸç†å’Œå®ç°
- [ ] äº†è§£Adapterçš„ç»“æ„
- [ ] ç†è§£Prompt/Prefix Tuningçš„åŒºåˆ«
- [ ] èƒ½é€‰æ‹©åˆé€‚çš„PEFTæ–¹æ³•
- [ ] èƒ½å°†PEFTåº”ç”¨åˆ°è§†è§‰ä»»åŠ¡

---

## ğŸ”— ç›¸å…³è®ºæ–‡æ¨è

### å¿…è¯»
1. **LoRA** (ICLR 2022) - ä½ç§©é€‚é…
2. **Prefix Tuning** (ACL 2021) - å‰ç¼€å¾®è°ƒ
3. **Adapter** (ICML 2019) - é€‚é…å™¨

### æ‰©å±•é˜…è¯»
1. **(IA)Â³** (EMNLP 2022) - ç¼©æ”¾å‘é‡
2. **Prompt Tuning** (EMNLP 2021) - è½¯æç¤º
3. **BitFit** (ACL 2022) - åç½®å¾®è°ƒ
4. **tCURLoRA** (ICML 2024) - å¼ é‡CURåˆ†è§£é€‚é…

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
**ä¸‹ä¸€æ­¥**: åœ¨äº•ç›–æ£€æµ‹ä¸Šå®è·µLoRAå¾®è°ƒ
