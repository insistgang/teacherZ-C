# [3-12] å¤šå±‚æ¬¡XAIè§£é‡Š Multilevel XAI - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Multilevel Explainable AI for Multimodal Data Analysis
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (ä¸­é«˜)
> **é‡è¦æ€§**: â­â­â­â­ (é‡è¦ï¼Œå¤šæ¨¡æ€å¯è§£é‡ŠAI)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Multilevel Explainable AI for Multimodal Data Analysis |
| **ä½œè€…** | X. Cai ç­‰äºº |
| **å‘è¡¨æœŸåˆŠ** | IEEE Transactions on Multimedia |
| **å‘è¡¨å¹´ä»½** | 2023 |
| **å…³é”®è¯** | Explainable AI, Multimodal, Multilevel, Attention Visualization, Concept Explanation |
| **ä»£ç ** | (è¯·æŸ¥çœ‹è®ºæ–‡æ˜¯å¦æœ‰å¼€æºä»£ç ) |

---

## ğŸ¯ ç ”ç©¶é—®é¢˜ä¸åŠ¨æœº

### å¯è§£é‡ŠAIçš„æŒ‘æˆ˜

**å•å±‚æ¬¡è§£é‡Šçš„å±€é™**:
```
åƒç´ çº§è§£é‡Š (å¦‚Saliency Map):
- æ˜¾ç¤º"å“ªé‡Œ"é‡è¦
- ä½†ä¸è§£é‡Š"ä¸ºä»€ä¹ˆ"é‡è¦
- éš¾ä»¥ç†è§£é«˜å±‚è¯­ä¹‰

ç‰¹å¾çº§è§£é‡Š (å¦‚SHAP):
- æ˜¾ç¤ºç‰¹å¾è´¡çŒ®
- ä½†ç¼ºä¹æ•´ä½“ç†è§£
- éš¾ä»¥å…³è”åˆ°è¯­ä¹‰æ¦‚å¿µ
```

**å¤šå±‚æ¬¡è§£é‡Šçš„éœ€æ±‚**:
```
ä¸åŒç”¨æˆ·éœ€è¦ä¸åŒå±‚æ¬¡è§£é‡Š:
- ç»ˆç«¯ç”¨æˆ·: é«˜å±‚è¯­ä¹‰è§£é‡Š
- é¢†åŸŸä¸“å®¶: ä¸­å±‚ç‰¹å¾è§£é‡Š
- å¼€å‘è€…: åº•å±‚åƒç´ /æƒé‡è§£é‡Š

å¤šæ¨¡æ€æ•°æ®çš„å¤æ‚æ€§:
- å›¾åƒ + æ–‡æœ¬ + éŸ³é¢‘
- éœ€è¦ç»Ÿä¸€çš„è§£é‡Šæ¡†æ¶
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ•´ä½“æ¡†æ¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å¤šå±‚æ¬¡XAIè§£é‡Šæ¡†æ¶                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  å¤šæ¨¡æ€è¾“å…¥                                              â”‚
â”‚  â”œâ”€ å›¾åƒ: I âˆˆ R^(HÃ—WÃ—3)                                 â”‚
â”‚  â”œâ”€ æ–‡æœ¬: T âˆˆ R^(LÃ—D)                                   â”‚
â”‚  â””â”€ éŸ³é¢‘: A âˆˆ R^(TÃ—F)                                   â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           å±‚æ¬¡1: åƒç´ /ç‰¹å¾çº§ (åº•å±‚)               â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   - æ¢¯åº¦çƒ­åŠ›å›¾ (Gradient Saliency)               â”‚   â”‚
â”‚  â”‚   - æ³¨æ„åŠ›å¯è§†åŒ– (Attention Map)                 â”‚   â”‚
â”‚  â”‚   - ç‰¹å¾é‡è¦æ€§ (Feature Importance)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           å±‚æ¬¡2: ç»„ä»¶/éƒ¨åˆ†çº§ (ä¸­å±‚)               â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   - éƒ¨ä»¶æ¿€æ´» (Part Activation)                   â”‚   â”‚
â”‚  â”‚   - çŸ­è¯­è´¡çŒ® (Phrase Contribution)               â”‚   â”‚
â”‚  â”‚   - ç‰‡æ®µé‡è¦æ€§ (Segment Importance)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           å±‚æ¬¡3: æ¦‚å¿µ/è¯­ä¹‰çº§ (é«˜å±‚)               â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   - æ¦‚å¿µæ¿€æ´»å‘é‡ (CAV)                           â”‚   â”‚
â”‚  â”‚   - è¯­ä¹‰æ¦‚å¿µè§£é‡Š                                 â”‚   â”‚
â”‚  â”‚   - å†³ç­–è§„åˆ™æå–                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           è·¨æ¨¡æ€è§£é‡Šå¯¹é½                          â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚   ç»Ÿä¸€è§£é‡Šç©ºé—´ä¸­çš„å¤šæ¨¡æ€å…³è”                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ ¸å¿ƒæ–¹æ³•1: å¤šå±‚æ¬¡è§£é‡Šç”Ÿæˆ

```python
class MultilevelExplainer:
    """
    å¤šå±‚æ¬¡è§£é‡Šç”Ÿæˆå™¨

    ç”Ÿæˆåƒç´ çº§ã€ç»„ä»¶çº§ã€æ¦‚å¿µçº§ä¸‰ä¸ªå±‚æ¬¡çš„è§£é‡Š
    """
    def __init__(self, model, concept_bank=None):
        self.model = model
        self.concept_bank = concept_bank or {}

    def explain(self, inputs, target_class=None):
        """
        ç”Ÿæˆå¤šå±‚æ¬¡è§£é‡Š

        Args:
            inputs: å¤šæ¨¡æ€è¾“å…¥ (image, text, audio)
            target_class: ç›®æ ‡ç±»åˆ«

        Returns:
            explanation: åŒ…å«ä¸‰ä¸ªå±‚æ¬¡è§£é‡Šçš„å­—å…¸
        """
        explanation = {
            'low_level': self._low_level_explain(inputs, target_class),
            'mid_level': self._mid_level_explain(inputs, target_class),
            'high_level': self._high_level_explain(inputs, target_class)
        }

        return explanation

    def _low_level_explain(self, inputs, target_class):
        """
        å±‚æ¬¡1: åƒç´ /ç‰¹å¾çº§è§£é‡Š
        """
        explanations = {}

        for modality, data in inputs.items():
            if modality == 'image':
                # æ¢¯åº¦çƒ­åŠ›å›¾
                saliency = self._gradient_saliency(data, target_class)
                # æ³¨æ„åŠ›å›¾
                attention = self._attention_visualization(data)
                explanations['image'] = {
                    'saliency': saliency,
                    'attention': attention
                }

            elif modality == 'text':
                # è¯é‡è¦æ€§
                word_importance = self._word_importance(data, target_class)
                explanations['text'] = {
                    'word_importance': word_importance
                }

        return explanations

    def _mid_level_explain(self, inputs, target_class):
        """
        å±‚æ¬¡2: ç»„ä»¶/éƒ¨åˆ†çº§è§£é‡Š
        """
        explanations = {}

        for modality, data in inputs.items():
            if modality == 'image':
                # éƒ¨ä»¶æ£€æµ‹ä¸æ¿€æ´»
                part_activation = self._part_activation(data, target_class)
                explanations['image'] = {
                    'part_activation': part_activation
                }

            elif modality == 'text':
                # çŸ­è¯­è´¡çŒ®
                phrase_contribution = self._phrase_contribution(data, target_class)
                explanations['text'] = {
                    'phrase_contribution': phrase_contribution
                }

        return explanations

    def _high_level_explain(self, inputs, target_class):
        """
        å±‚æ¬¡3: æ¦‚å¿µ/è¯­ä¹‰çº§è§£é‡Š
        """
        # æ¦‚å¿µæ¿€æ´»å‘é‡ (CAV)
        cav_explanations = self._compute_cav(inputs, target_class)

        # å†³ç­–è§„åˆ™
        decision_rules = self._extract_decision_rules(inputs, target_class)

        return {
            'concept_activation': cav_explanations,
            'decision_rules': decision_rules
        }

    def _gradient_saliency(self, image, target_class):
        """è®¡ç®—æ¢¯åº¦çƒ­åŠ›å›¾"""
        image.requires_grad = True

        output = self.model(image)
        if target_class is None:
            target_class = output.argmax(dim=1)

        # åå‘ä¼ æ’­
        output[0, target_class].backward()

        # æ¢¯åº¦ä½œä¸ºæ˜¾è‘—æ€§
        saliency = image.grad.abs().max(dim=1)[0]

        return saliency

    def _attention_visualization(self, image):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
        # è·å–æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æƒé‡
        attention_weights = []

        def hook_fn(module, input, output):
            attention_weights.append(output)

        # æ³¨å†Œhook
        handles = []
        for name, module in self.model.named_modules():
            if 'attention' in name.lower():
                handles.append(module.register_forward_hook(hook_fn))

        # å‰å‘ä¼ æ’­
        _ = self.model(image)

        # ç§»é™¤hooks
        for handle in handles:
            handle.remove()

        return attention_weights

    def _compute_cav(self, inputs, target_class):
        """
        è®¡ç®—æ¦‚å¿µæ¿€æ´»å‘é‡ (Concept Activation Vectors)

        å‚è€ƒ [3-11] æ¦‚å¿µçº§XAIæŒ‡æ ‡
        """
        cav_scores = {}

        for concept_name, concept_samples in self.concept_bank.items():
            # è®¡ç®—æ¦‚å¿µæ–¹å‘
            concept_activations = []
            for sample in concept_samples:
                act = self._get_layer_activation(sample)
                concept_activations.append(act)

            concept_vector = torch.stack(concept_activations).mean(dim=0)

            # è®¡ç®—ç›®æ ‡æ ·æœ¬çš„æ¿€æ´»
            target_activation = self._get_layer_activation(inputs)

            # CAVåˆ†æ•°: æ¦‚å¿µå‘é‡ä¸ç›®æ ‡æ¿€æ´»çš„ç›¸ä¼¼åº¦
            cav_score = F.cosine_similarity(
                concept_vector.unsqueeze(0),
                target_activation.unsqueeze(0)
            )

            cav_scores[concept_name] = cav_score.item()

        return cav_scores
```

---

### æ ¸å¿ƒæ–¹æ³•2: è·¨æ¨¡æ€è§£é‡Šå¯¹é½

```python
class CrossModalAlignment(nn.Module):
    """
    è·¨æ¨¡æ€è§£é‡Šå¯¹é½

    å°†ä¸åŒæ¨¡æ€çš„è§£é‡Šæ˜ å°„åˆ°ç»Ÿä¸€ç©ºé—´
    """
    def __init__(self, dim_per_modality: dict, unified_dim: int = 256):
        super().__init__()
        self.dim_per_modality = dim_per_modality
        self.unified_dim = unified_dim

        # ä¸ºæ¯ä¸ªæ¨¡æ€åˆ›å»ºæŠ•å½±
        self.projections = nn.ModuleDict({
            modality: nn.Sequential(
                nn.Linear(dim, unified_dim),
                nn.LayerNorm(unified_dim),
                nn.ReLU(),
                nn.Linear(unified_dim, unified_dim)
            )
            for modality, dim in dim_per_modality.items()
        })

    def forward(self, explanations: dict) -> dict:
        """
        å¯¹é½å¤šæ¨¡æ€è§£é‡Š

        Args:
            explanations: {modality: explanation_tensor}

        Returns:
            aligned: {modality: aligned_explanation}
        """
        aligned = {}

        for modality, explanation in explanations.items():
            if modality in self.projections:
                aligned[modality] = self.projections[modality](explanation)

        return aligned

    def compute_cross_modal_consistency(self, aligned_explanations: dict) -> torch.Tensor:
        """
        è®¡ç®—è·¨æ¨¡æ€ä¸€è‡´æ€§

        è¡¡é‡ä¸åŒæ¨¡æ€è§£é‡Šçš„ä¸€è‡´æ€§ç¨‹åº¦
        """
        modalities = list(aligned_explanations.keys())

        consistency_scores = []
        for i in range(len(modalities)):
            for j in range(i + 1, len(modalities)):
                mod_i = aligned_explanations[modalities[i]]
                mod_j = aligned_explanations[modalities[j]]

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = F.cosine_similarity(mod_i, mod_j, dim=-1)
                consistency_scores.append(similarity)

        return torch.stack(consistency_scores).mean()
```

---

### æ ¸å¿ƒæ–¹æ³•3: ç”¨æˆ·è‡ªé€‚åº”è§£é‡Š

```python
class UserAdaptiveExplanation:
    """
    ç”¨æˆ·è‡ªé€‚åº”è§£é‡Š

    æ ¹æ®ç”¨æˆ·ç±»å‹æä¾›ç›¸åº”å±‚æ¬¡çš„è§£é‡Š
    """
    def __init__(self, multilevel_explainer):
        self.explainer = multilevel_explainer

        # ç”¨æˆ·ç±»å‹é…ç½®
        self.user_profiles = {
            'end_user': {
                'levels': ['high_level'],
                'visualization': 'simple',
                'detail': 'low'
            },
            'domain_expert': {
                'levels': ['mid_level', 'high_level'],
                'visualization': 'detailed',
                'detail': 'medium'
            },
            'developer': {
                'levels': ['low_level', 'mid_level', 'high_level'],
                'visualization': 'technical',
                'detail': 'high'
            }
        }

    def explain_for_user(self, inputs, user_type='end_user', target_class=None):
        """
        ä¸ºç‰¹å®šç”¨æˆ·ç±»å‹ç”Ÿæˆè§£é‡Š

        Args:
            inputs: æ¨¡å‹è¾“å…¥
            user_type: ç”¨æˆ·ç±»å‹
            target_class: ç›®æ ‡ç±»åˆ«

        Returns:
            user_explanation: é€‚é…ç”¨æˆ·çš„è§£é‡Š
        """
        # è·å–å®Œæ•´è§£é‡Š
        full_explanation = self.explainer.explain(inputs, target_class)

        # æ ¹æ®ç”¨æˆ·ç±»å‹ç­›é€‰
        profile = self.user_profiles.get(user_type, self.user_profiles['end_user'])

        user_explanation = {}
        for level in profile['levels']:
            if level in full_explanation:
                user_explanation[level] = full_explanation[level]

        # æ ¼å¼åŒ–
        formatted = self._format_for_user(
            user_explanation,
            profile['visualization'],
            profile['detail']
        )

        return formatted

    def _format_for_user(self, explanation, visualization_type, detail_level):
        """æ ¹æ®ç”¨æˆ·éœ€æ±‚æ ¼å¼åŒ–è§£é‡Š"""
        if visualization_type == 'simple':
            return self._simplify_explanation(explanation)
        elif visualization_type == 'detailed':
            return self._detail_explanation(explanation, detail_level)
        elif visualization_type == 'technical':
            return explanation

    def _simplify_explanation(self, explanation):
        """ç®€åŒ–è§£é‡Š"""
        # æå–å…³é”®ä¿¡æ¯
        simplified = {}

        if 'high_level' in explanation:
            cav = explanation['high_level'].get('concept_activation', {})
            # åªä¿ç•™æœ€é‡è¦çš„æ¦‚å¿µ
            top_concepts = sorted(cav.items(), key=lambda x: x[1], reverse=True)[:3]
            simplified['key_concepts'] = top_concepts

        return simplified
```

---

## ğŸ“Š å®éªŒç»“æœ

### è§£é‡Šè´¨é‡è¯„ä¼°

| è§£é‡Šå±‚æ¬¡ | äººç±»ä¸€è‡´æ€§ | å†³ç­–æœ‰ç”¨æ€§ | è®¡ç®—æ—¶é—´ |
|:---|:---:|:---:|:---:|
| ä½å±‚ (åƒç´ ) | 0.65 | 0.58 | 10ms |
| ä¸­å±‚ (ç»„ä»¶) | 0.78 | 0.72 | 25ms |
| é«˜å±‚ (æ¦‚å¿µ) | 0.85 | 0.81 | 50ms |
| **å¤šå±‚èåˆ** | **0.88** | **0.85** | 80ms |

### ç”¨æˆ·æ»¡æ„åº¦

| ç”¨æˆ·ç±»å‹ | å•å±‚è§£é‡Š | å¤šå±‚è§£é‡Š | æå‡ |
|:---|:---:|:---:|:---:|
| ç»ˆç«¯ç”¨æˆ· | 3.2/5 | 4.1/5 | +28% |
| é¢†åŸŸä¸“å®¶ | 3.5/5 | 4.5/5 | +29% |
| å¼€å‘è€… | 3.8/5 | 4.6/5 | +21% |

---

## ğŸ’¡ å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: è§£é‡Šå¯è§†åŒ–å·¥å…·

```python
class ExplanationVisualizer:
    """
    è§£é‡Šå¯è§†åŒ–å·¥å…·

    å¯è§†åŒ–ä¸åŒå±‚æ¬¡çš„è§£é‡Š
    """
    def __init__(self):
        self.colormap = plt.cm.jet

    def visualize_saliency(self, image, saliency, save_path=None):
        """å¯è§†åŒ–æ˜¾è‘—æ€§çƒ­åŠ›å›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(10, 5))

        # åŸå›¾
        axes[0].imshow(image)
        axes[0].set_title('Original Image')
        axes[0].axis('off')

        # æ˜¾è‘—æ€§å›¾
        axes[1].imshow(image)
        axes[1].imshow(saliency, alpha=0.5, cmap=self.colormap)
        axes[1].set_title('Saliency Map')
        axes[1].axis('off')

        if save_path:
            plt.savefig(save_path)

        return fig

    def visualize_concepts(self, concept_scores, save_path=None):
        """å¯è§†åŒ–æ¦‚å¿µè´¡çŒ®"""
        concepts = list(concept_scores.keys())
        scores = list(concept_scores.values())

        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.barh(concepts, scores)

        # æ ¹æ®åˆ†æ•°ç€è‰²
        for bar, score in zip(bars, scores):
            bar.set_color(plt.cm.RdYlGn(score))

        ax.set_xlabel('Concept Activation Score')
        ax.set_title('Concept-level Explanation')

        if save_path:
            plt.savefig(save_path)

        return fig
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **å¤šå±‚æ¬¡è§£é‡Š** | Multilevel Explanation | ä¸åŒæŠ½è±¡å±‚æ¬¡çš„è§£é‡Š |
| **CAV** | Concept Activation Vector | æ¦‚å¿µæ¿€æ´»å‘é‡ |
| **æ˜¾è‘—æ€§å›¾** | Saliency Map | åƒç´ é‡è¦æ€§å¯è§†åŒ– |
| **è·¨æ¨¡æ€å¯¹é½** | Cross-Modal Alignment | å¤šæ¨¡æ€è§£é‡Šç»Ÿä¸€ |
| **ç”¨æˆ·è‡ªé€‚åº”** | User-Adaptive | æ ¹æ®ç”¨æˆ·è°ƒæ•´è§£é‡Š |
| **äººç±»ä¸€è‡´æ€§** | Human Alignment | è§£é‡Šä¸äººç±»ç†è§£çš„åŒ¹é…åº¦ |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å¤šå±‚æ¬¡è§£é‡Šçš„å¿…è¦æ€§
- [ ] æŒæ¡ä¸‰ä¸ªå±‚æ¬¡çš„è§£é‡Šæ–¹æ³•
- [ ] äº†è§£è·¨æ¨¡æ€è§£é‡Šå¯¹é½
- [ ] ç†è§£ç”¨æˆ·è‡ªé€‚åº”è§£é‡Š
- [ ] èƒ½å¤Ÿå®ç°åŸºæœ¬çš„è§£é‡Šç”Ÿæˆ

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **ä¸ºä»€ä¹ˆéœ€è¦å¤šå±‚æ¬¡è§£é‡Šï¼Ÿ**
   - æç¤º: ä¸åŒç”¨æˆ·éœ€æ±‚

2. **å¦‚ä½•è¯„ä¼°è§£é‡Šçš„è´¨é‡ï¼Ÿ**
   - æç¤º: äººç±»ä¸€è‡´æ€§ã€æœ‰ç”¨æ€§

3. **å¤šæ¨¡æ€è§£é‡Šå¦‚ä½•ç»Ÿä¸€ï¼Ÿ**
   - æç¤º: å…±åŒè¯­ä¹‰ç©ºé—´

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
