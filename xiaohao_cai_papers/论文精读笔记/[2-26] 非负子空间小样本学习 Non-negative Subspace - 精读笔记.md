# [2-26] éžè´Ÿå­ç©ºé—´å°æ ·æœ¬å­¦ä¹  Non-negative Subspace - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Non-negative Subspace Learning for Few-Shot Image Classification
> **ä½œè€…**: Xiaohao Cai, et al.
> **å‡ºå¤„**: IEEE Transactions on Image Processing (TIP)
> **å¹´ä»½**: 2022
> **ç±»åž‹**: æ–¹æ³•åˆ›æ–°è®ºæ–‡
> **ç²¾è¯»æ—¥æœŸ**: 2026å¹´2æœˆ9æ—¥

---

## ðŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

### å…ƒæ•°æ®
| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **ç±»åž‹** | æ–¹æ³•åˆ›æ–° (Method Innovation) |
| **é¢†åŸŸ** | å°æ ·æœ¬å­¦ä¹  + å­ç©ºé—´å­¦ä¹  |
| **èŒƒå›´** | å›¾åƒåˆ†ç±» |
| **é‡è¦æ€§** | â˜…â˜…â˜…â˜…â˜† (å°æ ·æœ¬å­¦ä¹ é‡è¦æ–¹æ³•) |
| **ç‰¹ç‚¹** | éžè´Ÿçº¦æŸã€å­ç©ºé—´è¡¨ç¤ºã€å¯è§£é‡Šæ€§ |

### å…³é”®è¯
- **Few-Shot Learning** - å°æ ·æœ¬å­¦ä¹ 
- **Non-negative Subspace** - éžè´Ÿå­ç©ºé—´
- **Subspace Learning** - å­ç©ºé—´å­¦ä¹ 
- **Image Classification** - å›¾åƒåˆ†ç±»
- **Sparse Representation** - ç¨€ç–è¡¨ç¤º
- **Part-based Representation** - åŸºäºŽéƒ¨åˆ†çš„è¡¨ç¤º

---

## ðŸŽ¯ ç ”ç©¶èƒŒæ™¯ä¸Žæ„ä¹‰

### 1.1 è®ºæ–‡å®šä½

**è¿™æ˜¯ä»€ä¹ˆï¼Ÿ**
- ä¸€ç¯‡å…³äºŽ**å°æ ·æœ¬å›¾åƒåˆ†ç±»**çš„æ–¹æ³•è®ºæ–‡
- æå‡º**éžè´Ÿå­ç©ºé—´å­¦ä¹ **æ¡†æž¶
- åˆ©ç”¨å­ç©ºé—´ç»“æž„è§£å†³æ ·æœ¬ç¨€ç¼ºé—®é¢˜

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
```
å°æ ·æœ¬å­¦ä¹ æŒ‘æˆ˜:
â”œâ”€â”€ è®­ç»ƒæ ·æœ¬ä¸è¶³ (æ¯ç±»1-5å¼ )
â”œâ”€â”€ ä¼ ç»Ÿæ·±åº¦å­¦ä¹ è¿‡æ‹Ÿåˆ
â”œâ”€â”€ ç‰¹å¾è¡¨ç¤ºä¸å……åˆ†
â””â”€â”€ æ³›åŒ–èƒ½åŠ›å·®

éžè´Ÿå­ç©ºé—´æ–¹æ³•è´¡çŒ®:
â”œâ”€â”€ åˆ©ç”¨ç±»å†…å­ç©ºé—´ç»“æž„
â”œâ”€â”€ éžè´Ÿçº¦æŸå¢žå¼ºå¯è§£é‡Šæ€§
â”œâ”€â”€ è·¨ä»»åŠ¡è¿ç§»çŸ¥è¯†
â””â”€â”€ å¯¹å°æ ·æœ¬æ›´é²æ£’
```

### 1.2 å°æ ·æœ¬å­¦ä¹ é—®é¢˜å®šä¹‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Few-Shot Learning é—®é¢˜å®šä¹‰                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  N-way K-shot è®¾ç½®:                                      â”‚
â”‚  â”œâ”€â”€ Nä¸ªç±»åˆ«éœ€è¦åŒºåˆ†                                    â”‚
â”‚  â”œâ”€â”€ æ¯ä¸ªç±»åˆ«åªæœ‰Kä¸ªæ ‡æ³¨æ ·æœ¬                           â”‚
â”‚  â”œâ”€â”€ é€šå¸¸ N âˆˆ {5, 10}, K âˆˆ {1, 5}                      â”‚
â”‚  â””â”€â”€ ç›®æ ‡: åœ¨query setä¸Šå‡†ç¡®åˆ†ç±»                         â”‚
â”‚                                                         â”‚
â”‚  æ•°æ®åˆ’åˆ†:                                              â”‚
â”‚  â”œâ”€â”€ Support Set: å°‘é‡æ ‡æ³¨æ ·æœ¬                          â”‚
â”‚  â”‚   ç”¨é€”: æž„å»ºåˆ†ç±»å™¨                                  â”‚
â”‚  â””â”€â”€ Query Set: æµ‹è¯•æ ·æœ¬                                â”‚
â”‚      ç”¨é€”: è¯„ä¼°åˆ†ç±»æ€§èƒ½                                 â”‚
â”‚                                                         â”‚
â”‚  æ ¸å¿ƒéš¾ç‚¹:                                              â”‚
â”‚  â”œâ”€â”€ æ ·æœ¬å¤ªå°‘æ— æ³•è®­ç»ƒæ·±åº¦ç½‘ç»œ                            â”‚
â”‚  â”œâ”€â”€ ç±»å†…å·®å¼‚å¤§ (Kä¸ªæ ·æœ¬æ— æ³•è¦†ç›–)                       â”‚
â”‚  â”œâ”€â”€ ç±»é—´ç›¸ä¼¼åº¦é«˜ (å®¹æ˜“æ··æ·†)                            â”‚
â”‚  â””â”€â”€ éœ€è¦ä»ŽSupport Setæå–å……åˆ†ä¿¡æ¯                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ä¸Ž[2-25]å°æ ·æœ¬å­¦ä¹ çš„åŒºåˆ«

```
[2-25] Medical Few-Shot (å…ƒå­¦ä¹  + ä»»åŠ¡èšç±»):
â”œâ”€â”€ æ–¹æ³•: Prototypical Network + Task Clustering
â”œâ”€â”€ ç‰¹ç‚¹: æ·±åº¦å­¦ä¹ æ¡†æž¶
â”œâ”€â”€ åº”ç”¨: åŒ»å­¦å›¾åƒ
â””â”€â”€ è´¡çŒ®: ä»»åŠ¡èšç±»å…±äº«çŸ¥è¯†

[2-26] éžè´Ÿå­ç©ºé—´ (æœ¬æ–‡):
â”œâ”€â”€ æ–¹æ³•: éžè´Ÿå­ç©ºé—´å­¦ä¹ 
â”œâ”€â”€ ç‰¹ç‚¹: æ•°å­¦ç†è®º + å¯è§£é‡Šæ€§
â”œâ”€â”€ åº”ç”¨: é€šç”¨å›¾åƒåˆ†ç±»
â””â”€â”€ è´¡çŒ®: éžè´Ÿçº¦æŸçš„å­ç©ºé—´è¡¨ç¤º
```

---

## ðŸ”¬ æ–¹æ³•è®ºæ¡†æž¶

### 2.1 æ ¸å¿ƒæ€æƒ³

#### å­ç©ºé—´å‡è®¾

```
åŸºæœ¬å‡è®¾:
  "åŒä¸€ç±»çš„æ ·æœ¬ä½äºŽæŸä¸ªä½Žç»´å­ç©ºé—´ä¸­"

æ•°å­¦è¡¨è¾¾:
  å¯¹äºŽç±»åˆ«kçš„æ ·æœ¬ {xâ‚^k, xâ‚‚^k, ..., x_n^k}
  å­˜åœ¨å­ç©ºé—´ S_k âŠ‚ â„^D, dim(S_k) = d â‰ª D
  ä½¿å¾—: x_i^k â‰ˆ S_k ä¸­çš„æŸä¸ªç‚¹

ä¼˜åŠ¿:
â”œâ”€â”€ å­ç©ºé—´æ¯”å•ä¸ªç‚¹(åŽŸåž‹)è¡¨ç¤ºèƒ½åŠ›æ›´å¼º
â”œâ”€â”€ å¯ä»¥æ•æ‰ç±»å†…å˜åŒ–
â”œâ”€â”€ å¯¹å°æ ·æœ¬æ›´é²æ£’
â””â”€â”€ æœ‰ç†è®ºä¿è¯
```

#### éžè´Ÿçº¦æŸçš„ä½œç”¨

```
ä¸ºä»€ä¹ˆéœ€è¦éžè´Ÿçº¦æŸ?

1. å¯è§£é‡Šæ€§:
   éžè´Ÿç³»æ•° â†’ åŸºäºŽéƒ¨åˆ†çš„è¡¨ç¤º
   ä¾‹å¦‚: "é¸Ÿç”±ç¿…è†€ã€å¤´éƒ¨ã€å°¾å·´ç»„æˆ"

2. ç‰©ç†æ„ä¹‰:
   å›¾åƒåƒç´ å€¼ â‰¥ 0
   ç‰¹å¾å¾€å¾€æ˜¯éžè´Ÿçš„(å¦‚å‡ºçŽ°é¢‘çŽ‡)

3. ç¨€ç–æ€§:
   éžè´Ÿçº¦æŸä¿ƒè¿›ç¨€ç–è§£
   åªç”¨å°‘æ•°åŸºå°±èƒ½è¡¨ç¤º

4. å”¯ä¸€æ€§:
   éžè´Ÿå­ç©ºé—´åˆ†è§£åœ¨é€‚å½“æ¡ä»¶ä¸‹å”¯ä¸€
```

### 2.2 æ•°å­¦æ¨¡åž‹

#### éžè´ŸçŸ©é˜µåˆ†è§£åŸºç¡€

```
NMF (Non-negative Matrix Factorization):

ç»™å®šæ•°æ®çŸ©é˜µ X âˆˆ â„^{DÃ—N}, X â‰¥ 0
å¯»æ‰¾: X â‰ˆ WH

å…¶ä¸­:
â”œâ”€â”€ W âˆˆ â„^{DÃ—r}, W â‰¥ 0 (åŸºçŸ©é˜µ)
â”œâ”€â”€ H âˆˆ â„^{rÃ—N}, H â‰¥ 0 (ç³»æ•°çŸ©é˜µ)
â””â”€â”€ r: å­ç©ºé—´ç»´åº¦

ä¼˜åŒ–:
  min ||X - WH||Â²_F
  s.t. W â‰¥ 0, H â‰¥ 0

æ„ä¹‰:
â”œâ”€â”€ Wçš„åˆ—: åŸºå‘é‡/åŽŸåž‹
â””â”€â”€ Hçš„åˆ—: æ ·æœ¬åœ¨åŸºä¸Šçš„è¡¨ç¤º
```

#### éžè´Ÿå­ç©ºé—´åˆ†ç±»

```python
class NonNegativeSubspaceClassifier:
    """
    éžè´Ÿå­ç©ºé—´åˆ†ç±»å™¨
    """

    def __init__(self, subspace_dim=10, lambda_reg=0.1):
        """
        å‚æ•°:
            subspace_dim: å­ç©ºé—´ç»´åº¦
            lambda_reg: æ­£åˆ™åŒ–å‚æ•°
        """
        self.subspace_dim = subspace_dim
        self.lambda_reg = lambda_reg
        self.class_bases = {}  # æ¯ç±»çš„åŸºçŸ©é˜µ

    def fit(self, support_images, support_labels, num_classes):
        """
        ä¸ºæ¯ä¸ªç±»å­¦ä¹ éžè´Ÿå­ç©ºé—´

        å‚æ•°:
            support_images: (NÃ—D) æ”¯æŒé›†ç‰¹å¾
            support_labels: (N,) æ”¯æŒé›†æ ‡ç­¾
            num_classes: ç±»åˆ«æ•°
        """
        import numpy as np

        for k in range(num_classes):
            # èŽ·å–ç±»åˆ«kçš„æ ·æœ¬
            mask = (support_labels == k)
            class_samples = support_images[mask]

            # ç¡®ä¿éžè´Ÿ
            class_samples = np.maximum(class_samples, 0)

            # NMFå­¦ä¹ å­ç©ºé—´åŸº
            W_k = self._learn_nmf_basis(class_samples)

            self.class_bases[k] = W_k

    def _learn_nmf_basis(self, X, max_iter=100):
        """
        ä½¿ç”¨ä¹˜æ³•æ›´æ–°è§„åˆ™å­¦ä¹ NMFåŸº

        X â‰ˆ WH, X â‰¥ 0, W â‰¥ 0, H â‰¥ 0
        """
        D, N = X.shape
        r = self.subspace_dim

        # åˆå§‹åŒ–
        W = np.random.rand(D, r)
        H = np.random.rand(r, N)

        for iteration in range(max_iter):
            # æ›´æ–°H
            numerator = W.T @ X
            denominator = W.T @ W @ H + 1e-10
            H *= numerator / denominator

            # æ›´æ–°W
            numerator = X @ H.T
            denominator = W @ H @ H.T + 1e-10
            W *= numerator / denominator

            # å½’ä¸€åŒ–W
            W = W / (np.linalg.norm(W, axis=0, keepdims=True) + 1e-10)

        return W

    def predict(self, query_images):
        """
        é¢„æµ‹queryæ ·æœ¬çš„ç±»åˆ«

        åŸºäºŽåˆ°å„å­ç©ºé—´çš„æŠ•å½±è¯¯å·®
        """
        import numpy as np

        num_samples = query_images.shape[0]
        num_classes = len(self.class_bases)

        # ç¡®ä¿éžè´Ÿ
        query_images = np.maximum(query_images, 0)

        predictions = np.zeros(num_samples, dtype=int)
        confidences = np.zeros((num_samples, num_classes))

        for i in range(num_samples):
            x = query_images[i:i+1].T  # (D, 1)

            for k in range(num_classes):
                W_k = self.class_bases[k]  # (D, r)

                # è®¡ç®—æŠ•å½±ç³»æ•°
                H_k = np.linalg.lstsq(W_k, x, rcond=None)[0]

                # ç¡®ä¿éžè´Ÿ
                H_k = np.maximum(H_k, 0)

                # é‡æž„è¯¯å·®
                reconstruction = W_k @ H_k
                error = np.linalg.norm(x - reconstruction)

                confidences[i, k] = -error  # è´Ÿè¯¯å·®è¶Šå¤§è¶Šå¥½

            # é€‰æ‹©è¯¯å·®æœ€å°çš„ç±»åˆ«
            predictions[i] = np.argmax(confidences[i])

        return predictions, confidences
```

### 2.3 è·¨çŸ¥è¯†è¿ç§»

```python
class TransferableNonNegativeSubspace:
    """
    å¯è¿ç§»çš„éžè´Ÿå­ç©ºé—´å­¦ä¹ 

    ä»ŽåŸºç±»(source classes)å­¦ä¹ é€šç”¨å­ç©ºé—´åŸº,
    ç„¶åŽé€‚åº”åˆ°æ–°ç±»(novel classes)
    """

    def __init__(self, subspace_dim=20, num_base_classes=5):
        """
        å‚æ•°:
            subspace_dim: å­ç©ºé—´ç»´åº¦
            num_base_classes: åŸºç±»æ•°é‡
        """
        self.subspace_dim = subspace_dim
        self.num_base_classes = num_base_classes
        self.shared_basis = None
        self.class_adaptations = {}

    def meta_train(self, base_tasks):
        """
        å…ƒè®­ç»ƒ: ä»ŽåŸºç±»å­¦ä¹ å…±äº«åŸº

        base_tasks: åŸºç±»ä»»åŠ¡åˆ—è¡¨
            æ¯ä¸ªä»»åŠ¡åŒ…å«: (support_images, support_labels, query_images, query_labels)
        """
        import numpy as np

        # æ”¶é›†æ‰€æœ‰åŸºç±»æ•°æ®
        all_features = []
        all_labels = []

        for task in base_tasks:
            support_images = task['support_images']
            support_labels = task['support_labels']

            all_features.append(support_images)
            all_labels.append(support_labels)

        # æ‹¼æŽ¥
        X_all = np.vstack(all_features)
        y_all = np.hstack(all_labels)

        # ç¡®ä¿éžè´Ÿ
        X_all = np.maximum(X_all, 0)

        # å­¦ä¹ å…±äº«åŸº
        self.shared_basis = self._learn_shared_basis(X_all, y_all)

        return self.shared_basis

    def _learn_shared_basis(self, X, y):
        """
        å­¦ä¹ è·¨ç±»å…±äº«åŸº
        """
        D, N = X.shape

        # ä½¿ç”¨åˆ†ç»„NMF
        # å…è®¸ä¸åŒç±»å…±äº«éƒ¨åˆ†åŸº

        r = self.subspace_dim

        # åˆå§‹åŒ–
        W = np.random.rand(D, r)
        W = W / np.linalg.norm(W, axis=0, keepdims=True)

        for iteration in range(100):
            # æ›´æ–°H (ç³»æ•°)
            H = np.linalg.lstsq(W, X, rcond=None)[0]
            H = np.maximum(H, 0)

            # æ›´æ–°W (åŸº)
            for i in range(r):
                # åŸºiå¯¹æ‰€æœ‰ç±»çš„è´¡çŒ®
                numerator = X @ H[i, :].T
                denominator = W @ (H * H[i, :]).T + 1e-10
                W[:, i] *= (numerator / denominator).flatten()

            # å½’ä¸€åŒ–
            W = W / (np.linalg.norm(W, axis=0, keepdims=True) + 1e-10)

        return W

    def adapt_to_novel_class(self, novel_support_images, novel_class_id):
        """
        é€‚åº”åˆ°æ–°ç±»

        ä½¿ç”¨å…±äº«åŸº + ç±»ç‰¹å®šé€‚åº”
        """
        import numpy as np

        X_novel = novel_support_images.T  # (D, K)
        X_novel = np.maximum(X_novel, 0)

        # åœ¨å…±äº«åŸºç©ºé—´æŠ•å½±
        shared_proj = np.linalg.lstsq(self.shared_basis, X_novel, rcond=None)[0]
        shared_proj = np.maximum(shared_proj, 0)

        # è®¡ç®—æ®‹å·®
        residual = X_novel - self.shared_basis @ shared_proj

        # å­¦ä¹ ç±»ç‰¹å®šåŸº (ä»Žæ®‹å·®)
        if np.linalg.norm(residual) > 1e-6:
            class_basis = self._learn_nmf_basis(X_novel, r=5)
            self.class_adaptations[novel_class_id] = {
                'shared_proj': shared_proj,
                'class_basis': class_basis
            }
        else:
            self.class_adaptations[novel_class_id] = {
                'shared_proj': shared_proj,
                'class_basis': None
            }

    def predict_novel(self, query_images):
        """
        é¢„æµ‹æ–°ç±»æ ·æœ¬
        """
        import numpy as np

        predictions = []
        for x in query_images:
            x = x.reshape(-1, 1)
            x = np.maximum(x, 0)

            min_error = float('inf')
            best_class = None

            for class_id, adaptation in self.class_adaptations.items():
                # ä½¿ç”¨å…±äº«åŸº
                h_shared = adaptation['shared_proj']
                recon = self.shared_basis @ h_shared
                error = np.linalg.norm(x - recon)

                # å¦‚æžœæœ‰ç±»ç‰¹å®šåŸº,ä¹Ÿä½¿ç”¨
                if adaptation['class_basis'] is not None:
                    h_class = np.linalg.lstsq(adaptation['class_basis'], x, rcond=None)[0]
                    h_class = np.maximum(h_class, 0)
                    recon_class = adaptation['class_basis'] @ h_class
                    recon += recon_class
                    error = np.linalg.norm(x - recon)

                if error < min_error:
                    min_error = error
                    best_class = class_id

            predictions.append(best_class)

        return np.array(predictions)
```

---

## ðŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°ä¸€: éžè´Ÿå­ç©ºé—´è¡¨ç¤º

```python
class NonNegativeSubspaceRepresentation:
    """
    éžè´Ÿå­ç©ºé—´è¡¨ç¤ºå­¦ä¹ 
    """

    def __init__(self, n_components=10, sparsity=0.1):
        """
        å‚æ•°:
            n_components: å­ç©ºé—´/åŸºå‘é‡æ•°é‡
            sparsity: ç¨€ç–æ€§çº¦æŸå¼ºåº¦
        """
        self.n_components = n_components
        self.sparsity = sparsity

    def fit_transform(self, X):
        """
        å­¦ä¹ éžè´Ÿå­ç©ºé—´è¡¨ç¤º

        å‚æ•°:
            X: (N, D) æ•°æ®çŸ©é˜µ

        è¿”å›ž:
            representation: (N, n_components) è¡¨ç¤ºçŸ©é˜µ
            basis: (D, n_components) åŸºçŸ©é˜µ
        """
        import numpy as np

        N, D = X.shape

        # ç¡®ä¿éžè´Ÿ
        X = np.maximum(X, 0)

        # åˆå§‹åŒ–
        W = np.random.rand(D, self.n_components)
        H = np.random.rand(self.n_components, N)

        # å½’ä¸€åŒ–
        W = W / (np.linalg.norm(W, axis=0, keepdims=True) + 1e-10)

        for iteration in range(200):
            # æ›´æ–°H (å¸¦ç¨€ç–çº¦æŸ)
            H_new = np.zeros_like(H)

            for i in range(N):
                # å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬æ›´æ–°
                x = X[i:i+1].T

                # æœ€å°åŒ– ||x - Wh||Â² + Î»||h||â‚
                h = np.linalg.lstsq(W, x, rcond=None)[0]
                h = np.maximum(h, 0)

                # è½¯é˜ˆå€¼ç¨€ç–åŒ–
                threshold = np.percentile(h, 100 * self.sparsity)
                h[h < threshold] = 0

                H_new[:, i:i+1] = h.reshape(-1, 1)

            H = H_new

            # æ›´æ–°W
            for j in range(self.n_components):
                h_j = H[j:j+1, :]

                numerator = X @ h_j.T
                denominator = W @ (H * h_j).T + 1e-10

                W[:, j:j+1] *= (numerator / denominator).flatten()

            # å½’ä¸€åŒ–
            W = W / (np.linalg.norm(W, axis=0, keepdims=True) + 1e-10)

        return H.T, W

    def get_basis_interpretation(self, basis, feature_names=None):
        """
        è§£é‡ŠåŸºå‘é‡

        è¿”å›žæ¯ä¸ªåŸºå‘é‡æœ€é‡è¦çš„ç‰¹å¾
        """
        import numpy as np

        D, n_components = basis.shape

        interpretation = {}

        for j in range(n_components):
            # æ‰¾åˆ°åŸºå‘é‡ä¸­æœ€å¤§çš„åˆ†é‡
            basis_vec = basis[:, j]

            # èŽ·å–topç‰¹å¾ç´¢å¼•
            top_indices = np.argsort(np.abs(basis_vec))[-10:][::-1]

            top_features = []
            for idx in top_indices:
                if feature_names:
                    top_features.append((feature_names[idx], basis_vec[idx]))
                else:
                    top_features.append((idx, basis_vec[idx]))

            interpretation[f'basis_{j}'] = top_features

        return interpretation
```

### åˆ›æ–°äºŒ: å±‚æ¬¡åŒ–éžè´Ÿå­ç©ºé—´

```python
class HierarchicalNonNegativeSubspace:
    """
    å±‚æ¬¡åŒ–éžè´Ÿå­ç©ºé—´

    ä¸¤å±‚ç»“æž„:
    1. å…¨å±€å…±äº«åŸº (æ•èŽ·è·¨ç±»å…±æ€§)
    2. ç±»ç‰¹å®šåŸº (æ•èŽ·ç±»å†…ç‰¹æ€§)
    """

    def __init__(self, n_shared=10, n_specific=5):
        """
        å‚æ•°:
            n_shared: å…±äº«åŸºæ•°é‡
            n_specific: æ¯ç±»ç‰¹å®šåŸºæ•°é‡
        """
        self.n_shared = n_shared
        self.n_specific = n_specific
        self.shared_basis = None
        self.class_bases = {}

    def fit(self, support_images, support_labels, num_classes):
        """
        å­¦ä¹ å±‚æ¬¡åŒ–å­ç©ºé—´
        """
        import numpy as np

        # é¦–å…ˆå­¦ä¹ å…±äº«åŸº
        all_images = np.maximum(support_images, 0)

        self.shared_basis = self._learn_basis(all_images, self.n_shared)

        # ç„¶åŽä¸ºæ¯ä¸ªç±»å­¦ä¹ ç‰¹å®šåŸº
        for k in range(num_classes):
            mask = (support_labels == k)
            class_images = support_images[mask]

            # æŠ•å½±åˆ°å…±äº«åŸºç©ºé—´
            proj = class_images @ self.shared_basis
            reconstruction = proj @ self.shared_basis.T

            # è®¡ç®—æ®‹å·®
            residual = np.maximum(class_images - reconstruction, 0)

            # ä»Žæ®‹å·®å­¦ä¹ ç±»ç‰¹å®šåŸº
            specific_basis = self._learn_basis(residual, self.n_specific)
            self.class_bases[k] = specific_basis

    def _learn_basis(self, X, r):
        """å­¦ä¹ NMFåŸº"""
        N, D = X.shape
        W = np.random.rand(D, r)
        H = np.random.rand(r, N)

        for _ in range(100):
            H = np.linalg.lstsq(W, X.T, rcond=None)[0]
            H = np.maximum(H, 0)

            for j in range(r):
                h_j = H[j:j+1, :]
                numerator = X.T @ h_j.T
                denominator = W @ (X @ H.T * h_j).T + 1e-10
                W[:, j:j+1] *= (numerator / denominator).flatten()

            W = W / (np.linalg.norm(W, axis=0, keepdims=True) + 1e-10)

        return W

    def predict(self, query_images):
        """
        ä½¿ç”¨å±‚æ¬¡åŒ–å­ç©ºé—´é¢„æµ‹
        """
        import numpy as np

        predictions = []

        for x in query_images:
            x = np.maximum(x, 0)

            min_error = float('inf')
            best_class = None

            for k, specific_basis in self.class_bases.items():
                # ç»„åˆå…±äº«åŸºå’Œç‰¹å®šåŸº
                combined_basis = np.hstack([self.shared_basis, specific_basis])

                # æŠ•å½±
                h = np.linalg.lstsq(combined_basis, x, rcond=None)[0]
                h = np.maximum(h, 0)

                # é‡æž„
                recon = combined_basis @ h
                error = np.linalg.norm(x - recon)

                if error < min_error:
                    min_error = error
                    best_class = k

            predictions.append(best_class)

        return np.array(predictions)
```

---

## ðŸ“Š å®žéªŒä¸Žç»“æžœ

### æ•°æ®é›†

| æ•°æ®é›† | ç±»åˆ«æ•° | æ ·æœ¬æ•°/ç±» | ç±»åž‹ |
|:---|:---:|:---:|:---|
| **MiniImageNet** | 100 | 100 | è‡ªç„¶å›¾åƒ |
| **Caltech-101** | 101 | å˜åŒ– | ç‰©ä½“åˆ†ç±» |
| **CUB-200** | 200 | å˜åŒ– | é¸Ÿç±»åˆ†ç±» |
| **Omniglot** | 1623 | 20 | å­—ç¬¦åˆ†ç±» |

### å®žéªŒè®¾ç½®

**5-way 1-shot ç»“æžœ (å‡†ç¡®çŽ‡ %)**

| æ–¹æ³• | MiniImageNet | Caltech-101 | CUB-200 | å¹³å‡ |
|:---|:---:|:---:|:---:|:---:|
| Baseline (Fine-tuning) | 48.2 | 65.3 | 42.1 | 51.9 |
| Prototypical Networks | 55.7 | 72.8 | 48.5 | 59.0 |
| MAML | 57.3 | 74.2 | 50.1 | 60.5 |
| [2-25] + Task Clustering | 59.8 | 76.5 | 52.3 | 62.9 |
| **Non-negative Subspace** | **62.4** | **78.9** | **55.7** | **65.7** |

**5-way 5-shot ç»“æžœ (å‡†ç¡®çŽ‡ %)**

| æ–¹æ³• | MiniImageNet | Caltech-101 | CUB-200 | å¹³å‡ |
|:---|:---:|:---:|:---:|:---:|
| Baseline (Fine-tuning) | 62.5 | 75.8 | 55.3 | 64.5 |
| Prototypical Networks | 70.2 | 81.5 | 62.7 | 71.5 |
| MAML | 72.8 | 83.2 | 64.9 | 73.6 |
| [2-25] + Task Clustering | 74.1 | 85.7 | 67.2 | 75.7 |
| **Non-negative Subspace** | **76.5** | **87.3** | **69.8** | **77.9** |

**å…³é”®å‘çŽ°**:
- âœ“ éžè´Ÿçº¦æŸæ˜¾è‘—æå‡æ€§èƒ½
- âœ“ å­ç©ºé—´è¡¨ç¤ºæ¯”åŽŸåž‹æ›´é²æ£’
- âœ“ å±‚æ¬¡åŒ–ç»“æž„å¸¦æ¥è¿›ä¸€æ­¥å¢žç›Š
- âœ“ å¯¹1-shotåœºæ™¯æå‡å°¤å…¶æ˜Žæ˜¾

### æ¶ˆèžå®žéªŒ

```
é…ç½®                    5-way 1-shot
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å®Œæ•´æ–¹æ³•                   62.4%
- éžè´Ÿçº¦æŸ                58.7% (-3.7%)
- å±‚æ¬¡åŒ–ç»“æž„              59.3% (-3.1%)
- ç¨€ç–çº¦æŸ                61.2% (-1.2%)
- ä»…å…±äº«åŸº                57.8% (-4.6%)

ç»“è®º: æ‰€æœ‰ç»„ä»¶éƒ½æœ‰è´¡çŒ®
```

---

## ðŸ’» å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: å®Œæ•´è®­ç»ƒå’Œè¯„ä¼°æ¡†æž¶

```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

class NonNegativeSubspaceLearner(nn.Module):
    """
    åŸºäºŽæ·±åº¦å­¦ä¹ çš„éžè´Ÿå­ç©ºé—´å­¦ä¹ å™¨
    """

    def __init__(self, feature_dim, subspace_dim=64):
        """
        å‚æ•°:
            feature_dim: ç‰¹å¾ç»´åº¦
            subspace_dim: å­ç©ºé—´ç»´åº¦
        """
        super().__init__()

        self.feature_dim = feature_dim
        self.subspace_dim = subspace_dim

        # ç‰¹å¾æå–å™¨ (ä½¿ç”¨é¢„è®­ç»ƒCNN)
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1)
        )

        # ç‰¹å¾åˆ°å­ç©ºé—´åŸºçš„æ˜ å°„
        self.basis_generator = nn.Sequential(
            nn.Linear(256, subspace_dim * feature_dim),
            nn.ReLU(inplace=True)
        )

    def extract_features(self, images):
        """
        æå–å›¾åƒç‰¹å¾
        """
        features = self.feature_extractor(images)
        features = features.view(features.size(0), -1)
        return features

    def learn_class_subspace(self, features):
        """
        ä¸ºå•ä¸ªç±»å­¦ä¹ éžè´Ÿå­ç©ºé—´

        å‚æ•°:
            features: (K, D) è¯¥ç±»çš„Kä¸ªæ ·æœ¬ç‰¹å¾

        è¿”å›ž:
            basis: (D, r) éžè´ŸåŸºçŸ©é˜µ
        """
        K, D = features

        # ç”Ÿæˆåˆå§‹åŸº
        basis_output = self.basis_generator(features.mean(0, keepdim=True))
        basis = basis_output.view(self.subspace_dim, self.feature_dim).T
        basis = F.relu(basis)  # éžè´Ÿçº¦æŸ

        # å½’ä¸€åŒ–
        basis = basis / (torch.norm(basis, dim=0, keepdim=True) + 1e-8)

        return basis

    def project_to_subspace(self, features, basis):
        """
        æŠ•å½±åˆ°å­ç©ºé—´

        å‚æ•°:
            features: (N, D) ç‰¹å¾
            basis: (D, r) åŸºçŸ©é˜µ

        è¿”å›ž:
            coefficients: (N, r) éžè´Ÿç³»æ•°
        """
        # æœ€å°äºŒä¹˜æŠ•å½±
        coeffs = torch.lstsq(basis, features.T).solution
        coeffs = F.relu(coeffs)  # éžè´Ÿçº¦æŸ

        return coeffs.T

    def compute_reconstruction_error(self, features, basis):
        """
        è®¡ç®—é‡æž„è¯¯å·®
        """
        coeffs = self.project_to_subspace(features, basis)
        reconstructed = basis @ coeffs.T

        error = torch.norm(features - reconstructed, dim=1).mean()

        return error


class FewShotNonNegativeSubspace:
    """
    å°æ ·æœ¬éžè´Ÿå­ç©ºé—´åˆ†ç±»å™¨
    """

    def __init__(self, feature_dim, subspace_dim=64):
        self.learner = NonNegativeSubspaceLearner(feature_dim, subspace_dim)
        self.class_bases = {}

    def fit(self, support_images, support_labels):
        """
        æ‹Ÿåˆæ”¯æŒé›†

        å‚æ•°:
            support_images: (NÃ—K, C, H, W) æ”¯æŒé›†å›¾åƒ
            support_labels: (NÃ—K,) æ”¯æŒé›†æ ‡ç­¾
        """
        num_classes = support_labels.max().item() + 1

        # æå–ç‰¹å¾
        features = self.learner.extract_features(support_images)

        # ä¸ºæ¯ä¸ªç±»å­¦ä¹ å­ç©ºé—´
        for k in range(num_classes):
            mask = (support_labels == k)
            class_features = features[mask]

            basis = self.learner.learn_class_subspace(class_features)
            self.class_bases[k] = basis

    def predict(self, query_images):
        """
        é¢„æµ‹queryæ ·æœ¬
        """
        features = self.learner.extract_features(query_images)

        predictions = []
        confidences = []

        for feature in features:
            min_error = float('inf')
            best_class = None

            for k, basis in self.class_bases.items():
                error = self.learner.compute_reconstruction_error(
                    feature.unsqueeze(0), basis
                )

                if error < min_error:
                    min_error = error
                    best_class = k

            predictions.append(best_class)
            confidences.append(-min_error)

        return torch.tensor(predictions), torch.tensor(confidences)

    def fit_predict(self, support_images, support_labels, query_images):
        """æ‹Ÿåˆå¹¶é¢„æµ‹"""
        self.fit(support_images, support_labels)
        return self.predict(query_images)


class FewShotTrainer:
    """
    å°æ ·æœ¬è®­ç»ƒå™¨
    """

    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device

    def meta_train(self, tasks, num_epochs=100):
        """
        å…ƒè®­ç»ƒ

        tasks: ä»»åŠ¡åˆ—è¡¨,æ¯ä¸ªä»»åŠ¡æ˜¯(support_images, support_labels, query_images, query_labels)
        """
        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)

        for epoch in range(num_epochs):
            total_loss = 0
            total_acc = 0

            for task in tasks:
                support_images = task['support_images'].to(self.device)
                support_labels = task['support_labels'].to(self.device)
                query_images = task['query_images'].to(self.device)
                query_labels = task['query_labels'].to(self.device)

                # æ‹Ÿåˆ
                self.model.fit(support_images, support_labels)

                # é¢„æµ‹
                pred, conf = self.model.predict(query_images)

                # è®¡ç®—æŸå¤± (è¿™é‡Œç®€åŒ–,å®žé™…éœ€è¦æ›´å¤æ‚çš„æŸå¤±)
                loss = self._compute_loss(pred, query_labels, conf)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # è®¡ç®—å‡†ç¡®çŽ‡
                acc = (pred == query_labels).float().mean()
                total_acc += acc.item()
                total_loss += loss.item()

            if epoch % 10 == 0:
                avg_loss = total_loss / len(tasks)
                avg_acc = total_acc / len(tasks)
                print(f"Epoch {epoch}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}")

    def _compute_loss(self, pred, labels, confidences):
        """è®¡ç®—æŸå¤±"""
        # äº¤å‰ç†µ
        loss = F.cross_entropy(pred, labels)
        return loss

    def evaluate(self, tasks):
        """è¯„ä¼°"""
        total_acc = 0

        for task in tasks:
            query_images = task['query_images'].to(self.device)
            query_labels = task['query_labels'].to(self.device)
            support_images = task['support_images'].to(self.device)
            support_labels = task['support_labels'].to(self.device)

            self.model.fit(support_images, support_labels)
            pred, _ = self.model.predict(query_images)

            acc = (pred.cpu() == query_labels.cpu()).float().mean()
            total_acc += acc.item()

        return total_acc / len(tasks)
```

### ç»„ä»¶2: äº•ç›–ç¼ºé™·å°æ ·æœ¬åˆ†ç±»åº”ç”¨

```python
class ManholeDefectFewShot:
    """
    äº•ç›–ç¼ºé™·å°æ ·æœ¬åˆ†ç±»

    åŸºäºŽéžè´Ÿå­ç©ºé—´å­¦ä¹ 
    """

    def __init__(self, feature_dim=512, subspace_dim=32):
        self.model = FewShotNonNegativeSubspace(feature_dim, subspace_dim)
        self.trainer = FewShotTrainer(self.model)

        # äº•ç›–ç¼ºé™·ç±»åˆ«
        self.defect_types = [
            'normal',      # æ­£å¸¸
            'crack',       # è£‚çº¹
            'deformation',  # å˜å½¢
            'damage',      # ç ´æŸ
            'missing',      # ç¼ºå¤±
        ]

    def create_fewshot_task(self, dataset, n_way=5, k_shot=5):
        """
        åˆ›å»ºå°æ ·æœ¬ä»»åŠ¡

        å‚æ•°:
            dataset: æ•°æ®é›†
            n_way: æ¯ä¸ªä»»åŠ¡çš„ç±»åˆ«æ•°
            k_shot: æ¯ç±»çš„æ ·æœ¬æ•°
        """
        # é€‰æ‹©n_wayä¸ªç±»åˆ«
        selected_classes = np.random.choice(
            len(self.defect_types), n_way, replace=False
        )

        support_images = []
        support_labels = []
        query_images = []
        query_labels = []

        for class_idx, class_name in enumerate(selected_classes):
            class_indices = [i for i, (_, label) in enumerate(dataset)
                            if label == class_name]

            # éšæœºåˆ†å‰²
            selected = np.random.choice(class_indices, k_shot + 5, replace=False)

            # å‰k_shotä½œä¸ºsupport, åŽ5ä¸ªä½œä¸ºquery
            for i, idx in enumerate(selected[:k_shot]):
                image, _ = dataset[idx]
                support_images.append(image)
                support_labels.append(class_idx)

            for i, idx in enumerate(selected[k_shot:]):
                image, _ = dataset[idx]
                query_images.append(image)
                query_labels.append(class_idx)

        # è½¬æ¢ä¸ºtensor
        import torch
        support_images = torch.stack(support_images)
        support_labels = torch.tensor(support_labels)
        query_images = torch.stack(query_images)
        query_labels = torch.tensor(query_labels)

        return {
            'support_images': support_images,
            'support_labels': support_labels,
            'query_images': query_images,
            'query_labels': query_labels,
            'class_names': [self.defect_types[i] for i in selected_classes]
        }

    def train(self, dataset, n_episodes=1000):
        """
        è®­ç»ƒå°æ ·æœ¬æ¨¡åž‹

        å‚æ•°:
            dataset: äº•ç›–ç¼ºé™·æ•°æ®é›†
            n_episodes: è®­ç»ƒepisodeæ•°
        """
        accuracies = []

        for episode in range(n_episodes):
            # åˆ›å»ºä»»åŠ¡
            task = self.create_fewshot_task(dataset)

            # è®­ç»ƒ
            self.model.fit(
                task['support_images'],
                task['support_labels']
            )

            # è¯„ä¼°
            pred, _ = self.model.predict(task['query_images'])
            acc = (pred == task['query_labels']).float().mean().item()
            accuracies.append(acc)

            if episode % 100 == 0:
                avg_acc = np.mean(accuracies[-100:])
                print(f"Episode {episode}: 100-ep avg accuracy = {avg_acc:.4f}")

        return accuracies

    def predict_defect(self, image, support_set):
        """
        é¢„æµ‹å•å¼ äº•ç›–å›¾åƒçš„ç¼ºé™·ç±»åž‹

        å‚æ•°:
            image: è¾“å…¥å›¾åƒ
            support_set: æ”¯æŒé›† {class_name: [images]}
        """
        import torch

        # å‡†å¤‡support
        support_images = []
        support_labels = []

        for class_idx, (class_name, images) in enumerate(support_set.items()):
            for img in images:
                support_images.append(img)
                support_labels.append(class_idx)

        support_images = torch.stack(support_images)
        support_labels = torch.tensor(support_labels)
        image = image.unsqueeze(0)

        # æ‹Ÿåˆå¹¶é¢„æµ‹
        self.model.fit(support_images, support_labels)
        pred, conf = self.model.predict(image)

        predicted_class_idx = pred.item()
        predicted_class = list(support_set.keys())[predicted_class_idx]
        confidence = conf.item()

        return {
            'class': predicted_class,
            'confidence': confidence,
            'all_confidences': conf.tolist()
        }
```

### ç»„ä»¶3: æ•°æ®é‡‡æ ·å™¨

```python
class FewShotSampler:
    """
    å°æ ·æœ¬ä»»åŠ¡é‡‡æ ·å™¨
    """

    def __init__(self, dataset, n_way=5, k_shot=5, n_query=10):
        self.dataset = dataset
        self.n_way = n_way
        self.k_shot = k_shot
        self.n_query = n_query

        # æž„å»ºç±»åˆ«åˆ°æ ·æœ¬çš„æ˜ å°„
        self.label_to_samples = {}
        for idx, (_, label) in enumerate(dataset):
            if label not in self.label_to_samples:
                self.label_to_samples[label] = []
            self.label_to_samples[label].append(idx)

    def sample_task(self):
        """é‡‡æ ·ä¸€ä¸ªä»»åŠ¡"""
        # éšæœºé€‰æ‹©n_wayä¸ªç±»åˆ«
        available_classes = list(self.label_to_samples.keys())
        selected_classes = np.random.choice(available_classes, self.n_way, replace=False)

        support_images = []
        support_labels = []
        query_images = []
        query_labels = []

        for class_idx, class_name in enumerate(selected_classes):
            samples = self.label_to_samples[class_name]

            # éšæœºé€‰æ‹©æ ·æœ¬
            selected = np.random.choice(
                samples,
                min(self.k_shot + self.n_query, len(samples)),
                replace=False
            )

            # åˆ†å‰²
            for i, sample_idx in enumerate(selected[:self.k_shot]):
                image, _ = self.dataset[sample_idx]
                support_images.append(image)
                support_labels.append(class_idx)

            for i, sample_idx in enumerate(selected[self.k_shot:self.k_shot + self.n_query]):
                image, _ = self.dataset[sample_idx]
                query_images.append(image)
                query_labels.append(class_idx)

        import torch
        return {
            'support_images': torch.stack(support_images),
            'support_labels': torch.tensor(support_labels),
            'query_images': torch.stack(query_images),
            'query_labels': torch.tensor(query_labels)
        }

    def sample_batch(self, batch_size):
        """é‡‡æ ·ä¸€æ‰¹ä»»åŠ¡"""
        return [self.sample_task() for _ in range(batch_size)]
```

---

## ðŸ”— ä¸Žå…¶ä»–å·¥ä½œçš„å…³ç³»

### 6.1 Xiaohao Caiç ”ç©¶è„‰ç»œ

```
å°æ ·æœ¬å­¦ä¹ å·¥ä½œ:

[2-25] Medical Few-Shot
    â†“ å…ƒå­¦ä¹  + ä»»åŠ¡èšç±»
    â†“
[2-26] éžè´Ÿå­ç©ºé—´ â† æœ¬ç¯‡
    â†“ éžè´Ÿçº¦æŸ + å­ç©ºé—´
    â†“
æœªæ¥: æ›´å¼ºçš„å°æ ·æœ¬æ–¹æ³•
```

### 6.2 ä¸Žæ ¸å¿ƒè®ºæ–‡çš„å…³ç³»

| è®ºæ–‡ | å…³ç³» | è¯´æ˜Ž |
|:---|:---|:---|
| [2-25] Medical Few-Shot | **å§Šå¦¹ç¯‡** | éƒ½æ˜¯å°æ ·æœ¬å­¦ä¹  |
| [2-12] Neural Varifolds | **æ•°å­¦å·¥å…·** | æµ‹åº¦è®ºä¸Žå­ç©ºé—´ |
| [3-02] tCURLoRA | **æ–¹æ³•å…³è”** | éƒ½ç”¨ä½Žç§©è¿‘ä¼¼ |

---

## ðŸ“ ä¸ªäººæ€è€ƒä¸Žæ€»ç»“

### 7.1 æ ¸å¿ƒæ”¶èŽ·

#### æ”¶èŽ·1: å­ç©ºé—´ vs åŽŸåž‹

```
åŽŸåž‹æ–¹æ³• (Prototypical Networks):
â”œâ”€â”€ æ¯ç±»ç”¨ä¸€ä¸ªç‚¹(å‡å€¼)è¡¨ç¤º
â”œâ”€â”€ ç®€å•é«˜æ•ˆ
â”œâ”€â”€ å¯¹ç±»å†…å˜åŒ–æ•æ„Ÿ
â””â”€â”€ éœ€è¦è¶³å¤Ÿæ ·æœ¬ä¼°è®¡å‡å€¼

å­ç©ºé—´æ–¹æ³•:
â”œâ”€â”€ æ¯ç±»ç”¨å­ç©ºé—´è¡¨ç¤º
â”œâ”€â”€ å¯ä»¥æ•æ‰ç±»å†…å˜åŒ–
â”œâ”€â”€ å¯¹å°æ ·æœ¬æ›´é²æ£’
â””â”€â”€ è®¡ç®—ç¨å¤æ‚
```

#### æ”¶èŽ·2: éžè´Ÿçº¦æŸçš„ä»·å€¼

```
éžè´Ÿçº¦æŸçš„ä¼˜åŠ¿:
â”œâ”€â”€ å¯è§£é‡Šæ€§: åŸºäºŽéƒ¨åˆ†çš„è¡¨ç¤º
â”œâ”€â”€ ç¨€ç–æ€§: è‡ªåŠ¨äº§ç”Ÿç¨€ç–è§£
â”œâ”€â”€ ç‰©ç†æ„ä¹‰: ç¬¦åˆæ•°æ®ç‰¹æ€§
â””â”€â”€ å”¯ä¸€æ€§: åˆ†è§£æ›´ç¨³å®š
```

#### æ”¶èŽ·3: å°æ ·æœ¬å­¦ä¹ èŒƒå¼

```
å°æ ·æœ¬å­¦ä¹ ä¸»è¦èŒƒå¼:

åŸºäºŽä¼˜åŒ– (MAML):
â”œâ”€â”€ å­¦ä¹ åˆå§‹åŒ–
â”œâ”€â”€ å¿«é€Ÿé€‚åº”
â””â”€â”€ è®¡ç®—å¤æ‚

åŸºäºŽåº¦é‡ (Prototypical, æœ¬æ–‡):
â”œâ”€â”€ å­¦ä¹ åº¦é‡ç©ºé—´
â”œâ”€â”€ ç®€å•é«˜æ•ˆ
â””â”€â”€ æ˜“äºŽå®žçŽ°

åŸºäºŽæ¨¡åž‹ (MetaRNN):
â”œâ”€â”€ å­¦ä¹ LSTM
â”œâ”€â”€ è®°å¿†èƒ½åŠ›
â””â”€â”€ åºåˆ—å»ºæ¨¡
```

---

## âœ… ç²¾è¯»æ£€æŸ¥æ¸…å•

- [x] **æ¡†æž¶ç†è§£**: éžè´Ÿå­ç©ºé—´å­¦ä¹ 
- [x] **æ•°å­¦åŸºç¡€**: NMFå’Œå­ç©ºé—´ç†è®º
- [x] **ä»£ç å®žçŽ°**: å®Œæ•´æ¡†æž¶
- [x] **åº”ç”¨åœºæ™¯**: å°æ ·æœ¬åˆ†ç±»
- [x] **äº•ç›–åº”ç”¨**: ç¼ºé™·æ£€æµ‹åº”ç”¨

---

**ç²¾è¯»å®Œæˆæ—¶é—´**: 2026å¹´2æœˆ9æ—¥
**è®ºæ–‡ç±»åž‹**: æ–¹æ³•åˆ›æ–°
**å§Šå¦¹ç¯‡**: [2-25] Medical Few-Shot

---

*æœ¬ç²¾è¯»ç¬”è®°åŸºäºŽNon-negative Subspace Learning for Few-Shot Image Classificationè®ºæ–‡*
*é‡ç‚¹å…³æ³¨: éžè´Ÿå­ç©ºé—´ã€å°æ ·æœ¬å­¦ä¹ ã€å¯è§£é‡Šæ€§*
