# 代码组件库

> **创建日期**: 2026年2月9日
> **范围**: 可复用Python/PyTorch代码组件
> **基于**: Xiaohao Cai师门83篇论文的代码实现

---

## 目录

1. [基础工具组件](#基础工具组件)
2. [数据处理组件](#数据处理组件)
3. [损失函数组件](#损失函数组件)
4. [评估指标组件](#评估指标组件)
5. [可视化组件](#可视化组件)
6. [训练工具组件](#训练工具组件)

---

## 基础工具组件

### 随机种子设置

```python
import random
import numpy as np
import torch

def set_seed(seed=42):
    """
    设置随机种子以确保可复现性

    Args:
        seed: 随机种子值
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

### 设备管理

```python
def get_device(prefer_mps=True):
    """
    获取最佳可用设备

    Args:
        prefer_mps: 是否优先使用MPS(Apple Silicon)

    Returns:
        torch.device: 最佳设备
    """
    if prefer_mps and torch.backends.mps.is_available():
        return torch.device('mps')
    elif torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')


class DeviceManager:
    """设备管理器"""
    def __init__(self):
        self.device = get_device()
        self.device_count = torch.cuda.device_count() if self.device.type == 'cuda' else 0

    def to_device(self, data):
        """将数据移动到设备"""
        if isinstance(data, (list, tuple)):
            return [self.to_device(d) for d in data]
        elif isinstance(data, dict):
            return {k: self.to_device(v) for k, v in data.items()}
        elif isinstance(data, torch.Tensor):
            return data.to(self.device)
        return data

    def get_memory_usage(self):
        """获取GPU内存使用"""
        if self.device.type == 'cuda':
            allocated = torch.cuda.memory_allocated(self.device) / 1024**3
            reserved = torch.cuda.memory_reserved(self.device) / 1024**3
            return {"allocated": allocated, "reserved": reserved}
        return None
```

### 模型工具

```python
def count_parameters(model, only_trainable=False):
    """
    统计模型参数量

    Args:
        model: PyTorch模型
        only_trainable: 是否只统计可训练参数

    Returns:
        dict: 参数统计信息
    """
    if only_trainable:
        params = [p for p in model.parameters() if p.requires_grad]
    else:
        params = list(model.parameters())

    total = sum(p.numel() for p in params)

    return {
        "total": total,
        "M": f"{total/1e6:.2f}M",
        "params": params
    }


def freeze_module(model, module_name):
    """冻结指定模块的参数"""
    for name, param in model.named_parameters():
        if module_name in name:
            param.requires_grad = False


def unfreeze_module(model, module_name):
    """解冻指定模块的参数"""
    for name, param in model.named_parameters():
        if module_name in name:
            param.requires_grad = True


def get_lr(optimizer):
    """获取当前学习率"""
    for param_group in optimizer.param_groups:
        return param_group['lr']
```

---

## 数据处理组件

### 图像数据增强

```python
import torchvision.transforms as T
from torchvision.transforms import functional as TF

class SegmentationAugmentation:
    """
    分割任务数据增强

    保证图像和掩码同步变换
    """
    def __init__(self, crop_size=256, flip_prob=0.5):
        self.crop_size = crop_size
        self.flip_prob = flip_prob

    def __call__(self, image, mask):
        """
        Args:
            image: PIL Image or Tensor (H, W, C)
            mask: PIL Image or Tensor (H, W)

        Returns:
            augmented_image, augmented_mask
        """
        # 随机裁剪
        i, j, h, w = T.RandomCrop.get_params(image, (self.crop_size, self.crop_size))
        image = TF.crop(image, i, j, h, w)
        mask = TF.crop(mask, i, j, h, w)

        # 随机水平翻转
        if random.random() < self.flip_prob:
            image = TF.hflip(image)
            mask = TF.hflip(mask)

        # 随机垂直翻转
        if random.random() < self.flip_prob:
            image = TF.vflip(image)
            mask = TF.vflip(mask)

        # 颜色抖动 (仅对图像)
        image = TF.adjust_brightness(image, random.uniform(0.8, 1.2))
        image = TF.adjust_contrast(image, random.uniform(0.8, 1.2))

        return image, mask


class MedicalImageAugmentation:
    """
    医学图像数据增强

    包含弹性形变等医学图像专用增强
    """
    def __init__(self, rotate_range=15, elastic_alpha=1000, elastic_sigma=30):
        self.rotate_range = rotate_range
        self.elastic_alpha = elastic_alpha
        self.elastic_sigma = elastic_sigma

    def elastic_transform(self, image, mask):
        """弹性形变"""
        from scipy.ndimage import gaussian_filter, map_coordinates

        shape = image.shape[:2]

        # 生成随机位移场
        dx = gaussian_filter(
            (np.random.rand(*shape) * 2 - 1),
            self.elastic_sigma
        ) * self.elastic_alpha
        dy = gaussian_filter(
            (np.random.rand(*shape) * 2 - 1),
            self.elastic_sigma
        ) * self.elastic_alpha

        # 创建网格
        x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))
        indices = (y + dy).astype(float), (x + dx).astype(float)

        # 应用形变
        if len(image.shape) == 3:
            transformed = np.stack([
                map_coordinates(image[:,:,c], indices, order=1, mode='reflect')
                for c in range(image.shape[2])
            ], axis=2)
        else:
            transformed = map_coordinates(image, indices, order=1, mode='reflect')

        mask_transformed = map_coordinates(mask, indices, order=1, mode='reflect')

        return transformed, mask_transformed

    def __call__(self, image, mask):
        # 随机旋转
        angle = random.uniform(-self.rotate_range, self.rotate_range)
        image = TF.rotate(image, angle)
        mask = TF.rotate(mask, angle)

        # 弹性形变
        if random.random() < 0.3:
            image, mask = self.elastic_transform(np.array(image), np.array(mask))
            image = TF.to_pil_image(image.astype(np.uint8))
            mask = TF.to_pil_image((mask * 255).astype(np.uint8))

        return image, mask
```

### 点云数据处理

```python
class PointCloudPreprocessor:
    """点云数据预处理器"""

    def __init__(self, num_points=2048, normalize=True):
        self.num_points = num_points
        self.normalize = normalize

    def normalize_point_cloud(self, points):
        """
        归一化点云到单位球

        Args:
            points: (N, 3) 点云坐标

        Returns:
            normalized_points: 归一化后的点云
        """
        # 中心化
        centroid = np.mean(points, axis=0)
        points = points - centroid

        # 缩放到单位球
        max_dist = np.max(np.linalg.norm(points, axis=1))
        if max_dist > 0:
            points = points / max_dist

        return points

    def estimate_normals(self, points, k=20):
        """
        估计法线 (PCA方法)

        Args:
            points: (N, 3) 点云
            k: 最近邻数量

        Returns:
            normals: (N, 3) 法线
        """
        from sklearn.neighbors import NearestNeighbors

        nbrs = NearestNeighbors(n_neighbors=k).fit(points)
        distances, indices = nbrs.kneighbors(points)

        normals = []
        for i in range(len(points)):
            neighbors = points[indices[i]]
            centroid = np.mean(neighbors, axis=0)
            cov = np.cov((neighbors - centroid).T)
            eigenvalues, eigenvectors = np.linalg.eigh(cov)

            # 最小特征值对应的特征向量
            normal = eigenvectors[:, 0]

            # 确保法线方向一致
            if np.dot(normal, centroid) > 0:
                normal = -normal

            normals.append(normal)

        return np.array(normals)

    def sample_points(self, points, normals=None):
        """随机采样固定数量的点"""
        num_input = points.shape[0]

        if num_input >= self.num_points:
            choice = np.random.choice(num_input, self.num_points, replace=False)
        else:
            choice = np.random.choice(num_input, self.num_points, replace=True)

        points_sampled = points[choice]

        if normals is not None:
            normals_sampled = normals[choice]
        else:
            normals_sampled = None

        return points_sampled, normals_sampled

    def __call__(self, points, normals=None):
        """完整预处理流程"""
        if self.normalize:
            points = self.normalize_point_cloud(points)

        if normals is None:
            normals = self.estimate_normals(points)

        points, normals = self.sample_points(points, normals)

        return points, normals
```

### DataLoader工厂

```python
from torch.utils.data import Dataset, DataLoader

def create_dataloader(dataset, batch_size=16, shuffle=True,
                      num_workers=4, pin_memory=True):
    """
    创建DataLoader的工厂函数

    Args:
        dataset: Dataset对象
        batch_size: 批次大小
        shuffle: 是否打乱
        num_workers: 工作进程数
        pin_memory: 是否锁页内存

    Returns:
        DataLoader对象
    """
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=shuffle
    )


class SimpleDataset(Dataset):
    """简单的数据集包装器"""

    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        label = self.labels[idx]

        if self.transform:
            sample = self.transform(sample)

        return sample, label
```

---

## 损失函数组件

### 组合损失

```python
class CombinedLoss(nn.Module):
    """
    可配置的组合损失函数

    支持多种损失的加权组合
    """
    def __init__(self, loss_config):
        """
        Args:
            loss_config: {
                'dice': {'weight': 1.0, 'params': {}},
                'focal': {'weight': 0.5, 'params': {'alpha': 0.25, 'gamma': 2}},
                ...
            }
        """
        super().__init__()
        self.losses = nn.ModuleDict()
        self.weights = {}

        for name, config in loss_config.items():
            if name == 'dice':
                self.losses[name] = DiceLoss(**config.get('params', {}))
            elif name == 'focal':
                self.losses[name] = FocalLoss(**config.get('params', {}))
            elif name == 'ce':
                self.losses[name] = nn.CrossEntropyLoss(**config.get('params', {}))
            elif name == 'boundary':
                self.losses[name] = BoundaryLoss(**config.get('params', {}))

            self.weights[name] = config.get('weight', 1.0)

    def forward(self, pred, target, **kwargs):
        """
        Args:
            pred: 预测 (B, C, H, W)
            target: 目标 (B, H, W)
            **kwargs: 其他参数

        Returns:
            total_loss: 总损失
            loss_dict: 各损失的详细信息
        """
        total_loss = 0
        loss_dict = {}

        for name, loss_fn in self.losses.items():
            loss = loss_fn(pred, target, **kwargs)
            weighted_loss = self.weights[name] * loss

            total_loss += weighted_loss
            loss_dict[name] = loss.item()

        loss_dict['total'] = total_loss.item()

        return total_loss, loss_dict
```

### Dice损失

```python
class DiceLoss(nn.Module):
    """
    Dice损失

    适用于类别不平衡的分割任务
    """
    def __init__(self, smooth=1e-6, apply_sigmoid=False):
        super().__init__()
        self.smooth = smooth
        self.apply_sigmoid = apply_sigmoid

    def forward(self, pred, target):
        """
        Args:
            pred: (B, C, H, W) 预测logits
            target: (B, H, W) 目标标签

        Returns:
            dice_loss: Dice损失值
        """
        if self.apply_sigmoid:
            pred = torch.sigmoid(pred)

        # one-hot编码
        num_classes = pred.shape[1]
        target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()

        # 计算Dice
        pred_flat = pred.reshape(pred.shape[0], pred.shape[1], -1)
        target_flat = target_one_hot.reshape(target_one_hot.shape[0], target_one_hot.shape[1], -1)

        intersection = (pred_flat * target_flat).sum(dim=-1)
        union = pred_flat.sum(dim=-1) + target_flat.sum(dim=-1)

        dice = (2. * intersection + self.smooth) / (union + self.smooth)

        return 1 - dice.mean()


class BoundaryLoss(nn.Module):
    """
    边界损失

    关注分割边界的精度
    """
    def __init__(self, theta0=3, theta=5):
        super().__init__()
        self.theta0 = theta0
        self.theta = theta

    def forward(self, pred, target):
        """
        Args:
            pred: (B, C, H, W) 预测
            target: (B, H, W) 目标

        Returns:
            boundary_loss: 边界损失值
        """
        # 计算边界
        target_boundary = self._compute_boundary(target.float())
        pred_boundary = self._compute_boundary(pred.softmax(dim=1)[:, 1])

        # 计算距离变换
        target_dist = self._distance_transform(target_boundary)
        pred_dist = self._distance_transform(pred_boundary)

        # 损失
        loss = (pred_boundary * target_dist).mean() + \
               (target_boundary * pred_dist).mean()

        return loss

    def _compute_boundary(self, mask):
        """计算边界"""
        kernel = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]],
                              dtype=mask.dtype, device=mask.device)
        kernel = kernel.view(1, 1, 3, 3)

        mask_pad = F.pad(mask.unsqueeze(1), (1, 1, 1, 1), mode='replicate')
        boundary = F.conv2d(mask_pad, kernel)
        boundary = boundary.squeeze(1)

        return torch.abs(boundary) > 0

    def _distance_transform(self, boundary):
        """距离变换"""
        # 简化实现
        return boundary.float()
```

### Focal损失

```python
class FocalLoss(nn.Module):
    """
    Focal损失

    关注难分类样本
    """
    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, pred, target):
        """
        Args:
            pred: (B, C, H, W) 预测logits
            target: (B, H, W) 目标标签

        Returns:
            focal_loss: Focal损失值
        """
        ce_loss = F.cross_entropy(pred, target, reduction='none')
        pt = torch.exp(-ce_loss)

        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss
```

---

## 评估指标组件

### 分割指标

```python
class SegmentationMetrics:
    """分割评估指标计算器"""

    def __init__(self, num_classes):
        self.num_classes = num_classes

    def compute_iou(self, pred, target):
        """计算IoU"""
        intersection = (pred & target).sum()
        union = (pred | target).sum()
        return intersection / (union + 1e-8)

    def compute_dice(self, pred, target):
        """计算Dice系数"""
        intersection = (pred & target).sum()
        return (2. * intersection) / (pred.sum() + target.sum() + 1e-8)

    def compute_hd95(self, pred, target):
        """计算95% Hausdorff距离"""
        from scipy.spatial.distance import directed_hausdorff

        pred_points = np.argwhere(pred)
        target_points = np.argwhere(target)

        if len(pred_points) == 0 or len(target_points) == 0:
            return 0.0

        d1 = directed_hausdorff(pred_points, target_points)[0]
        d2 = directed_hausdorff(target_points, pred_points)[0]

        return max(d1, d2)

    def evaluate(self, pred, target):
        """
        综合评估

        Returns:
            metrics_dict: 指标字典
        """
        metrics = {
            'iou': self.compute_iou(pred, target),
            'dice': self.compute_dice(pred, target),
            'hd95': self.compute_hd95(pred, target)
        }
        return metrics

    def batch_evaluate(self, preds, targets):
        """批量评估"""
        results = {
            'iou': [],
            'dice': [],
            'hd95': []
        }

        for pred, target in zip(preds, targets):
            metrics = self.evaluate(pred, target)
            for k, v in metrics.items():
                results[k].append(v)

        # 计算均值和标准差
        summary = {}
        for k, v in results.items():
            summary[f'{k}_mean'] = np.mean(v)
            summary[f'{k}_std'] = np.std(v)

        return summary
```

---

## 可视化组件

### 分割结果可视化

```python
import matplotlib.pyplot as plt
import numpy as np

def visualize_segmentation(image, pred, target, save_path=None):
    """
    可视化分割结果

    Args:
        image: 原始图像 (H, W, C)
        pred: 预测掩码 (H, W)
        target: 目标掩码 (H, W)
        save_path: 保存路径

    Returns:
        fig: matplotlib图形对象
    """
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))

    # 原始图像
    axes[0].imshow(image)
    axes[0].set_title('Original Image')
    axes[0].axis('off')

    # 目标掩码
    axes[1].imshow(target, cmap='jet')
    axes[1].set_title('Ground Truth')
    axes[1].axis('off')

    # 预测掩码
    axes[2].imshow(pred, cmap='jet')
    axes[2].set_title('Prediction')
    axes[2].axis('off')

    # 叠加显示
    axes[3].imshow(image)
    axes[3].imshow(target, cmap='jet', alpha=0.3)
    axes[3].imshow(pred, cmap='jet', alpha=0.3)
    axes[3].set_title('Overlay')
    axes[3].axis('off')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    return fig


def plot_training_history(history, save_path=None):
    """
    绘制训练历史

    Args:
        history: {'train_loss': [], 'val_loss': [], 'train_metric': [], 'val_metric': []}
        save_path: 保存路径
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # 损失曲线
    axes[0].plot(history['train_loss'], label='Train Loss')
    axes[0].plot(history['val_loss'], label='Val Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Loss Curve')
    axes[0].legend()
    axes[0].grid(True)

    # 指标曲线
    axes[1].plot(history['train_metric'], label='Train Metric')
    axes[1].plot(history['val_metric'], label='Val Metric')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Metric')
    axes[1].set_title('Metric Curve')
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    return fig
```

### 点云可视化

```python
def visualize_point_cloud(points, normals=None, colors=None, save_path=None):
    """
    可视化点云

    Args:
        points: (N, 3) 点云坐标
        normals: (N, 3) 法线 (可选)
        colors: (N, 3) 颜色 (可选)
        save_path: 保存路径
    """
    try:
        import open3d as o3d

        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)

        if colors is not None:
            pcd.colors = o3d.utility.Vector3dVector(colors)

        if normals is not None:
            pcd.normals = o3d.utility.Vector3dVector(normals)

        o3d.visualization.draw_geometries([pcd])

        if save_path:
            o3d.io.write_point_cloud(save_path, pcd)

    except ImportError:
        # 回退到matplotlib
        fig = plt.figure(figsize=(8, 8))
        ax = fig.add_subplot(111, projection='3d')

        if colors is not None:
            ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=colors, s=1)
        else:
            ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=1)

        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        return fig
```

---

## 训练工具组件

### 训练器基类

```python
class BaseTrainer:
    """
    训练器基类

    提供标准的训练流程模板
    """
    def __init__(self, model, criterion, optimizer, scheduler=None,
                 device='cuda', logger=None):
        self.model = model.to(device)
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.logger = logger

        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_metric': [],
            'val_metric': []
        }

    def train_epoch(self, dataloader):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        total_metric = 0
        num_batches = 0

        for batch in dataloader:
            self.optimizer.zero_grad()
            loss, metric = self._process_batch(batch)
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            total_metric += metric
            num_batches += 1

        return total_loss / num_batches, total_metric / num_batches

    def validate(self, dataloader):
        """验证"""
        self.model.eval()
        total_loss = 0
        total_metric = 0
        num_batches = 0

        with torch.no_grad():
            for batch in dataloader:
                loss, metric = self._process_batch(batch)
                total_loss += loss.item()
                total_metric += metric
                num_batches += 1

        return total_loss / num_batches, total_metric / num_batches

    def _process_batch(self, batch):
        """处理单个批次 (子类实现)"""
        raise NotImplementedError

    def fit(self, train_loader, val_loader, epochs, callbacks=None):
        """完整训练流程"""
        best_metric = 0

        for epoch in range(epochs):
            # 训练
            train_loss, train_metric = self.train_epoch(train_loader)

            # 验证
            val_loss, val_metric = self.validate(val_loader)

            # 更新学习率
            if self.scheduler:
                self.scheduler.step()

            # 记录历史
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_metric'].append(train_metric)
            self.history['val_metric'].append(val_metric)

            # 日志
            if self.logger:
                self.logger.info(
                    f"Epoch {epoch+1}/{epochs}: "
                    f"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, "
                    f"train_metric={train_metric:.4f}, val_metric={val_metric:.4f}"
                )

            # 保存最佳模型
            if val_metric > best_metric:
                best_metric = val_metric
                self._save_checkpoint('best_model.pth')

            # 回调
            if callbacks:
                for callback in callbacks:
                    callback(epoch, self.history)

        return self.history

    def _save_checkpoint(self, path):
        """保存检查点"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'history': self.history
        }, path)


class SegmentationTrainer(BaseTrainer):
    """分割任务训练器"""

    def __init__(self, *args, metrics_calculator=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.metrics_calculator = metrics_calculator or SegmentationMetrics(num_classes=2)

    def _process_batch(self, batch):
        """
        处理分割批次

        batch: (images, masks) 或 (images, masks, extra_data)
        """
        if isinstance(batch, (list, tuple)) and len(batch) >= 2:
            images, masks = batch[0], batch[1]
        else:
            images, masks = batch['image'], batch['mask']

        images = images.to(self.device)
        masks = masks.to(self.device)

        # 前向传播
        outputs = self.model(images)

        # 计算损失
        if isinstance(self.criterion, CombinedLoss):
            loss, loss_dict = self.criterion(outputs, masks)
        else:
            loss = self.criterion(outputs, masks)

        # 计算指标
        with torch.no_grad():
            preds = outputs.argmax(dim=1)
            metrics = []
            for pred, mask in zip(preds, masks):
                metric = self.metrics_calculator.evaluate(
                    pred.cpu().numpy(),
                    mask.cpu().numpy()
                )
                metrics.append(metric['dice'])

            avg_metric = np.mean(metrics)

        return loss, avg_metric
```

### 学习率调度器

```python
def get_scheduler(optimizer, scheduler_name='cosine', **kwargs):
    """
    获取学习率调度器

    Args:
        optimizer: 优化器
        scheduler_name: 调度器名称
        **kwargs: 调度器参数

    Returns:
        scheduler: 学习率调度器
    """
    if scheduler_name == 'cosine':
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=kwargs.get('T_max', 100),
            eta_min=kwargs.get('eta_min', 1e-6)
        )

    elif scheduler_name == 'step':
        return torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=kwargs.get('step_size', 30),
            gamma=kwargs.get('gamma', 0.1)
        )

    elif scheduler_name == 'multistep':
        return torch.optim.lr_scheduler.MultiStepLR(
            optimizer,
            milestones=kwargs.get('milestones', [50, 80]),
            gamma=kwargs.get('gamma', 0.1)
        )

    elif scheduler_name == 'exponential':
        return torch.optim.lr_scheduler.ExponentialLR(
            optimizer,
            gamma=kwargs.get('gamma', 0.95)
        )

    elif scheduler_name == 'plateau':
        return torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode=kwargs.get('mode', 'max'),
            factor=kwargs.get('factor', 0.5),
            patience=kwargs.get('patience', 10)
        )

    else:
        raise ValueError(f"Unknown scheduler: {scheduler_name}")
```

### 早停回调

```python
class EarlyStopping:
    """
    早停回调

    Args:
        patience: 等待改善的epoch数
        min_delta: 最小改善量
        mode: 'min' 或 'max'
    """
    def __init__(self, patience=10, min_delta=0, mode='min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score):
        """
        检查是否应该早停

        Args:
            score: 当前指标值

        Returns:
            bool: 是否应该早停
        """
        if self.best_score is None:
            self.best_score = score
        elif self._is_better(score, self.best_score):
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

        return self.early_stop

    def _is_better(self, new_score, best_score):
        """判断新分数是否更好"""
        if self.mode == 'min':
            return new_score < best_score - self.min_delta
        else:
            return new_score > best_score + self.min_delta
```

---

## 附录

### 完整训练示例

```python
# 完整的训练流程示例
def train_segmentation_model():
    # 1. 设置随机种子
    set_seed(42)

    # 2. 创建模型
    model = ImprovedUNet(in_channels=3, num_classes=2)

    # 3. 创建损失函数
    criterion = CombinedLoss({
        'dice': {'weight': 1.0},
        'focal': {'weight': 0.5, 'alpha': 0.25, 'gamma': 2}
    })

    # 4. 创建优化器
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)

    # 5. 创建调度器
    scheduler = get_scheduler(optimizer, 'cosine', T_max=100)

    # 6. 创建训练器
    trainer = SegmentationTrainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device='cuda'
    )

    # 7. 创建数据加载器
    train_loader = create_dataloader(train_dataset, batch_size=16, shuffle=True)
    val_loader = create_dataloader(val_dataset, batch_size=16, shuffle=False)

    # 8. 创建回调
    early_stopping = EarlyStopping(patience=15, mode='max')

    def callback_fn(epoch, history):
        early_stopping(history['val_metric'][-1])
        if early_stopping.early_stop:
            print(f"Early stopping at epoch {epoch+1}")
            return True  # 停止训练
        return False

    # 9. 训练
    history = trainer.fit(
        train_loader,
        val_loader,
        epochs=100,
        callbacks=[callback_fn]
    )

    # 10. 可视化
    plot_training_history(history, save_path='training_history.png')

    return model, history
```

---

*文档创建时间: 2026年2月9日*
*基于: Xiaohao Cai 师门论文代码实现*
