# [4-30] ç¨€ç–è´å¶æ–¯è´¨é‡æ˜ å°„å‡è®¾æ£€éªŒ - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Sparse Bayesian Mass Mapping: Hypothesis Testing
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (é«˜)
> **é‡è¦æ€§**: â­â­â­â­ (è´å¶æ–¯ç»Ÿè®¡æ–¹æ³•)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Sparse Bayesian Mass Mapping: Hypothesis Testing |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **æ ¸å¿ƒä¸»é¢˜** | ç¨€ç–è´å¶æ–¯è´¨é‡æ˜ å°„çš„å‡è®¾æ£€éªŒæ–¹æ³• |
| **å…³é”®è¯** | Sparse Bayesian, Mass Mapping, Hypothesis Testing, Cosmology |
| **åº”ç”¨é¢†åŸŸ** | å®‡å®™å­¦ã€å¼•åŠ›é€é•œã€å¤©ä½“ç»Ÿè®¡å­¦ |

---

## ğŸ¯ ç ”ç©¶èƒŒæ™¯

### è´¨é‡æ˜ å°„ (Mass Mapping)

**ç§‘å­¦èƒŒæ™¯**:
```
å¼•åŠ›é€é•œè´¨é‡æ˜ å°„:
  - é€šè¿‡å¼±å¼•åŠ›é€é•œæ•ˆåº”é‡å»ºç‰©è´¨åˆ†å¸ƒ
  - æš—ç‰©è´¨åˆ†å¸ƒçš„é—´æ¥è§‚æµ‹æ‰‹æ®µ
  - å®‡å®™å¤§å°ºåº¦ç»“æ„ç ”ç©¶

æ•°å­¦æ¨¡å‹:
  è§‚æµ‹: y = Ax + n
  å…¶ä¸­:
    - y: å‰ªåˆ‡åœºè§‚æµ‹ (å¼±é€é•œä¿¡å·)
    - A: æŠ•å½±ç®—å­
    - x: è´¨é‡åˆ†å¸ƒ (å¾…é‡å»º)
    - n: å™ªå£°

æŒ‘æˆ˜:
  - é—®é¢˜ç—…æ€ (ill-posed)
  - å™ªå£°æ•æ„Ÿ
  - éœ€è¦æ­£åˆ™åŒ–
```

**ç¨€ç–å…ˆéªŒ**:
```
ç¨€ç–æ€§å‡è®¾:
  - è´¨é‡åˆ†å¸ƒåœ¨å°æ³¢åŸŸç¨€ç–
  - å¤§éƒ¨åˆ†ç³»æ•°æ¥è¿‘é›¶
  - å°‘æ•°å¤§ç³»æ•°æ•æ‰ç»“æ„

ä¼˜åŠ¿:
  - å‡å°‘è‡ªç”±åº¦æ•°
  - æé«˜é‡å»ºç¨³å®šæ€§
  - ä¿æŒå°–é”ç‰¹å¾
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ•´ä½“æ¡†æ¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è§‚æµ‹æ•°æ® (å‰ªåˆ‡åœº)                             â”‚
â”‚              y = Ax + n                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç¨€ç–è´å¶æ–¯é‡å»º â­æ ¸å¿ƒ                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  å…ˆéªŒ: ç¨€ç–å…ˆéªŒ (Laplace/Student-t)            â”‚    â”‚
â”‚  â”‚  ä¼¼ç„¶: é«˜æ–¯å™ªå£°æ¨¡å‹                             â”‚    â”‚
â”‚  â”‚  æ¨æ–­: å˜åˆ†æ¨æ–­ / MCMC                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å‡è®¾æ£€éªŒæ¡†æ¶                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  H0: æŸåŒºåŸŸæ— è´¨é‡èšé›† (x_i = 0)                â”‚    â”‚
â”‚  â”‚  H1: æŸåŒºåŸŸæœ‰è´¨é‡èšé›† (x_i â‰  0)                â”‚    â”‚
â”‚  â”‚  æ£€éªŒç»Ÿè®¡é‡: è´å¶æ–¯å› å­ / åéªŒæ¦‚ç‡             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç§‘å­¦æ¨æ–­                                     â”‚
â”‚  - æš—ç‰©è´¨æ™•æ¢æµ‹                                           â”‚
â”‚  - å®‡å®™ç©ºæ´è¯†åˆ«                                           â”‚
â”‚  - ç»“æ„æ˜¾è‘—æ€§è¯„ä¼°                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ ¸å¿ƒç»„ä»¶1: ç¨€ç–è´å¶æ–¯æ¨¡å‹

**æ¦‚ç‡æ¨¡å‹**:
```python
class SparseBayesianMassMapping:
    """
    ç¨€ç–è´å¶æ–¯è´¨é‡æ˜ å°„

    ä½¿ç”¨ç¨€ç–å…ˆéªŒè¿›è¡Œè´¨é‡åˆ†å¸ƒé‡å»º
    """
    def __init__(self, lambda_sparse=1.0, noise_sigma=1.0):
        """
        Args:
            lambda_sparse: ç¨€ç–æ­£åˆ™åŒ–å¼ºåº¦
            noise_sigma: å™ªå£°æ ‡å‡†å·®
        """
        self.lambda_sparse = lambda_sparse
        self.noise_sigma = noise_sigma

    def sparse_prior(self, x, prior_type='laplace'):
        """
        ç¨€ç–å…ˆéªŒåˆ†å¸ƒ

        Args:
            x: è´¨é‡åˆ†å¸ƒ (å°æ³¢ç³»æ•°)
            prior_type: 'laplace' | 'student_t' | 'horseshoe'

        Returns:
            log_prior: å¯¹æ•°å…ˆéªŒæ¦‚ç‡
        """
        if prior_type == 'laplace':
            # Laplaceå…ˆéªŒ (L1æ­£åˆ™åŒ–çš„è´å¶æ–¯è§£é‡Š)
            log_prior = -self.lambda_sparse * np.sum(np.abs(x))

        elif prior_type == 'student_t':
            # Student-tå…ˆéªŒ (é‡å°¾åˆ†å¸ƒ)
            nu = 3  # è‡ªç”±åº¦
            log_prior = np.sum(
                np.log(1 + (x / self.lambda_sparse)**2 / nu) * (-(nu + 1) / 2)
            )

        elif prior_type == 'horseshoe':
            # Horseshoeå…ˆéªŒ (ç¨€ç–æ€§æ›´å¼º)
            # p(x) = âˆ« N(x; 0, tau^2) * C+(tau; 0, 1) dtau
            log_prior = self._horseshoe_log_prior(x)

        return log_prior

    def likelihood(self, y, x, A):
        """
        ä¼¼ç„¶å‡½æ•°

        p(y|x) = N(y; Ax, sigma^2 I)
        """
        residual = y - A @ x
        log_likelihood = -0.5 * np.sum(residual**2) / (self.noise_sigma**2)

        return log_likelihood

    def posterior(self, x, y, A):
        """
        éå½’ä¸€åŒ–åéªŒ

        p(x|y) âˆ p(y|x) * p(x)
        """
        log_lik = self.likelihood(y, x, A)
        log_prior = self.sparse_prior(x)

        return log_lik + log_prior
```

---

### æ ¸å¿ƒç»„ä»¶2: å˜åˆ†æ¨æ–­

**å˜åˆ†è´å¶æ–¯æ¨æ–­**:
```python
class VariationalInference:
    """
    å˜åˆ†æ¨æ–­æ±‚è§£ç¨€ç–è´å¶æ–¯æ¨¡å‹

    ç”¨ç®€å•åˆ†å¸ƒè¿‘ä¼¼å¤æ‚åéªŒ
    """
    def __init__(self, max_iterations=1000, tolerance=1e-6):
        self.max_iterations = max_iterations
        self.tolerance = tolerance

    def fit(self, y, A, initial_x=None):
        """
        æ‰§è¡Œå˜åˆ†æ¨æ–­

        Args:
            y: è§‚æµ‹æ•°æ®
            A: æŠ•å½±çŸ©é˜µ
            initial_x: åˆå§‹å€¼

        Returns:
            posterior_mean: åéªŒå‡å€¼ (ç‚¹ä¼°è®¡)
            posterior_var: åéªŒæ–¹å·® (ä¸ç¡®å®šæ€§)
        """
        n = A.shape[1]

        # åˆå§‹åŒ–å˜åˆ†åˆ†å¸ƒ (é«˜æ–¯)
        if initial_x is None:
            mu = np.zeros(n)
        else:
            mu = initial_x.copy()

        sigma_sq = np.ones(n)  # å˜åˆ†æ–¹å·®

        # è¿­ä»£ä¼˜åŒ–
        for iteration in range(self.max_iterations):
            mu_old = mu.copy()

            # åæ ‡ä¸Šå‡æ›´æ–°
            for i in range(n):
                # è®¡ç®—ç¬¬iä¸ªåæ ‡çš„æ¡ä»¶åˆ†å¸ƒ
                mu[i], sigma_sq[i] = self._update_coordinate(
                    i, y, A, mu, sigma_sq
                )

            # æ£€æŸ¥æ”¶æ•›
            delta = np.linalg.norm(mu - mu_old) / np.linalg.norm(mu_old)
            if delta < self.tolerance:
                print(f"Converged at iteration {iteration}")
                break

        return mu, sigma_sq

    def _update_coordinate(self, i, y, A, mu, sigma_sq):
        """
        æ›´æ–°å•ä¸ªåæ ‡

        ä½¿ç”¨ç¨€ç–å…ˆéªŒçš„ç‰¹å®šæ›´æ–°è§„åˆ™
        """
        # è®¡ç®—æ®‹å·®
        residual = y - A @ mu + A[:, i] * mu[i]

        # è®¡ç®—å……åˆ†ç»Ÿè®¡é‡
        Sigma_i = np.sum(A[:, i]**2)
        r_i = np.sum(A[:, i] * residual)

        # åéªŒå‚æ•° (é«˜æ–¯ä¼¼ç„¶)
        post_var = 1.0 / (Sigma_i + 1e-10)
        post_mean = post_var * r_i

        # åº”ç”¨ç¨€ç–é˜ˆå€¼ (Laplaceå…ˆéªŒ)
        # è½¯é˜ˆå€¼ç®—å­
        threshold = self.lambda_sparse * post_var
        post_mean = np.sign(post_mean) * np.maximum(0, np.abs(post_mean) - threshold)

        return post_mean, post_var
```

---

### æ ¸å¿ƒç»„ä»¶3: å‡è®¾æ£€éªŒæ¡†æ¶

**è´å¶æ–¯å‡è®¾æ£€éªŒ**:
```python
class BayesianHypothesisTesting:
    """
    è´å¶æ–¯å‡è®¾æ£€éªŒ

    æ£€éªŒç‰¹å®šåŒºåŸŸæ˜¯å¦æœ‰æ˜¾è‘—è´¨é‡èšé›†
    """
    def __init__(self, vi_result):
        """
        Args:
            vi_result: å˜åˆ†æ¨æ–­ç»“æœ (mu, sigma_sq)
        """
        self.mu, self.sigma_sq = vi_result

    def test_point(self, index, threshold=0.95):
        """
        å•ç‚¹å‡è®¾æ£€éªŒ

        H0: x_i = 0 (æ— è´¨é‡)
        H1: x_i â‰  0 (æœ‰è´¨é‡)

        Args:
            index: æ£€éªŒçš„åƒç´ /ç³»æ•°ç´¢å¼•
            threshold: æ˜¾è‘—æ€§é˜ˆå€¼

        Returns:
            result: {
                'reject_h0': æ˜¯å¦æ‹’ç»H0,
                'bayes_factor': è´å¶æ–¯å› å­,
                'posterior_prob': åéªŒæ¦‚ç‡,
                'credibility': å¯ä¿¡åŒºé—´
            }
        """
        # è®¡ç®—åéªŒæ¦‚ç‡ P(x_i â‰  0 | y)
        # åŸºäºå˜åˆ†åéªŒè¿‘ä¼¼
        post_mean = self.mu[index]
        post_std = np.sqrt(self.sigma_sq[index])

        # è®¡ç®—P(x_i > 0 | y) å’Œ P(x_i < 0 | y)
        prob_positive = 1 - stats.norm.cdf(0, post_mean, post_std)
        prob_negative = stats.norm.cdf(0, post_mean, post_std)
        prob_nonzero = prob_positive + prob_negative

        # è´å¶æ–¯å› å­è¿‘ä¼¼
        # BF_10 = p(y|H1) / p(y|H0)
        bayes_factor = self._approximate_bayes_factor(index)

        # å†³ç­–
        reject_h0 = prob_nonzero > threshold

        # å¯ä¿¡åŒºé—´
        credibility_interval = (
            post_mean - 2 * post_std,
            post_mean + 2 * post_std
        )

        return {
            'reject_h0': reject_h0,
            'bayes_factor': bayes_factor,
            'posterior_prob_nonzero': prob_nonzero,
            'credibility_interval': credibility_interval,
            'posterior_mean': post_mean,
            'posterior_std': post_std
        }

    def test_region(self, region_mask, threshold=0.95):
        """
        åŒºåŸŸå‡è®¾æ£€éªŒ

        H0: åŒºåŸŸå†…æ‰€æœ‰x_i = 0
        H1: åŒºåŸŸå†…è‡³å°‘ä¸€ä¸ªx_i â‰  0
        """
        indices = np.where(region_mask)[0]

        # è®¡ç®—åŒºåŸŸç»Ÿè®¡é‡
        region_mean = np.mean(self.mu[indices])
        region_var = np.mean(self.sigma_sq[indices])

        # è”åˆæ£€éªŒ (ç®€åŒ–ç‰ˆ)
        test_statistic = np.sum(self.mu[indices]**2 / self.sigma_sq[indices])

        # è¿‘ä¼¼på€¼ (åŸºäºå¡æ–¹åˆ†å¸ƒ)
        from scipy.stats import chi2
        p_value = 1 - chi2.cdf(test_statistic, df=len(indices))

        return {
            'reject_h0': p_value < (1 - threshold),
            'test_statistic': test_statistic,
            'p_value': p_value,
            'region_mean': region_mean,
            'region_variance': region_var
        }

    def multiple_testing_correction(self, test_results, method='fdr'):
        """
        å¤šé‡æ£€éªŒæ ¡æ­£

        Args:
            test_results: å¤šä¸ªæ£€éªŒçš„ç»“æœåˆ—è¡¨
            method: 'fdr' (é”™è¯¯å‘ç°ç‡) | 'bonferroni'
        """
        if method == 'fdr':
            # Benjamini-Hochberg FDRæ§åˆ¶
            p_values = [r['p_value'] for r in test_results]
            n = len(p_values)

            sorted_indices = np.argsort(p_values)
            corrected = np.zeros(n)

            for i, idx in enumerate(sorted_indices):
                corrected[idx] = min(1, p_values[idx] * n / (i + 1))

            return corrected

        elif method == 'bonferroni':
            p_values = [r['p_value'] for r in test_results]
            return np.minimum(np.array(p_values) * len(p_values), 1.0)
```

---

## ğŸ“Š å®éªŒç»“æœ

### æ¨¡æ‹Ÿæ•°æ®éªŒè¯

| æ–¹æ³• | é‡å»ºè¯¯å·® | æ¢æµ‹ç‡ | è¯¯æŠ¥ç‡ |
|:---|:---:|:---:|:---:|
| ç›´æ¥é€† | 0.85 | 45% | 35% |
| Wieneræ»¤æ³¢ | 0.52 | 62% | 18% |
| L2æ­£åˆ™åŒ– | 0.38 | 70% | 12% |
| **ç¨€ç–è´å¶æ–¯** | **0.25** | **88%** | **5%** |

### å‡è®¾æ£€éªŒæ€§èƒ½

```
ROCåˆ†æ:

é˜ˆå€¼é€‰æ‹©:
  - ä¸¥æ ¼ (BF > 10): é«˜ç‰¹å¼‚æ€§ï¼Œå¯èƒ½æ¼æ£€
  - ä¸­ç­‰ (BF > 3): å¹³è¡¡
  - å®½æ¾ (BF > 1): é«˜çµæ•åº¦ï¼Œå¯èƒ½è¯¯æŠ¥

å®é™…åº”ç”¨å»ºè®®:
  - æš—ç‰©è´¨æ™•æ¢æµ‹: ä¸­ç­‰é˜ˆå€¼
  - å®‡å®™ç©ºæ´è¯†åˆ«: ä¸¥æ ¼é˜ˆå€¼
  - åˆæ­¥ç­›é€‰: å®½æ¾é˜ˆå€¼
```

---

## ğŸ’¡ å¯¹ç»Ÿè®¡å­¦ä¹ çš„å¯ç¤º

### ç¨€ç–è´å¶æ–¯çš„ä¸€èˆ¬æ¡†æ¶

```python
class SparseBayesianFramework:
    """
    é€šç”¨ç¨€ç–è´å¶æ–¯æ¡†æ¶

    å¯åº”ç”¨äºå„ç§é€†é—®é¢˜
    """
    def __init__(self, forward_model, prior_type='laplace'):
        """
        Args:
            forward_model: å‰å‘æ¨¡å‹ y = A(x) + n
            prior_type: ç¨€ç–å…ˆéªŒç±»å‹
        """
        self.forward_model = forward_model
        self.prior_type = prior_type

    def solve(self, y, **kwargs):
        """
        æ±‚è§£é€†é—®é¢˜

        é€šç”¨æµç¨‹:
        1. å®šä¹‰æ¦‚ç‡æ¨¡å‹
        2. é€‰æ‹©æ¨æ–­æ–¹æ³•
        3. è®¡ç®—åéªŒ
        4. ä¸ç¡®å®šæ€§é‡åŒ–
        """
        # 1. å®šä¹‰æ¨¡å‹
        model = self._define_model()

        # 2. æ¨æ–­
        if kwargs.get('method') == 'vi':
            result = self._variational_inference(y, model)
        elif kwargs.get('method') == 'mcmc':
            result = self._mcmc_sampling(y, model)

        # 3. åå¤„ç†
        result = self._post_process(result)

        return result

    def hypothesis_test(self, result, test_specification):
        """
        å‡è®¾æ£€éªŒ

        æ£€éªŒé‡å»ºç»“æœçš„æ˜¾è‘—æ€§
        """
        tester = BayesianHypothesisTesting(result)
        return tester.test_region(test_specification)
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **è´¨é‡æ˜ å°„** | Mass Mapping | ä»é€é•œä¿¡å·é‡å»ºç‰©è´¨åˆ†å¸ƒ |
| **ç¨€ç–å…ˆéªŒ** | Sparse Prior | å‡è®¾è§£åœ¨å°æ³¢åŸŸç¨€ç– |
| **å˜åˆ†æ¨æ–­** | Variational Inference | ç”¨ç®€å•åˆ†å¸ƒè¿‘ä¼¼åéªŒ |
| **è´å¶æ–¯å› å­** | Bayes Factor | æ¨¡å‹æ¯”è¾ƒçš„è´å¶æ–¯æŒ‡æ ‡ |
| **å¯ä¿¡åŒºé—´** | Credible Interval | è´å¶æ–¯ç½®ä¿¡åŒºé—´ |
| **é”™è¯¯å‘ç°ç‡** | False Discovery Rate | å¤šé‡æ£€éªŒæ§åˆ¶æŒ‡æ ‡ |
| **é€†é—®é¢˜** | Inverse Problem | ä»è§‚æµ‹æ¨æ–­åŸå› çš„é—®é¢˜ |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£è´¨é‡æ˜ å°„çš„ç‰©ç†èƒŒæ™¯
- [ ] æŒæ¡ç¨€ç–è´å¶æ–¯æ¨¡å‹çš„æ„å»º
- [ ] äº†è§£å˜åˆ†æ¨æ–­çš„åŸºæœ¬åŸç†
- [ ] ç†è§£è´å¶æ–¯å‡è®¾æ£€éªŒæ–¹æ³•
- [ ] äº†è§£å¤šé‡æ£€éªŒæ ¡æ­£
- [ ] èƒ½å¤Ÿåº”ç”¨åˆ°å…¶ä»–é€†é—®é¢˜

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **ç¨€ç–å…ˆéªŒä¸ºä»€ä¹ˆé€‚åˆè´¨é‡æ˜ å°„ï¼Ÿ**
   - æç¤º: å®‡å®™ç»“æ„ã€å°æ³¢å˜æ¢ã€è‡ªç”±åº¦

2. **å˜åˆ†æ¨æ–­ç›¸æ¯”MCMCçš„ä¼˜åŠ¿ï¼Ÿ**
   - æç¤º: è®¡ç®—æˆæœ¬ã€å¯æ‰©å±•æ€§ã€è¿‘ä¼¼è´¨é‡

3. **è´å¶æ–¯å› å­å¦‚ä½•è§£é‡Šï¼Ÿ**
   - æç¤º: è¯æ®å¼ºåº¦ã€Jeffreyså°ºåº¦

4. **å¦‚ä½•å¹³è¡¡æ¢æµ‹ç‡å’Œè¯¯æŠ¥ç‡ï¼Ÿ**
   - æç¤º: é˜ˆå€¼é€‰æ‹©ã€å…ˆéªŒè®¾å®šã€æŸå¤±å‡½æ•°

---

## ğŸ”— ç›¸å…³è®ºæ–‡æ¨è

### å¿…è¯»
1. **Sparse Bayesian Learning** - ç¨€ç–è´å¶æ–¯å­¦ä¹ åŸºç¡€
2. **Weak Lensing Mass Mapping** - å¼±é€é•œè´¨é‡æ˜ å°„ç»¼è¿°
3. **Variational Inference Review** - å˜åˆ†æ¨æ–­ç»¼è¿°

### æ‰©å±•é˜…è¯»
1. **Horseshoe Prior** - Horseshoeå…ˆéªŒè®ºæ–‡
2. **Bayesian Hypothesis Testing** - è´å¶æ–¯å‡è®¾æ£€éªŒ
3. **Cosmological Parameter Estimation** - å®‡å®™å­¦å‚æ•°ä¼°è®¡

---

## ğŸ“ ä¸ªäººç¬”è®°åŒº

### æˆ‘çš„ç†è§£



### ç–‘é—®ä¸å¾…æ¾„æ¸…



### ä¸å½“å‰é¡¹ç›®çš„ç»“åˆç‚¹



### å®ç°è®¡åˆ’



---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
**ä¸‹ä¸€æ­¥**: æ¢ç´¢ç¨€ç–è´å¶æ–¯æ–¹æ³•åœ¨å›¾åƒé‡å»ºä¸­çš„åº”ç”¨
