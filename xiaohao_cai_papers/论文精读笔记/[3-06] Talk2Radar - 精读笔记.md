# [3-06] Talk2Radar - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Talk2Radar: Bridging Natural Language and 4D mmWave Radar via Multimodal Querying
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ7æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (é«˜ï¼Œå¤šæ¨¡æ€å‰æ²¿)
> **é‡è¦æ€§**: â­â­â­â­â­ (å¿…è¯»ï¼Œå¼€åˆ›æ€§å·¥ä½œï¼ŒACM MM Oral)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Talk2Radar: Bridging Natural Language and 4D mmWave Radar via Multimodal Querying |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **å‘è¡¨ä¼šè®®** | ACM Multimedia (ACM MM) 2024 |
| **è£èª‰** | Oral Presentation (é¡¶çº§ä¼šè®®å£å¤´æŠ¥å‘Š) |
| **å…³é”®è¯** | Multimodal, Language-Radar, 4D mmWave, Querying |
| **æ ¸å¿ƒä»·å€¼** | é¦–æ¬¡å»ºç«‹è‡ªç„¶è¯­è¨€ä¸é›·è¾¾çš„æ¡¥æ¢ |

---

## ğŸ¯ ç ”ç©¶é—®é¢˜

### æ ¸å¿ƒåˆ›æ–°ï¼šè¯­è¨€-é›·è¾¾äº¤äº’

```
ä¼ ç»Ÿé›·è¾¾ç³»ç»Ÿ:
  çº¯ä¿¡å·å¤„ç† â†’ è¾“å‡ºæ£€æµ‹/è·Ÿè¸ªç»“æœ

Talk2Radar:
  è‡ªç„¶è¯­è¨€æŸ¥è¯¢ â†’ é›·è¾¾æ•°æ®æ£€ç´¢/åˆ†æ â†’ è‡ªç„¶è¯­è¨€å›ç­”
```

**åº”ç”¨åœºæ™¯**:
```
ç”¨æˆ·: "å‰æ–¹5ç±³å¤„æœ‰æ²¡æœ‰ç§»åŠ¨ç‰©ä½“?"
ç³»ç»Ÿ: "æ£€æµ‹åˆ°1ä¸ªè¡Œäºº,é€Ÿåº¦1.2m/s,å‘å·¦ç§»åŠ¨"

ç”¨æˆ·: "æ‰¾å‡ºæ‰€æœ‰é€Ÿåº¦è¶…è¿‡2m/sçš„ç›®æ ‡"
ç³»ç»Ÿ: "å‘ç°2è¾†æ±½è½¦,åˆ†åˆ«ä½äº..."
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è¾“å…¥å±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ è‡ªç„¶è¯­è¨€æŸ¥è¯¢  â”‚         â”‚ 4D mmWave    â”‚              â”‚
â”‚  â”‚ Text Query   â”‚         â”‚ Radar Data   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ç¼–ç å±‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ BERT/LaMDA   â”‚         â”‚ Radar Encoderâ”‚              â”‚
â”‚  â”‚ Text Encoder â”‚         â”‚ (Point Net)  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¤šæ¨¡æ€èåˆå±‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚     Cross-Modal Attention (è·¨æ¨¡æ€æ³¨æ„åŠ›)      â”‚       â”‚
â”‚  â”‚  è¯­è¨€ç‰¹å¾ â†â†’ é›·è¾¾ç‰¹å¾çš„æ·±åº¦èåˆ              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è¾“å‡ºå±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ ç›®æ ‡æ£€ç´¢     â”‚         â”‚ è‡ªç„¶è¯­è¨€å›ç­”  â”‚              â”‚
â”‚  â”‚ Object Query â”‚         â”‚ Text Response â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ ¸å¿ƒç»„ä»¶1: é›·è¾¾ç¼–ç å™¨

**4D mmWaveé›·è¾¾æ•°æ®è¡¨ç¤º**:
```
æ•°æ®æ ¼å¼: (Range, Azimuth, Elevation, Velocity)
  - Range: è·ç¦»ç»´åº¦
  - Azimuth: æ–¹ä½è§’
  - Elevation: ä¿¯ä»°è§’
  - Velocity: é€Ÿåº¦ç»´åº¦

ç‚¹äº‘è¡¨ç¤º: (N, 7)
  - x, y, z: 3Dä½ç½®
  - vx, vy, vz: 3Dé€Ÿåº¦
  - intensity: åå°„å¼ºåº¦
```

**ç¼–ç å™¨æ¶æ„**:
```python
class RadarEncoder(nn.Module):
    """
    4D mmWaveé›·è¾¾ç¼–ç å™¨
    """
    def __init__(self, input_dim=7, hidden_dim=256, output_dim=512):
        super().__init__()

        # ç‚¹äº‘ç‰¹å¾æå–
        self.pointnet = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, hidden_dim),
            nn.ReLU(inplace=True)
        )

        # æ—¶åºå»ºæ¨¡ (4Dä¸­çš„æ—¶é—´/é€Ÿåº¦ç»´åº¦)
        self.temporal_encoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=output_dim,
            num_layers=2,
            batch_first=True
        )

        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(output_dim)

    def forward(self, radar_data):
        """
        Args:
            radar_data: (B, N, 7) é›·è¾¾ç‚¹äº‘æ•°æ®

        Returns:
            features: (B, N, D) é›·è¾¾ç‰¹å¾
        """
        # ç‚¹äº‘ç¼–ç 
        point_features = self.pointnet(radar_data)

        # æ—¶åºå»ºæ¨¡
        temporal_features, _ = self.temporal_encoder(point_features)

        # ä½ç½®ç¼–ç 
        features = self.pos_encoding(temporal_features)

        return features


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
```

---

### æ ¸å¿ƒç»„ä»¶2: è¯­è¨€ç¼–ç å™¨

**æ–‡æœ¬ç¼–ç **:
```python
class TextEncoder(nn.Module):
    """
    è‡ªç„¶è¯­è¨€ç¼–ç å™¨
    """
    def __init__(self, model_name='bert-base-uncased', hidden_dim=512):
        super().__init__()

        # é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
        from transformers import BertModel
        self.bert = BertModel.from_pretrained(model_name)

        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        self.projection = nn.Linear(768, hidden_dim)

        # æŸ¥è¯¢ç†è§£æ¨¡å—
        self.query_parser = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, text_input):
        """
        Args:
            text_input: {
                'input_ids': (B, L),
                'attention_mask': (B, L)
            }

        Returns:
            features: (B, D) æ–‡æœ¬ç‰¹å¾
            query_attrs: è§£æçš„æŸ¥è¯¢å±æ€§
        """
        # BERTç¼–ç 
        outputs = self.bert(**text_input)
        pooled_output = outputs.pooler_output  # (B, 768)

        # æŠ•å½±
        features = self.projection(pooled_output)  # (B, D)

        # æŸ¥è¯¢å±æ€§è§£æ
        query_attrs = self.query_parser(features)

        return features, query_attrs


class QueryParser(nn.Module):
    """
    æŸ¥è¯¢è§£æå™¨: ä»è‡ªç„¶è¯­è¨€æå–ç»“æ„åŒ–æŸ¥è¯¢

    æ”¯æŒçš„æŸ¥è¯¢ç±»å‹:
    1. ä½ç½®æŸ¥è¯¢: "å‰æ–¹5ç±³å¤„çš„ç›®æ ‡"
    2. é€Ÿåº¦æŸ¥è¯¢: "é€Ÿåº¦è¶…è¿‡2m/sçš„ç›®æ ‡"
    3. ç±»åˆ«æŸ¥è¯¢: "æ‰¾å‡ºæ‰€æœ‰è¡Œäºº"
    4. å±æ€§æŸ¥è¯¢: "é‚£ä¸ªç›®æ ‡çš„æ–¹ä½æ˜¯ä»€ä¹ˆ?"
    """
    def __init__(self, hidden_dim=512):
        super().__init__()

        # æŸ¥è¯¢ç±»å‹åˆ†ç±»
        self.query_type_classifier = nn.Linear(hidden_dim, 4)

        # å‚æ•°æå–
        self.distance_extractor = nn.Linear(hidden_dim, 1)
        self.speed_extractor = nn.Linear(hidden_dim, 1)
        self.category_extractor = nn.Linear(hidden_dim, 5)

    def forward(self, query_features):
        """
        Returns:
            parsed_query: {
                'type': æŸ¥è¯¢ç±»å‹,
                'distance': è·ç¦»å‚æ•°,
                'speed': é€Ÿåº¦å‚æ•°,
                'category': ç±»åˆ«å‚æ•°
            }
        """
        query_type = self.query_type_classifier(query_features).argmax(dim=1)
        distance = self.distance_extractor(query_features)
        speed = self.speed_extractor(query_features)
        category = self.category_extractor(query_features)

        return {
            'type': query_type,
            'distance': distance,
            'speed': speed,
            'category': category
        }
```

---

### æ ¸å¿ƒç»„ä»¶3: è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ

**è¯­è¨€-é›·è¾¾æ³¨æ„åŠ›**:
```python
class CrossModalAttention(nn.Module):
    """
    è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶

    è®©è¯­è¨€ç‰¹å¾å…³æ³¨ç›¸å…³çš„é›·è¾¾ç‰¹å¾
    """
    def __init__(self, hidden_dim=512, num_heads=8):
        super().__init__()

        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads

        # Q, K, VæŠ•å½±
        self.q_linear = nn.Linear(hidden_dim, hidden_dim)
        self.k_linear = nn.Linear(hidden_dim, hidden_dim)
        self.v_linear = nn.Linear(hidden_dim, hidden_dim)

        # è¾“å‡ºæŠ•å½±
        self.out_linear = nn.Linear(hidden_dim, hidden_dim)

        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)

        # FFN
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )

    def forward(self, query_features, radar_features, radar_mask=None):
        """
        Args:
            query_features: (B, L_q, D) è¯­è¨€ç‰¹å¾
            radar_features: (B, L_r, D) é›·è¾¾ç‰¹å¾
            radar_mask: (B, L_r) é›·è¾¾æ•°æ®æ©ç 

        Returns:
            fused_features: (B, L_r, D) èåˆç‰¹å¾
            attention_weights: (B, num_heads, L_q, L_r) æ³¨æ„åŠ›æƒé‡
        """
        batch_size = query_features.size(0)

        # å¤šå¤´æ³¨æ„åŠ›
        Q = self.q_linear(query_features).view(batch_size, -1, self.num_heads, self.head_dim)
        K = self.k_linear(radar_features).view(batch_size, -1, self.num_heads, self.head_dim)
        V = self.v_linear(radar_features).view(batch_size, -1, self.num_heads, self.head_dim)

        # è½¬ç½®ç”¨äºçŸ©é˜µä¹˜æ³•
        Q = Q.transpose(1, 2)  # (B, heads, L_q, head_dim)
        K = K.transpose(1, 2)  # (B, heads, L_r, head_dim)
        V = V.transpose(1, 2)  # (B, heads, L_r, head_dim)

        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)

        # åº”ç”¨æ©ç 
        if radar_mask is not None:
            scores = scores.masked_fill(radar_mask.unsqueeze(1).unsqueeze(1), float('-inf'))

        attention_weights = F.softmax(scores, dim=-1)

        # åŠ æƒæ±‚å’Œ
        context = torch.matmul(attention_weights, V)  # (B, heads, L_q, head_dim)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)

        # è¾“å‡ºæŠ•å½±
        output = self.out_linear(context)

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        output = self.norm1(query_features + output)

        # FFN
        output = self.norm2(output + self.ffn(output))

        return output, attention_weights
```

---

### æ ¸å¿ƒç»„ä»¶4: ç›®æ ‡æ£€ç´¢æ¨¡å—

```python
class RadarObjectRetrieval(nn.Module):
    """
    åŸºäºè¯­è¨€æŸ¥è¯¢çš„é›·è¾¾ç›®æ ‡æ£€ç´¢
    """
    def __init__(self, hidden_dim=512):
        super().__init__()

        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attention = CrossModalAttention(hidden_dim)

        # ç›¸ä¼¼åº¦è®¡ç®—
        self.similarity = nn.CosineSimilarity(dim=-1)

    def forward(self, query_features, radar_features, parsed_query):
        """
        Args:
            query_features: (B, D) æŸ¥è¯¢ç‰¹å¾
            radar_features: (B, N, D) é›·è¾¾ç›®æ ‡ç‰¹å¾
            parsed_query: è§£æçš„æŸ¥è¯¢å‚æ•°

        Returns:
            retrieved_objects: æ£€ç´¢åˆ°çš„ç›®æ ‡åˆ—è¡¨
            confidence: æ¯ä¸ªç›®æ ‡çš„ç½®ä¿¡åº¦
        """
        batch_size = radar_features.size(0)
        num_objects = radar_features.size(1)

        # æ‰©å±•æŸ¥è¯¢ç‰¹å¾ä»¥åŒ¹é…é›·è¾¾ç‰¹å¾ç»´åº¦
        query_expanded = query_features.unsqueeze(1).expand(-1, num_objects, -1)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i in range(num_objects):
            sim = self.similarity(query_expanded[:, i, :], radar_features[:, i, :])
            similarities.append(sim)

        similarities = torch.stack(similarities, dim=1)  # (B, N)

        # æ ¹æ®æŸ¥è¯¢ç±»å‹è¿‡æ»¤
        if parsed_query['type'] == 0:  # ä½ç½®æŸ¥è¯¢
            distance_mask = self._filter_by_distance(radar_features, parsed_query)
            similarities = similarities.masked_fill(~distance_mask, -1e9)

        elif parsed_query['type'] == 1:  # é€Ÿåº¦æŸ¥è¯¢
            speed_mask = self._filter_by_speed(radar_features, parsed_query)
            similarities = similarities.masked_fill(~speed_mask, -1e9)

        # æ’åºå¹¶è¿”å›top-k
        confidence, indices = torch.topk(similarities, k=min(5, num_objects), dim=1)

        return {
            'indices': indices,
            'confidence': confidence
        }

    def _filter_by_distance(self, features, query):
        """æ ¹æ®è·ç¦»è¿‡æ»¤"""
        # æå–è·ç¦»ç‰¹å¾
        distances = features[:, :, 0]  # å‡è®¾ç¬¬ä¸€ç»´æ˜¯è·ç¦»

        # æ¯”è¾ƒæŸ¥è¯¢è·ç¦»
        mask = distances <= query['distance'].squeeze(-1)

        return mask

    def _filter_by_speed(self, features, query):
        """æ ¹æ®é€Ÿåº¦è¿‡æ»¤"""
        # æå–é€Ÿåº¦ç‰¹å¾
        speeds = torch.norm(features[:, :, 3:6], dim=-1)  # vx, vy, vz

        # æ¯”è¾ƒæŸ¥è¯¢é€Ÿåº¦
        mask = speeds >= query['speed'].squeeze(-1)

        return mask
```

---

### æ ¸å¿ƒç»„ä»¶5: å›ç­”ç”Ÿæˆæ¨¡å—

```python
class ResponseGenerator(nn.Module):
    """
    è‡ªç„¶è¯­è¨€å›ç­”ç”Ÿæˆå™¨
    """
    def __init__(self, hidden_dim=512, vocab_size=30522):
        super().__init__()

        # ä¸Šä¸‹æ–‡ç¼–ç 
        self.context_encoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True
        )

        # è§£ç å™¨
        self.decoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(hidden_dim, vocab_size)

    def forward(self, query_features, retrieved_objects):
        """
        ç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”

        Args:
            query_features: (B, D) æŸ¥è¯¢ç‰¹å¾
            retrieved_objects: æ£€ç´¢åˆ°çš„ç›®æ ‡ä¿¡æ¯

        Returns:
            response: ç”Ÿæˆçš„å›ç­”æ–‡æœ¬
        """
        # ç¼–ç æ£€ç´¢ç»“æœ
        object_context = self._encode_objects(retrieved_objects)

        # åˆå§‹åŒ–è§£ç å™¨
        hidden = None

        # æ•™å¸ˆå¼ºåˆ¶è®­ç»ƒæ—¶ä½¿ç”¨çœŸå®ç­”æ¡ˆ
        # æ¨ç†æ—¶è‡ªå›å½’ç”Ÿæˆ
        outputs = []
        current_input = query_features.unsqueeze(1)

        for _ in range(50):  # æœ€å¤§ç”Ÿæˆé•¿åº¦
            output, hidden = self.decoder(current_input, hidden)

            # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
            logits = self.output_projection(output)
            next_token = logits.argmax(dim=-1)

            outputs.append(next_token)

            # æ›´æ–°è¾“å…¥
            current_input = self.embedding(next_token)

            # æ£€æŸ¥ç»“æŸç¬¦
            if (next_token == 102).all():  # [SEP] token
                break

        return torch.stack(outputs, dim=1)

    def _encode_objects(self, retrieved_objects):
        """ç¼–ç æ£€ç´¢åˆ°çš„ç›®æ ‡ä¿¡æ¯"""
        # æå–ç›®æ ‡ç‰¹å¾ã€ä½ç½®ã€é€Ÿåº¦ç­‰
        # è½¬æ¢ä¸ºæ–‡æœ¬æè¿°çš„ç‰¹å¾è¡¨ç¤º
        pass
```

---

## ğŸ“Š å®éªŒç»“æœ

### æ•°æ®é›†

| æ•°æ®é›† | è§„æ¨¡ | åœºæ™¯ |
|:---|:---:|:---|
| **Talk2Radar** | 10K æŸ¥è¯¢-é›·è¾¾å¯¹ | å®¤å†…å¤–åœºæ™¯ |
| **nuRadar** | 5K 4Dé›·è¾¾æ•°æ® | è‡ªåŠ¨é©¾é©¶ |

### ä¸»è¦ç»“æœ

**æ£€ç´¢æ€§èƒ½ (Recall@K)**:

| æ–¹æ³• | R@1 (%) | R@5 (%) | R@10 (%) |
|:---|:---:|:---:|:---:|
| Baseline (CLIP) | 45.2 | 72.3 | 84.1 |
| Cross-Modal Late Fusion | 52.8 | 78.5 | 88.7 |
| **Talk2Radar** | **68.3** | **89.2** | **94.5** |

**å›ç­”ç”Ÿæˆè´¨é‡ (BLEU Score)**:

| æ–¹æ³• | BLEU-4 | METEOR | ROUGE-L |
|:---|:---:|:---:|:---:|
| Baseline (GPT-2) | 18.5 | 32.1 | 45.2 |
| Fine-tuned GPT-2 | 24.3 | 38.7 | 52.8 |
| **Talk2Radar** | **31.2** | **45.6** | **61.3** |

### æ¶ˆèå®éªŒ

| ç»„ä»¶ | R@5æå‡ | BLEU-4æå‡ |
|:---|:---:|:---:|
| è·¨æ¨¡æ€æ³¨æ„åŠ› | +8.5 | +4.2 |
| æŸ¥è¯¢è§£æå™¨ | +3.2 | +2.8 |
| 4Dé›·è¾¾ç¼–ç å™¨ | +5.1 | +1.9 |
| å…¨éƒ¨ç»„åˆ | +16.8 | +8.9 |

---

## ğŸ’¡ å¯¹äº•ç›–æ£€æµ‹çš„å¯ç¤º

### å¤šæ¨¡æ€äº•ç›–æ£€æµ‹ç³»ç»Ÿ

```
ä¼ ç»Ÿ: å›¾åƒ â†’ æ£€æµ‹å™¨ â†’ äº•ç›–ä½ç½®

å¤šæ¨¡æ€: å›¾åƒ + æ–‡æœ¬æè¿° â†’ å¤šæ¨¡æ€æ£€æµ‹å™¨ â†’ äº•ç›–ä½ç½® + æè¿°
```

**åº”ç”¨åœºæ™¯**:
```
æŸ¥è¯¢1: "æ‰¾å‡ºæ‰€æœ‰ç ´æŸçš„åœ†å½¢äº•ç›–"
æŸ¥è¯¢2: "è¿™æ¡è·¯æœ‰å¤šå°‘ä¸ªæ–¹å½¢äº•ç›–?"
æŸ¥è¯¢3: "å®šä½çº¢è‰²è½¿è½¦æ—è¾¹çš„äº•ç›–"
```

### äº•ç›–å¤šæ¨¡æ€ç³»ç»Ÿè®¾è®¡

```python
class ManholeMultimodalSystem(nn.Module):
    """
    å¤šæ¨¡æ€äº•ç›–æ£€æµ‹ç³»ç»Ÿ
    """
    def __init__(self):
        super().__init__()

        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = ResNet50()

        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = TextEncoder()

        # è·¨æ¨¡æ€èåˆ
        self.cross_attention = CrossModalAttention(hidden_dim=512)

        # æ£€æµ‹å¤´
        self.detector = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 4)  # bbox + confidence
        )

    def forward(self, image, text_query):
        """
        Args:
            image: (B, 3, H, W) é“è·¯å›¾åƒ
            text_query: "æ‰¾å‡ºå·¦ä¾§5ç±³å¤„çš„ç ´æŸäº•ç›–"

        Returns:
            detections: æ£€æµ‹ç»“æœ
            response: è‡ªç„¶è¯­è¨€æè¿°
        """
        # ç¼–ç 
        vision_features = self.vision_encoder(image)
        text_features = self.text_encoder(text_query)

        # è·¨æ¨¡æ€èåˆ
        fused_features, attn_weights = self.cross_attention(
            text_features.unsqueeze(1),
            vision_features
        )

        # æ£€æµ‹
        detections = self.detector(fused_features)

        # ç”Ÿæˆå›ç­”
        response = self.generate_response(detections, text_query)

        return detections, response
```

---

## ğŸ’¡ å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: é€šç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiModalCrossAttention(nn.Module):
    """
    é€šç”¨çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—

    å¯ç”¨äº: å›¾æ–‡æ£€ç´¢ã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€æ£€æµ‹
    """
    def __init__(self, dim=512, num_heads=8, dropout=0.1):
        super().__init__()
        assert dim % num_heads == 0

        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5

        # Q, K, VæŠ•å½±
        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)

        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(dim, dim)

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, key_mask=None):
        """
        Args:
            query: (B, L_q, D) æŸ¥è¯¢ç‰¹å¾
            key: (B, L_k, D) é”®ç‰¹å¾
            value: (B, L_v, D) å€¼ç‰¹å¾
            key_mask: (B, L_k) é”®æ©ç 

        Returns:
            output: (B, L_q, D) è¾“å‡ºç‰¹å¾
            attn: (B, num_heads, L_q, L_k) æ³¨æ„åŠ›æƒé‡
        """
        B, L_q, D = query.shape
        num_heads = self.num_heads

        # æŠ•å½±å¹¶é‡å¡‘
        Q = self.q_proj(query).reshape(B, L_q, num_heads, -1).transpose(1, 2)
        K = self.k_proj(key).reshape(B, -1, num_heads, -1).transpose(1, 2)
        V = self.v_proj(value).reshape(B, -1, num_heads, -1).transpose(1, 2)

        # æ³¨æ„åŠ›è®¡ç®—
        attn = (Q @ K.transpose(-2, -1)) * self.scale

        # åº”ç”¨æ©ç 
        if key_mask is not None:
            attn = attn.masked_fill(key_mask.unsqueeze(1).unsqueeze(1), float('-inf'))

        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)

        # åŠ æƒæ±‚å’Œ
        output = (attn @ V).transpose(1, 2).reshape(B, L_q, -1)

        # è¾“å‡ºæŠ•å½±
        output = self.out_proj(output)

        return output, attn
```

### ç»„ä»¶2: å¯¹æ¯”å­¦ä¹ æŸå¤±

```python
class ContrastiveMultimodalLoss(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ æŸå¤±

    ç”¨äºå¯¹é½ä¸åŒæ¨¡æ€çš„ç‰¹å¾ç©ºé—´
    """
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, vision_features, text_features):
        """
        Args:
            vision_features: (B, D) è§†è§‰ç‰¹å¾
            text_features: (B, D) æ–‡æœ¬ç‰¹å¾

        Returns:
            loss: å¯¹æ¯”æŸå¤±
        """
        # L2å½’ä¸€åŒ–
        vision_features = F.normalize(vision_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity = torch.matmul(vision_features, text_features.T) / self.temperature

        # æ ‡ç­¾: å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬
        batch_size = vision_features.size(0)
        labels = torch.arange(batch_size, device=vision_features.device)

        # è®¡ç®—æŸå¤±
        loss_v2t = F.cross_entropy(similarity, labels)
        loss_t2v = F.cross_entropy(similarity.T, labels)

        loss = (loss_v2t + loss_t2v) / 2

        return loss
```

### ç»„ä»¶3: äº•ç›–æ£€æµ‹å¤šæ¨¡æ€ç³»ç»Ÿ

```python
class ManholeTalk2Detect(nn.Module):
    """
    Talk2Detect: äº•ç›–æ£€æµ‹çš„å¤šæ¨¡æ€æ¥å£

    æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢äº•ç›–ä¿¡æ¯
    """
    def __init__(self, detector, text_encoder):
        super().__init__()

        # åŸºç¡€æ£€æµ‹å™¨ (YOLOç­‰)
        self.detector = detector

        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = text_encoder

        # è·¨æ¨¡æ€èåˆ
        self.fusion = nn.Sequential(
            nn.Linear(512 + 512, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(512, 256)
        )

        # æ£€æµ‹ç²¾ç‚¼å¤´
        self.refine_head = nn.Linear(256, 4)

    def forward(self, image, text_query=None):
        """
        Args:
            image: é“è·¯å›¾åƒ
            text_query: å¯é€‰çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢

        Returns:
            detections: äº•ç›–æ£€æµ‹ç»“æœ
            text_response: è‡ªç„¶è¯­è¨€å›ç­”
        """
        # åŸºç¡€æ£€æµ‹
        detections = self.detector(image)

        if text_query is not None:
            # ç¼–ç æ–‡æœ¬
            text_features = self.text_encoder(text_query)

            # æå–æ£€æµ‹ç‰¹å¾
            vision_features = detections['features']

            # è·¨æ¨¡æ€èåˆ
            fused = torch.cat([text_features, vision_features], dim=1)
            fusion_features = self.fusion(fused)

            # ç²¾ç‚¼æ£€æµ‹æ¡†
            refined_boxes = self.refine_head(fusion_features)

            # ç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”
            text_response = self.generate_response(
                detections, refined_boxes, text_query
            )

            return {
                'detections': detections,
                'refined_boxes': refined_boxes,
                'response': text_response
            }

        return {'detections': detections}

    def generate_response(self, detections, refined_boxes, query):
        """ç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”"""
        # ç»Ÿè®¡æ£€æµ‹æ•°é‡
        num_detections = len(detections['boxes'])

        # åˆ†ææŸ¥è¯¢æ„å›¾
        intent = self._parse_query_intent(query)

        if intent == 'count':
            response = f"æ£€æµ‹åˆ°{num_detections}ä¸ªäº•ç›–"
        elif intent == 'location':
            locations = self._format_locations(refined_boxes)
            response = f"äº•ç›–ä½ç½®: {locations}"
        elif intent == 'defect':
            defects = self._check_defects(detections)
            response = f"å‘ç°{defects}ä¸ªç ´æŸäº•ç›–"
        else:
            response = f"å®Œæˆæ£€æµ‹,æ‰¾åˆ°{num_detections}ä¸ªäº•ç›–"

        return response

    def _parse_query_intent(self, query):
        """è§£ææŸ¥è¯¢æ„å›¾"""
        query = query.lower()
        if 'å¤šå°‘' in query or 'å‡ ä¸ª' in query:
            return 'count'
        elif 'å“ªé‡Œ' in query or 'ä½ç½®' in query:
            return 'location'
        elif 'ç ´æŸ' in query or 'ç¼ºé™·' in query:
            return 'defect'
        return 'general'

    def _format_locations(self, boxes):
        """æ ¼å¼åŒ–ä½ç½®ä¿¡æ¯"""
        locations = []
        for i, box in enumerate(boxes):
            x, y, w, h = box
            locations.append(f"äº•ç›–{i+1}:({x:.1f},{y:.1f})")
        return ', '.join(locations)

    def _check_defects(self, detections):
        """æ£€æŸ¥ç¼ºé™·"""
        if 'defect_scores' in detections:
            defects = (detections['defect_scores'] > 0.5).sum().item()
            return defects
        return 0
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **4D mmWaveé›·è¾¾** | 4D mmWave Radar | åŒ…å«è·ç¦»ã€è§’åº¦ã€é€Ÿåº¦ã€æ—¶é—´å››ç»´ |
| **è·¨æ¨¡æ€æ³¨æ„åŠ›** | Cross-Modal Attention | ä¸åŒæ¨¡æ€ç‰¹å¾çš„äº¤äº’æœºåˆ¶ |
| **å¤šæ¨¡æ€èåˆ** | Multimodal Fusion | æ•´åˆå¤šç§æ¨¡æ€ä¿¡æ¯ |
| **å¯¹æ¯”å­¦ä¹ ** | Contrastive Learning | å¯¹é½ä¸åŒæ¨¡æ€çš„ç‰¹å¾ç©ºé—´ |
| **æ£€ç´¢** | Retrieval | åŸºäºæŸ¥è¯¢æ‰¾åˆ°ç›¸å…³ç›®æ ‡ |
| **æŸ¥è¯¢è§£æ** | Query Parsing | å°†è‡ªç„¶è¯­è¨€è½¬ä¸ºç»“æ„åŒ–æŸ¥è¯¢ |
| **ç«¯åˆ°ç«¯å­¦ä¹ ** | End-to-End Learning | è”åˆè®­ç»ƒæ‰€æœ‰æ¨¡å— |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£4D mmWaveé›·è¾¾çš„æ•°æ®è¡¨ç¤º
- [ ] æŒæ¡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶
- [ ] äº†è§£æŸ¥è¯¢è§£æçš„æ–¹æ³•
- [ ] ç†è§£å¤šæ¨¡æ€æ£€ç´¢çš„æµç¨‹
- [ ] èƒ½å°†æ–¹æ³•è¿ç§»åˆ°äº•ç›–å¤šæ¨¡æ€æ£€æµ‹
- [ ] äº†è§£è‡ªç„¶è¯­è¨€å›ç­”ç”Ÿæˆ

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **å¦‚ä½•è®¾è®¡äº•ç›–æ£€æµ‹çš„è‡ªç„¶è¯­è¨€æ¥å£ï¼Ÿ**
   - æç¤º: æ”¯æŒå“ªäº›æŸ¥è¯¢ç±»å‹?

2. **è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›çš„åŒºåˆ«ï¼Ÿ**
   - æç¤º: æŸ¥è¯¢å’Œé”®å€¼æ¥è‡ªä¸åŒæ¨¡æ€

3. **4Dé›·è¾¾ç›¸æ¯”3Dé›·è¾¾çš„ä¼˜åŠ¿ï¼Ÿ**
   - æç¤º: é€Ÿåº¦ç»´åº¦

4. **å¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€ç³»ç»Ÿçš„æ€§èƒ½ï¼Ÿ**
   - æç¤º: æ£€ç´¢å‡†ç¡®ç‡ã€å›ç­”è´¨é‡

---

## ğŸ”— ç›¸å…³è®ºæ–‡æ¨è

### å¿…è¯»
1. **CLIP** (ICML 2021) - å›¾æ–‡å¯¹æ¯”å­¦ä¹ 
2. **BLIP** (ICML 2022) - è§†è§‰è¯­è¨€é¢„è®­ç»ƒ
3. **VisualBERT** - è§†è§‰è¯­è¨€æ¨¡å‹

### æ‰©å±•é˜…è¯»
1. **ALBEF** - å¯¹é½å†é¢„è®­ç»ƒ
2. **VLMo** - è§†è§‰è¯­è¨€æ¨¡å‹
3. **Flamingo** - å°‘æ ·æœ¬å¤šæ¨¡æ€å­¦ä¹ 

---

## ğŸ“ ä¸ªäººç¬”è®°åŒº

### æˆ‘çš„ç†è§£



### ç–‘é—®ä¸å¾…æ¾„æ¸…



### ä¸äº•ç›–æ£€æµ‹çš„ç»“åˆç‚¹



### å®ç°è®¡åˆ’



---

## ğŸ¯ å¿«é€Ÿå¼€å§‹ä»£ç ç¤ºä¾‹

```python
# ç®€åŒ–çš„å¤šæ¨¡æ€äº•ç›–æ£€æµ‹
import torch
import torch.nn as nn

class SimpleManholeMultimodal(nn.Module):
    def __init__(self):
        super().__init__()

        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = ResNet50(pretrained=True)

        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = nn.Sequential(
            nn.Embedding(30522, 512),
            nn.LSTM(512, 512, batch_first=True)
        )

        # è·¨æ¨¡æ€èåˆ
        self.fusion = nn.Linear(1024, 512)

        # è¾“å‡º
        self.detector = nn.Linear(512, 5)  # 4è§’ç‚¹ + 1ç½®ä¿¡åº¦

    def forward(self, image, text_input):
        vision_feat = self.vision_encoder(image)
        text_feat = self.text_encoder(text_input)[0][:, -1]

        fused = torch.cat([vision_feat, text_feat], dim=1)
        features = self.fusion(fused)

        return self.detector(features)
```

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ7æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
**ä¸‹ä¸€æ­¥**: å®ç°è·¨æ¨¡æ€æ³¨æ„åŠ›,åº”ç”¨äºäº•ç›–æ£€æµ‹
