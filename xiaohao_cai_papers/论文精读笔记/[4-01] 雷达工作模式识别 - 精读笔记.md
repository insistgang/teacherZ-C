# [4-01] é›·è¾¾å·¥ä½œæ¨¡å¼è¯†åˆ« - ç²¾è¯»ç¬”è®°

> **è®ºæ–‡æ ‡é¢˜**: Radar Work Mode Recognition based on Bayesian Attention Mechanism
> **é˜…è¯»æ—¥æœŸ**: 2026å¹´2æœˆ10æ—¥
> **éš¾åº¦è¯„çº§**: â­â­â­â­ (è¾ƒéš¾)
> **é‡è¦æ€§**: â­â­â­â­ (é›·è¾¾ä¿¡å·å¤„ç†å¿…è¯»)

---

## ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **æ ‡é¢˜** | Radar Work Mode Recognition based on Bayesian Attention Mechanism |
| **ä½œè€…** | Xiaohao Cai ç­‰äºº |
| **å‘è¡¨æœŸåˆŠ** | IEEE Transactions on Aerospace and Electronic Systems |
| **å‘è¡¨å¹´ä»½** | 2020 |
| **æ–‡ç« ç±»å‹** | å…¨æ–‡è®ºæ–‡ |
| **å…³é”®è¯** | Radar Work Mode Recognition, Bayesian Attention, Uncertainty Quantification |
| **å½±å“å› å­** | IEEE TAES (2020) ~3.0 |

---

## ğŸ¯ ç ”ç©¶é—®é¢˜

### é›·è¾¾å·¥ä½œæ¨¡å¼è¯†åˆ«æŒ‘æˆ˜

**æ ¸å¿ƒé—®é¢˜**: å¦‚ä½•åœ¨å¤æ‚ç”µç£ç¯å¢ƒä¸‹å‡†ç¡®è¯†åˆ«é›·è¾¾å·¥ä½œæ¨¡å¼

**å·¥ä½œæ¨¡å¼ç±»å‹**:
```
é›·è¾¾å·¥ä½œæ¨¡å¼åˆ†ç±»:
â”œâ”€â”€ æœç´¢æ¨¡å¼ (Search)
â”‚   â””â”€â”€ å¤§èŒƒå›´æ‰«ææ¢æµ‹
â”œâ”€â”€ è·Ÿè¸ªæ¨¡å¼ (Track)
â”‚   â””â”€â”€ ç›®æ ‡é”å®šè·Ÿè¸ª
â”œâ”€â”€ åˆ¶å¯¼æ¨¡å¼ (Guidance)
â”‚   â””â”€â”€ å¯¼å¼¹åˆ¶å¯¼ç…§å°„
â””â”€â”€ å¹²æ‰°æ¨¡å¼ (Jamming)
    â””â”€â”€ ç”µå­å¯¹æŠ—
```

**æŠ€æœ¯æŒ‘æˆ˜**:
```
1. ä¿¡å·ç‰¹å¾å¤æ‚å¤šå˜
   - è„‰å†²é‡å¤é¢‘ç‡(PRF)å˜åŒ–
   - æ³¢å½¢å‚æ•°è°ƒåˆ¶

2. ç”µç£ç¯å¢ƒå¹²æ‰°
   - å™ªå£°å¹²æ‰°
   - å¤šå¾„æ•ˆåº”

3. ä¸ç¡®å®šæ€§å»ºæ¨¡å›°éš¾
   - è§‚æµ‹ä¸ç¡®å®šæ€§
   - æ¨¡å‹ä¸ç¡®å®šæ€§
```

---

## ğŸ”¬ æ–¹æ³•è®ºè¯¦è§£

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    é›·è¾¾ä¿¡å·è¾“å…¥                           â”‚
â”‚         (è„‰å†²åºåˆ—ã€é¢‘ç‡ã€è„‰å®½ã€PRIç­‰å‚æ•°)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç‰¹å¾æå–å±‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ æ—¶åŸŸç‰¹å¾: è„‰å†²é—´éš”(PRI)ã€è„‰å®½(PW)                  â”‚    â”‚
â”‚  â”‚ é¢‘åŸŸç‰¹å¾: è½½é¢‘(RF)ã€å¸¦å®½(BW)                       â”‚    â”‚
â”‚  â”‚ è°ƒåˆ¶ç‰¹å¾: é¢‘ç‡è°ƒåˆ¶ã€ç›¸ä½è°ƒåˆ¶                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è´å¶æ–¯æ³¨æ„åŠ›æœºåˆ¶ â­æ ¸å¿ƒ                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ä¼ ç»Ÿæ³¨æ„åŠ›: ç¡®å®šæ€§æƒé‡                            â”‚    â”‚
â”‚  â”‚ è´å¶æ–¯æ³¨æ„åŠ›: æƒé‡åˆ†å¸ƒ p(w|data)                 â”‚    â”‚
â”‚  â”‚                                                  â”‚    â”‚
â”‚  â”‚ ä¸ç¡®å®šæ€§æŒ‡å¯¼çš„ç‰¹å¾é€‰æ‹©                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è´å¶æ–¯åˆ†ç±»å™¨                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ åéªŒåˆ†å¸ƒ: p(mode|features) âˆ p(features|mode)p(mode)â”‚    â”‚
â”‚  â”‚ ä¸ç¡®å®šæ€§é‡åŒ–: é¢„æµ‹æ–¹å·®ã€å¯ä¿¡åŒºé—´                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    å·¥ä½œæ¨¡å¼è¯†åˆ«ç»“æœ + ä¸ç¡®å®šæ€§ä¼°è®¡
```

---

### æ ¸å¿ƒç»„ä»¶1: é›·è¾¾ä¿¡å·ç‰¹å¾æå–

```python
import numpy as np
from scipy import signal
from scipy.stats import entropy

class RadarFeatureExtractor:
    """
    é›·è¾¾ä¿¡å·ç‰¹å¾æå–å™¨

    ä»é›·è¾¾è„‰å†²åºåˆ—æå–è¯†åˆ«ç‰¹å¾
    """
    def __init__(self):
        self.feature_dim = 12

    def extract_features(self, pulse_train):
        """
        ä»è„‰å†²åºåˆ—æå–ç‰¹å¾

        Args:
            pulse_train: è„‰å†²å‚æ•°åºåˆ—
                - toa: åˆ°è¾¾æ—¶é—´ (Time of Arrival)
                - rf: è½½é¢‘ (Radio Frequency)
                - pw: è„‰å®½ (Pulse Width)
                - pa: è„‰å†²å¹…åº¦ (Pulse Amplitude)

        Returns:
            features: ç‰¹å¾å‘é‡ (12ç»´)
        """
        features = {}

        # 1. PRIç‰¹å¾ (Pulse Repetition Interval)
        toa = pulse_train['toa']
        pri = np.diff(toa)  # è„‰å†²é—´éš”

        features['pri_mean'] = np.mean(pri)
        features['pri_std'] = np.std(pri)
        features['pri_entropy'] = self._compute_entropy(pri)

        # PRIè°ƒåˆ¶ç±»å‹è¯†åˆ«
        features['pri_modulation'] = self._identify_pri_modulation(pri)

        # 2. RFç‰¹å¾ (Radio Frequency)
        rf = pulse_train['rf']
        features['rf_mean'] = np.mean(rf)
        features['rf_std'] = np.std(rf)
        features['rf_sweep_rate'] = self._compute_sweep_rate(rf)

        # 3. PWç‰¹å¾ (Pulse Width)
        pw = pulse_train['pw']
        features['pw_mean'] = np.mean(pw)
        features['pw_std'] = np.std(pw)

        # 4. è”åˆç‰¹å¾
        features['rf_pw_correlation'] = np.corrcoef(rf, pw)[0, 1]

        # 5. æ—¶é¢‘ç‰¹å¾
        features['spectral_centroid'] = self._spectral_centroid(rf)

        return features

    def _compute_entropy(self, data, bins=10):
        """è®¡ç®—ä¿¡æ¯ç†µ"""
        hist, _ = np.histogram(data, bins=bins, density=True)
        hist = hist[hist > 0]
        return entropy(hist)

    def _identify_pri_modulation(self, pri):
        """
        è¯†åˆ«PRIè°ƒåˆ¶ç±»å‹

        Returns:
            0: å›ºå®šPRI
            1: æŠ–åŠ¨PRI
            2: å‚å·®PRI
            3: æ»‘å˜PRI
        """
        pri_diff = np.diff(pri)
        pri_std = np.std(pri)
        pri_mean = np.mean(pri)

        # å˜å¼‚ç³»æ•°
        cv = pri_std / (pri_mean + 1e-6)

        if cv < 0.05:
            return 0  # å›ºå®šPRI
        elif cv < 0.2:
            # æ£€æŸ¥å‘¨æœŸæ€§
            autocorr = np.correlate(pri_diff, pri_diff, mode='full')
            peaks = signal.find_peaks(autocorr[len(autocorr)//2:])[0]

            if len(peaks) > 0:
                return 2  # å‚å·®PRI
            else:
                return 1  # æŠ–åŠ¨PRI
        else:
            return 3  # æ»‘å˜PRI

    def _compute_sweep_rate(self, rf):
        """è®¡ç®—é¢‘ç‡æ‰«æé€Ÿç‡"""
        rf_diff = np.diff(rf)
        return np.mean(np.abs(rf_diff))

    def _spectral_centroid(self, rf):
        """è®¡ç®—é¢‘è°±è´¨å¿ƒ"""
        return np.sum(rf * np.arange(len(rf))) / (np.sum(rf) + 1e-6)
```

---

### æ ¸å¿ƒç»„ä»¶2: è´å¶æ–¯æ³¨æ„åŠ›æœºåˆ¶

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BayesianAttention(nn.Module):
    """
    è´å¶æ–¯æ³¨æ„åŠ›æœºåˆ¶

    æ ¸å¿ƒåˆ›æ–°: å°†æ³¨æ„åŠ›æƒé‡å»ºæ¨¡ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    è€Œéç¡®å®šæ€§å€¼
    """
    def __init__(self, feature_dim, num_modes=4, num_samples=10):
        """
        Args:
            feature_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            num_modes: å·¥ä½œæ¨¡å¼ç±»åˆ«æ•°
            num_samples: è’™ç‰¹å¡æ´›é‡‡æ ·æ¬¡æ•°
        """
        super().__init__()
        self.feature_dim = feature_dim
        self.num_modes = num_modes
        self.num_samples = num_samples

        # å‡å€¼ç½‘ç»œ (ç¡®å®šæ€§éƒ¨åˆ†)
        self.mean_net = nn.Sequential(
            nn.Linear(feature_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )

        # æ–¹å·®ç½‘ç»œ (ä¸ç¡®å®šæ€§éƒ¨åˆ†)
        self.log_var_net = nn.Sequential(
            nn.Linear(feature_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )

        # æ³¨æ„åŠ›æŸ¥è¯¢/é”®/å€¼æŠ•å½±
        self.query_proj = nn.Linear(32, 16)
        self.key_proj = nn.Linear(32, 16)
        self.value_proj = nn.Linear(32, 16)

        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(16, num_modes)

    def forward(self, features, return_uncertainty=True):
        """
        å‰å‘ä¼ æ’­

        Args:
            features: (batch_size, feature_dim)
            return_uncertainty: æ˜¯å¦è¿”å›ä¸ç¡®å®šæ€§ä¼°è®¡

        Returns:
            logits: åˆ†ç±»logits
            uncertainty: é¢„æµ‹ä¸ç¡®å®šæ€§ (å¦‚æœreturn_uncertainty=True)
        """
        # 1. æå–å‡å€¼ç‰¹å¾ (ç¡®å®šæ€§)
        mean_features = self.mean_net(features)

        # 2. æå–æ–¹å·®ç‰¹å¾ (ä¸ç¡®å®šæ€§)
        log_var = self.log_var_net(features)
        var = torch.exp(log_var)

        # 3. é‡å‚æ•°åŒ–é‡‡æ · (è’™ç‰¹å¡æ´›)
        attention_weights = []
        for _ in range(self.num_samples):
            # é‡‡æ ·: z = Î¼ + Ïƒ * Îµ
            epsilon = torch.randn_like(mean_features)
            sampled_features = mean_features + torch.sqrt(var) * epsilon

            # è®¡ç®—æ³¨æ„åŠ›
            Q = self.query_proj(sampled_features)
            K = self.key_proj(sampled_features)
            V = self.value_proj(sampled_features)

            # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
            scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(16)
            attn_weights = F.softmax(scores, dim=-1)

            attention_weights.append(attn_weights)

        # 4. èšåˆæ³¨æ„åŠ›æƒé‡
        attention_weights = torch.stack(attention_weights, dim=0)
        mean_attention = attention_weights.mean(dim=0)
        attention_uncertainty = attention_weights.std(dim=0)

        # 5. åº”ç”¨æ³¨æ„åŠ›
        attended = torch.matmul(mean_attention, V)

        # 6. åˆ†ç±»
        logits = self.classifier(attended)

        if return_uncertainty:
            # è®¡ç®—é¢„æµ‹ä¸ç¡®å®šæ€§
            epistemic_uncertainty = attention_uncertainty.mean()
            aleatoric_uncertainty = var.mean()

            uncertainty = {
                'epistemic': epistemic_uncertainty,
                'aleatoric': aleatoric_uncertainty,
                'total': epistemic_uncertainty + aleatoric_uncertainty
            }
            return logits, uncertainty

        return logits
```

---

### æ ¸å¿ƒç»„ä»¶3: è´å¶æ–¯åˆ†ç±»ä¸ä¸ç¡®å®šæ€§é‡åŒ–

```python
class BayesianRadarModeClassifier(nn.Module):
    """
    è´å¶æ–¯é›·è¾¾å·¥ä½œæ¨¡å¼åˆ†ç±»å™¨

    ç»“åˆè´å¶æ–¯ç¥ç»ç½‘ç»œè¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–
    """
    def __init__(self, feature_dim=12, num_modes=4, num_mc_samples=20):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_modes = num_modes
        self.num_mc_samples = num_mc_samples

        # è´å¶æ–¯æ³¨æ„åŠ›å±‚
        self.bayesian_attention = BayesianAttention(
            feature_dim, num_modes, num_samples=num_mc_samples
        )

        # è´å¶æ–¯çº¿æ€§å±‚ (Dropoutä½œä¸ºå˜åˆ†æ¨æ–­è¿‘ä¼¼)
        self.fc1 = nn.Linear(16, 32)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(32, num_modes)

    def forward(self, features, training=True):
        """
        å‰å‘ä¼ æ’­

        Args:
            features: è¾“å…¥ç‰¹å¾
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼

        Returns:
            predictions: é¢„æµ‹ç»“æœ
            uncertainty: ä¸ç¡®å®šæ€§ä¼°è®¡
        """
        if training:
            # è®­ç»ƒæ¨¡å¼: å•æ¬¡å‰å‘
            logits, uncertainty = self.bayesian_attention(features)
            return logits, uncertainty
        else:
            # æµ‹è¯•æ¨¡å¼: MC Dropouté‡‡æ ·
            predictions = []

            self.train()  # ä¿æŒdropoutå¼€å¯
            for _ in range(self.num_mc_samples):
                logits, _ = self.bayesian_attention(features, return_uncertainty=False)
                probs = F.softmax(logits, dim=-1)
                predictions.append(probs)

            self.eval()

            # èšåˆé¢„æµ‹
            predictions = torch.stack(predictions, dim=0)
            mean_pred = predictions.mean(dim=0)
            pred_variance = predictions.var(dim=0)

            # é¢„æµ‹ç†µ (ä¸ç¡®å®šæ€§åº¦é‡)
            predictive_entropy = -torch.sum(
                mean_pred * torch.log(mean_pred + 1e-10), dim=-1
            )

            uncertainty = {
                'predictive_entropy': predictive_entropy,
                'mutual_information': self._compute_mi(predictions, mean_pred),
                'variance': pred_variance
            }

            return mean_pred, uncertainty

    def _compute_mi(self, predictions, mean_pred):
        """
        è®¡ç®—äº’ä¿¡æ¯ (Mutual Information)

        MI = H[E[p(y|x,w)]] - E[H[p(y|x,w)]]

        è¡¡é‡è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic Uncertainty)
        """
        # æœŸæœ›é¢„æµ‹çš„ç†µ
        entropy_mean = -torch.sum(
            mean_pred * torch.log(mean_pred + 1e-10), dim=-1
        )

        # æœŸæœ›çš„ç†µ
        entropies = -torch.sum(
            predictions * torch.log(predictions + 1e-10), dim=-1
        )
        mean_entropy = entropies.mean(dim=0)

        # äº’ä¿¡æ¯
        mutual_info = entropy_mean - mean_entropy

        return mutual_info

    def recognize_mode(self, pulse_train, confidence_threshold=0.8):
        """
        è¯†åˆ«é›·è¾¾å·¥ä½œæ¨¡å¼

        Args:
            pulse_train: é›·è¾¾è„‰å†²åºåˆ—
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            result: {
                'mode': è¯†åˆ«çš„å·¥ä½œæ¨¡å¼,
                'confidence': ç½®ä¿¡åº¦,
                'uncertainty': ä¸ç¡®å®šæ€§ä¼°è®¡,
                'is_reliable': æ˜¯å¦å¯é 
            }
        """
        # ç‰¹å¾æå–
        extractor = RadarFeatureExtractor()
        features = extractor.extract_features(pulse_train)
        features_tensor = torch.tensor(list(features.values())).float().unsqueeze(0)

        # è´å¶æ–¯æ¨æ–­
        with torch.no_grad():
            pred_probs, uncertainty = self.forward(features_tensor, training=False)

        # å†³ç­–
        max_prob, predicted_mode = pred_probs.max(dim=-1)
        confidence = max_prob.item()

        mode_names = ['æœç´¢', 'è·Ÿè¸ª', 'åˆ¶å¯¼', 'å¹²æ‰°']

        result = {
            'mode': mode_names[predicted_mode.item()],
            'mode_id': predicted_mode.item(),
            'confidence': confidence,
            'all_probs': {
                name: prob.item()
                for name, prob in zip(mode_names, pred_probs[0])
            },
            'uncertainty': {
                k: v.item() if isinstance(v, torch.Tensor) else v
                for k, v in uncertainty.items()
            },
            'is_reliable': confidence > confidence_threshold
        }

        return result
```

---

## ğŸ“Š å®éªŒç»“æœ

### æ•°æ®é›†

| æ•°æ®é›† | å·¥ä½œæ¨¡å¼ | è„‰å†²æ ·æœ¬æ•° | ä¿¡å™ªæ¯”èŒƒå›´ |
|:---|:---:|:---:|:---:|
| **è‡ªå»ºæ•°æ®é›†** | 4ç±» | 10,000 | 0-20 dB |
| **å…¬å¼€æ•°æ®é›†** | 4ç±» | 5,000 | 5-15 dB |

### ä¸»è¦ç»“æœ (è¯†åˆ«å‡†ç¡®ç‡ %)

| æ–¹æ³• | æœç´¢æ¨¡å¼ | è·Ÿè¸ªæ¨¡å¼ | åˆ¶å¯¼æ¨¡å¼ | å¹²æ‰°æ¨¡å¼ | å¹³å‡ |
|:---|:---:|:---:|:---:|:---:|:---:|
| **ä¼ ç»Ÿæ–¹æ³• (SVM)** | 75.2 | 72.8 | 68.5 | 70.1 | 71.7 |
| **æ·±åº¦å­¦ä¹ æ–¹æ³• (CNN)** | 82.5 | 80.3 | 78.2 | 79.5 | 80.1 |
| **æ³¨æ„åŠ›æ–¹æ³•** | 86.3 | 84.7 | 82.1 | 83.5 | 84.2 |
| **[4-01] è´å¶æ–¯æ³¨æ„åŠ›** | **91.2** | **89.5** | **87.8** | **88.3** | **89.2** |

### ä¸ç¡®å®šæ€§é‡åŒ–æ•ˆæœ

| ä¿¡å™ªæ¯” | å‡†ç¡®ç‡ | å¹³å‡ä¸ç¡®å®šæ€§ | æ‹’è¯†ç‡(é«˜ä¸ç¡®å®šæ€§) |
|:---:|:---:|:---:|:---:|
| 20 dB | 94.5% | 0.12 | 2% |
| 10 dB | 89.2% | 0.28 | 8% |
| 5 dB | 81.3% | 0.45 | 18% |
| 0 dB | 72.1% | 0.62 | 35% |

---

## ğŸ’¡ å¯¹è¿å»ºæ£€æµ‹çš„è¿ç§»

### é›·è¾¾ä¿¡å·è¯†åˆ« â†’ é¥æ„Ÿå˜åŒ–æ£€æµ‹

```
ç›¸ä¼¼æ€§åˆ†æ:

é›·è¾¾å·¥ä½œæ¨¡å¼è¯†åˆ«          é¥æ„Ÿè¿å»ºæ£€æµ‹
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
è„‰å†²åºåˆ—ç‰¹å¾               æ—¶åºå›¾åƒç‰¹å¾
    â†“                        â†“
æ—¶åŸŸ/é¢‘åŸŸåˆ†æ              æ—¶é—´åºåˆ—åˆ†æ
    â†“                        â†“
æ¨¡å¼åˆ†ç±»                  å˜åŒ–/æœªå˜åŒ–åˆ†ç±»
    â†“                        â†“
ä¸ç¡®å®šæ€§é‡åŒ–              æ£€æµ‹ç½®ä¿¡åº¦ä¼°è®¡
```

### è´å¶æ–¯æ³¨æ„åŠ›è¿ç§»

```python
class BayesianChangeDetector(nn.Module):
    """
    è´å¶æ–¯å˜åŒ–æ£€æµ‹å™¨

    åŸºäº[4-01]è´å¶æ–¯æ³¨æ„åŠ›æœºåˆ¶
    ç”¨äºé¥æ„Ÿå›¾åƒå˜åŒ–æ£€æµ‹
    """
    def __init__(self, in_channels=3, num_classes=2):
        super().__init__()

        # ç‰¹å¾ç¼–ç å™¨ (åŒæ—¶ç›¸å›¾åƒ)
        self.encoder_t1 = self._build_encoder(in_channels)
        self.encoder_t2 = self._build_encoder(in_channels)

        # è´å¶æ–¯æ³¨æ„åŠ› (æ ¸å¿ƒç»„ä»¶)
        self.bayesian_attention = BayesianAttention(
            feature_dim=256,
            num_modes=num_classes,
            num_samples=10
        )

    def forward(self, img_t1, img_t2):
        """
        å˜åŒ–æ£€æµ‹

        Args:
            img_t1: æ—¶ç›¸1å›¾åƒ
            img_t2: æ—¶ç›¸2å›¾åƒ

        Returns:
            change_map: å˜åŒ–å›¾
            uncertainty: ä¸ç¡®å®šæ€§å›¾
        """
        # æå–åŒæ—¶ç›¸ç‰¹å¾
        feat_t1 = self.encoder_t1(img_t1)
        feat_t2 = self.encoder_t2(img_t2)

        # å·®å¼‚ç‰¹å¾
        diff_feat = torch.abs(feat_t1 - feat_t2)

        # è´å¶æ–¯æ³¨æ„åŠ›åˆ†ç±»
        logits, uncertainty = self.bayesian_attention(diff_feat)

        return logits, uncertainty

    def detect_with_rejection(self, img_t1, img_t2, uncertainty_threshold=0.3):
        """
        å¸¦æ‹’è¯†çš„å˜åŒ–æ£€æµ‹

        é«˜ä¸ç¡®å®šæ€§åŒºåŸŸæ ‡è®°ä¸º"éœ€äººå·¥å®¡æ ¸"
        """
        logits, uncertainty = self.forward(img_t1, img_t2)
        probs = F.softmax(logits, dim=1)

        # å˜åŒ–é¢„æµ‹
        change_pred = probs.argmax(dim=1)

        # ä¸ç¡®å®šæ€§æ©ç 
        high_uncertainty = uncertainty['total'] > uncertainty_threshold

        # æ ‡è®°ä¸ç¡®å®šåŒºåŸŸ (ç±»åˆ«-1è¡¨ç¤ºéœ€äººå·¥å®¡æ ¸)
        final_pred = change_pred.clone()
        final_pred[high_uncertainty] = -1

        return final_pred, uncertainty
```

---

## ğŸ’¡ å¯å¤ç”¨ä»£ç ç»„ä»¶

### ç»„ä»¶1: è´å¶æ–¯ä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—

```python
class UncertaintyEstimator:
    """
    ä¸ç¡®å®šæ€§ä¼°è®¡å™¨

    å¯å¤ç”¨äºä»»ä½•åˆ†ç±»ä»»åŠ¡
    """
    def __init__(self, model, num_samples=20):
        self.model = model
        self.num_samples = num_samples

    def estimate(self, x):
        """
        ä¼°è®¡é¢„æµ‹ä¸ç¡®å®šæ€§

        Returns:
            mean_pred: å¹³å‡é¢„æµ‹
            uncertainties: {
                'aleatoric': å¶ç„¶ä¸ç¡®å®šæ€§,
                'epistemic': è®¤çŸ¥ä¸ç¡®å®šæ€§,
                'total': æ€»ä¸ç¡®å®šæ€§
            }
        """
        self.model.train()  # å¯ç”¨Dropout

        predictions = []
        for _ in range(self.num_samples):
            with torch.no_grad():
                pred = self.model(x)
                predictions.append(F.softmax(pred, dim=-1))

        predictions = torch.stack(predictions)

        # å¹³å‡é¢„æµ‹
        mean_pred = predictions.mean(dim=0)

        # å¶ç„¶ä¸ç¡®å®šæ€§ (æ•°æ®å™ªå£°)
        aleatoric = predictions.mean(dim=0) * (1 - predictions.mean(dim=0))

        # è®¤çŸ¥ä¸ç¡®å®šæ€§ (æ¨¡å‹ä¸ç¡®å®šæ€§)
        epistemic = predictions.var(dim=0)

        uncertainties = {
            'aleatoric': aleatoric.mean().item(),
            'epistemic': epistemic.mean().item(),
            'total': (aleatoric + epistemic).mean().item()
        }

        return mean_pred, uncertainties
```

---

## ğŸ“– å…³é”®æ¦‚å¿µä¸æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|:---|:---|:---|
| **PRI** | Pulse Repetition Interval | è„‰å†²é‡å¤é—´éš” |
| **RF** | Radio Frequency | è½½é¢‘ |
| **PW** | Pulse Width | è„‰å®½ |
| **è´å¶æ–¯æ³¨æ„åŠ›** | Bayesian Attention | æƒé‡ä¸ºåˆ†å¸ƒçš„æ³¨æ„åŠ› |
| **è®¤çŸ¥ä¸ç¡®å®šæ€§** | Epistemic Uncertainty | æ¨¡å‹çŸ¥è¯†ä¸è¶³å¯¼è‡´ |
| **å¶ç„¶ä¸ç¡®å®šæ€§** | Aleatoric Uncertainty | æ•°æ®å™ªå£°å¯¼è‡´ |
| **äº’ä¿¡æ¯** | Mutual Information | ä¸ç¡®å®šæ€§åˆ†è§£æŒ‡æ ‡ |

---

## âœ… å¤ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£é›·è¾¾å·¥ä½œæ¨¡å¼è¯†åˆ«çš„æ ¸å¿ƒæŒ‘æˆ˜
- [ ] æŒæ¡PRI/RF/PWç­‰å…³é”®ç‰¹å¾æå–æ–¹æ³•
- [ ] ç†è§£è´å¶æ–¯æ³¨æ„åŠ›vsä¼ ç»Ÿæ³¨æ„åŠ›çš„åŒºåˆ«
- [ ] æŒæ¡ä¸¤ç§ä¸ç¡®å®šæ€§(è®¤çŸ¥/å¶ç„¶)çš„åŒºåˆ«
- [ ] èƒ½å°†è´å¶æ–¯æ–¹æ³•è¿ç§»åˆ°å˜åŒ–æ£€æµ‹ä»»åŠ¡

---

## ğŸ¤” æ€è€ƒé—®é¢˜

1. **ä¸ºä»€ä¹ˆè´å¶æ–¯æ³¨æ„åŠ›æ¯”ç¡®å®šæ€§æ³¨æ„åŠ›æ›´å¥½ï¼Ÿ**
   - æç¤º: ä¸ç¡®å®šæ€§æŒ‡å¯¼çš„ç‰¹å¾é€‰æ‹©

2. **å¦‚ä½•è§£é‡Šè®¤çŸ¥ä¸ç¡®å®šæ€§å’Œå¶ç„¶ä¸ç¡®å®šæ€§ï¼Ÿ**
   - æç¤º: æ¨¡å‹çŸ¥è¯†vsæ•°æ®å™ªå£°

3. **é«˜ä¸ç¡®å®šæ€§æ ·æœ¬åº”è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ**
   - æç¤º: æ‹’è¯†ã€ä¸»åŠ¨å­¦ä¹ ã€äººå·¥å®¡æ ¸

---

## ğŸ”— ç›¸å…³è®ºæ–‡æ¨è

### å¿…è¯»
1. **[4-02] DNCNeté›·è¾¾å»å™ª** - é›·è¾¾ä¿¡å·é¢„å¤„ç†
2. **[4-07] é«˜ç»´é€†é—®é¢˜ä¸ç¡®å®šæ€§** - è´å¶æ–¯ç†è®ºåŸºç¡€
3. **[4-08] è¿‘ç«¯åµŒå¥—é‡‡æ ·** - è´å¶æ–¯æ¨æ–­ç®—æ³•

### æ‰©å±•é˜…è¯»
1. **MC Dropout** - Gal et al. ä¸ç¡®å®šæ€§ä¼°è®¡
2. **Bayesian Neural Networks** - è´å¶æ–¯ç¥ç»ç½‘ç»œç»¼è¿°

---

**ç¬”è®°åˆ›å»ºæ—¶é—´**: 2026å¹´2æœˆ10æ—¥
**çŠ¶æ€**: å·²å®Œæˆç²¾è¯» âœ…
