# 变分法方法手册

> **创建日期**: 2026年2月9日
> **范围**: 图像分割与去噪的变分方法
> **基于论文**: [1-04] Mumford-Shah与ROF, [2-01] 凸优化分割

---

## 目录

1. [方法概述](#方法概述)
2. [核心理论](#核心理论)
3. [算法模板](#算法模板)
4. [代码实现](#代码实现)
5. [实验设计](#实验设计)
6. [论文应用案例](#论文应用案例)

---

## 方法概述

### 核心思想

变分法将图像处理问题转化为**能量泛函最小化问题**：

```
图像处理问题 → 能量泛函 E(u) → 寻找 u* = argmin E(u)
```

### 适用场景

| 场景 | 适用方法 | 特点 |
|------|----------|------|
| 图像去噪 | ROF模型 | 保持边缘的去噪 |
| 图像分割 | Mumford-Shah | 分片光滑逼近 |
| 医学图像 | 凸优化分割 | 全局最优解 |
| 退化图像 | SLaT三阶段 | 鲁棒分割 |

### 方法演进

```
变分法基础 (ROF, Mumford-Shah 1989-1992)
    ↓
凸优化突破 (Chan-Esedoğlu-Nikolova 2006)
    ↓
高效算法 (Split Bregman, 原始-对偶 2009-2013)
    ↓
深度学习融合 (可学习变分层 2019-2023)
```

---

## 核心理论

### 1. ROF模型 (Rudin-Osher-Fatemi)

#### 能量泛函

```
E_ROF(u) = ∫_Ω |∇u| dx + λ ∫_Ω (u - f)² dx
```

**物理意义**：
- 第一项 `∫|∇u|dx`: 全变分正则化，惩罚过度振荡
- 第二项 `λ∫(u-f)²dx`: 数据保真项，保持与原图相似

#### 欧拉-拉格朗日方程

```
-div(∇u/|∇u|) + 2λ(u - f) = 0
```

### 2. Mumford-Shah泛函

#### 能量泛函

```
E_MS(u, Γ) = ∫_Ω\Γ |∇u|² dx + μ ∫_Ω (u - f)² dx + ν|Γ|
```

**三项解释**：
- `∫|∇u|²`: 平滑项 - 同质区域内部光滑
- `μ∫(u-f)²`: 数据项 - 逼近原图像
- `ν|Γ|`: 边缘长度惩罚 - 控制边缘复杂性

### 3. 凸松弛理论

#### Chan-Esedoğlu-Nikolova模型

**松弛后的凸泛函**：

```
E_CE(u, v) = ∫_Ω |v|² dx + μ ∫_Ω (u - f)² dx + β ∫_Ω |∇u - v|² dx
```

**凸性证明**：
1. 关于u是二次的 → 凸
2. 关于v是L2范数平方 → 凸
3. 耦合项也是二次的 → 联合凸

---

## 算法模板

### 模板1: 梯度下降法

```python
def gradient_descent_template(f, lambda_param=0.1, tau=0.01, iterations=100):
    """
    通用梯度下降模板

    适用于: ROF去噪、基本能量最小化
    """
    u = f.copy()

    for i in range(iterations):
        # 1. 计算梯度
        grad_u = compute_gradient(u)

        # 2. 计算散度
        div_term = compute_divergence(grad_u / (grad_u.norm() + eps))

        # 3. 梯度下降更新
        u = u + tau * (div_term - 2 * lambda_param * (u - f))

        # 4. 收敛检查
        if convergence_check(u, prev_u, tol=1e-4):
            break

    return u
```

### 模板2: 原始-对偶算法

```python
def primal_dual_template(f, lambda_param=0.1, iterations=100):
    """
    原始-对偶算法模板

    适用于: ROF模型、TV-L1问题
    """
    # 初始化
    u = f.copy()
    p_x = np.zeros_like(f)
    p_y = np.zeros_like(f)

    tau = 0.1  # 原始步长
    sigma = 0.1  # 对偶步长

    for i in range(iterations):
        # 1. 对偶变量更新
        grad_u = compute_gradient(u)
        p_x = project_to_unit_ball(p_x + sigma * grad_u[0])
        p_y = project_to_unit_ball(p_y + sigma * grad_u[1])

        # 2. 原始变量更新
        div_p = compute_divergence(p_x, p_y)
        u = (u + tau * div_p + tau * lambda_param * f) / (1 + tau * lambda_param)

    return u
```

### 模板3: Split Bregman算法

```python
def split_bregman_template(f, mu=0.1, beta=1.0, alpha=0.01, max_iter=100):
    """
    Split Bregman算法模板

    适用于: 凸优化分割、ROF、TV-L1
    """
    # 初始化
    u = f.copy()
    v = np.zeros_like(f)
    d_x = np.zeros_like(f)
    d_y = np.zeros_like(f)
    b_x = np.zeros_like(f)
    b_y = np.zeros_like(f)

    for i in range(max_iter):
        # 1. u子问题: 求解Poisson方程
        u = solve_poisson(f, d_x, d_y, b_x, b_y, mu, beta)

        # 2. v子问题: 软阈值
        grad_u = compute_gradient(u)
        v = soft_threshold(grad_u - np.stack([b_x, b_y]), alpha/beta)

        # 3. d子问题: 辅助变量更新
        d_x = grad_u[0] + b_x
        d_y = grad_u[1] + b_y

        # 4. b子问题: Bregman参数更新
        grad_u = compute_gradient(u)
        b_x = b_x + grad_u[0] - v[0]
        b_y = b_y + grad_u[1] - v[1]

    return u
```

---

## 代码实现

### 组件1: 全变分正则化层

```python
import torch
import torch.nn as nn

class TotalVariation2D(nn.Module):
    """
    2D全变分正则化层

    用途: 深度网络中的正则化项
    """
    def __init__(self, reduction='mean'):
        super().__init__()
        self.reduction = reduction

    def forward(self, x):
        """
        计算全变分

        Args:
            x: (B, C, H, W) 输入特征图
        Returns:
            tv: 全变分值
        """
        # x方向差分
        diff_x = x[:, :, :, 1:] - x[:, :, :, :-1]

        # y方向差分
        diff_y = x[:, :, 1:, :] - x[:, :, :-1, :]

        # 全变分
        tv = torch.abs(diff_x).sum(dim=[1, 2, 3]) + \
             torch.abs(diff_y).sum(dim=[1, 2, 3])

        if self.reduction == 'mean':
            tv = tv.mean()
        elif self.reduction == 'sum':
            tv = tv.sum()

        return tv


class ROFDenoisingLayer(nn.Module):
    """
    可学习的ROF去噪层

    将ROF模型集成到深度网络中
    """
    def __init__(self, in_channels, init_lambda=0.1):
        super().__init__()
        self.in_channels = in_channels

        # 可学习的lambda参数
        self.lambda_param = nn.Parameter(torch.tensor(init_lambda))

        # 可学习的迭代权重
        self.weights = nn.ModuleList([
            nn.Conv2d(in_channels, in_channels, 3, padding=1)
            for _ in range(5)
        ])

    def forward(self, x):
        """ROF去噪前向传播"""
        u = x

        for i, weight in enumerate(self.weights):
            # 计算梯度
            grad_x = torch.zeros_like(u)
            grad_x[:, :, :, 1:] = u[:, :, :, 1:] - u[:, :, :, :-1]

            grad_y = torch.zeros_like(u)
            grad_y[:, :, 1:, :] = u[:, :, 1:, :] - u[:, :, :-1, :]

            # 散度
            div = grad_x[:, :, :, :-1] - grad_x[:, :, :, 1:] + \
                  grad_y[:, :, :-1, :] - grad_y[:, :, 1:, :]

            # ROF更新
            u = u + 0.01 * (div - self.lambda_param * (u - x))

            # 应用可学习权重
            u = u + weight(u)

        return u
```

### 组件2: Mumford-Shah分割网络

```python
class MumfordShahSegmentation(nn.Module):
    """
    基于Mumford-Shah的分割网络

    结合深度学习和变分法
    """
    def __init__(self, in_channels=3, num_classes=2, iterations=10):
        super().__init__()

        # 特征提取
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )

        # 解码器
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 2, stride=2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, num_classes, 2, stride=2),
        )

        # 水平集演化迭代次数
        self.iterations = iterations

    def forward(self, x):
        # 编码
        features = self.encoder(x)

        # 解码得到初始分割
        logits = self.decoder(features)

        # Mumford-Shah能量最小化
        for _ in range(self.iterations):
            probs = torch.softmax(logits, dim=1)
            mask = probs[:, 1:2, :, :]  # 前景概率

            # 计算区域均值
            fg_mean = (x * mask).sum([2, 3], keepdim=True) / \
                      (mask.sum([2, 3], keepdim=True) + 1e-8)
            bg_mean = (x * (1 - mask)).sum([2, 3], keepdim=True) / \
                      ((1 - mask).sum([2, 3], keepdim=True) + 1e-8)

            # 数据项
            data_term_fg = ((x - fg_mean)**2).sum(1, keepdim=True)
            data_term_bg = ((x - bg_mean)**2).sum(1, keepdim=True)

            # 曲率项
            grad_mask_x = mask[:, :, :, 1:] - mask[:, :, :, :-1]
            grad_mask_y = mask[:, :, 1:, :] - mask[:, :, :-1, :]
            curvature = grad_mask_x[:, :, :-1, :] + grad_mask_y[:, :, :, :-1]

            # 更新logits
            edge_force = data_term_bg - data_term_fg
            logits = logits + 0.01 * (edge_force + 0.1 * curvature)

        return logits


class MumfordShahLoss(nn.Module):
    """
    Mumford-Shah能量损失函数
    """
    def __init__(self, mu=1.0, nu=0.1):
        super().__init__()
        self.mu = mu
        self.nu = nu
        self.tv = TotalVariation2D()

    def forward(self, pred, target, image):
        """
        Args:
            pred: 预测分割 (B, C, H, W)
            target: 真实分割 (B, H, W)
            image: 原始图像 (B, C, H, W)
        """
        probs = torch.softmax(pred, dim=1)

        # 数据项: 区域内方差
        foreground_mask = probs[:, 1:2, :, :]
        background_mask = probs[:, 0:1, :, :]

        fg_mean = (image * foreground_mask).sum([2, 3], keepdim=True) / \
                  (foreground_mask.sum([2, 3], keepdim=True) + 1e-8)
        bg_mean = (image * background_mask).sum([2, 3], keepdim=True) / \
                  (background_mask.sum([2, 3], keepdim=True) + 1e-8)

        data_loss = ((image - fg_mean)**2 * foreground_mask).sum() + \
                    ((image - bg_mean)**2 * background_mask).sum()

        # 边缘长度项
        edge_loss = self.tv(probs[:, 1:2, :, :])

        # 交叉熵
        ce_loss = nn.functional.cross_entropy(pred, target)

        # 总损失
        total_loss = ce_loss + self.mu * data_loss + self.nu * edge_loss

        return total_loss
```

### 组件3: 凸优化分割

```python
class ConvexMumfordShah(nn.Module):
    """
    凸优化Mumford-Shah分割
    """
    def __init__(self, mu=0.1, beta=1.0, alpha=0.01, max_iter=100):
        super().__init__()
        self.mu = mu
        self.beta = beta
        self.alpha = alpha
        self.max_iter = max_iter

    def segment(self, f):
        """
        分割图像

        Args:
            f: 输入图像 (B, 1, H, W)
        Returns:
            u: 分割结果
            edges: 边缘图
        """
        B, C, H, W = f.shape

        # 初始化
        u = f.clone()
        b_x = torch.zeros_like(f)
        b_y = torch.zeros_like(f)

        for i in range(self.max_iter):
            # 计算梯度
            grad_u_x = torch.zeros_like(u)
            grad_u_x[:, :, :, :-1] = u[:, :, :, 1:] - u[:, :, :, :-1]

            grad_u_y = torch.zeros_like(u)
            grad_u_y[:, :, :-1, :] = u[:, :, 1:, :] - u[:, :, :-1, :]

            # 软阈值
            threshold = 1.0 / (self.beta + 1e-8)
            v_x = torch.sign(grad_u_x - b_x) * torch.relu(
                torch.abs(grad_u_x - b_x) - threshold)
            v_y = torch.sign(grad_u_y - b_y) * torch.relu(
                torch.abs(grad_u_y - b_y) - threshold)

            # 更新u
            d_x = v_x + b_x
            d_y = v_y + b_y

            div = torch.zeros_like(u)
            div[:, :, :, :-1] -= d_x[:, :, :, 1:]
            div[:, :, :, 1:] += d_x[:, :, :, :-1]
            div[:, :, :-1, :] -= d_y[:, :, 1:, :]
            div[:, :, 1:, :] += d_y[:, :, :-1, :]

            rhs = self.mu * f + self.beta * div
            u = u + 0.1 * (rhs - self.mu * u)

            # 更新Bregman参数
            grad_u_x = torch.zeros_like(u)
            grad_u_x[:, :, :, :-1] = u[:, :, :, 1:] - u[:, :, :, :-1]

            grad_u_y = torch.zeros_like(u)
            grad_u_y[:, :, :-1, :] = u[:, :, 1:, :] - u[:, :, :-1, :]

            b_x = b_x + grad_u_x - v_x
            b_y = b_y + grad_u_y - v_y

        # 计算边缘
        edges = torch.sqrt((b_x**2 + b_y**2).sum(1, keepdim=True) + 1e-8)

        return u, edges


class ConvexSegmentationLoss(nn.Module):
    """
    凸优化分割损失
    """
    def __init__(self, mu=0.1, beta=1.0, alpha=0.01):
        super().__init__()
        self.mu = mu
        self.beta = beta
        self.alpha = alpha

    def forward(self, pred, target):
        """
        凸分割能量损失
        """
        # 1. 数据项
        data_term = self.mu * torch.sum((pred - target)**2)

        # 2. 梯度稀疏性项
        grad_pred_x = pred[:, :, :, 1:] - pred[:, :, :, :-1]
        grad_pred_y = pred[:, :, 1:, :] - pred[:, :, :-1, :]
        gradient_term = self.alpha * (torch.abs(grad_pred_x).mean() +
                                     torch.abs(grad_pred_y).mean())

        # 3. 分片光滑项
        smoothness = torch.sum(grad_pred_x**2) + torch.sum(grad_pred_y**2)

        # 总损失
        total_loss = data_term + gradient_term + smoothness

        return total_loss
```

---

## 实验设计

### 数据集选择

| 数据集类型 | 典型数据集 | 适用场景 | 特点 |
|------------|------------|----------|------|
| 合成图像 | 自生成 | 算法验证 | Ground truth已知 |
| 自然图像 | BSDS500 | 通用分割 | 多样化场景 |
| 医学图像 | ISIC, BRATS | 医学应用 | 边界模糊 |

### 评估指标

```python
变分法评估指标集 = {
    "分割类": {
        "IoU": "交并比",
        "Dice": "Dice系数",
        "HD95": "95%Hausdorff距离",
        "ASD": "平均表面距离"
    },
    "去噪类": {
        "PSNR": "峰值信噪比",
        "SSIM": "结构相似性",
        "MAE": "平均绝对误差"
    },
    "收敛性": {
        "迭代次数": "达到收敛的迭代数",
        "计算时间": "CPU/GPU时间"
    }
}
```

### 消融实验设计

| 变量 | 设置 | 目的 |
|------|------|------|
| 优化算法 | 梯度下降/原始-对偶/Split Bregman | 验证算法效率 |
| 正则化参数 | λ = [0.01, 0.1, 1.0] | 参数敏感性 |
| 迭代次数 | [50, 100, 200] | 收敛速度分析 |
| 初始化 | 随机/启发式 | 初始化独立性 |

---

## 论文应用案例

| 论文 | 应用方式 | 改进点 | 结果 |
|------|----------|--------|------|
| [1-04] | ROF模型基础 | 全变分去噪理论 | PSNR +4-7dB |
| [2-01] | 凸优化分割 | Split Bregman算法 | IoU 82.3% |
| [2-03] | SLaT三阶段 | RGB+Lab颜色空间 | 退化图像鲁棒分割 |
| [2-05] | 语义比例分割 | 语义信息融入 | IEEE TIP发表 |

---

## 附录

### 常用能量泛函表

| 泛函名称 | 公式 | 应用 |
|----------|------|------|
| ROF | E = ∫\|∇u\| + λ∫(u-f)² | 去噪 |
| Mumford-Shah | E = ∫\|∇u\|² + μ∫(u-f)² + ν\|Γ\| | 分割 |
| TV-L1 | E = ∫\|u-f\| + λ∫\|∇u\| | 去噪+边缘保持 |
| Chan-Vese | E = ∫(u-c₁)² + ∫(u-c₂)² + ν\|Γ\| | 二值分割 |

### Python实现资源

```python
# 推荐库
import numpy as np
import torch
import torch.nn as nn
from scipy.fft import fft2, ifft2  # 快速Poisson求解
from scipy.sparse import spdiags  # 稀疏矩阵
```

---

*文档创建时间: 2026年2月9日*
*基于: Xiaohao Cai 师门论文分析*
